<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 16 September 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 16 September 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 16 September 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2509.12531",
      "title": "Pre-trained Visual Representations Generalize Where it Matters in\n  Model-Based Reinforcement Learning",
      "authors": [
        "Scott Jones",
        "Liyou Zhou",
        "Sebastian W. Pattinson"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "In visuomotor policy learning, the control policy for the robotic agent is\nderived directly from visual inputs. The typical approach, where a policy and\nvision encoder are trained jointly from scratch, generalizes poorly to novel\nvisual scene changes. Using pre-trained vision models (PVMs) to inform a policy\nnetwork improves robustness in model-free reinforcement learning (MFRL). Recent\ndevelopments in Model-based reinforcement learning (MBRL) suggest that MBRL is\nmore sample-efficient than MFRL. However, counterintuitively, existing work has\nfound PVMs to be ineffective in MBRL. Here, we investigate PVM's effectiveness\nin MBRL, specifically on generalization under visual domain shifts. We show\nthat, in scenarios with severe shifts, PVMs perform much better than a baseline\nmodel trained from scratch. We further investigate the effects of varying\nlevels of fine-tuning of PVMs. Our results show that partial fine-tuning can\nmaintain the highest average task performance under the most extreme\ndistribution shifts. Our results demonstrate that PVMs are highly successful in\npromoting robustness in visual policy learning, providing compelling evidence\nfor their wider adoption in model-based robotic learning applications.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12531v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12531v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.452,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.372,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained visual representations in Model-Based Reinforcement Learning (MBRL) for robotic tasks, emphasizing generalization to visual domain shifts. It does not involve human feedback, reward models trained on human-ranked data, or any mechanism for aligning AI with human preferences. Therefore, it does not relate to Reinforcement Learning from Human Feedback (RLHF).",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12534",
      "title": "DeepEyeNet: Generating Medical Report for Retinal Images",
      "authors": [
        "Jia-Hong Huang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The increasing prevalence of retinal diseases poses a significant challenge\nto the healthcare system, as the demand for ophthalmologists surpasses the\navailable workforce. This imbalance creates a bottleneck in diagnosis and\ntreatment, potentially delaying critical care. Traditional methods of\ngenerating medical reports from retinal images rely on manual interpretation,\nwhich is time-consuming and prone to errors, further straining\nophthalmologists' limited resources. This thesis investigates the potential of\nArtificial Intelligence (AI) to automate medical report generation for retinal\nimages. AI can quickly analyze large volumes of image data, identifying subtle\npatterns essential for accurate diagnosis. By automating this process, AI\nsystems can greatly enhance the efficiency of retinal disease diagnosis,\nreducing doctors' workloads and enabling them to focus on more complex cases.\nThe proposed AI-based methods address key challenges in automated report\ngeneration: (1) A multi-modal deep learning approach captures interactions\nbetween textual keywords and retinal images, resulting in more comprehensive\nmedical reports; (2) Improved methods for medical keyword representation\nenhance the system's ability to capture nuances in medical terminology; (3)\nStrategies to overcome RNN-based models' limitations, particularly in capturing\nlong-range dependencies within medical descriptions; (4) Techniques to enhance\nthe interpretability of the AI-based report generation system, fostering trust\nand acceptance in clinical practice. These methods are rigorously evaluated\nusing various metrics and achieve state-of-the-art performance. This thesis\ndemonstrates AI's potential to revolutionize retinal disease diagnosis by\nautomating medical report generation, ultimately improving clinical efficiency,\ndiagnostic accuracy, and patient care.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12534v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12534v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.348,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on multi-modal deep learning for automated medical report generation from retinal images, including techniques like attention mechanisms and keyword representations, but it does not involve reinforcement learning. There is no mention of training a reward model with human-ranked data or using RL to fine-tune a model based on human feedback. Elements like manually labeled datasets are present, but they do not constitute RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contributions are in deep learning for image captioning and report generation, such as using DNNs and attention-based fusion, but it does not adapt diffusion models for iterative refinement or multi-step logical reasoning. There is no component for treating a 'Chain-of-Thought' as an entity or holistic correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12541",
      "title": "zELO: ELO-inspired Training Method for Rerankers and Embedding Models",
      "authors": [
        "Nicholas Pipitone",
        "Ghita Houir Alami",
        "Advaith Avadhanam",
        "Anton Kaminskyi",
        "Ashley Khoo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce a novel training methodology named zELO, which optimizes\nretrieval performance via the analysis that ranking tasks are statically\nequivalent to a Thurstone model. Based on the zELO method, we use unsupervised\ndata in order train a suite of state-of-the-art open-weight reranker models:\nzerank-1 and zerank-1-small. These models achieve the highest retrieval scores\nin multiple domains, including finance, legal, code, and STEM, outperforming\nclosed-source proprietary rerankers on both NDCG@10 and Recall. These models\nalso demonstrate great versatility, maintaining their 0-shot performance on\nout-of-domain and private customer datasets. The training data included 112,000\nqueries and 100 documents per query, and was trained end-to-end from\nunannotated queries and documents in less than 10,000 H100-hours.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12541v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.41,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a zELO training method using unsupervised data for rerankers, with no mention of human feedback, reward models, or reinforcement learning. It relies on unannotated queries and documents, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper mentions training the models in less than 10,000 H100-hours, which implies significant computational resources potentially involving parallel computing. However, the main contribution is the zELO methodology for ranking, not strategies for distributed training, data partitioning, or multi-node systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12543",
      "title": "Human + AI for Accelerating Ad Localization Evaluation",
      "authors": [
        "Harshit Rajgarhia",
        "Shivali Dalmia",
        "Mengyang Zhao",
        "Mukherji Abhishek",
        "Kiran Ganesh"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Adapting advertisements for multilingual audiences requires more than simple\ntext translation; it demands preservation of visual consistency, spatial\nalignment, and stylistic integrity across diverse languages and formats. We\nintroduce a structured framework that combines automated components with human\noversight to address the complexities of advertisement localization. To the\nbest of our knowledge, this is the first work to integrate scene text\ndetection, inpainting, machine translation (MT), and text reimposition\nspecifically for accelerating ad localization evaluation workflows. Qualitative\nresults across six locales demonstrate that our approach produces semantically\naccurate and visually coherent localized advertisements, suitable for\ndeployment in real-world workflows.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12543v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12543v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.476,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.404,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "The paper describes a human-in-the-loop mechanism for text detection to improve accuracy and reduce annotation time, but it does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune a main model. This is a general human oversight process, not RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses Stable Diffusion for image inpainting in ad localization, but this is for visual restoration, not for multi-step logical reasoning or treating a chain-of-thought as an entity to refine. There is no component for iterative reasoning tasks.",
      "distributed_training_justification": "The paper focuses on an end-to-end framework for ad localization using existing AI components, but it does not discuss distributed training, parallel computing, or partitioning data/computation across multiple nodes for model training.",
      "datasets_justification": "The paper presents a framework for ad localization and mentions qualitative results across locales, but it does not involve creating, analyzing, benchmarking, or evaluating datasets. The main contribution is the pipeline, not dataset-related work.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12544",
      "title": "Neural Collapse-Inspired Multi-Label Federated Learning under\n  Label-Distribution Skew",
      "authors": [
        "Can Peng",
        "Yuyuan Liu",
        "Yingyu Yang",
        "Pramit Saha",
        "Qianye Yang",
        "J. Alison Noble"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. However, the performance of\ndeep learning often deteriorates in FL due to decentralized and heterogeneous\ndata. This challenge is further amplified in multi-label scenarios, where data\nexhibit complex characteristics such as label co-occurrence, inter-label\ndependency, and discrepancies between local and global label relationships.\nWhile most existing FL research primarily focuses on single-label\nclassification, many real-world applications, particularly in domains such as\nmedical imaging, often involve multi-label settings. In this paper, we address\nthis important yet underexplored scenario in FL, where clients hold multi-label\ndata with skewed label distributions. Neural Collapse (NC) describes a\ngeometric structure in the latent feature space where features of each class\ncollapse to their class mean with vanishing intra-class variance, and the class\nmeans form a maximally separated configuration. Motivated by this theory, we\npropose a method to align feature distributions across clients and to learn\nhigh-quality, well-clustered representations. To make the NC-structure\napplicable to multi-label settings, where image-level features may contain\nmultiple semantic concepts, we introduce a feature disentanglement module that\nextracts semantically specific features. The clustering of these disentangled\nclass-wise features is guided by a predefined shared NC structure, which\nmitigates potential conflicts between client models due to diverse local data\ndistributions. In addition, we design regularisation losses to encourage\ncompact clustering in the latent feature space. Experiments conducted on four\nbenchmark datasets across eight diverse settings demonstrate that our approach\noutperforms existing methods, validating its effectiveness in this challenging\nFL scenario.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12544v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12544v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.423,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.454,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on Federated Learning for multi-label classification with label-distribution skew, emphasizing feature alignment and regularization to handle heterogeneous data. It does not involve programmatically generating labels from noisy or imprecise sources, nor does it rely on weak supervision techniques for training; instead, it assumes existing labeled data across clients.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a method for Federated Learning, which is a form of distributed training that partitions data and computation across decentralized clients to collaboratively train a model without sharing raw data. It directly addresses challenges in distributed settings, such as data heterogeneity and parallel processing across nodes, making it highly aligned with distributed training concepts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of multi-label Federated Learning (FL) under label-distribution skew, where clients have heterogeneous data, by proposing a novel method called FedNCAlign-ML inspired by Neural Collapse theory. The approach involves feature disentanglement to extract class-specific representations, alignment with a predefined Equiangular Tight Frame structure to promote consistent feature clustering across clients, and additional regularization losses to enhance intra-class compactness and inter-class separation, resulting in significant improvements in performance metrics like AUC and F1 scores on four benchmark datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting Neural Collapse theory to multi-label FL with label skew, combining existing ideas in a new way for an underexplored scenario, though it does not introduce entirely new concepts. This makes it a clever extension rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in multi-label FL, particularly in fields like medical imaging with heterogeneous data, by providing a robust method that could be built upon. However, its applicability is somewhat limited to specific subfields, reducing broader influence.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to FL for multi-label scenarios with empirical validation, making it essential for researchers in computer vision and related areas to be aware of. While insightful, it is not revolutionary enough to be deemed must-read for all.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ae8ee5b1dc284b83a23d32f0046a8bbd7edba300",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 13,
      "average_h_index": 4.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Can Peng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2331769743"
        },
        {
          "name": "Yuyuan Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380793713"
        },
        {
          "name": "Yingyu Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375157917"
        },
        {
          "name": "Pramit Saha",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2254295532"
        },
        {
          "name": "Qianye Yang",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/9900287"
        },
        {
          "name": "J. Noble",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2254292901"
        }
      ]
    },
    {
      "id": "2509.12546",
      "title": "Agent4FaceForgery: Multi-Agent LLM Framework for Realistic Face Forgery\n  Detection",
      "authors": [
        "Yingxin Lai",
        "Zitong Yu",
        "Jun Wang",
        "Linlin Shen",
        "Yong Xu",
        "Xiaochun Cao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Face forgery detection faces a critical challenge: a persistent gap between\noffline benchmarks and real-world efficacy,which we attribute to the ecological\ninvalidity of training data.This work introduces Agent4FaceForgery to address\ntwo fundamental problems: (1) how to capture the diverse intents and iterative\nprocesses of human forgery creation, and (2) how to model the complex, often\nadversarial, text-image interactions that accompany forgeries in social media.\nTo solve this,we propose a multi-agent framework where LLM-poweredagents,\nequipped with profile and memory modules, simulate the forgery creation\nprocess. Crucially, these agents interact in a simulated social environment to\ngenerate samples labeled for nuanced text-image consistency, moving beyond\nsimple binary classification. An Adaptive Rejection Sampling (ARS) mechanism\nensures data quality and diversity. Extensive experiments validate that the\ndata generated by our simulationdriven approach brings significant performance\ngains to detectors of multiple architectures, fully demonstrating the\neffectiveness and value of our framework.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12546v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12546v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.368,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12553",
      "title": "iCD: A Implicit Clustering Distillation Mathod for Structural\n  Information Mining",
      "authors": [
        "Xiang Xue",
        "Yatu Ji",
        "Qing-dao-er-ji Ren",
        "Bao Shi",
        "Min Lu",
        "Nier Wu",
        "Xufei Zhuang",
        "Haiteng Xu",
        "Gan-qi-qi-ge Cha"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Logit Knowledge Distillation has gained substantial research interest in\nrecent years due to its simplicity and lack of requirement for intermediate\nfeature alignment; however, it suffers from limited interpretability in its\ndecision-making process. To address this, we propose implicit Clustering\nDistillation (iCD): a simple and effective method that mines and transfers\ninterpretable structural knowledge from logits, without requiring ground-truth\nlabels or feature-space alignment. iCD leverages Gram matrices over decoupled\nlocal logit representations to enable student models to learn latent semantic\nstructural patterns. Extensive experiments on benchmark datasets demonstrate\nthe effectiveness of iCD across diverse teacher-student architectures, with\nparticularly strong performance in fine-grained classification tasks --\nachieving a peak improvement of +5.08% over the baseline. The code is available\nat: https://github.com/maomaochongaa/iCD.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12553v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12553v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.376,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a method for knowledge distillation using implicit clustering and Gram matrices to transfer structural information from teacher to student models in classification tasks. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12554",
      "title": "Explicit Multimodal Graph Modeling for Human-Object Interaction\n  Detection",
      "authors": [
        "Wenxuan Ji",
        "Haichao Shi",
        "Xiao-Yu zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Transformer-based methods have recently become the prevailing approach for\nHuman-Object Interaction (HOI) detection. However, the Transformer architecture\ndoes not explicitly model the relational structures inherent in HOI detection,\nwhich impedes the recognition of interactions. In contrast, Graph Neural\nNetworks (GNNs) are inherently better suited for this task, as they explicitly\nmodel the relationships between human-object pairs. Therefore, in this paper,\nwe propose \\textbf{M}ultimodal \\textbf{G}raph \\textbf{N}etwork\n\\textbf{M}odeling (MGNM) that leverages GNN-based relational structures to\nenhance HOI detection. Specifically, we design a multimodal graph network\nframework that explicitly models the HOI task in a four-stage graph structure.\nFurthermore, we introduce a multi-level feature interaction mechanism within\nour graph network. This mechanism leverages multi-level vision and language\nfeatures to enhance information propagation across human-object pairs.\nConsequently, our proposed MGNM achieves state-of-the-art performance on two\nwidely used benchmarks: HICO-DET and V-COCO. Moreover, when integrated with a\nmore advanced object detector, our method demonstrates a significant\nperformance gain and maintains an effective balance between rare and non-rare\nclasses.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12554v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12554v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.328,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using Graph Neural Networks (GNNs) for explicit relational modeling in Human-Object Interaction (HOI) detection, incorporating multimodal features from vision and language models like CLIP. It does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are core to diffusion-based reasoning. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12556",
      "title": "VQT-Light:Lightweight HDR Illumination Map Prediction with Richer\n  Texture.pdf",
      "authors": [
        "Kunliang Xie"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate lighting estimation is a significant yet challenging task in\ncomputer vision and graphics. However, existing methods either struggle to\nrestore detailed textures of illumination map, or face challenges in running\nspeed and texture fidelity. To tackle this problem, we propose a novel\nframework (VQT-Light) based on VQVAE and ViT architecture. VQT-Light includes\ntwo modules: feature extraction and lighting estimation. First, we take\nadvantages of VQVAE to extract discrete features of illumination map rather\nthan continuous features to avoid \"posterior collapse\". Second, we capture\nglobal context and dependencies of input image through ViT rather than CNNs to\nimprove the prediction of illumination outside the field of view. Combining the\nabove two modules, we formulate the lighting estimation as a multiclass\nclassification task, which plays a key role in our pipeline. As a result, our\nmodel predicts light map with richer texture and better fidelity while keeping\nlightweight and fast. VQT-Light achieves an inference speed of 40FPS and\nimproves multiple evaluation metrics. Qualitative and quantitative experiments\ndemonstrate that the proposed method realizes superior results compared to\nexisting state-of-the-art methods.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12556v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12556v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.292,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.29,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12569",
      "title": "Adaptive Sampling Scheduler",
      "authors": [
        "Qi Wang",
        "Shuliang Zhu",
        "Jinjia Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Consistent distillation methods have evolved into effective techniques that\nsignificantly accelerate the sampling process of diffusion models. Although\nexisting methods have achieved remarkable results, the selection of target\ntimesteps during distillation mainly relies on deterministic or stochastic\nstrategies, which often require sampling schedulers to be designed specifically\nfor different distillation processes. Moreover, this pattern severely limits\nflexibility, thereby restricting the full sampling potential of diffusion\nmodels in practical applications. To overcome these limitations, this paper\nproposes an adaptive sampling scheduler that is applicable to various\nconsistency distillation frameworks. The scheduler introduces three innovative\nstrategies: (i) dynamic target timestep selection, which adapts to different\nconsistency distillation frameworks by selecting timesteps based on their\ncomputed importance; (ii) Optimized alternating sampling along the solution\ntrajectory by guiding forward denoising and backward noise addition based on\nthe proposed time step importance, enabling more effective exploration of the\nsolution space to enhance generation performance; and (iii) Utilization of\nsmoothing clipping and color balancing techniques to achieve stable and\nhigh-quality generation results at high guidance scales, thereby expanding the\napplicability of consistency distillation models in complex generation\nscenarios. We validated the effectiveness and flexibility of the adaptive\nsampling scheduler across various consistency distillation methods through\ncomprehensive experimental evaluations. Experimental results consistently\ndemonstrated significant improvements in generative performance, highlighting\nthe strong adaptability achieved by our method.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12569v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12569v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.478,
      "distributed_training_score": 0.389,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an adaptive sampling scheduler for improving efficiency in diffusion models for image generation, focusing on techniques like dynamic timestep selection and handling issues in consistency distillation. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, as required by the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12589",
      "title": "Redefining CX with Agentic AI: Minerva CQ Case Study",
      "authors": [
        "Garima Agrawal",
        "Riccardo De Maria",
        "Kiran Davuluri",
        "Daniele Spera",
        "Charlie Read",
        "Cosimo Spera",
        "Jack Garrett",
        "Don Miller"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite advances in AI for contact centers, customer experience (CX)\ncontinues to suffer from high average handling time (AHT), low first-call\nresolution, and poor customer satisfaction (CSAT). A key driver is the\ncognitive load on agents, who must navigate fragmented systems, troubleshoot\nmanually, and frequently place customers on hold. Existing AI-powered\nagent-assist tools are often reactive driven by static rules, simple prompting,\nor retrieval-augmented generation (RAG) without deeper contextual reasoning. We\nintroduce Agentic AI goal-driven, autonomous, tool-using systems that\nproactively support agents in real time. Unlike conventional approaches,\nAgentic AI identifies customer intent, triggers modular workflows, maintains\nevolving context, and adapts dynamically to conversation state. This paper\npresents a case study of Minerva CQ, a real-time Agent Assist product deployed\nin voice-based customer support. Minerva CQ integrates real-time transcription,\nintent and sentiment detection, entity recognition, contextual retrieval,\ndynamic customer profiling, and partial conversational summaries enabling\nproactive workflows and continuous context-building. Deployed in live\nproduction, Minerva CQ acts as an AI co-pilot, delivering measurable\nimprovements in agent efficiency and customer experience across multiple\ndeployments.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12589v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12589v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.322,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12592",
      "title": "Match Chat: Real Time Generative AI and Generative Computing for Tennis",
      "authors": [
        "Aaron Baughman",
        "Gozde Akay",
        "Eduardo Morales",
        "Rahul Agarwal",
        "Preetika Srivastava"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We present Match Chat, a real-time, agent-driven assistant designed to\nenhance the tennis fan experience by delivering instant, accurate responses to\nmatch-related queries. Match Chat integrates Generative Artificial Intelligence\n(GenAI) with Generative Computing (GenComp) techniques to synthesize key\ninsights during live tennis singles matches. The system debuted at the 2025\nWimbledon Championships and the 2025 US Open, where it provided about 1 million\nusers with seamless access to streaming and static data through natural\nlanguage queries. The architecture is grounded in an Agent-Oriented\nArchitecture (AOA) combining rule engines, predictive models, and agents to\npre-process and optimize user queries before passing them to GenAI components.\nThe Match Chat system had an answer accuracy of 92.83% with an average response\ntime of 6.25 seconds under loads of up to 120 requests per second (RPS). Over\n96.08% of all queries were guided using interactive prompt design, contributing\nto a user experience that prioritized clarity, responsiveness, and minimal\neffort. The system was designed to mask architectural complexity, offering a\nfrictionless and intuitive interface that required no onboarding or technical\nfamiliarity. Across both Grand Slam deployments, Match Chat maintained 100%\nuptime and supported nearly 1 million unique users, underscoring the\nscalability and reliability of the platform. This work introduces key design\npatterns for real-time, consumer-facing AI systems that emphasize speed,\nprecision, and usability that highlights a practical path for deploying\nperformant agentic systems in dynamic environments.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12592v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12592v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.327,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12594",
      "title": "The Better You Learn, The Smarter You Prune: Towards Efficient\n  Vision-language-action Models via Differentiable Token Pruning",
      "authors": [
        "Titong Jiang",
        "Xuefeng Jiang",
        "Yuan Ma",
        "Xin Wen",
        "Bailin Li",
        "Kun Zhan",
        "Peng Jia",
        "Yahui Liu",
        "Sheng Sun",
        "Xianpeng Lang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present LightVLA, a simple yet effective differentiable token pruning\nframework for vision-language-action (VLA) models. While VLA models have shown\nimpressive capability in executing real-world robotic tasks, their deployment\non resource-constrained platforms is often bottlenecked by the heavy\nattention-based computation over large sets of visual tokens. LightVLA\naddresses this challenge through adaptive, performance-driven pruning of visual\ntokens: It generates dynamic queries to evaluate visual token importance, and\nadopts Gumbel softmax to enable differentiable token selection. Through\nfine-tuning, LightVLA learns to preserve the most informative visual tokens\nwhile pruning tokens which do not contribute to task execution, thereby\nimproving efficiency and performance simultaneously. Notably, LightVLA requires\nno heuristic magic numbers and introduces no additional trainable parameters,\nmaking it compatible with modern inference frameworks. Experimental results\ndemonstrate that LightVLA outperforms different VLA models and existing token\npruning methods across diverse tasks on the LIBERO benchmark, achieving higher\nsuccess rates with substantially reduced computational overhead. Specifically,\nLightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.6%\nimprovement in task success rate. Meanwhile, we also investigate the learnable\nquery-based token pruning method LightVLA* with additional trainable\nparameters, which also achieves satisfactory performance. Our work reveals that\nas VLA pursues optimal performance, LightVLA spontaneously learns to prune\ntokens from a performance-driven perspective. To the best of our knowledge,\nLightVLA is the first work to apply adaptive visual token pruning to VLA tasks\nwith the collateral goals of efficiency and performance, marking a significant\nstep toward more efficient, powerful and practical real-time robotic systems.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12594v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12594v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.404,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on efficient token pruning in vision-language-action (VLA) models for robotics, emphasizing differentiable techniques and fine-tuning for performance. It does not involve human feedback, reward models, or reinforcement learning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for visual token pruning in VLA models to improve efficiency, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It deals with attention-based computation and task execution in robotics.",
      "distributed_training_justification": "The paper addresses inference efficiency through adaptive token pruning in VLA models, not distributed training, parallel computing, or strategies for accelerating model training across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12595",
      "title": "DisorientLiDAR: Physical Attacks on LiDAR-based Localization",
      "authors": [
        "Yizhen Lao",
        "Yu Zhang",
        "Ziting Wang",
        "Chengbo Wang",
        "Yifei Xue",
        "Wanpeng Shao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deep learning models have been shown to be susceptible to adversarial attacks\nwith visually imperceptible perturbations. Even this poses a serious security\nchallenge for the localization of self-driving cars, there has been very little\nexploration of attack on it, as most of adversarial attacks have been applied\nto 3D perception. In this work, we propose a novel adversarial attack framework\ncalled DisorientLiDAR targeting LiDAR-based localization. By\nreverse-engineering localization models (e.g., feature extraction networks),\nadversaries can identify critical keypoints and strategically remove them,\nthereby disrupting LiDAR-based localization. Our proposal is first evaluated on\nthree state-of-the-art point-cloud registration models (HRegNet, D3Feat, and\nGeoTransformer) using the KITTI dataset. Experimental results demonstrate that\nremoving regions containing Top-K keypoints significantly degrades their\nregistration accuracy. We further validate the attack's impact on the Autoware\nautonomous driving platform, where hiding merely a few critical regions induces\nnoticeable localization drift. Finally, we extended our attacks to the physical\nworld by hiding critical regions with near-infrared absorptive materials,\nthereby successfully replicate the attack effects observed in KITTI data. This\nstep has been closer toward the realistic physical-world attack that\ndemonstrate the veracity and generality of our proposal.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12595v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12595v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.31,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12600",
      "title": "A Multimodal Foundation Model to Enhance Generalizability and Data\n  Efficiency for Pan-cancer Prognosis Prediction",
      "authors": [
        "Huajun Zhou",
        "Fengtao Zhou",
        "Jiabo Ma",
        "Yingxue Xu",
        "Xi Wang",
        "Xiuming Zhang",
        "Li Liang",
        "Zhenhui Li",
        "Hao Chen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multimodal data provides heterogeneous information for a holistic\nunderstanding of the tumor microenvironment. However, existing AI models often\nstruggle to harness the rich information within multimodal data and extract\npoorly generalizable representations. Here we present MICE (Multimodal data\nIntegration via Collaborative Experts), a multimodal foundation model that\neffectively integrates pathology images, clinical reports, and genomics data\nfor precise pan-cancer prognosis prediction. Instead of conventional\nmulti-expert modules, MICE employs multiple functionally diverse experts to\ncomprehensively capture both cross-cancer and cancer-specific insights.\nLeveraging data from 11,799 patients across 30 cancer types, we enhanced MICE's\ngeneralizability by coupling contrastive and supervised learning. MICE\noutperformed both unimodal and state-of-the-art multi-expert-based multimodal\nmodels, demonstrating substantial improvements in C-index ranging from 3.8% to\n11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts,\nrespectively. Moreover, it exhibited remarkable data efficiency across diverse\nclinical scenarios. With its enhanced generalizability and data efficiency,\nMICE establishes an effective and scalable foundation for pan-cancer prognosis\nprediction, holding strong potential to personalize tailored therapies and\nimprove treatment outcomes.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12600v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12600v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.355,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12602",
      "title": "DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large\n  Language Models",
      "authors": [
        "Minyu Chen",
        "Guoqiang Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The performance of Conflict-Driven Clause Learning solvers hinges on internal\nheuristics, yet the heterogeneity of SAT problems makes a single, universally\noptimal configuration unattainable. While prior automated methods can find\nspecialized configurations for specific problem families, this dataset-specific\napproach lacks generalizability and requires costly re-optimization for new\nproblem types. We introduce DaSAThco, a framework that addresses this challenge\nby learning a generalizable mapping from instance features to tailored\nheuristic ensembles, enabling a train-once, adapt-broadly model. Our framework\nuses a Large Language Model, guided by systematically defined Problem\nArchetypes, to generate a diverse portfolio of specialized heuristic ensembles\nand subsequently learns an adaptive selection mechanism to form the final\nmapping. Experiments show that DaSAThco achieves superior performance and, most\nnotably, demonstrates robust out-of-domain generalization where non-adaptive\nmethods show limitations. Our work establishes a more scalable and practical\npath toward automated algorithm design for complex, configurable systems.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12602v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12602v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.394,
      "datasets_score": 0.429,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on using Large Language Models for generating and optimizing SAT heuristics via evolutionary search, without any involvement of human feedback, reward models, or reinforcement learning based on human preferences. There is no element of aligning AI with human-ranked data, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for logical reasoning. It centers on SAT heuristic optimization using LLMs and evolutionary frameworks, with no mention of multi-step logical reasoning via diffusion, such as Chain-of-Thought refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses datasets to define Problem Archetypes and evaluate SAT heuristics, including benchmarking performance, but its main contribution is the DaSAThco framework for optimization, not the creation, analysis, or curation of datasets. It touches on datasets peripherally through experimental setup rather than focusing on them as a primary research area.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12603",
      "title": "EconProver: Towards More Economical Test-Time Scaling for Automated\n  Theorem Proving",
      "authors": [
        "Mukai Li",
        "Linfeng Song",
        "Zhenwen Liang",
        "Jiahao Xu",
        "Shansan Gong",
        "Qi Liu",
        "Haitao Mi",
        "Dong Yu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have recently advanced the field of Automated\nTheorem Proving (ATP), attaining substantial performance gains through widely\nadopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)\nreasoning and increased sampling passes. However, they both introduce\nsignificant computational overhead for inference. Moreover, existing cost\nanalyses typically regulate only the number of sampling passes, while\nneglecting the substantial disparities in sampling costs introduced by\ndifferent scaling strategies. In this paper, we systematically compare the\nefficiency of different test-time scaling strategies for ATP models and\ndemonstrate the inefficiency of the current state-of-the-art (SOTA) open-source\napproaches. We then investigate approaches to significantly reduce token usage\nand sample passes while maintaining the original performance. Specifically, we\npropose two complementary methods that can be integrated into a unified EconRL\npipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching\nmechanism designed to mitigate unnecessary token consumption, and (2) Diverse\nparallel-scaled reinforcement learning (RL) with trainable prefixes to enhance\npass rates under constrained sampling passes. Experiments on miniF2F and\nProofNet demonstrate that our EconProver achieves comparable performance to\nbaseline methods with only 12% of the computational cost. This work provides\nactionable insights for deploying lightweight ATP models without sacrificing\nperformance.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12603v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12603v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.4,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on optimizing test-time scaling for Automated Theorem Proving using Chain-of-Thought switching and reinforcement learning, without any mention or adaptation of diffusion models or their iterative refinement processes. There is no evidence of treating reasoning paths as entities for holistic correction over multiple steps using diffusion techniques.",
      "distributed_training_justification": "The paper discusses parallel scaling for sampling passes during inference and uses reinforcement learning techniques like PPO, which could involve parallel computing elements, but it primarily addresses test-time efficiency rather than distributed training algorithms, data partitioning, or multi-node training systems for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12610",
      "title": "ScaleDoc: Scaling LLM-based Predicates over Large Document Collections",
      "authors": [
        "Hengrui Zhang",
        "Yulong Hui",
        "Yihao Liu",
        "Huanchen Zhang"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Predicates are foundational components in data analysis systems. However,\nmodern workloads increasingly involve unstructured documents, which demands\nsemantic understanding, beyond traditional value-based predicates. Given\nenormous documents and ad-hoc queries, while Large Language Models (LLMs)\ndemonstrate powerful zero-shot capabilities, their high inference cost leads to\nunacceptable overhead. Therefore, we introduce \\textsc{ScaleDoc}, a novel\nsystem that addresses this by decoupling predicate execution into an offline\nrepresentation phase and an optimized online filtering phase. In the offline\nphase, \\textsc{ScaleDoc} leverages a LLM to generate semantic representations\nfor each document. Online, for each query, it trains a lightweight proxy model\non these representations to filter the majority of documents, forwarding only\nthe ambiguous cases to the LLM for final decision. Furthermore,\n\\textsc{ScaleDoc} proposes two core innovations to achieve significant\nefficiency: (1) a contrastive-learning-based framework that trains the proxy\nmodel to generate reliable predicating decision scores; (2) an adaptive cascade\nmechanism that determines the effective filtering policy while meeting specific\naccuracy targets. Our evaluations across three datasets demonstrate that\n\\textsc{ScaleDoc} achieves over a 2$\\times$ end-to-end speedup and reduces\nexpensive LLM invocations by up to 85\\%, making large-scale semantic analysis\npractical and efficient.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12610v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12610v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.437,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a system for efficient LLM-based predicate execution over documents, using offline representations and online filtering with a proxy model. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no mention of adapting diffusion for Chain-of-Thought or holistic correction.",
      "distributed_training_justification": "The paper addresses efficiency in querying large document collections using LLMs and proxy models, but it does not discuss distributed training, parallel computing, or multi-node machine learning techniques. It focuses on inference optimization and model training for filtering, without partitioning data, architecture, or computation across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12611",
      "title": "Analogy-Driven Financial Chain-of-Thought (AD-FCoT): A Prompting\n  Approach for Financial Sentiment Analysis",
      "authors": [
        "Anmol Singhal Navya Singhal"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Financial news sentiment analysis is crucial for anticipating market\nmovements. With the rise of AI techniques such as Large Language Models (LLMs),\nwhich demonstrate strong text understanding capabilities, there has been\nrenewed interest in enhancing these systems. Existing methods, however, often\nstruggle to capture the complex economic context of news and lack transparent\nreasoning, which undermines their reliability. We propose Analogy-Driven\nFinancial Chain-of-Thought (AD-FCoT), a prompting framework that integrates\nanalogical reasoning with chain-of-thought (CoT) prompting for sentiment\nprediction on historical financial news. AD-FCoT guides LLMs to draw parallels\nbetween new events and relevant historical scenarios with known outcomes,\nembedding these analogies into a structured, step-by-step reasoning chain. To\nour knowledge, this is among the first approaches to explicitly combine\nanalogical examples with CoT reasoning in finance. Operating purely through\nprompting, AD-FCoT requires no additional training data or fine-tuning and\nleverages the model's internal financial knowledge to generate rationales that\nmirror human analytical reasoning. Experiments on thousands of news articles\nshow that AD-FCoT outperforms strong baselines in sentiment classification\naccuracy and achieves substantially higher correlation with market returns. Its\ngenerated explanations also align with domain expertise, providing\ninterpretable insights suitable for real-world financial analysis.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12611v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12611v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.282,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a prompting technique for LLMs in financial sentiment analysis, specifically AD-FCoT, which uses analogical reasoning and chain-of-thought without any involvement of human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces AD-FCoT, which integrates analogical reasoning with chain-of-thought prompting, but it does not use diffusion models, iterative refinement processes, or treat reasoning as a holistically corrected entity over multiple steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12612",
      "title": "GBV-SQL: Guided Generation and SQL2Text Back-Translation Validation for\n  Multi-Agent Text2SQL",
      "authors": [
        "Daojun Chen",
        "Xi Wang",
        "Shenyuan Ren",
        "Qingzhi Ma",
        "Pengpeng Zhao",
        "An Liu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While Large Language Models have significantly advanced Text2SQL generation,\na critical semantic gap persists where syntactically valid queries often\nmisinterpret user intent. To mitigate this challenge, we propose GBV-SQL, a\nnovel multi-agent framework that introduces Guided Generation with SQL2Text\nBack-translation Validation. This mechanism uses a specialized agent to\ntranslate the generated SQL back into natural language, which verifies its\nlogical alignment with the original question. Critically, our investigation\nreveals that current evaluation is undermined by a systemic issue: the poor\nquality of the benchmarks themselves. We introduce a formal typology for \"Gold\nErrors\", which are pervasive flaws in the ground-truth data, and demonstrate\nhow they obscure true model performance. On the challenging BIRD benchmark,\nGBV-SQL achieves 63.23% execution accuracy, a 5.8% absolute improvement. After\nremoving flawed examples, GBV-SQL achieves 96.5% (dev) and 97.6% (test)\nexecution accuracy on the Spider benchmark. Our work offers both a robust\nframework for semantic validation and a critical perspective on benchmark\nintegrity, highlighting the need for more rigorous dataset curation.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12612v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12612v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.327,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a multi-agent framework for Text2SQL that involves iterative refinement and chain-of-thought processes, but it does not adapt the iterative refinement process of diffusion models. There is no mention of diffusion-based techniques, treating the chain-of-thought as a single entity for holistic correction via diffusion, or any multi-step logical reasoning explicitly linked to diffusion models. Thus, the paper's contributions are unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12615",
      "title": "Mob-based cattle weight gain forecasting using ML models",
      "authors": [
        "Muhammad Riaz Hasib Hossain",
        "Rafiqul Islam",
        "Shawn R McGrath",
        "Md Zahidul Islam",
        "David Lamb"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Forecasting mob based cattle weight gain (MB CWG) may benefit large livestock\nfarms, allowing farmers to refine their feeding strategies, make educated\nbreeding choices, and reduce risks linked to climate variability and market\nfluctuations. In this paper, a novel technique termed MB CWG is proposed to\nforecast the one month advanced weight gain of herd based cattle using\nhistorical data collected from the Charles Sturt University Farm. This research\nemploys a Random Forest (RF) model, comparing its performance against Support\nVector Regression (SVR) and Long Short Term Memory (LSTM) models for monthly\nweight gain prediction. Four datasets were used to evaluate the performance of\nmodels, using 756 sample data from 108 herd-based cattle, along with weather\ndata (rainfall and temperature) influencing CWG. The RF model performs better\nthan the SVR and LSTM models across all datasets, achieving an R^2 of 0.973,\nRMSE of 0.040, and MAE of 0.033 when both weather and age factors were\nincluded. The results indicate that including both weather and age factors\nsignificantly improves the accuracy of weight gain predictions, with the RF\nmodel outperforming the SVR and LSTM models in all scenarios. These findings\ndemonstrate the potential of RF as a robust tool for forecasting cattle weight\ngain in variable conditions, highlighting the influence of age and climatic\nfactors on herd based weight trends. This study has also developed an\ninnovative automated pre processing tool to generate a benchmark dataset for MB\nCWG predictive models. The tool is publicly available on GitHub and can assist\nin preparing datasets for current and future analytical research..",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12615v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12615v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.295,
      "distributed_training_score": 0.309,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12618",
      "title": "ActiveVLN: Towards Active Exploration via Multi-Turn RL in\n  Vision-and-Language Navigation",
      "authors": [
        "Zekai Zhang",
        "Weiye Zhu",
        "Hewei Pan",
        "Xiangchen Wang",
        "Rongtao Xu",
        "Xing Sun",
        "Feng Zheng"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The Vision-and-Language Navigation (VLN) task requires an agent to follow\nnatural language instructions and navigate through complex environments.\nExisting MLLM-based VLN methods primarily rely on imitation learning (IL) and\noften use DAgger for post-training to mitigate covariate shift. While\neffective, these approaches incur substantial data collection and training\ncosts. Reinforcement learning (RL) offers a promising alternative. However,\nprior VLN RL methods lack dynamic interaction with the environment and depend\non expert trajectories for reward shaping, rather than engaging in open-ended\nactive exploration. This restricts the agent's ability to discover diverse and\nplausible navigation routes. To address these limitations, we propose\nActiveVLN, a VLN framework that explicitly enables active exploration through\nmulti-turn RL. In the first stage, a small fraction of expert trajectories is\nused for IL to bootstrap the agent. In the second stage, the agent iteratively\npredicts and executes actions, automatically collects diverse trajectories, and\noptimizes multiple rollouts via the GRPO objective. To further improve RL\nefficiency, we introduce a dynamic early-stopping strategy to prune long-tail\nor likely failed trajectories, along with additional engineering optimizations.\nExperiments show that ActiveVLN achieves the largest performance gains over IL\nbaselines compared to both DAgger-based and prior RL-based post-training\nmethods, while reaching competitive performance with state-of-the-art\napproaches despite using a smaller model. Code and data will be released soon.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12618v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12618v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.452,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.387,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on reinforcement learning for active exploration in VLN, using environmental rewards and a small amount of expert trajectories for initial bootstrapping via imitation learning. However, it does not involve training a reward model on human-ranked data or using human feedback for fine-tuning, which are core to RLHF. Thus, it does not align with RLHF principles.",
      "weak_supervision_justification": "The paper uses a small fraction of expert trajectories for initial imitation learning and then relies on self-generated trajectories in RL, reducing dependence on fully labeled data. This indirectly relates to weak supervision by minimizing precise expert input, but the main approach is RL-based exploration rather than programmatic label generation for supervised learning.",
      "diffusion_reasoning_justification": "The paper proposes a multi-turn RL framework for VLN, involving iterative action prediction and optimization, but it does not incorporate diffusion models, iterative refinement of reasoning paths, or any multi-step logical reasoning via diffusion processes. The focus is solely on navigation and RL, with no elements of diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12625",
      "title": "ECG-aBcDe: Overcoming Model Dependence, Encoding ECG into a Universal\n  Language for Any LLM",
      "authors": [
        "Yong Xia",
        "Jingxuan Li",
        "YeTeng Sun",
        "Jiarui Bu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) hold significant promise for electrocardiogram\n(ECG) analysis, yet challenges remain regarding transferability, time-scale\ninformation learning, and interpretability. Current methods suffer from\nmodel-specific ECG encoders, hindering transfer across LLMs. Furthermore, LLMs\nstruggle to capture crucial time-scale information inherent in ECGs due to\nTransformer limitations. And their black-box nature limits clinical adoption.\nTo address these limitations, we introduce ECG-aBcDe, a novel ECG encoding\nmethod that transforms ECG signals into a universal ECG language readily\ninterpretable by any LLM. By constructing a hybrid dataset of ECG language and\nnatural language, ECG-aBcDe enables direct fine-tuning of pre-trained LLMs\nwithout architectural modifications, achieving \"construct once, use anywhere\"\ncapability. Moreover, the bidirectional convertibility between ECG and ECG\nlanguage of ECG-aBcDe allows for extracting attention heatmaps from ECG\nsignals, significantly enhancing interpretability. Finally, ECG-aBcDe\nexplicitly represents time-scale information, mitigating Transformer\nlimitations. This work presents a new paradigm for integrating ECG analysis\nwith LLMs. Compared with existing methods, our method achieves competitive\nperformance on ROUGE-L and METEOR. Notably, it delivers significant\nimprovements in the BLEU-4, with improvements of 2.8 times and 3.9 times in\nin-dataset and cross-dataset evaluations, respectively, reaching scores of\n42.58 and 30.76. These results provide strong evidence for the feasibility of\nthe new paradigm.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12625v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12625v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.363,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of ECG-aBcDe, a method for encoding ECG signals into a universal language for use with LLMs, focusing on transferability, time-scale information, and interpretability. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical tasks such as holistic Chain-of-Thought correction. Therefore, the paper lacks any clear component related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12626",
      "title": "DoubleAgents: Exploring Mechanisms of Building Trust with Proactive AI",
      "authors": [
        "Tao Long",
        "Xuanming Zhang",
        "Sitong Wang",
        "Zhou Yu",
        "Lydia B Chilton"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "Agentic workflows promise efficiency, but adoption hinges on whether people\nactually trust systems that act on their behalf. We present DoubleAgents, an\nagentic planning tool that embeds transparency and control through user\nintervention, value-reflecting policies, rich state visualizations, and\nuncertainty flagging for human coordination tasks. A built-in respondent\nsimulation generates realistic scenarios, allowing users to rehearse, refine\npolicies, and calibrate their reliance before live use. We evaluate\nDoubleAgents in a two-day lab study (n=10), two deployments (n=2), and a\ntechnical evaluation. Results show that participants initially hesitated to\ndelegate but grew more reliant as they experienced transparency, control, and\nadaptive learning during simulated cases. Deployment results demonstrate\nDoubleAgents' real-world relevance and usefulness, showing that the effort\nrequired scaled appropriately with task complexity and contextual data. We\ncontribute trust-by-design patterns and mechanisms for proactive AI --\nconsistency, controllability, and explainability -- along with simulation as a\nsafe path to build and calibrate trust over time.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12626v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12626v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.477,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.344,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces DoubleAgents, a system for building trust in AI agents through simulation, policy refinement, and human oversight, which incorporates human feedback to adjust AI behavior in real-time. While this involves elements of human interaction and alignment (e.g., users refining policies based on simulated scenarios), it does not describe the core RLHF process of training a reward model on human-ranked data and fine-tuning a main model via reinforcement learning. Instead, the focus is on interactive workflows and trust mechanisms, making it only loosely connected to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12627",
      "title": "Exploring Spectral Characteristics for Single Image Reflection Removal",
      "authors": [
        "Pengbo Guo",
        "Chengxu Liu",
        "Guoshuai Zhao",
        "Xingsong Hou",
        "Jialie Shen",
        "Xueming Qian"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Eliminating reflections caused by incident light interacting with reflective\nmedium remains an ill-posed problem in the image restoration area. The primary\nchallenge arises from the overlapping of reflection and transmission components\nin the captured images, which complicates the task of accurately distinguishing\nand recovering the clean background. Existing approaches typically address\nreflection removal solely in the image domain, ignoring the spectral property\nvariations of reflected light, which hinders their ability to effectively\ndiscern reflections. In this paper, we start with a new perspective on spectral\nlearning, and propose the Spectral Codebook to reconstruct the optical spectrum\nof the reflection image. The reflections can be effectively distinguished by\nperceiving the wavelength differences between different light sources in the\nspectrum. To leverage the reconstructed spectrum, we design two spectral prior\nrefinement modules to re-distribute pixels in the spatial dimension and\nadaptively enhance the spectral differences along the wavelength dimension.\nFurthermore, we present the Spectrum-Aware Transformer to jointly recover the\ntransmitted content in spectral and pixel domains. Experimental results on\nthree different reflection benchmarks demonstrate the superiority and\ngeneralization ability of our method compared to state-of-the-art models.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12627v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12627v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.279,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.273,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12632",
      "title": "Maps for Autonomous Driving: Full-process Survey and Frontiers",
      "authors": [
        "Pengxin Chen",
        "Zhipeng Luo",
        "Xiaoqi Jiang",
        "Zhangcai Yin",
        "Jonathan Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Maps have always been an essential component of autonomous driving. With the\nadvancement of autonomous driving technology, both the representation and\nproduction process of maps have evolved substantially. The article categorizes\nthe evolution of maps into three stages: High-Definition (HD) maps, Lightweight\n(Lite) maps, and Implicit maps. For each stage, we provide a comprehensive\nreview of the map production workflow, with highlighting technical challenges\ninvolved and summarizing relevant solutions proposed by the academic community.\nFurthermore, we discuss cutting-edge research advances in map representations\nand explore how these innovations can be integrated into end-to-end autonomous\ndriving frameworks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12632v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12632v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.339,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12633",
      "title": "CIARD: Cyclic Iterative Adversarial Robustness Distillation",
      "authors": [
        "Liming Lu",
        "Shuchao Pang",
        "Xu Zheng",
        "Xiang Gu",
        "Anan Du",
        "Yunhuai Liu",
        "Yongbin Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Adversarial robustness distillation (ARD) aims to transfer both performance\nand robustness from teacher model to lightweight student model, enabling\nresilient performance on resource-constrained scenarios. Though existing ARD\napproaches enhance student model's robustness, the inevitable by-product leads\nto the degraded performance on clean examples. We summarize the causes of this\nproblem inherent in existing methods with dual-teacher framework as: 1. The\ndivergent optimization objectives of dual-teacher models, i.e., the clean and\nrobust teachers, impede effective knowledge transfer to the student model, and\n2. The iteratively generated adversarial examples during training lead to\nperformance deterioration of the robust teacher model. To address these\nchallenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key\ninnovations: a. A multi-teacher framework with contrastive push-loss alignment\nto resolve conflicts in dual-teacher optimization objectives, and b. Continuous\nadversarial retraining to maintain dynamic teacher robustness against\nperformance degradation from the varying adversarial examples. Extensive\nexperiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD\nachieves remarkable performance with an average 3.53 improvement in adversarial\ndefense rates across various attack scenarios and a 5.87 increase in clean\nsample accuracy, establishing a new benchmark for balancing model robustness\nand generalization. Our code is available at https://github.com/eminentgu/CIARD",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12633v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12633v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.399,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12635",
      "title": "Positional Encoding via Token-Aware Phase Attention",
      "authors": [
        "Yu Wang",
        "Sheng Shen",
        "Rémi Munos",
        "Hongyuan Zhan",
        "Yuandong Tian"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We prove under practical assumptions that Rotary Positional Embedding (RoPE)\nintroduces an intrinsic distance-dependent bias in attention scores that limits\nRoPE's ability to model long-context. RoPE extension methods may alleviate this\nissue, but they typically require post-hoc adjustments after pretraining, such\nas rescaling or hyperparameters retuning. This paper introduces Token-Aware\nPhase Attention (TAPA), a new positional encoding method that incorporates a\nlearnable phase function into the attention mechanism. TAPA preserves token\ninteractions over long range, extends to longer contexts with direct and light\nfine-tuning, extrapolates to unseen lengths, and attains significantly lower\nperplexity on long-context than RoPE families.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12635v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12635v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.332,
      "datasets_score": 0.24,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12643",
      "title": "Learn to Relax with Large Language Models: Solving Nonlinear\n  Combinatorial Optimization Problems via Bidirectional Coevolution",
      "authors": [
        "Beidan Liu",
        "Zhengqiu Zhu",
        "Chen Gao",
        "Yong Zhao",
        "Wei Qi",
        "Quanjun Yin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Nonlinear Combinatorial Optimization Problems (NCOPs) present a formidable\ncomputational hurdle in practice, as their nonconvex nature gives rise to\nmulti-modal solution spaces that defy efficient optimization. Traditional\nconstraint relaxation approaches rely heavily on expert-driven, iterative\ndesign processes that lack systematic automation and scalable adaptability.\nWhile recent Large Language Model (LLM)-based optimization methods show promise\nfor autonomous problem-solving, they predominantly function as passive\nconstraint validators rather than proactive strategy architects, failing to\nhandle the sophisticated constraint interactions inherent to NCOPs.To address\nthese limitations, we introduce the first end-to-end \\textbf{Auto}mated\n\\textbf{C}onstraint \\textbf{O}ptimization (AutoCO) method, which revolutionizes\nNCOPs resolution through learning to relax with LLMs.Specifically, we leverage\nstructured LLM reasoning to generate constraint relaxation strategies, which\nare dynamically evolving with algorithmic principles and executable code\nthrough a unified triple-representation scheme. We further establish a novel\nbidirectional (global-local) coevolution mechanism that synergistically\nintegrates Evolutionary Algorithms for intensive local refinement with Monte\nCarlo Tree Search for systematic global strategy space exploration, ensuring\noptimal balance between intensification and diversification in fragmented\nsolution spaces. Finally, comprehensive experiments on three challenging NCOP\nbenchmarks validate AutoCO's consistent effectiveness and superior performance\nover the baselines.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12643v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12643v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.388,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an automated method using LLMs for constraint relaxation in optimization problems, incorporating evolutionary algorithms and Monte Carlo Tree Search. It does not involve human feedback, a reward model trained on human-ranked data, or reinforcement learning for aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes LLMs for generating strategies and a bidirectional coevolution mechanism, but it does not employ diffusion models, iterative refinement processes, or treat a Chain-of-Thought as a single entity for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12645",
      "title": "Large Language Models Imitate Logical Reasoning, but at what Cost?",
      "authors": [
        "Lachlan McGinness",
        "Peter Baumgartner"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "We present a longitudinal study which evaluates the reasoning capability of\nfrontier Large Language Models over an eighteen month period. We measured the\naccuracy of three leading models from December 2023, September 2024 and June\n2025 on true or false questions from the PrOntoQA dataset and their\nfaithfulness to reasoning strategies provided through in-context learning. The\nimprovement in performance from 2023 to 2024 can be attributed to hidden Chain\nof Thought prompting. The introduction of thinking models allowed for\nsignificant improvement in model performance between 2024 and 2025.\n  We then present a neuro-symbolic architecture which uses LLMs of less than 15\nbillion parameters to translate the problems into a standardised form. We then\nparse the standardised forms of the problems into a program to be solved by Z3,\nan SMT solver, to determine the satisfiability of the query. We report the\nnumber of prompt and completion tokens as well as the computational cost in\nFLOPs for open source models. The neuro-symbolic approach significantly reduces\nthe computational cost while maintaining near perfect performance. The common\napproximation that the number of inference FLOPs is double the product of the\nactive parameters and total tokens was accurate within 10\\% for all\nexperiments.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12645v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12645v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.581,
      "distributed_training_score": 0.383,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates LLMs' reasoning capabilities using in-context learning and a neuro-symbolic framework, but it does not involve training models with human feedback, reward models, or reinforcement learning for alignment. There is no mention of RLHF elements.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Chain of Thought prompting and neuro-symbolic architectures for reasoning, but it does not adapt diffusion models or use iterative refinement processes for logical tasks. There is no component for holistically correcting reasoning paths via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12649",
      "title": "A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for\n  the Security of Code LLMs",
      "authors": [
        "Kiho Lee",
        "Jungkon Kim",
        "Doowon Kim",
        "Hyoungshick Kim"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Code-generating Large Language Models (LLMs) significantly accelerate\nsoftware development. However, their frequent generation of insecure code\npresents serious risks. We present a comprehensive evaluation of seven\nparameter-efficient fine-tuning (PEFT) techniques, demonstrating substantial\ngains in secure code generation without compromising functionality. Our\nresearch identifies prompt-tuning as the most effective PEFT method, achieving\nan 80.86% Overall-Secure-Rate on CodeGen2 16B, a 13.5-point improvement over\nthe 67.28% baseline. Optimizing decoding strategies through sampling\ntemperature further elevated security to 87.65%. This equates to a reduction of\napproximately 203,700 vulnerable code snippets per million generated. Moreover,\nprompt and prefix tuning increase robustness against poisoning attacks in our\nTrojanPuzzle evaluation, with strong performance against CWE-79 and CWE-502\nattack vectors. Our findings generalize across Python and Java, confirming\nprompt-tuning's consistent effectiveness. This study provides essential\ninsights and practical guidance for building more resilient software systems\nwith LLMs.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12649v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12649v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.396,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates parameter-efficient fine-tuning (PEFT) methods like prompt-tuning and LoRA to enhance the security of code-generating LLMs, focusing on empirical analysis and vulnerability reduction. It does not involve reinforcement learning, human feedback, reward models, or any alignment with human preferences, which are core to RLHF. Therefore, there is no connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12650",
      "title": "Leveraging Intermediate Representations of Time Series Foundation Models\n  for Anomaly Detection",
      "authors": [
        "Chan Sik Han",
        "Keon Myung Lee"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Detecting anomalies in time series data is essential for the reliable\noperation of many real-world systems. Recently, time series foundation models\n(TSFMs) have emerged as a powerful tool for anomaly detection. However,\nexisting methods typically rely on the final layer's representations of TSFMs,\ncomputing the anomaly score as a reconstruction or forecasting error via a\ntask-specific head. Instead, we propose TimeRep, a novel anomaly detection\napproach that leverages the intermediate layer's representations of TSFMs,\ncomputing the anomaly score as the distance between these representations.\nGiven a pre-trained TSFM, TimeRep selects the intermediate layer and\npatch-token position that yield the most informative representation. TimeRep\nforms a reference collection of intermediate representations from the training\ndata and applies a core-set strategy to reduce its size while maintaining\ndistributional coverage. During inference, TimeRep computes the anomaly score\nfor incoming data by measuring the distance between its intermediate\nrepresentations and those of the collection. To address concept drift, TimeRep\nintegrates an adaptation mechanism that, at inference time, augments the\ncollection exclusively with non-redundant intermediate representations from\nincoming data. We conducted extensive experiments on the UCR Anomaly Archive,\nwhich contains 250 univariate time series. TimeRep consistently outperforms a\nbroad spectrum of state-of-the-art baselines, including non-DL, DL, and\nfoundation model-based methods.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12650v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12650v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.313,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12652",
      "title": "Don't Change My View: Ideological Bias Auditing in Large Language Models",
      "authors": [
        "Paul Kröger",
        "Emilio Barkett"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As large language models (LLMs) become increasingly embedded in products used\nby millions, their outputs may influence individual beliefs and, cumulatively,\nshape public opinion. If the behavior of LLMs can be intentionally steered\ntoward specific ideological positions, such as political or religious views,\nthen those who control these systems could gain disproportionate influence over\npublic discourse. Although it remains an open question whether LLMs can\nreliably be guided toward coherent ideological stances and whether such\nsteering can be effectively prevented, a crucial first step is to develop\nmethods for detecting when such steering attempts occur. In this work, we adapt\na previously proposed statistical method to the new context of ideological bias\nauditing. Our approach carries over the model-agnostic design of the original\nframework, which does not require access to the internals of the language\nmodel. Instead, it identifies potential ideological steering by analyzing\ndistributional shifts in model outputs across prompts that are thematically\nrelated to a chosen topic. This design makes the method particularly suitable\nfor auditing proprietary black-box systems. We validate our approach through a\nseries of experiments, demonstrating its practical applicability and its\npotential to support independent post hoc audits of LLM behavior.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12652v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12652v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.475,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.331,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is developing a method for auditing ideological bias in LLMs by detecting distributional shifts in outputs, without requiring access to model internals. It focuses on statistical analysis for bias detection and does not mention, involve, or relate to Reinforcement Learning from Human Feedback (RLHF), which specifically entails training models using human-ranked data and reinforcement learning for alignment. There is no discussion of human feedback mechanisms, reward models, or fine-tuning via RLHF in the paper.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12653",
      "title": "Beyond Artificial Misalignment: Detecting and Grounding\n  Semantic-Coordinated Multimodal Manipulations",
      "authors": [
        "Jinjie Shen",
        "Yaxiong Wang",
        "Lechao Cheng",
        "Nan Pu",
        "Zhun Zhong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The detection and grounding of manipulated content in multimodal data has\nemerged as a critical challenge in media forensics. While existing benchmarks\ndemonstrate technical progress, they suffer from misalignment artifacts that\npoorly reflect real-world manipulation patterns: practical attacks typically\nmaintain semantic consistency across modalities, whereas current datasets\nartificially disrupt cross-modal alignment, creating easily detectable\nanomalies. To bridge this gap, we pioneer the detection of\nsemantically-coordinated manipulations where visual edits are systematically\npaired with semantically consistent textual descriptions. Our approach begins\nwith constructing the first Semantic-Aligned Multimodal Manipulation (SAMM)\ndataset, generated through a two-stage pipeline: 1) applying state-of-the-art\nimage manipulations, followed by 2) generation of contextually-plausible\ntextual narratives that reinforce the visual deception. Building on this\nfoundation, we propose a Retrieval-Augmented Manipulation Detection and\nGrounding (RamDG) framework. RamDG commences by harnessing external knowledge\nrepositories to retrieve contextual evidence, which serves as the auxiliary\ntexts and encoded together with the inputs through our image forgery grounding\nand deep manipulation detection modules to trace all manipulations. Extensive\nexperiments demonstrate our framework significantly outperforms existing\nmethods, achieving 2.06\\% higher detection accuracy on SAMM compared to\nstate-of-the-art approaches. The dataset and code are publicly available at\nhttps://github.com/shen8424/SAMM-RamDG-CAP.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12653v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12653v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.323,
      "datasets_score": 0.419,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on multimodal manipulation detection and introduces a new dataset and framework (RamDG) using external knowledge retrieval, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. There is no mention of adapting diffusion for chain-of-thought reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating and introducing the SAMM dataset, detailing its curation methodology (e.g., two-stage pipeline for image and text manipulation), and discussing its use for benchmarking and evaluation in multimodal manipulation detection, which aligns directly with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of existing multimodal manipulation detection benchmarks by introducing the Semantic-Aligned Multimodal Manipulation (SAMM) dataset, which features semantically coordinated visual and textual manipulations to better reflect real-world scenarios, and proposes the Retrieval-Augmented Manipulation Detection and Grounding (RamDG) framework that leverages external knowledge bases for improved detection and grounding of manipulations. The methodology involves a two-stage pipeline for dataset creation—applying image manipulations followed by generating consistent textual descriptions—and RamDG's use of retrieval mechanisms and contrastive learning to enhance accuracy, with experiments showing it outperforms state-of-the-art methods by achieving 2.06% higher detection accuracy on SAMM.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the SAMM dataset and RamDG framework, which cleverly combine existing techniques like image manipulation and retrieval-augmented methods to address the gap in semantically aligned multimodal manipulations, though it builds on known problems in media forensics.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in multimodal manipulation detection and media forensics by providing a more realistic dataset and effective framework, potentially leading to advancements in AI safety and fake news detection within its subfield.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong and valuable contribution to computer vision and AI through its innovative dataset and framework, making it essential for researchers in media forensics and multimodal analysis to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f8ed0d626a2d0981c4250c03a22dcc238844872d",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 3,
      "average_h_index": 1.6,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jinjie Shen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380830741"
        },
        {
          "name": "Yaxiong Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2303413831"
        },
        {
          "name": "Lechao Cheng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2303427080"
        },
        {
          "name": "Nan Pu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2327049243"
        },
        {
          "name": "Zhun Zhong",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2290969758"
        }
      ]
    },
    {
      "id": "2509.12658",
      "title": "Sustainable LSTM-Based Precoding for RIS-Aided mmWave MIMO Systems with\n  Implicit CSI",
      "authors": [
        "Po-Heng Chou",
        "Jiun-Jia Wu",
        "Wan-Jen Huang",
        "Ronald Y. Chang"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.IT (Information Theory)",
        "cs.LG (Machine Learning)",
        "cs.NI (Networking and Internet Architecture)",
        "math.IT (Information Theory)"
      ],
      "abstract": "In this paper, we propose a sustainable long short-term memory (LSTM)-based\nprecoding framework for reconfigurable intelligent surface (RIS)-assisted\nmillimeter-wave (mmWave) MIMO systems. Instead of explicit channel state\ninformation (CSI) estimation, the framework exploits uplink pilot sequences to\nimplicitly learn channel characteristics, reducing both pilot overhead and\ninference complexity. Practical hardware constraints are addressed by\nincorporating the phase-dependent amplitude model of RIS elements, while a\nmulti-label training strategy improves robustness when multiple near-optimal\ncodewords yield comparable performance. Simulations show that the proposed\ndesign achieves over 90% of the spectral efficiency of exhaustive search (ES)\nwith only 2.2% of its computation time, cutting energy consumption by nearly\ntwo orders of magnitude. The method also demonstrates resilience under\ndistribution mismatch and scalability to larger RIS arrays, making it a\npractical and energy-efficient solution for sustainable 6G wireless networks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12658v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12658v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.421,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is an LSTM-based precoding framework for RIS-aided mmWave MIMO systems, focusing on implicit CSI learning, hardware constraints, and energy-efficient inference. It does not address distributed training, parallel computing, or multi-node machine learning techniques, as there is no mention of partitioning data, model architecture, or computation across multiple processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12669",
      "title": "Exact alternative optima for nonlinear optimization problems defined\n  with maximum component objective function constrained by the Sugeno-Weber\n  fuzzy relational inequalities",
      "authors": [
        "Amin Ghodousian",
        "Sara Zal",
        "Minoo Ahmadi"
      ],
      "categories": [
        "math.OC (Optimization and Control)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we study a latticized optimization problem with fuzzy\nrelational inequality constraints where the feasible region is formed as the\nintersection of two inequality fuzzy systems and Sugeno-Weber family of t-norms\nis considered as fuzzy composition. Sugeno-Weber family of t-norms and\nt-conorms is one of the most applied one in various fuzzy modelling problems.\nThis family of t-norms and t-conorms was suggested by Weber for modeling\nintersection and union of fuzzy sets. Also, the t-conorms were suggested as\naddition rules by Sugeno for so-called alpha-fuzzy measures. The resolution of\nthe feasible region of the problem is firstly investigated when it is defined\nwith max-Sugeno-Weber composition and a necessary and sufficient condition is\npresented for determining the feasibility. Then, based on some theoretical\nproperties of the problem, an algorithm is presented for solving this nonlinear\nproblem. It is proved that the algorithm can find the exact optimal solution\nand an example is presented to illustrate the proposed algorithm.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12669v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12669v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.27,
      "weak_supervision_score": 0.267,
      "diffusion_reasoning_score": 0.252,
      "distributed_training_score": 0.22,
      "datasets_score": 0.175,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12673",
      "title": "MFAF: An EVA02-Based Multi-scale Frequency Attention Fusion Method for\n  Cross-View Geo-Localization",
      "authors": [
        "YiTong Liu",
        "TianZhu Liu",
        "YanFeng GU"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Cross-view geo-localization aims to determine the geographical location of a\nquery image by matching it against a gallery of images. This task is\nchallenging due to the significant appearance variations of objects observed\nfrom variable views, along with the difficulty in extracting discriminative\nfeatures. Existing approaches often rely on extracting features through feature\nmap segmentation while neglecting spatial and semantic information. To address\nthese issues, we propose the EVA02-based Multi-scale Frequency Attention Fusion\n(MFAF) method. The MFAF method consists of Multi-Frequency Branch-wise Block\n(MFB) and the Frequency-aware Spatial Attention (FSA) module. The MFB block\neffectively captures both low-frequency structural features and high-frequency\nedge details across multiple scales, improving the consistency and robustness\nof feature representations across various viewpoints. Meanwhile, the FSA module\nadaptively focuses on the key regions of frequency features, significantly\nmitigating the interference caused by background noise and viewpoint\nvariability. Extensive experiments on widely recognized benchmarks, including\nUniversity-1652, SUES-200, and Dense-UAV, demonstrate that the MFAF method\nachieves competitive performance in both drone localization and drone\nnavigation tasks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12673v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12673v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.363,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12678",
      "title": "Instance-level Randomization: Toward More Stable LLM Evaluations",
      "authors": [
        "Yiyang Li",
        "Yonghuang Wu",
        "Ying Luo",
        "Liangtai Sun",
        "Zishu Qin",
        "Lin Qiu",
        "Xuezhi Cao",
        "Xunliang Cai"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Evaluations of large language models (LLMs) suffer from instability, where\nsmall changes of random factors such as few-shot examples can lead to drastic\nfluctuations of scores and even model rankings. Moreover, different LLMs can\nhave different preferences for a certain setting of random factors. As a\nresult, using a fixed setting of random factors, which is often adopted as the\nparadigm of current evaluations, can lead to potential unfair comparisons\nbetween LLMs. To mitigate the volatility of evaluations, we first theoretically\nanalyze the sources of variance induced by changes in random factors. Targeting\nthese specific sources, we then propose the instance-level randomization (ILR)\nmethod to reduce variance and enhance fairness in model comparisons. Instead of\nusing a fixed setting across the whole benchmark in a single experiment, we\nrandomize all factors that affect evaluation scores for every single instance,\nrun multiple experiments and report the averaged score. Theoretical analyses\nand empirical results demonstrate that ILR can reduce the variance and unfair\ncomparisons caused by random factors, as well as achieve similar robustness\nlevel with less than half computational cost compared with previous methods.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12678v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12678v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.443,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.372,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a method for stabilizing evaluations of large language models by reducing variance through instance-level randomization, focusing on random factors in benchmarking. It does not involve training models with human feedback, reward models, or reinforcement learning for alignment.",
      "weak_supervision_justification": "The paper deals with improving the robustness of LLM evaluations by addressing random factors, such as few-shot examples, but it does not cover training models using programmatically generated labels from noisy sources, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12682",
      "title": "A Comparative Study of YOLOv8 to YOLOv11 Performance in Underwater\n  Vision Tasks",
      "authors": [
        "Gordon Hung",
        "Ivan Felipe Rodriguez"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autonomous underwater vehicles (AUVs) increasingly rely on on-board\ncomputer-vision systems for tasks such as habitat mapping, ecological\nmonitoring, and infrastructure inspection. However, underwater imagery is\nhindered by light attenuation, turbidity, and severe class imbalance, while the\ncomputational resources available on AUVs are limited. One-stage detectors from\nthe YOLO family are attractive because they fuse localization and\nclassification in a single, low-latency network; however, their terrestrial\nbenchmarks (COCO, PASCAL-VOC, Open Images) leave open the question of how\nsuccessive YOLO releases perform in the marine domain. We curate two openly\navailable datasets that span contrasting operating conditions: a Coral Disease\nset (4,480 images, 18 classes) and a Fish Species set (7,500 images, 20\nclasses). For each dataset, we create four training regimes (25 %, 50 %, 75 %,\n100 % of the images) while keeping balanced validation and test partitions\nfixed. We train YOLOv8-s, YOLOv9-s, YOLOv10-s, and YOLOv11-s with identical\nhyperparameters (100 epochs, 640 px input, batch = 16, T4 GPU) and evaluate\nprecision, recall, mAP50, mAP50-95, per-image inference time, and\nframes-per-second (FPS). Post-hoc Grad-CAM visualizations probe feature\nutilization and localization faithfulness. Across both datasets, accuracy\nsaturates after YOLOv9, suggesting architectural innovations primarily target\nefficiency rather than accuracy. Inference speed, however, improves markedly.\nOur results (i) provide the first controlled comparison of recent YOLO variants\non underwater imagery, (ii) show that lightweight YOLOv10 offers the best\nspeed-accuracy trade-off for embedded AUV deployment, and (iii) deliver an\nopen, reproducible benchmark and codebase to accelerate future marine-vision\nresearch.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12682v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12682v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.272,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.322,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12683",
      "title": "StereoCarla: A High-Fidelity Driving Dataset for Generalizable Stereo",
      "authors": [
        "Xianda Guo",
        "Chenming Zhang",
        "Ruilin Wang",
        "Youmin Zhang",
        "Wenzhao Zheng",
        "Matteo Poggi",
        "Hao Zhao",
        "Qin Zou",
        "Long Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Stereo matching plays a crucial role in enabling depth perception for\nautonomous driving and robotics. While recent years have witnessed remarkable\nprogress in stereo matching algorithms, largely driven by learning-based\nmethods and synthetic datasets, the generalization performance of these models\nremains constrained by the limited diversity of existing training data. To\naddress these challenges, we present StereoCarla, a high-fidelity synthetic\nstereo dataset specifically designed for autonomous driving scenarios. Built on\nthe CARLA simulator, StereoCarla incorporates a wide range of camera\nconfigurations, including diverse baselines, viewpoints, and sensor placements\nas well as varied environmental conditions such as lighting changes, weather\neffects, and road geometries. We conduct comprehensive cross-domain experiments\nacross four standard evaluation datasets (KITTI2012, KITTI2015, Middlebury,\nETH3D) and demonstrate that models trained on StereoCarla outperform those\ntrained on 11 existing stereo datasets in terms of generalization accuracy\nacross multiple benchmarks. Furthermore, when integrated into multi-dataset\ntraining, StereoCarla contributes substantial improvements to generalization\naccuracy, highlighting its compatibility and scalability. This dataset provides\na valuable benchmark for developing and evaluating stereo algorithms under\nrealistic, diverse, and controllable settings, facilitating more robust depth\nperception systems for autonomous vehicles. Code can be available at\nhttps://github.com/XiandaGuo/OpenStereo, and data can be available at\nhttps://xiandaguo.net/StereoCarla.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12683v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12683v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.266,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.274,
      "distributed_training_score": 0.353,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and evaluation of a new synthetic dataset, StereoCarla, designed for stereo matching in autonomous driving. It covers dataset creation using the CARLA simulator, includes diverse configurations for enhanced generalization, and provides benchmarking against existing datasets through experiments on multiple benchmarks. This directly aligns with research on creating, benchmarking, and evaluating datasets for machine learning and AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces StereoCarla, a high-fidelity synthetic dataset created using the CARLA simulator, aimed at improving the generalization of stereo matching algorithms for autonomous driving by incorporating diverse camera configurations, baselines, viewpoints, and environmental conditions such as weather and lighting. Through extensive cross-domain experiments on benchmarks like KITTI2012, KITTI2015, Middlebury, and ETH3D, the authors demonstrate that models trained on StereoCarla outperform those trained on 11 existing datasets in terms of generalization accuracy and robustness, and it enhances performance when integrated into multi-dataset training, providing a valuable resource for developing more reliable depth perception systems.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a more diverse synthetic dataset that addresses the limitations of existing ones for stereo matching generalization, though it builds on established simulators and techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and utilized within the subfield of computer vision for autonomous driving, as it provides a scalable dataset that improves model generalization, but its influence may remain confined to specific applications in stereo vision research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by offering a new dataset that enhances stereo matching robustness, making it essential for researchers in autonomous driving and computer vision to consider for advancing their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1d7ac971fbd3dd409294a328980aead4ffab766a",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 6,
      "average_h_index": 2.5555555555555554,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Xianda Guo",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2284674027"
        },
        {
          "name": "Chenming Zhang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2212835056"
        },
        {
          "name": "Ruilin Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2331694048"
        },
        {
          "name": "Youmin Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2309083394"
        },
        {
          "name": "Wenzhao Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376388690"
        },
        {
          "name": "Matteo Poggi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380606131"
        },
        {
          "name": "Hao Zhao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2363599402"
        },
        {
          "name": "Qin Zou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362628158"
        },
        {
          "name": "Long Chen",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2284693771"
        }
      ]
    },
    {
      "id": "2509.12701",
      "title": "SmokeBench: A Real-World Dataset for Surveillance Image Desmoking in\n  Early-Stage Fire Scenes",
      "authors": [
        "Wenzhuo Jin",
        "Qianfeng Yang",
        "Xianhao Wu",
        "Hongming Chen",
        "Pengpeng Li",
        "Xiang Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Early-stage fire scenes (0-15 minutes after ignition) represent a crucial\ntemporal window for emergency interventions. During this stage, the smoke\nproduced by combustion significantly reduces the visibility of surveillance\nsystems, severely impairing situational awareness and hindering effective\nemergency response and rescue operations. Consequently, there is an urgent need\nto remove smoke from images to obtain clear scene information. However, the\ndevelopment of smoke removal algorithms remains limited due to the lack of\nlarge-scale, real-world datasets comprising paired smoke-free and\nsmoke-degraded images. To address these limitations, we present a real-world\nsurveillance image desmoking benchmark dataset named SmokeBench, which contains\nimage pairs captured under diverse scenes setup and smoke concentration. The\ncurated dataset provides precisely aligned degraded and clean images, enabling\nsupervised learning and rigorous evaluation. We conduct comprehensive\nexperiments by benchmarking a variety of desmoking methods on our dataset. Our\ndataset provides a valuable foundation for advancing robust and practical image\ndesmoking in real-world fire scenes. This dataset has been released to the\npublic and can be downloaded from https://github.com/ncfjd/SmokeBench.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12701v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12701v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.269,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.314,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new real-world dataset called SmokeBench, specifically designed for machine learning applications in image desmoking. It details the dataset creation process, including curation methodologies via the smoke acquisition system, and includes benchmarking of various desmoking methods. This directly aligns with research on creating, analyzing, benchmarking, and evaluating datasets for AI, making it a core focus of the paper.",
      "llm_score_status": "completed",
      "summary": "SmokeBench addresses the critical need for real-world datasets in image desmoking for early-stage fire scenes by introducing a benchmark dataset of paired smoke-free and smoke-degraded surveillance images, captured using a specialized smoke acquisition system that accounts for diverse scenes and smoke concentrations. The methodology involves scene setup, data acquisition, and enhancement to create precisely aligned image pairs, enabling supervised learning and evaluation of various desmoking methods, with the dataset's release fostering advancements in practical fire emergency response systems.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the first real-world dataset for surveillance image desmoking, which cleverly addresses the limitations of synthetic datasets, though it does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in computer vision subfields focused on fire safety and image processing, as it provides a foundational dataset for developing more effective desmoking algorithms in real-world applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution by offering a practical dataset that could enhance research in emergency response technologies, making it essential for specialists in computer vision but not for the general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fdc0f7bcc4647b6ffc1727ffdee7b79f47e97d3a",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 5,
      "average_h_index": 1.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Wenzhuo Jin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381084901"
        },
        {
          "name": "Qianfeng Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379764343"
        },
        {
          "name": "Xianhao Wu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2265181165"
        },
        {
          "name": "Hongming Chen",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2237384440"
        },
        {
          "name": "Pengpeng Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380792179"
        },
        {
          "name": "Xiang Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2301258787"
        }
      ]
    },
    {
      "id": "2509.12710",
      "title": "RIS-FUSION: Rethinking Text-Driven Infrared and Visible Image Fusion\n  from the Perspective of Referring Image Segmentation",
      "authors": [
        "Siju Ma",
        "Changsiyu Gong",
        "Xiaofeng Fan",
        "Yong Ma",
        "Chengjie Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-driven infrared and visible image fusion has gained attention for\nenabling natural language to guide the fusion process. However, existing\nmethods lack a goal-aligned task to supervise and evaluate how effectively the\ninput text contributes to the fusion outcome. We observe that referring image\nsegmentation (RIS) and text-driven fusion share a common objective:\nhighlighting the object referred to by the text. Motivated by this, we propose\nRIS-FUSION, a cascaded framework that unifies fusion and RIS through joint\noptimization. At its core is the LangGatedFusion module, which injects textual\nfeatures into the fusion backbone to enhance semantic alignment. To support\nmultimodal referring image segmentation task, we introduce MM-RIS, a\nlarge-scale benchmark with 12.5k training and 3.5k testing triplets, each\nconsisting of an infrared-visible image pair, a segmentation mask, and a\nreferring expression. Extensive experiments show that RIS-FUSION achieves\nstate-of-the-art performance, outperforming existing methods by over 11% in\nmIoU. Code and dataset will be released at\nhttps://github.com/SijuMa2003/RIS-FUSION.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12710v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12710v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.299,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for text-driven infrared and visible image fusion, integrated with referring image segmentation, using components like BERT for text encoding and cross-attention. It does not involve diffusion models, iterative refinement for logical reasoning, or treating a 'Chain-of-Thought' as an entity for multi-step correction. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12711",
      "title": "Learning by Imagining: Debiased Feature Augmentation for Compositional\n  Zero-Shot Learning",
      "authors": [
        "Haozhe Zhang",
        "Chenchen Jing",
        "Mingyu Liu",
        "Qingsheng Wang",
        "Hao Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Compositional Zero-Shot Learning (CZSL) aims to recognize unseen\nattribute-object compositions by learning prior knowledge of seen primitives,\n\\textit{i.e.}, attributes and objects. Learning generalizable compositional\nrepresentations in CZSL remains challenging due to the entangled nature of\nattributes and objects as well as the prevalence of long-tailed distributions\nin real-world data. Inspired by neuroscientific findings that imagination and\nperception share similar neural processes, we propose a novel approach called\nDebiased Feature Augmentation (DeFA) to address these challenges. The proposed\nDeFA integrates a disentangle-and-reconstruct framework for feature\naugmentation with a debiasing strategy. DeFA explicitly leverages the prior\nknowledge of seen attributes and objects by synthesizing high-fidelity\ncomposition features to support compositional generalization. Extensive\nexperiments on three widely used datasets demonstrate that DeFA achieves\nstate-of-the-art performance in both \\textit{closed-world} and\n\\textit{open-world} settings.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12711v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12711v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.355,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Debiased Feature Augmentation (DeFA) for Compositional Zero-Shot Learning, which involves synthesizing features through a disentangle-and-reconstruct framework inspired by neuroscientific concepts. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12715",
      "title": "AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in\n  Large Vision-Language Models",
      "authors": [
        "Heng Zhang",
        "Haichuan Hu",
        "Yaomin Shen",
        "Weihao Yu",
        "Yilei Yuan",
        "Haochen You",
        "Guo Cheng",
        "Zijian Zhang",
        "Lubin Gan",
        "Huihui Wei",
        "Hao Zhang",
        "Jin Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive performance\non multimodal tasks through scaled architectures and extensive training.\nHowever, existing Mixture of Experts (MoE) approaches face challenges due to\nthe asymmetry between visual and linguistic processing. Visual information is\nspatially complete, while language requires maintaining sequential context. As\na result, MoE models struggle to balance modality-specific features and\ncross-modal interactions. Through systematic analysis, we observe that language\nexperts in deeper layers progressively lose contextual grounding and rely more\non parametric knowledge rather than utilizing the provided visual and\nlinguistic information. To address this, we propose AsyMoE, a novel\narchitecture that models this asymmetry using three specialized expert groups.\nWe design intra-modality experts for modality-specific processing, hyperbolic\ninter-modality experts for hierarchical cross-modal interactions, and\nevidence-priority language experts to suppress parametric biases and maintain\ncontextual grounding. Extensive experiments demonstrate that AsyMoE achieves\n26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific\nMoE respectively, with 25.45% fewer activated parameters than dense models.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12715v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12715v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.352,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing Mixture of Experts (MoE) architectures in Large Vision-Language Models (LVLMs) by addressing asymmetries between visual and linguistic processing, including specialized experts and hyperbolic spaces. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12716",
      "title": "Joint AoI and Handover Optimization in Space-Air-Ground Integrated\n  Network",
      "authors": [
        "Zifan Lang",
        "Guixia Liu",
        "Geng Sun",
        "Jiahui Li",
        "Jiacheng Wang",
        "Weijie Yuan",
        "Dusit Niyato",
        "Dong In Kim"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite the widespread deployment of terrestrial networks, providing reliable\ncommunication services to remote areas and maintaining connectivity during\nemergencies remains challenging. Low Earth orbit (LEO) satellite constellations\noffer promising solutions with their global coverage capabilities and reduced\nlatency, yet struggle with intermittent coverage and limited communication\nwindows due to orbital dynamics. This paper introduces an age of information\n(AoI)-aware space-air-ground integrated network (SAGIN) architecture that\nleverages a high-altitude platform (HAP) as intelligent relay between the LEO\nsatellites and ground terminals. Our three-layer design employs hybrid\nfree-space optical (FSO) links for high-capacity satellite-to-HAP communication\nand reliable radio frequency (RF) links for HAP-to-ground transmission, and\nthus addressing the temporal discontinuity in LEO satellite coverage while\nserving diverse user priorities. Specifically, we formulate a joint\noptimization problem to simultaneously minimize the AoI and satellite handover\nfrequency through optimal transmit power distribution and satellite selection\ndecisions. This highly dynamic, non-convex problem with time-coupled\nconstraints presents significant computational challenges for traditional\napproaches. To address these difficulties, we propose a novel diffusion model\n(DM)-enhanced dueling double deep Q-network with action decomposition and state\ntransformer encoder (DD3QN-AS) algorithm that incorporates transformer-based\ntemporal feature extraction and employs a DM-based latent prompt generative\nmodule to refine state-action representations through conditional denoising.\nSimulation results highlight the superior performance of the proposed approach\ncompared with policy-based methods and some other deep reinforcement learning\n(DRL) benchmarks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12716v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12716v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.387,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a deep reinforcement learning (DRL) algorithm, specifically a Diffusion Model-enhanced Dueling Double Deep Q-Network, for optimizing age of information and handover frequency in a space-air-ground integrated network. However, it does not involve human feedback, such as training a reward model on human-ranked data or fine-tuning based on human preferences, which are essential elements of RLHF. Therefore, the paper's contributions are unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12718",
      "title": "EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer",
      "authors": [
        "Pukun Zhao",
        "Longxiang Wang",
        "Miaowei Wang",
        "Chen Chen",
        "Fanqing Zhou",
        "Haojian Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Most existing spatial reasoning benchmarks focus on static or globally\nobservable environments, failing to capture the challenges of long-horizon\nreasoning and memory utilization under partial observability and dynamic\nchanges. We introduce two dynamic spatial benchmarks, locally observable maze\nnavigation and match-2 elimination that systematically evaluate models'\nabilities in spatial understanding and adaptive planning when local perception,\nenvironment feedback, and global objectives are tightly coupled. Each action\ntriggers structural changes in the environment, requiring continuous update of\ncognition and strategy. We further propose a subjective experience-based memory\nmechanism for cross-task experience transfer and validation. Experiments show\nthat our benchmarks reveal key limitations of mainstream models in dynamic\nspatial reasoning and long-term memory, providing a comprehensive platform for\nfuture methodological advances. Our code and data are available at\nhttps://anonymous.4open.science/r/EvoEmpirBench-143C/.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12718v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12718v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.546,
      "distributed_training_score": 0.342,
      "datasets_score": 0.384,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on dynamic spatial reasoning benchmarks, agent-based learning frameworks, and memory mechanisms for LLMs in interactive environments like maze navigation and match-2 games. It does not mention or utilize diffusion models, iterative refinement processes for reasoning, or any adaptation of diffusion techniques for logical tasks such as treating a Chain-of-Thought as a single entity. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12721",
      "title": "SPGen: Spherical Projection as Consistent and Flexible Representation\n  for Single Image 3D Shape Generation",
      "authors": [
        "Jingdong Zhang",
        "Weikai Chen",
        "Yuan Liu",
        "Jionghao Wang",
        "Zhengming Yu",
        "Zhuowen Shen",
        "Bo Yang",
        "Wenping Wang",
        "Xin Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing single-view 3D generative models typically adopt multiview diffusion\npriors to reconstruct object surfaces, yet they remain prone to inter-view\ninconsistencies and are unable to faithfully represent complex internal\nstructure or nontrivial topologies. In particular, we encode geometry\ninformation by projecting it onto a bounding sphere and unwrapping it into a\ncompact and structural multi-layer 2D Spherical Projection (SP) representation.\nOperating solely in the image domain, SPGen offers three key advantages\nsimultaneously: (1) Consistency. The injective SP mapping encodes surface\ngeometry with a single viewpoint which naturally eliminates view inconsistency\nand ambiguity; (2) Flexibility. Multi-layer SP maps represent nested internal\nstructures and support direct lifting to watertight or open 3D surfaces; (3)\nEfficiency. The image-domain formulation allows the direct inheritance of\npowerful 2D diffusion priors and enables efficient finetuning with limited\ncomputational resources. Extensive experiments demonstrate that SPGen\nsignificantly outperforms existing baselines in geometric quality and\ncomputational efficiency.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12721v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12721v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.271,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.348,
      "datasets_score": 0.246,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for 3D shape generation from single images via Spherical Projection maps, emphasizing generative tasks like reconstructing geometry. It does not involve adapting diffusion for complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as an entity, as required by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12724",
      "title": "Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks\n  in Vision-Language Models",
      "authors": [
        "Yunhan Zhao",
        "Xiang Zheng",
        "Xingjun Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite their superb capabilities, Vision-Language Models (VLMs) have been\nshown to be vulnerable to jailbreak attacks. While recent jailbreaks have\nachieved notable progress, their effectiveness and efficiency can still be\nimproved. In this work, we reveal an interesting phenomenon: incorporating weak\ndefense into the attack pipeline can significantly enhance both the\neffectiveness and the efficiency of jailbreaks on VLMs. Building on this\ninsight, we propose Defense2Attack, a novel jailbreak method that bypasses the\nsafety guardrails of VLMs by leveraging defensive patterns to guide jailbreak\nprompt design. Specifically, Defense2Attack consists of three key components:\n(1) a visual optimizer that embeds universal adversarial perturbations with\naffirmative and encouraging semantics; (2) a textual optimizer that refines the\ninput using a defense-styled prompt; and (3) a red-team suffix generator that\nenhances the jailbreak through reinforcement fine-tuning. We empirically\nevaluate our method on four VLMs and four safety benchmarks. The results\ndemonstrate that Defense2Attack achieves superior jailbreak performance in a\nsingle attempt, outperforming state-of-the-art attack methods that often\nrequire multiple tries. Our work offers a new perspective on jailbreaking VLMs.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12724v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12724v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.312,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a jailbreak method for Vision-Language Models by incorporating weak defenses into adversarial prompt design, including visual and textual optimizers. It does not involve diffusion models, iterative refinement processes for logical tasks, or treating a Chain-of-Thought as a single entity for holistic correction. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12727",
      "title": "Unbiased Online Curvature Approximation for Regularized Graph Continual\n  Learning",
      "authors": [
        "Jie Yin",
        "Ke Sun",
        "Han Wu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Graph continual learning (GCL) aims to learn from a continuous sequence of\ngraph-based tasks. Regularization methods are vital for preventing catastrophic\nforgetting in GCL, particularly in the challenging replay-free,\nclass-incremental setting, where each task consists of a set of unique classes.\nIn this work, we first establish a general regularization framework for GCL\nbased on the curved parameter space induced by the Fisher information matrix\n(FIM). We show that the dominant Elastic Weight Consolidation (EWC) and its\nvariants are a special case within this framework, using a diagonal\napproximation of the empirical FIM based on parameters from previous tasks. To\novercome their limitations, we propose a new unbiased online curvature\napproximation of the full FIM based on the model's current learning state. Our\nmethod directly estimates the regularization term in an online manner without\nexplicitly evaluating and storing the FIM itself. This enables the model to\nbetter capture the loss landscape during learning new tasks while retaining the\nknowledge learned from previous tasks. Extensive experiments on three graph\ndatasets demonstrate that our method significantly outperforms existing\nregularization-based methods, achieving a superior trade-off between stability\n(retaining old knowledge) and plasticity (acquiring new knowledge).",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12727v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12727v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.369,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12728",
      "title": "Generalizable Holographic Reconstruction via Amplitude-Only Diffusion\n  Priors",
      "authors": [
        "Jeongsol Kim",
        "Chanseok Lee",
        "Jongin You",
        "Jong Chul Ye",
        "Mooseok Jang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Phase retrieval in inline holography is a fundamental yet ill-posed inverse\nproblem due to the nonlinear coupling between amplitude and phase in coherent\nimaging. We present a novel off-the-shelf solution that leverages a diffusion\nmodel trained solely on object amplitude to recover both amplitude and phase\nfrom diffraction intensities. Using a predictor-corrector sampling framework\nwith separate likelihood gradients for amplitude and phase, our method enables\ncomplex field reconstruction without requiring ground-truth phase data for\ntraining. We validate the proposed approach through extensive simulations and\nexperiments, demonstrating robust generalization across diverse object shapes,\nimaging system configurations, and modalities, including lensless setups.\nNotably, a diffusion prior trained on simple amplitude data (e.g., polystyrene\nbeads) successfully reconstructs complex biological tissue structures,\nhighlighting the method's adaptability. This framework provides a\ncost-effective, generalizable solution for nonlinear inverse problems in\ncomputational imaging, and establishes a foundation for broader coherent\nimaging applications beyond holography.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12728v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12728v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.347,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion model for holographic image reconstruction, specifically for phase retrieval in coherent imaging, involving iterative refinement for amplitude and phase recovery. However, this application focuses on physical inverse problems in computational imaging, not on adapting diffusion for multi-step logical reasoning tasks, such as holistically correcting a Chain-of-Thought for complex logical processes. As the topic requires a clear component for logical reasoning, which is absent here, the paper does not align with Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12730",
      "title": "A Graph Machine Learning Approach for Detecting Topological Patterns in\n  Transactional Graphs",
      "authors": [
        "Francesco Zola",
        "Jon Ander Medina",
        "Andrea Venturi",
        "Amaia Gil",
        "Raul Orduna"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)"
      ],
      "abstract": "The rise of digital ecosystems has exposed the financial sector to evolving\nabuse and criminal tactics that share operational knowledge and techniques both\nwithin and across different environments (fiat-based, crypto-assets, etc.).\nTraditional rule-based systems lack the adaptability needed to detect\nsophisticated or coordinated criminal behaviors (patterns), highlighting the\nneed for strategies that analyze actors' interactions to uncover suspicious\nactivities and extract their modus operandi. For this reason, in this work, we\npropose an approach that integrates graph machine learning and network analysis\nto improve the detection of well-known topological patterns within\ntransactional graphs. However, a key challenge lies in the limitations of\ntraditional financial datasets, which often provide sparse, unlabeled\ninformation that is difficult to use for graph-based pattern analysis.\nTherefore, we firstly propose a four-step preprocessing framework that involves\n(i) extracting graph structures, (ii) considering data temporality to manage\nlarge node sets, (iii) detecting communities within, and (iv) applying\nautomatic labeling strategies to generate weak ground-truth labels. Then, once\nthe data is processed, Graph Autoencoders are implemented to distinguish among\nthe well-known topological patterns. Specifically, three different GAE variants\nare implemented and compared in this analysis. Preliminary results show that\nthis pattern-focused, topology-driven method is effective for detecting complex\nfinancial crime schemes, offering a promising alternative to conventional\nrule-based detection systems.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12730v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12730v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.326,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12739",
      "title": "Deep Learning for Model-Free Prediction of Thermal States of Robot Joint\n  Motors",
      "authors": [
        "Trung Kien La",
        "Eric Guiffo Kaigom"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)",
        "cs.LG (Machine Learning)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "In this work, deep neural networks made up of multiple hidden Long Short-Term\nMemory (LSTM) and Feedforward layers are trained to predict the thermal\nbehavior of the joint motors of robot manipulators. A model-free and scalable\napproach is adopted. It accommodates complexity and uncertainty challenges\nstemming from the derivation, identification, and validation of a large number\nof parameters of an approximation model that is hardly available. To this end,\nsensed joint torques are collected and processed to foresee the thermal\nbehavior of joint motors. Promising prediction results of the machine learning\nbased capture of the temperature dynamics of joint motors of a redundant robot\nwith seven joints are presented.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12739v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12739v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.383,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12740",
      "title": "Deep Generative and Discriminative Digital Twin endowed with Variational\n  Autoencoder for Unsupervised Predictive Thermal Condition Monitoring of\n  Physical Robots in Industry 6.0 and Society 6.0",
      "authors": [
        "Eric Guiffo Kaigom"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)",
        "cs.LG (Machine Learning)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Robots are unrelentingly used to achieve operational efficiency in Industry\n4.0 along with symbiotic and sustainable assistance for the work-force in\nIndustry 5.0. As resilience, robustness, and well-being are required in\nanti-fragile manufacturing and human-centric societal tasks, an autonomous\nanticipation and adaption to thermal saturation and burns due to motors\noverheating become instrumental for human safety and robot availability. Robots\nare thereby expected to self-sustain their performance and deliver user\nexperience, in addition to communicating their capability to other agents in\nadvance to ensure fully automated thermally feasible tasks, and prolong their\nlifetime without human intervention. However, the traditional robot shutdown,\nwhen facing an imminent thermal saturation, inhibits productivity in factories\nand comfort in the society, while cooling strategies are hard to implement\nafter the robot acquisition. In this work, smart digital twins endowed with\ngenerative AI, i.e., variational autoencoders, are leveraged to manage\nthermally anomalous and generate uncritical robot states. The notion of thermal\ndifficulty is derived from the reconstruction error of variational\nautoencoders. A robot can use this score to predict, anticipate, and share the\nthermal feasibility of desired motion profiles to meet requirements from\nemerging applications in Industry 6.0 and Society 6.0.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12740v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12740v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.395,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12741",
      "title": "Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm\n  Motions",
      "authors": [
        "Alexis Yihong Hao",
        "Yufei Wang",
        "Navin Sriram Ravie",
        "Bharath Hegde",
        "David Held",
        "Zackory Erickson"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Robot-assisted dressing has the potential to significantly improve the lives\nof individuals with mobility impairments. To ensure an effective and\ncomfortable dressing experience, the robot must be able to handle challenging\ndeformable garments, apply appropriate forces, and adapt to limb movements\nthroughout the dressing process. Prior work often makes simplifying assumptions\n-- such as static human limbs during dressing -- which limits real-world\napplicability. In this work, we develop a robot-assisted dressing system\ncapable of handling partial observations with visual occlusions, as well as\nrobustly adapting to arm motions during the dressing process. Given a policy\ntrained in simulation with partial observations, we propose a method to\nfine-tune it in the real world using a small amount of data and multi-modal\nfeedback from vision and force sensing, to further improve the policy's\nadaptability to arm motions and enhance safety. We evaluate our method in\nsimulation with simplified articulated human meshes and in a real world human\nstudy with 12 participants across 264 dressing trials. Our policy successfully\ndresses two long-sleeve everyday garments onto the participants while being\nadaptive to various kinds of arm motions, and greatly outperforms prior\nbaselines in terms of task completion and user feedback. Video are available at\nhttps://dressing-motion.github.io/.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12741v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12741v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.328,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on training and fine-tuning a robot policy for assisted dressing using simulation data and multi-modal feedback from vision and force sensors. It does not involve training a separate reward model based on human-ranked data or preferences, nor does it use human feedback in the reinforcement learning process. Instead, human feedback is mentioned only in the context of evaluation through user studies, which does not align with the definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12742",
      "title": "Effective Gaussian Management for High-fidelity Object Reconstruction",
      "authors": [
        "Jiateng Liu",
        "Hao Gao",
        "Jiu-Cheng Xie",
        "Chi-Man Pun",
        "Jian Xiong",
        "Haolun Li",
        "Feng Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper proposes an effective Gaussian management approach for\nhigh-fidelity object reconstruction. Departing from recent Gaussian Splatting\n(GS) methods that employ indiscriminate attribute assignment, our approach\nintroduces a novel densification strategy that dynamically activates spherical\nharmonics (SHs) or normals under the supervision of a surface reconstruction\nmodule, which effectively mitigates the gradient conflicts caused by dual\nsupervision and achieves superior reconstruction results. To further improve\nrepresentation efficiency, we develop a lightweight Gaussian representation\nthat adaptively adjusts the SH orders of each Gaussian based on gradient\nmagnitudes and performs task-decoupled pruning to remove Gaussian with minimal\nimpact on a reconstruction task without sacrificing others, which balances the\nrepresentational capacity with parameter quantity. Notably, our management\napproach is model-agnostic and can be seamlessly integrated into other\nframeworks, enhancing performance while reducing model size. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art approaches in both reconstruction quality and efficiency,\nachieving superior performance with significantly fewer parameters.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12742v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12742v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.338,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12743",
      "title": "Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs",
      "authors": [
        "Hanqing Li",
        "Kiran Sheena Jyothi",
        "Henry Liang",
        "Sharika Mahadevan",
        "Diego Klabjan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We propose a new, training-free method, Graph Reasoning via Retrieval\nAugmented Framework (GRRAF), that harnesses retrieval-augmented generation\n(RAG) alongside the code-generation capabilities of large language models\n(LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target\ngraph is stored in a graph database, and the LLM is prompted to generate\nexecutable code queries that retrieve the necessary information. This approach\ncircumvents the limitations of existing methods that require extensive\nfinetuning or depend on predefined algorithms, and it incorporates an error\nfeedback loop with a time-out mechanism to ensure both correctness and\nefficiency. Experimental evaluations on the GraphInstruct dataset reveal that\nGRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle\ndetection, bipartite graph checks, shortest path computation, and maximum flow,\nwhile maintaining consistent token costs regardless of graph sizes. Imperfect\nbut still very high performance is observed on subgraph matching. Notably,\nGRRAF scales effectively to large graphs with up to 10,000 nodes.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12743v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12743v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.521,
      "distributed_training_score": 0.391,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces GRRAF, a zero-shot method using LLMs for graph reasoning with an error feedback loop, but it does not involve reinforcement learning, human feedback, or training a reward model based on human-ranked data. The feedback mechanism is automated and prompt-based, not aligned with RLHF principles.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's approach relies on LLMs generating code with an error feedback loop for graph tasks, but it does not use diffusion models, iterative refinement of a chain-of-thought as a holistic entity, or any adaptation of diffusion processes for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12746",
      "title": "Modelling and analysis of the 8 filters from the \"master key filters\n  hypothesis\" for depthwise-separable deep networks in relation to idealized\n  receptive fields based on scale-space theory",
      "authors": [
        "Tony Lindeberg",
        "Zahra Babaiee",
        "Peyman M. Kiasari"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper presents the results of analysing and modelling a set of 8\n``master key filters'', which have been extracted by applying a clustering\napproach to the receptive fields learned in depthwise-separable deep networks\nbased on the ConvNeXt architecture.\n  For this purpose, we first compute spatial spread measures in terms of\nweighted mean values and weighted variances of the absolute values of the\nlearned filters, which support the working hypotheses that: (i) the learned\nfilters can be modelled by separable filtering operations over the spatial\ndomain, and that (ii) the spatial offsets of the those learned filters that are\nnon-centered are rather close to half a grid unit. Then, we model the clustered\n``master key filters'' in terms of difference operators applied to a spatial\nsmoothing operation in terms of the discrete analogue of the Gaussian kernel,\nand demonstrate that the resulting idealized models of the receptive fields\nshow good qualitative similarity to the learned filters.\n  This modelling is performed in two different ways: (i) using possibly\ndifferent values of the scale parameters in the coordinate directions for each\nfilter, and (ii) using the same value of the scale parameter in both coordinate\ndirections. Then, we perform the actual model fitting by either (i) requiring\nspatial spread measures in terms of spatial variances of the absolute values of\nthe receptive fields to be equal, or (ii) minimizing the discrete $l_1$- or\n$l_2$-norms between the idealized receptive field models and the learned\nfilters.\n  Complementary experimental results then demonstrate the idealized models of\nreceptive fields have good predictive properties for replacing the learned\nfilters by idealized filters in depthwise-separable deep networks, thus showing\nthat the learned filters in depthwise-separable deep networks can be well\napproximated by discrete scale-space filters.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12746v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12746v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.328,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12750",
      "title": "What Makes a Good Generated Image? Investigating Human and Multimodal\n  LLM Image Preference Alignment",
      "authors": [
        "Rishab Parthasarathy",
        "Jasmine Collins",
        "Cory Stephenson"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Automated evaluation of generative text-to-image models remains a challenging\nproblem. Recent works have proposed using multimodal LLMs to judge the quality\nof images, but these works offer little insight into how multimodal LLMs make\nuse of concepts relevant to humans, such as image style or composition, to\ngenerate their overall assessment. In this work, we study what attributes of an\nimage--specifically aesthetics, lack of artifacts, anatomical accuracy,\ncompositional correctness, object adherence, and style--are important for both\nLLMs and humans to make judgments on image quality. We first curate a dataset\nof human preferences using synthetically generated image pairs. We use\ninter-task correlation between each pair of image quality attributes to\nunderstand which attributes are related in making human judgments. Repeating\nthe same analysis with LLMs, we find that the relationships between image\nquality attributes are much weaker. Finally, we study individual image quality\nattributes by generating synthetic datasets with a high degree of control for\neach axis. Humans are able to easily judge the quality of an image with respect\nto all of the specific image quality attributes (e.g. high vs. low aesthetic\nimage), however we find that some attributes, such as anatomical accuracy, are\nmuch more difficult for multimodal LLMs to learn to judge. Taken together,\nthese findings reveal interesting differences between how humans and multimodal\nLLMs perceive images.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12750v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12750v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.484,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.537,
      "distributed_training_score": 0.328,
      "datasets_score": 0.404,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper involves human feedback for evaluating image preferences, which is a concept related to RLHF, but it does not involve training a reward model or using reinforcement learning to fine-tune AI models. Instead, it focuses on comparing human and LLM judgments without implementing RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses diffusion models for image generation (e.g., Stable Diffusion) and their evaluation, but it does not adapt diffusion processes for multi-step logical reasoning or treat reasoning as an iterative entity. The focus is on image quality assessment, not reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution centers on creating, curating, and analyzing datasets for human and LLM image preference evaluations, including synthetic datasets for specific attributes. This directly aligns with research on dataset introduction, curation methodologies, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper examines the alignment between human and multimodal LLM preferences for evaluating generated images by analyzing attributes such as aesthetics, artifacts, anatomical accuracy, compositional correctness, object adherence, and style. Using a curated dataset of image pairs rated by humans and LLMs, the authors investigate inter-attribute correlations and create synthetic datasets to test specific attributes, finding that while LLMs align with humans on overall judgments, they exhibit weaker relationships between attributes and struggle with tasks like anatomical accuracy, revealing fundamental differences in image perception.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining human ratings with LLM evaluations to analyze attribute correlations in image quality assessment, offering a new perspective on automated evaluation methods. However, it builds on existing ideas of using LLMs for judging, rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in computer vision and AI alignment by highlighting discrepancies in how LLMs process image attributes, potentially leading to better evaluation metrics and model training. Nonetheless, its applicability is primarily within specific subfields like generative models and not broadly across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers significant insights into the limitations of multimodal LLMs in image evaluation, making it valuable for researchers in AI and computer vision to understand and address alignment issues. It is a strong contribution but not essential for those outside these areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3a0070db027526ebd3c961d20a1072c0c14f2c33",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 3,
      "average_h_index": 2.6666666666666665,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Rishab Parthasarathy",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2039903608"
        },
        {
          "name": "Jasmine Collins",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261497195"
        },
        {
          "name": "Cory Stephenson",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261492884"
        }
      ]
    },
    {
      "id": "2509.12754",
      "title": "Toward Ownership Understanding of Objects: Active Question Generation\n  with Large Language Model and Probabilistic Generative Model",
      "authors": [
        "Saki Hashimoto",
        "Shoichi Hasegawa",
        "Tomochika Ishikawa",
        "Akira Taniguchi",
        "Yoshinobu Hagiwara",
        "Lotfi El Hafi",
        "Tadahiro Taniguchi"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Robots operating in domestic and office environments must understand object\nownership to correctly execute instructions such as ``Bring me my cup.''\nHowever, ownership cannot be reliably inferred from visual features alone. To\naddress this gap, we propose Active Ownership Learning (ActOwL), a framework\nthat enables robots to actively generate and ask ownership-related questions to\nusers. ActOwL employs a probabilistic generative model to select questions that\nmaximize information gain, thereby acquiring ownership knowledge efficiently to\nimprove learning efficiency. Additionally, by leveraging commonsense knowledge\nfrom Large Language Models (LLM), objects are pre-classified as either shared\nor owned, and only owned objects are targeted for questioning. Through\nexperiments in a simulated home environment and a real-world laboratory\nsetting, ActOwL achieved significantly higher ownership clustering accuracy\nwith fewer questions than baseline methods. These findings demonstrate the\neffectiveness of combining active inference with LLM-guided commonsense\nreasoning, advancing the capability of robots to acquire ownership knowledge\nfor practical and socially appropriate task execution.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12754v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12754v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.318,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on active learning with a probabilistic generative model to select questions and update ownership knowledge based on user answers, but it does not involve training a reward model or using reinforcement learning to fine-tune an AI based on human preferences. There is no evidence of RLHF elements like policy optimization with rewards derived from human feedback.",
      "weak_supervision_justification": "The paper uses LLMs to programmatically generate initial labels for classifying objects as shared or owned, which can be seen as a form of weak supervision since it relies on noisy or high-level sources rather than precise hand-labeled data. However, the core method involves active questioning and direct user feedback for refinement, which goes beyond typical weak supervision paradigms.",
      "diffusion_reasoning_justification": "The paper employs a probabilistic generative model for question selection based on information gain and uses LLMs for commonsense reasoning, but there is no mention of diffusion models, iterative refinement of a reasoning chain, or multi-step logical processes characteristic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Active Ownership Learning (ActOwL), a framework designed to help robots understand object ownership in daily environments by efficiently generating and asking targeted questions to users. By integrating Large Language Models for commonsense-based pre-classification of objects as shared or owned and a probabilistic generative model to select questions that maximize information gain, ActOwL enables robots to acquire accurate ownership knowledge with fewer interactions; experiments in simulated and real-world settings demonstrate superior clustering accuracy and efficiency compared to baseline methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of active learning, Large Language Models, and probabilistic generative models to address object ownership in robotics, offering a notable improvement over existing methods by efficiently integrating these techniques for a specific problem. However, it builds on established concepts like active inference rather than introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in robotics and human-robot interaction by providing a practical method for robots to learn social contexts, potentially leading to more effective applications in domestic settings. Nonetheless, its impact may be confined to specific subfields like AI and robotics, limiting broader adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, innovative contribution to robot learning and interaction, making it valuable for researchers in AI and robotics to understand advancements in active questioning techniques. While not essential for all, it offers practical insights that could inspire further developments in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1fd7977efcd546779dc4a404fe087168cbb32caf",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 29,
      "average_h_index": 9.285714285714286,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Saki Hashimoto",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372718080"
        },
        {
          "name": "Shoichi Hasegawa",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2209284564"
        },
        {
          "name": "Tomochika Ishikawa",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2191613501"
        },
        {
          "name": "Akira Taniguchi",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/2049028872"
        },
        {
          "name": "Y. Hagiwara",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/2621346"
        },
        {
          "name": "Lotfi El Hafi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373463985"
        },
        {
          "name": "T. Taniguchi",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/1684099"
        }
      ]
    },
    {
      "id": "2509.12757",
      "title": "Recurrent Cross-View Object Geo-Localization",
      "authors": [
        "Xiaohan Zhang",
        "Si-Yuan Cao",
        "Xiaokai Bai",
        "Yiming Li",
        "Zhangkai Shen",
        "Zhe Wu",
        "Xiaoxi Hu",
        "Hui-liang Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cross-view object geo-localization (CVOGL) aims to determine the location of\na specific object in high-resolution satellite imagery given a query image with\na point prompt. Existing approaches treat CVOGL as a one-shot detection task,\ndirectly regressing object locations from cross-view information aggregation,\nbut they are vulnerable to feature noise and lack mechanisms for error\ncorrection. In this paper, we propose ReCOT, a Recurrent Cross-view Object\ngeo-localization Transformer, which reformulates CVOGL as a recurrent\nlocalization task. ReCOT introduces a set of learnable tokens that encode\ntask-specific intent from the query image and prompt embeddings, and\niteratively attend to the reference features to refine the predicted location.\nTo enhance this recurrent process, we incorporate two complementary modules:\n(1) a SAM-based knowledge distillation strategy that transfers segmentation\npriors from the Segment Anything Model (SAM) to provide clearer semantic\nguidance without additional inference cost, and (2) a Reference Feature\nEnhancement Module (RFEM) that introduces a hierarchical attention to emphasize\nobject-relevant regions in the reference features. Extensive experiments on\nstandard CVOGL benchmarks demonstrate that ReCOT achieves state-of-the-art\n(SOTA) performance while reducing parameters by 60% compared to previous SOTA\napproaches.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12757v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12757v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.32,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12759",
      "title": "A-TDOM: Active TDOM via On-the-Fly 3DGS",
      "authors": [
        "Yiwei Xu",
        "Xiang Wang",
        "Yifei Yu",
        "Wentian Gan",
        "Luca Morelli",
        "Giulio Perda",
        "Xiongwu Xiao",
        "Zongqian Zhan",
        "Xin Wang",
        "Fabio Remondino"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product in\nvarious fields such as urban management, city planning, land surveying, etc.\nHowever, traditional TDOM generation methods generally rely on a complex\noffline photogrammetric pipeline, resulting in delays that hinder real-time\napplications. Moreover, the quality of TDOM may degrade due to various\nchallenges, such as inaccurate camera poses or Digital Surface Model (DSM) and\nscene occlusions. To address these challenges, this work introduces A-TDOM, a\nnear real-time TDOM generation method based on On-the-Fly 3DGS optimization. As\neach image is acquired, its pose and sparse point cloud are computed via\nOn-the-Fly SfM. Then new Gaussians are integrated and optimized into previously\nunseen or coarsely reconstructed regions. By integrating with orthogonal\nsplatting, A-TDOM can render just after each update of a new 3DGS field.\nInitial experiments on multiple benchmarks show that the proposed A-TDOM is\ncapable of actively rendering TDOM in near real-time, with 3DGS optimization\nfor each new image in seconds while maintaining acceptable rendering quality\nand TDOM geometric accuracy.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12759v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12759v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.276,
      "weak_supervision_score": 0.28,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.337,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12763",
      "title": "DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for\n  Medical Image Segmentation",
      "authors": [
        "Yican Zhao",
        "Ce Wang",
        "You Hao",
        "Lei Li",
        "Tianli Liao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical image segmentation grapples with challenges including multi-scale\nlesion variability, ill-defined tissue boundaries, and computationally\nintensive processing demands. This paper proposes the DyGLNet, which achieves\nefficient and accurate segmentation by fusing global and local features with a\ndynamic upsampling mechanism. The model innovatively designs a hybrid feature\nextraction module (SHDCBlock), combining single-head self-attention and\nmulti-scale dilated convolutions to model local details and global context\ncollaboratively. We further introduce a dynamic adaptive upsampling module\n(DyFusionUp) to realize high-fidelity reconstruction of feature maps based on\nlearnable offsets. Then, a lightweight design is adopted to reduce\ncomputational overhead. Experiments on seven public datasets demonstrate that\nDyGLNet outperforms existing methods, particularly excelling in boundary\naccuracy and small-object segmentation. Meanwhile, it exhibits lower\ncomputation complexity, enabling an efficient and reliable solution for\nclinical medical image analysis. The code will be made available soon.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12763v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12763v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.38,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a hybrid neural network for medical image segmentation, emphasizing feature fusion, dynamic upsampling, and attention mechanisms to improve accuracy in tasks like lesion boundary detection. It does not involve diffusion models, iterative refinement for logical reasoning, or any multi-step reasoning processes. Therefore, it has no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12765",
      "title": "InfoGain-RAG: Boosting Retrieval-Augmented Generation via Document\n  Information Gain-based Reranking and Filtering",
      "authors": [
        "Zihan Wang",
        "Zihan Liang",
        "Zhou Shao",
        "Yufei Ma",
        "Huangyu Dai",
        "Ben Chen",
        "Lingtao Mao",
        "Chenyi Lei",
        "Yuqing Ding",
        "Han Li"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\naddress key limitations of Large Language Models (LLMs), such as hallucination,\noutdated knowledge, and lacking reference. However, current RAG frameworks\noften struggle with identifying whether retrieved documents meaningfully\ncontribute to answer generation. This shortcoming makes it difficult to filter\nout irrelevant or even misleading content, which notably impacts the final\nperformance. In this paper, we propose Document Information Gain (DIG), a novel\nmetric designed to quantify the contribution of retrieved documents to correct\nanswer generation. DIG measures a document's value by computing the difference\nof LLM's generation confidence with and without the document augmented.\nFurther, we introduce InfoGain-RAG, a framework that leverages DIG scores to\ntrain a specialized reranker, which prioritizes each retrieved document from\nexact distinguishing and accurate sorting perspectives. This approach can\neffectively filter out irrelevant documents and select the most valuable ones\nfor better answer generation. Extensive experiments across various models and\nbenchmarks demonstrate that InfoGain-RAG can significantly outperform existing\napproaches, on both single and multiple retrievers paradigm. Specifically on\nNaturalQA, it achieves the improvements of 17.9%, 4.5%, 12.5% in exact match\naccuracy against naive RAG, self-reflective RAG and modern ranking-based RAG\nrespectively, and even an average of 15.3% increment on advanced proprietary\nmodel GPT-4o across all datasets. These results demonstrate the feasibility of\nInfoGain-RAG as it can offer a reliable solution for RAG in multiple\napplications.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12765v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12765v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.338,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving Retrieval-Augmented Generation (RAG) through a metric called Document Information Gain (DIG) and a reranker, without involving human feedback, reward models, or reinforcement learning techniques. There is no mention of training on human-ranked data or aligning models with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a framework for document reranking in RAG based on information gain, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. There is no component for holistically correcting reasoning paths over steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12768",
      "title": "BATR-FST: Bi-Level Adaptive Token Refinement for Few-Shot Transformers",
      "authors": [
        "Mohammed Al-Habib",
        "Zuping Zhang",
        "Abdulrahman Noman"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Vision Transformers (ViTs) have shown significant promise in computer vision\napplications. However, their performance in few-shot learning is limited by\nchallenges in refining token-level interactions, struggling with limited\ntraining data, and developing a strong inductive bias. Existing methods often\ndepend on inflexible token matching or basic similarity measures, which limit\nthe effective incorporation of global context and localized feature refinement.\nTo address these challenges, we propose Bi-Level Adaptive Token Refinement for\nFew-Shot Transformers (BATR-FST), a two-stage approach that progressively\nimproves token representations and maintains a robust inductive bias for\nfew-shot classification. During the pre-training phase, Masked Image Modeling\n(MIM) provides Vision Transformers (ViTs) with transferable patch-level\nrepresentations by recreating masked image regions, providing a robust basis\nfor subsequent adaptation. In the meta-fine-tuning phase, BATR-FST incorporates\na Bi-Level Adaptive Token Refinement module that utilizes Token Clustering to\ncapture localized interactions, Uncertainty-Aware Token Weighting to prioritize\ndependable features, and a Bi-Level Attention mechanism to balance\nintra-cluster and inter-cluster relationships, thereby facilitating thorough\ntoken refinement. Furthermore, Graph Token Propagation ensures semantic\nconsistency between support and query instances, while a Class Separation\nPenalty preserves different class borders, enhancing discriminative capability.\nExtensive experiments on three benchmark few-shot datasets demonstrate that\nBATR-FST achieves superior results in both 1-shot and 5-shot scenarios and\nimproves the few-shot classification via transformers.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12768v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12768v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.376,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12772",
      "title": "MEGAN: Mixture of Experts for Robust Uncertainty Estimation in Endoscopy\n  Videos",
      "authors": [
        "Damola Agbelese",
        "Krishna Chaitanya",
        "Pushpak Pati",
        "Chaitanya Parmar",
        "Pooya Mobadersany",
        "Shreyas Fadnavis",
        "Lindsey Surace",
        "Shadi Yarandi",
        "Louis R. Ghanem",
        "Molly Lucas",
        "Tommaso Mansi",
        "Oana Gabriela Cula",
        "Pablo F. Damasceno",
        "Kristopher Standish"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reliable uncertainty quantification (UQ) is essential in medical AI.\nEvidential Deep Learning (EDL) offers a computationally efficient way to\nquantify model uncertainty alongside predictions, unlike traditional methods\nsuch as Monte Carlo (MC) Dropout and Deep Ensembles (DE). However, all these\nmethods often rely on a single expert's annotations as ground truth for model\ntraining, overlooking the inter-rater variability in healthcare. To address\nthis issue, we propose MEGAN, a Multi-Expert Gating Network that aggregates\nuncertainty estimates and predictions from multiple AI experts via EDL models\ntrained with diverse ground truths and modeling strategies. MEGAN's gating\nnetwork optimally combines predictions and uncertainties from each EDL model,\nenhancing overall prediction confidence and calibration. We extensively\nbenchmark MEGAN on endoscopy videos for Ulcerative colitis (UC) disease\nseverity estimation, assessed by visual labeling of Mayo Endoscopic Subscore\n(MES), where inter-rater variability is prevalent. In large-scale prospective\nUC clinical trial, MEGAN achieved a 3.5% improvement in F1-score and a 30.5%\nreduction in Expected Calibration Error (ECE) compared to existing methods.\nFurthermore, MEGAN facilitated uncertainty-guided sample stratification,\nreducing the annotation burden and potentially increasing efficiency and\nconsistency in UC trials.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12772v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12772v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.31,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12774",
      "title": "EmbeddedML: A New Optimized and Fast Machine Learning Library",
      "authors": [
        "Halil Hüseyin Çalışkan",
        "Talha Koruk"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Machine learning models and libraries can train datasets of different sizes\nand perform prediction and classification operations, but machine learning\nmodels and libraries cause slow and long training times on large datasets. This\narticle introduces EmbeddedML, a training-time-optimized and mathematically\nenhanced machine learning library. The speed was increased by approximately\ntimes compared to scikit-learn without any loss in terms of accuracy in\nregression models such as Multiple Linear Regression. Logistic Regression and\nSupport Vector Machines (SVM) algorithms have been mathematically rewritten to\nreduce training time and increase accuracy in classification models. With the\napplied mathematical improvements, training time has been reduced by\napproximately 2 times for SVM on small datasets and by around 800 times on\nlarge datasets, and by approximately 4 times for Logistic Regression, compared\nto the scikit-learn implementation. In summary, the EmbeddedML library offers\nregression, classification, clustering, and dimensionality reduction algorithms\nthat are mathematically rewritten and optimized to reduce training time.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12774v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12774v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.267,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.269,
      "distributed_training_score": 0.383,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12777",
      "title": "CECT-Mamba: a Hierarchical Contrast-enhanced-aware Model for Pancreatic\n  Tumor Subtyping from Multi-phase CECT",
      "authors": [
        "Zhifang Gong",
        "Shuo Gao",
        "Ben Zhao",
        "Yingjing Xu",
        "Yijun Yang",
        "Shenghong Ju",
        "Guangquan Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Contrast-enhanced computed tomography (CECT) is the primary imaging technique\nthat provides valuable spatial-temporal information about lesions, enabling the\naccurate diagnosis and subclassification of pancreatic tumors. However, the\nhigh heterogeneity and variability of pancreatic tumors still pose substantial\nchallenges for precise subtyping diagnosis. Previous methods fail to\neffectively explore the contextual information across multiple CECT phases\ncommonly used in radiologists' diagnostic workflows, thereby limiting their\nperformance. In this paper, we introduce, for the first time, an automatic way\nto combine the multi-phase CECT data to discriminate between pancreatic tumor\nsubtypes, among which the key is using Mamba with promising learnability and\nsimplicity to encourage both temporal and spatial modeling from multi-phase\nCECT. Specifically, we propose a dual hierarchical contrast-enhanced-aware\nMamba module incorporating two novel spatial and temporal sampling sequences to\nexplore intra and inter-phase contrast variations of lesions. A\nsimilarity-guided refinement module is also imposed into the temporal scanning\nmodeling to emphasize the learning on local tumor regions with more obvious\ntemporal variations. Moreover, we design the space complementary integrator and\nmulti-granularity fusion module to encode and aggregate the semantics across\ndifferent scales, achieving more efficient learning for subtyping pancreatic\ntumors. The experimental results on an in-house dataset of 270 clinical cases\nachieve an accuracy of 97.4% and an AUC of 98.6% in distinguishing between\npancreatic ductal adenocarcinoma (PDAC) and pancreatic neuroendocrine tumors\n(PNETs), demonstrating its potential as a more accurate and efficient tool.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12777v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12777v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.264,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.325,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12784",
      "title": "Contextualized Representation Learning for Effective Human-Object\n  Interaction Detection",
      "authors": [
        "Zhehao Li",
        "Yucheng Qian",
        "Chong Wang",
        "Yinghao Lu",
        "Zhihao Yang",
        "Jiafei Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Human-Object Interaction (HOI) detection aims to simultaneously localize\nhuman-object pairs and recognize their interactions. While recent two-stage\napproaches have made significant progress, they still face challenges due to\nincomplete context modeling. In this work, we introduce a Contextualized\nRepresentation Learning that integrates both affordance-guided reasoning and\ncontextual prompts with visual cues to better capture complex interactions. We\nenhance the conventional HOI detection framework by expanding it beyond simple\nhuman-object pairs to include multivariate relationships involving auxiliary\nentities like tools. Specifically, we explicitly model the functional role\n(affordance) of these auxiliary objects through triplet structures <human,\ntool, object>. This enables our model to identify tool-dependent interactions\nsuch as 'filling'. Furthermore, the learnable prompt is enriched with instance\ncategories and subsequently integrated with contextual visual features using an\nattention mechanism. This process aligns language with image content at both\nglobal and regional levels. These contextualized representations equip the\nmodel with enriched relational cues for more reliable reasoning over complex,\ncontext-dependent interactions. Our proposed method demonstrates superior\nperformance on both the HICO-Det and V-COCO datasets in most scenarios. The\nsource code is available at https://github.com/lzzhhh1019/CRL.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12784v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12784v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.464,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.304,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on contextualized representation learning for Human-Object Interaction (HOI) detection in computer vision, emphasizing affordance-guided reasoning, contextual prompts, and visual features to improve interaction detection. It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI models with human preferences. Therefore, there is no connection to Reinforcement Learning from Human Feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12787",
      "title": "Double Helix Diffusion for Cross-Domain Anomaly Image Generation",
      "authors": [
        "Linchun Wu",
        "Qin Zou",
        "Xianbiao Qi",
        "Bo Du",
        "Zhongyuan Wang",
        "Qingquan Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual anomaly inspection is critical in manufacturing, yet hampered by the\nscarcity of real anomaly samples for training robust detectors. Synthetic data\ngeneration presents a viable strategy for data augmentation; however, current\nmethods remain constrained by two principal limitations: 1) the generation of\nanomalies that are structurally inconsistent with the normal background, and 2)\nthe presence of undesirable feature entanglement between synthesized images and\ntheir corresponding annotation masks, which undermines the perceptual realism\nof the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel\ncross-domain generative framework designed to simultaneously synthesize\nhigh-fidelity anomaly images and their pixel-level annotation masks, explicitly\naddressing these challenges. DH-Diff employs a unique architecture inspired by\na double helix, cycling through distinct modules for feature separation,\nconnection, and merging. Specifically, a domain-decoupled attention mechanism\nmitigates feature entanglement by enhancing image and annotation features\nindependently, and meanwhile a semantic score map alignment module ensures\nstructural authenticity by coherently integrating anomaly foregrounds. DH-Diff\noffers flexible control via text prompts and optional graphical guidance.\nExtensive experiments demonstrate that DH-Diff significantly outperforms\nstate-of-the-art methods in diversity and authenticity, leading to significant\nimprovements in downstream anomaly detection performance.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12787v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12787v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.495,
      "distributed_training_score": 0.336,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Double Helix Diffusion (DH-Diff) for generating anomaly images and masks, utilizing diffusion models for iterative refinement in image synthesis. However, it does not adapt diffusion processes for solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for multi-step reasoning. The focus is on visual generation and data augmentation for anomaly detection, lacking any clear component for logical reasoning or holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12791",
      "title": "Superpixel Anything: A general object-based framework for accurate yet\n  regular superpixel segmentation",
      "authors": [
        "Julien Walther",
        "Rémi Giraud",
        "Michaël Clément"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Superpixels are widely used in computer vision to simplify image\nrepresentation and reduce computational complexity. While traditional methods\nrely on low-level features, deep learning-based approaches leverage high-level\nfeatures but also tend to sacrifice regularity of superpixels to capture\ncomplex objects, leading to accurate but less interpretable segmentations. In\nthis work, we introduce SPAM (SuperPixel Anything Model), a versatile framework\nfor segmenting images into accurate yet regular superpixels. We train a model\nto extract image features for superpixel generation, and at inference, we\nleverage a large-scale pretrained model for semantic-agnostic segmentation to\nensure that superpixels align with object masks. SPAM can handle any prior\nhigh-level segmentation, resolving uncertainty regions, and is able to\ninteractively focus on specific objects. Comprehensive experiments demonstrate\nthat SPAM qualitatively and quantitatively outperforms state-of-the-art methods\non segmentation tasks, making it a valuable and robust tool for various\napplications. Code and pre-trained models are available here:\nhttps://github.com/waldo-j/spam.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12791v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12791v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.331,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12798",
      "title": "LLM-Based Approach for Enhancing Maintainability of Automotive\n  Architectures",
      "authors": [
        "Nenad Petrovic",
        "Lukasz Mazur",
        "Alois Knoll"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "There are many bottlenecks that decrease the flexibility of automotive\nsystems, making their long-term maintenance, as well as updates and extensions\nin later lifecycle phases increasingly difficult, mainly due to long\nre-engineering, standardization, and compliance procedures, as well as\nheterogeneity and numerosity of devices and underlying software components\ninvolved. In this paper, we explore the potential of Large Language Models\n(LLMs) when it comes to the automation of tasks and processes that aim to\nincrease the flexibility of automotive systems. Three case studies towards\nachieving this goal are considered as outcomes of early-stage research: 1)\nupdates, hardware abstraction, and compliance, 2) interface compatibility\nchecking, and 3) architecture modification suggestions. For proof-of-concept\nimplementation, we rely on OpenAI's GPT-4o model.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12798v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12798v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.345,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained LLMs like GPT-4o for automating automotive maintenance tasks, such as hardware abstraction and interface checking, without any mention of training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes LLMs for text-based analysis and generation in automotive contexts, but it does not involve iterative refinement processes, chain-of-thought correction, or any diffusion-based models for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12810",
      "title": "H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents",
      "authors": [
        "Shicheng Ye",
        "Chao Yu",
        "Kaiqiang Ke",
        "Chengdong Xu",
        "Yinqi Wei"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language model (LLM)-based agents have shown strong potential in\nmulti-task scenarios, owing to their ability to transfer knowledge across\ndiverse tasks. However, existing approaches often treat prior experiences and\nknowledge as monolithic units, leading to inefficient and coarse-grained\nknowledge transfer. In this work, we propose a novel hierarchical memory\narchitecture that enables fine-grained knowledge transfer by decoupling\nhigh-level planning memory from low-level execution memory. To construct and\nrefine these hierarchical memories, we introduce Hierarchical Hindsight\nReflection (H$^2$R), a mechanism that distills reusable and hierarchical\nknowledge from past agent-environment interactions. At test time, H$^2$R\nperforms retrievals of high-level and low-level memories separately, allowing\nLLM-based agents to efficiently access and utilize task-relevant knowledge for\nnew tasks.Experimental results across two benchmarks demonstrate that H$^2$R\ncan improve generalization and decision-making performance, outperforming prior\nbaselines such as Expel.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12810v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12810v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.487,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.409,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on hierarchical memory and hindsight reflection for LLM agents in multi-task scenarios, using agent-environment interactions for knowledge transfer. It does not involve human feedback, a reward model trained on human-ranked data, or any reinforcement learning process aligned with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a hierarchical reflection mechanism for knowledge distillation in LLM agents, but it does not use diffusion models, iterative refinement processes, or treat reasoning as a holistic chain-of-thought for correction over steps.",
      "distributed_training_justification": "The paper addresses memory architecture and knowledge transfer for LLM agents, with no discussion of distributed training techniques, parallel computing, data partitioning, or multi-node systems for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12815",
      "title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset\n  Generation",
      "authors": [
        "Biwen Lei",
        "Yang Li",
        "Xinhai Liu",
        "Shuhui Yang",
        "Lixin Xu",
        "Jingwei Huang",
        "Ruining Tang",
        "Haohan Weng",
        "Jian Liu",
        "Jing Xu",
        "Zhen Zhou",
        "Yiling Zhu",
        "Jiankai Xing",
        "Jiachen Xu",
        "Changfeng Ma",
        "Xinhao Yan",
        "Yunhan Yang",
        "Chunshi Wang",
        "Duoteng Xu",
        "Xueqi Ma",
        "Yuguang Chen",
        "Jing Li",
        "Mingxin Yang",
        "Sheng Zhang",
        "Yifei Feng",
        "Xin Huang",
        "Di Luo",
        "Zebin He",
        "Puhua Jiang",
        "Changrong Hu",
        "Zihan Qin",
        "Shiwei Miao",
        "Haolin Liu",
        "Yunfei Zhao",
        "Zeqiang Lai",
        "Qingxiang Lin",
        "Zibo Zhao",
        "Kunhong Li",
        "Xianghui Yang",
        "Huiwen Shi",
        "Xin Yang",
        "Yuxuan Wang",
        "Zebin Yao",
        "Yihang Lian",
        "Sicong Liu",
        "Xintong Han",
        "Wangchen Qin",
        "Caisheng Ouyang",
        "Jianyin Liu",
        "Tianwen Yuan",
        "Shuai Jiang",
        "Hong Duan",
        "Yanqi Niu",
        "Wencong Lin",
        "Yifu Sun",
        "Shirui Huang",
        "Lin Niu",
        "Gu Gong",
        "Guojian Xiao",
        "Bojian Zheng",
        "Xiang Yuan",
        "Qi Chen",
        "Jie Xiao",
        "Dongyang Zheng",
        "Xiaofeng Yang",
        "Kai Liu",
        "Jianchen Zhu",
        "Lifu Wang",
        "Qinglin Lu",
        "Jie Liu",
        "Liang Dong",
        "Fan Jiang",
        "Ruibin Chen",
        "Lei Wang",
        "Chao Zhang",
        "Jiaxin Lin",
        "Hao Zhang",
        "Zheng Ye",
        "Peng He",
        "Runzhou Wu",
        "Yinhe Wu",
        "Jiayao Du",
        "Jupeng Chen",
        "Xinyue Mao",
        "Dongyuan Guo",
        "Yixuan Tang",
        "Yulin Tsai",
        "Yonghao Tan",
        "Jiaao Yu",
        "Junlin Yu",
        "Keren Zhang",
        "Yifan Li",
        "Peng Chen",
        "Tian Liu",
        "Di Wang",
        "Yuhong Liu",
        "Linus",
        "Jie Jiang",
        "Zhuo Chen",
        "Chunchao Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The creation of high-quality 3D assets, a cornerstone of modern game\ndevelopment, has long been characterized by labor-intensive and specialized\nworkflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered\ncontent creation platform designed to revolutionize the game production\npipeline by automating and streamlining the generation of game-ready 3D assets.\nAt its core, Hunyuan3D Studio integrates a suite of advanced neural modules\n(such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into\na cohesive and user-friendly system. This unified framework allows for the\nrapid transformation of a single concept image or textual description into a\nfully-realized, production-quality 3D model complete with optimized geometry\nand high-fidelity PBR textures. We demonstrate that assets generated by\nHunyuan3D Studio are not only visually compelling but also adhere to the\nstringent technical requirements of contemporary game engines, significantly\nreducing iteration time and lowering the barrier to entry for 3D content\ncreation. By providing a seamless bridge from creative intent to technical\nasset, Hunyuan3D Studio represents a significant leap forward for AI-assisted\nworkflows in game development and interactive media.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12815v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12815v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.293,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.345,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on an end-to-end AI pipeline for generating 3D assets using generative AI techniques, including diffusion models for tasks like geometry generation from images or text. However, it does not involve adapting diffusion models for multi-step logical reasoning, such as treating a Chain-of-Thought as a single entity for holistic correction in solving complex logical tasks. The core contributions are in visual and technical 3D asset creation, not reasoning processes, so it lacks the required components for this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12816",
      "title": "Gesture Evaluation in Virtual Reality",
      "authors": [
        "Axel Wiebe Werner",
        "Jonas Beskow",
        "Anna Deichler"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Gestures are central to human communication, enriching interactions through\nnon-verbal expression. Virtual avatars increasingly use AI-generated gestures\nto enhance life-likeness, yet evaluations have largely been confined to 2D.\nVirtual Reality (VR) provides an immersive alternative that may affect how\ngestures are perceived. This paper presents a comparative evaluation of\ncomputer-generated gestures in VR and 2D, examining three models from the 2023\nGENEA Challenge. Results show that gestures viewed in VR were rated slightly\nhigher on average, with the strongest effect observed for motion-capture \"true\nmovement.\" While model rankings remained consistent across settings, VR\ninfluenced participants' overall perception and offered unique benefits over\ntraditional 2D evaluation.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12816v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12816v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.268,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.252,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12817",
      "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention",
      "authors": [
        "Yuan Cao",
        "Dong Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12817v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12817v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.363,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12818",
      "title": "Data Scaling Laws for Radiology Foundation Models",
      "authors": [
        "Maximilian Ilse",
        "Harshita Sharma",
        "Anton Schwaighofer",
        "Sam Bond-Taylor",
        "Fernando Pérez-García",
        "Olesya Melnichenko",
        "Anne-Marie G. Sykes",
        "Kelly K. Horst",
        "Ashish Khandelwal",
        "Maxwell Reynolds",
        "Maria T. Wetscherek",
        "Noel C. F. Codella",
        "Javier Alvarez-Valle",
        "Korfiatis Panagiotis",
        "Valentina Salvatelli"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Foundation vision encoders such as CLIP and DINOv2, trained on web-scale\ndata, exhibit strong transfer performance across tasks and datasets. However,\nmedical imaging foundation models remain constrained by smaller datasets,\nlimiting our understanding of how data scale and pretraining paradigms affect\nperformance in this setting. In this work, we systematically study continual\npretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINO\nrepresenting the two major encoder paradigms CLIP and DINOv2, on up to 3.5M\nchest x-rays from a single institution, holding compute and evaluation\nprotocols constant. We evaluate on classification (radiology findings, lines\nand tubes), segmentation (lines and tubes), and radiology report generation.\nWhile prior work has primarily focused on tasks related to radiology findings,\nwe include lines and tubes tasks to counterbalance this bias and evaluate a\nmodel's ability to extract features that preserve continuity along elongated\nstructures. Our experiments show that MI2 scales more effectively for\nfinding-related tasks, while RAD-DINO is stronger on tube-related tasks.\nSurprisingly, continually pretraining MI2 with both reports and structured\nlabels using UniCL improves performance, underscoring the value of structured\nsupervision at scale. We further show that for some tasks, as few as 30k\nin-domain samples are sufficient to surpass open-weights foundation models.\nThese results highlight the utility of center-specific continual pretraining,\nenabling medical institutions to derive significant performance gains by\nutilizing in-domain data.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12818v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12818v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.435,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on vision encoders for medical imaging tasks like classification and segmentation, using CLIP-style and DINOv2-style pretraining. It does not involve diffusion models, iterative refinement for logical reasoning, or any multi-step reasoning processes.",
      "distributed_training_justification": "The paper mentions holding compute resources constant for fairness but does not discuss distributed training techniques, parallel computing, or strategies for partitioning data/models across multiple nodes. Its main focus is on data scaling laws and model performance, not training acceleration methods.",
      "datasets_justification": "The paper extensively analyzes and evaluates datasets, including creating a large internal dataset (INST-CXR-BENCH), varying dataset sizes to establish scaling laws, benchmarking models on classification, segmentation, and report generation tasks, and comparing to public benchmarks. This directly aligns with dataset analysis and evaluation in machine learning.",
      "llm_score_status": "completed",
      "summary": "This paper examines the impact of data scaling on radiology foundation models by continually pretraining two vision encoders, MedImageInsight (MI2) based on CLIP and RAD-DINO based on DINOv2, using up to 3.5 million chest X-rays from a single institution, while evaluating performance on classification, segmentation, and report generation tasks. Key findings indicate that MI2 scales more effectively for radiology findings, RAD-DINO performs better on lines and tubes tasks, incorporating structured labels via UniCL significantly improves MI2, and as few as 30,000 in-domain samples can surpass existing open-weight models, emphasizing the value of center-specific pretraining for medical imaging.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by systematically applying and comparing existing pretraining paradigms to medical imaging with a focus on data scaling laws, offering a clever adaptation to address domain-specific challenges. While it builds on established techniques like CLIP and DINOv2, it introduces new insights into their effectiveness in radiology contexts.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in medical AI by demonstrating the benefits of in-domain pretraining and data scaling, potentially guiding institutions to optimize their models. However, its applicability is primarily within the subfield of radiology foundation models, limiting broader reach.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable, practical insights into scaling laws for medical imaging models, making it a significant contribution for researchers in AI and computer vision focused on healthcare. It is high-quality but not essential for those outside this niche area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/19a81070e566eb1c6ab368cf645b898b90fa11ad",
      "total_authors": 15,
      "authors_found": 15,
      "highest_h_index": 12,
      "average_h_index": 4.066666666666666,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Maximilian Ilse",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2275353034"
        },
        {
          "name": "Harshita Sharma",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2261278350"
        },
        {
          "name": "Anton Schwaighofer",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2261277527"
        },
        {
          "name": "Sam Bond-Taylor",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2275352199"
        },
        {
          "name": "Fernando P'erez-Garc'ia",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2305484918"
        },
        {
          "name": "Olesya Melnichenko",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2283775268"
        },
        {
          "name": "Anne-Marie G. Sykes",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380605315"
        },
        {
          "name": "Kelly K. Horst",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380604867"
        },
        {
          "name": "Ashish Khandelwal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380605021"
        },
        {
          "name": "Maxwell Reynolds",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380606407"
        },
        {
          "name": "M. Wetscherek",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2223845249"
        },
        {
          "name": "Noel Codella",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2268318452"
        },
        {
          "name": "Javier Alvarez-Valle",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2029689505"
        },
        {
          "name": "Korfiatis Panagiotis",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2088861600"
        },
        {
          "name": "Valentina Salvatelli",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2275352224"
        }
      ]
    },
    {
      "id": "2509.12822",
      "title": "A Pressure-Based Diffusion Model for Influence Maximization on Social\n  Networks",
      "authors": [
        "Curt Stutsman",
        "Eliot W. Robson",
        "Abhishek K. Umrawal"
      ],
      "categories": [
        "cs.SI (Social and Information Networks)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In many real-world scenarios, an individual's local social network carries\nsignificant influence over the opinions they form and subsequently propagate to\nothers. In this paper, we propose a novel diffusion model -- the Pressure\nThreshold model (PT) -- for dynamically simulating the spread of influence\nthrough a social network. This new model extends the popular Linear Threshold\nModel (LT) by adjusting a node's outgoing influence proportional to the\ninfluence it receives from its activated neighbors. We address the Influence\nMaximization (IM) problem, which involves selecting the most effective seed\nnodes to achieve maximal graph coverage after a diffusion process, and how the\nproblem manifests with the PT Model. Experiments conducted on real-world\nnetworks, facilitated by enhancements to the open-source network-diffusion\nPython library, CyNetDiff, demonstrate unique seed node selection for the PT\nModel when compared to the LT Model. Moreover, analyses demonstrate that\ndensely connected networks amplify pressure effects more significantly than\nsparse networks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12822v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12822v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.329,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a Pressure Threshold diffusion model for influence maximization in social networks, focusing on how influence spreads based on neighbor activations. It does not involve adapting diffusion processes for multi-step logical reasoning, iterative refinement of reasoning paths, or treating a 'Chain-of-Thought' as an entity for correction. The content is centered on social network dynamics and epidemiology, lacking any component related to AI-based logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12831",
      "title": "A Lightweight Pipeline for Noisy Speech Voice Cloning and Accurate Lip\n  Sync Synthesis",
      "authors": [
        "Javeria Amir",
        "Farwa Attaria",
        "Mah Jabeen",
        "Umara Noor",
        "Zahid Rashid"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent developments in voice cloning and talking head generation demonstrate\nimpressive capabilities in synthesizing natural speech and realistic lip\nsynchronization. Current methods typically require and are trained on large\nscale datasets and computationally intensive processes using clean studio\nrecorded inputs that is infeasible in noisy or low resource environments. In\nthis paper, we introduce a new modular pipeline comprising Tortoise text to\nspeech. It is a transformer based latent diffusion model that can perform high\nfidelity zero shot voice cloning given only a few training samples. We use a\nlightweight generative adversarial network architecture for robust real time\nlip synchronization. The solution will contribute to many essential tasks\nconcerning less reliance on massive pre training generation of emotionally\nexpressive speech and lip synchronization in noisy and unconstrained scenarios.\nThe modular structure of the pipeline allows an easy extension for future multi\nmodal and text guided voice modulation and it could be used in real world\nsystems.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12831v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12831v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.365,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12836",
      "title": "Exploring Metric Fusion for Evaluation of NeRFs",
      "authors": [
        "Shreyas Shivakumara",
        "Gabriel Eilertsen",
        "Karljohan Lundin Palmerius"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Neural Radiance Fields (NeRFs) have demonstrated significant potential in\nsynthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however,\nremains a challenge due to the unique artifacts they exhibit, and no individual\nmetric performs well across all datasets. We hypothesize that combining two\nsuccessful metrics, Deep Image Structure and Texture Similarity (DISTS) and\nVideo Multi-Method Assessment Fusion (VMAF), based on different perceptual\nmethods, can overcome the limitations of individual metrics and achieve\nimproved correlation with subjective quality scores. We experiment with two\nnormalization strategies for the individual metrics and two fusion strategies\nto evaluate their impact on the resulting correlation with the subjective\nscores. The proposed pipeline is tested on two distinct datasets, Synthetic and\nOutdoor, and its performance is evaluated across three different\nconfigurations. We present a detailed analysis comparing the correlation\ncoefficients of fusion methods and individual scores with subjective scores to\ndemonstrate the robustness and generalizability of the fusion metrics.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12836v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12836v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.33,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12838",
      "title": "Multi-Robot Task Planning for Multi-Object Retrieval Tasks with\n  Distributed On-Site Knowledge via Large Language Models",
      "authors": [
        "Kento Murata",
        "Shoichi Hasegawa",
        "Tomochika Ishikawa",
        "Yoshinobu Hagiwara",
        "Akira Taniguchi",
        "Lotfi El Hafi",
        "Tadahiro Taniguchi"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "It is crucial to efficiently execute instructions such as \"Find an apple and\na banana\" or \"Get ready for a field trip,\" which require searching for multiple\nobjects or understanding context-dependent commands. This study addresses the\nchallenging problem of determining which robot should be assigned to which part\nof a task when each robot possesses different situational on-site\nknowledge-specifically, spatial concepts learned from the area designated to it\nby the user. We propose a task planning framework that leverages large language\nmodels (LLMs) and spatial concepts to decompose natural language instructions\ninto subtasks and allocate them to multiple robots. We designed a novel\nfew-shot prompting strategy that enables LLMs to infer required objects from\nambiguous commands and decompose them into appropriate subtasks. In our\nexperiments, the proposed method achieved 47/50 successful assignments,\noutperforming random (28/50) and commonsense-based assignment (26/50).\nFurthermore, we conducted qualitative evaluations using two actual mobile\nmanipulators. The results demonstrated that our framework could handle\ninstructions, including those involving ad hoc categories such as \"Get ready\nfor a field trip,\" by successfully performing task decomposition, assignment,\nsequential planning, and execution.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12838v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12838v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.383,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained LLMs for task decomposition and subtask assignment in multi-robot systems, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes LLMs with prompting for logical task decomposition and reasoning, but it does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity over multiple steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12845",
      "title": "Improving Anomalous Sound Detection with Attribute-aware Representation\n  from Domain-adaptive Pre-training",
      "authors": [
        "Xin Fang",
        "Guirui Zhong",
        "Qing Wang",
        "Fan Chu",
        "Lei Wang",
        "Mengui Qian",
        "Mingqi Cai",
        "Jiangzhao Wu",
        "Jianqing Gao",
        "Jun Du"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Anomalous Sound Detection (ASD) is often formulated as a machine attribute\nclassification task, a strategy necessitated by the common scenario where only\nnormal data is available for training. However, the exhaustive collection of\nmachine attribute labels is laborious and impractical. To address the challenge\nof missing attribute labels, this paper proposes an agglomerative hierarchical\nclustering method for the assignment of pseudo-attribute labels using\nrepresentations derived from a domain-adaptive pre-trained model, which are\nexpected to capture machine attribute characteristics. We then apply model\nadaptation to this pre-trained model through supervised fine-tuning for machine\nattribute classification, resulting in a new state-of-the-art performance.\nEvaluation on the Detection and Classification of Acoustic Scenes and Events\n(DCASE) 2025 Challenge dataset demonstrates that our proposed approach yields\nsignificant performance gains, ultimately outperforming our previous\ntop-ranking system in the challenge.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12845v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12845v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.439,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.364,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves generating pseudo-attribute labels using agglomerative hierarchical clustering on representations from a pre-trained model, which aligns directly with weak supervision. This approach programmatically creates labels from noisy or imprecise sources (e.g., clustering outputs) instead of relying on hand-labeled data, enabling model training for anomalous sound detection.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses existing datasets like the DCASE 2025 Challenge dataset for evaluation and mentions machine sound datasets for pre-training, but its primary focus is on the ASD method, not on creating, analyzing, benchmarking, or evaluating datasets. Thus, datasets are only peripherally involved.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of missing attribute labels in Anomalous Sound Detection (ASD) by proposing a method that uses agglomerative hierarchical clustering on representations from a domain-adaptive pre-trained model to generate pseudo-attribute labels. The approach involves pre-training on machine sound datasets to capture fine-grained attributes, followed by fine-tuning for machine attribute classification, which results in significant performance improvements and a new state-of-the-art on the DCASE 2025 Challenge dataset.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining domain-adaptive pre-training with agglomerative hierarchical clustering for pseudo-labeling in ASD, effectively addressing the issue of missing attribute labels in a new way. However, it builds on existing pre-trained models and clustering techniques rather than introducing a entirely novel problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the ASD subfield due to its state-of-the-art performance and practical approach to handling unlabeled data. Its influence may be limited to specialized applications in machine sound detection rather than broader AI or commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to ASD research with innovative techniques and benchmark improvements, making it essential for those working in acoustic event detection. While not groundbreaking across all AI fields, it provides insights that could enhance related methodologies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4a5b1fd11759b4b584b32db6b175289f6da799bc",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 3,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xin Fang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Guirui Zhong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2318645308"
        },
        {
          "name": "Qing Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376461454"
        },
        {
          "name": "Fan Chu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2320596042"
        },
        {
          "name": "Lei Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2318691102"
        },
        {
          "name": "Mengui Qian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380607391"
        },
        {
          "name": "Mingqi Cai",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2307916614"
        },
        {
          "name": "Jiangzhao Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380699007"
        },
        {
          "name": "Jianqing Gao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2344792827"
        },
        {
          "name": "Jun Du",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2307963618"
        }
      ]
    },
    {
      "id": "2509.12846",
      "title": "Unleashing the Power of Discrete-Time State Representation: Ultrafast\n  Target-based IMU-Camera Spatial-Temporal Calibration",
      "authors": [
        "Junlin Song",
        "Antoine Richard",
        "Miguel Olivares-Mendez"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual-inertial fusion is crucial for a large amount of intelligent and\nautonomous applications, such as robot navigation and augmented reality. To\nbootstrap and achieve optimal state estimation, the spatial-temporal\ndisplacements between IMU and cameras must be calibrated in advance. Most\nexisting calibration methods adopt continuous-time state representation, more\nspecifically the B-spline. Despite these methods achieve precise\nspatial-temporal calibration, they suffer from high computational cost caused\nby continuous-time state representation. To this end, we propose a novel and\nextremely efficient calibration method that unleashes the power of\ndiscrete-time state representation. Moreover, the weakness of discrete-time\nstate representation in temporal calibration is tackled in this paper. With the\nincreasing production of drones, cellphones and other visual-inertial\nplatforms, if one million devices need calibration around the world, saving one\nminute for the calibration of each device means saving 2083 work days in total.\nTo benefit both the research and industry communities, our code will be\nopen-source.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12846v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12846v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.252,
      "weak_supervision_score": 0.276,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.325,
      "datasets_score": 0.243,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12849",
      "title": "AI Factories: It's time to rethink the Cloud-HPC divide",
      "authors": [
        "Pedro Garcia Lopez",
        "Daniel Barcelona Pons",
        "Marcin Copik",
        "Torsten Hoefler",
        "Eduardo Quiñones",
        "Maciej Malawski",
        "Peter Pietzutch",
        "Alberto Marti",
        "Thomas Ohlson Timoudas",
        "Aleksander Slominski"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The strategic importance of artificial intelligence is driving a global push\ntoward Sovereign AI initiatives. Nationwide governments are increasingly\ndeveloping dedicated infrastructures, called AI Factories (AIF), to achieve\ntechnological autonomy and secure the resources necessary to sustain robust\nlocal digital ecosystems.\n  In Europe, the EuroHPC Joint Undertaking is investing hundreds of millions of\neuros into several AI Factories, built atop existing high-performance computing\n(HPC) supercomputers. However, while HPC systems excel in raw performance, they\nare not inherently designed for usability, accessibility, or serving as\npublic-facing platforms for AI services such as inference or agentic\napplications. In contrast, AI practitioners are accustomed to cloud-native\ntechnologies like Kubernetes and object storage, tools that are often difficult\nto integrate within traditional HPC environments.\n  This article advocates for a dual-stack approach within supercomputers:\nintegrating both HPC and cloud-native technologies. Our goal is to bridge the\ndivide between HPC and cloud computing by combining high performance and\nhardware acceleration with ease of use and service-oriented front-ends. This\nconvergence allows each paradigm to amplify the other. To this end, we will\nstudy the cloud challenges of HPC (Serverless HPC) and the HPC challenges of\ncloud technologies (High-performance Cloud).",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12849v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12849v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.5,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses the use of HPC systems for large-scale hardware acceleration in training AI models, such as large language models, which inherently involves distributed training techniques like parallel computing across nodes. It references tools like MPI and CUDA, commonly used in distributed training environments. However, the main contribution focuses on bridging HPC and cloud technologies for broader AI infrastructures rather than proposing new algorithms or systems specifically for distributed training, making it relevant but not central.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper advocates for integrating high-performance computing (HPC) and cloud-native technologies in AI Factories to address the limitations of existing HPC supercomputers, which are powerful but not user-friendly for AI practitioners. It proposes a dual-stack approach that combines HPC's raw performance and hardware acceleration with cloud tools like Kubernetes and object storage, detailing integrations in resource sharing, data infrastructure, compute environments, AI services, and external cloud federation to enhance usability, accessibility, and efficiency for AI workloads.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing HPC and cloud technologies to address AI Factory challenges, offering a clever integration rather than a completely new concept. While it doesn't introduce a groundbreaking problem or technique, it reframes known issues in the context of sovereign AI infrastructures.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and development in AI infrastructure, particularly in regions like Europe investing in AI Factories, by promoting integrated approaches that could enhance accessibility and efficiency. However, its applicability may be limited to specific subfields such as distributed computing and sovereign AI initiatives, rather than having widespread commercial impact.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers significant insights into reconciling HPC and cloud technologies for AI applications, making it valuable for researchers and policymakers in computing infrastructure. While not essential for all audiences, it represents a strong contribution that could inform ongoing efforts in sovereign AI development.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6dedeb0ee187d821fdb7adfa1c0c245c8e7f06b1",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 10,
      "average_h_index": 2.8,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Pedro García López",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2327040727"
        },
        {
          "name": "Daniel Barcelona-Pons",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/51151456"
        },
        {
          "name": "Marcin Copik",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/3475544"
        },
        {
          "name": "Torsten Hoefler",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2267726340"
        },
        {
          "name": "Eduardo Quinones",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380608813"
        },
        {
          "name": "Maciej Malawski",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380608311"
        },
        {
          "name": "Peter Pietzutch",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380608316"
        },
        {
          "name": "Alberto Marti",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380608130"
        },
        {
          "name": "Thomas Ohlson Timoudas",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/102400559"
        },
        {
          "name": "Aleksander Slominski",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380608692"
        }
      ]
    },
    {
      "id": "2509.12866",
      "title": "Leveraging Large Language Models to Effectively Generate Visual Data for\n  Canine Musculoskeletal Diagnoses",
      "authors": [
        "Martin Thißen",
        "Thi Ngoc Diep Tran",
        "Barbara Esteve Ratsch",
        "Ben Joel Schönbein",
        "Ute Trapp",
        "Beate Egner",
        "Romana Piat",
        "Elke Hergenröther"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "It is well-established that more data generally improves AI model\nperformance. However, data collection can be challenging for certain tasks due\nto the rarity of occurrences or high costs. These challenges are evident in our\nuse case, where we apply AI models to a novel approach for visually documenting\nthe musculoskeletal condition of dogs. Here, abnormalities are marked as\ncolored strokes on a body map of a dog. Since these strokes correspond to\ndistinct muscles or joints, they can be mapped to the textual domain in which\nlarge language models (LLMs) operate. LLMs have demonstrated impressive\ncapabilities across a wide range of tasks, including medical applications,\noffering promising potential for generating synthetic training data. In this\nwork, we investigate whether LLMs can effectively generate synthetic visual\ntraining data for canine musculoskeletal diagnoses. For this, we developed a\nmapping that segments visual documentations into over 200 labeled regions\nrepresenting muscles or joints. Using techniques like guided decoding,\nchain-of-thought reasoning, and few-shot prompting, we generated 1,000\nsynthetic visual documentations for patellar luxation (kneecap dislocation)\ndiagnosis, the diagnosis for which we have the most real-world data. Our\nanalysis shows that the generated documentations are sensitive to location and\nseverity of the diagnosis while remaining independent of the dog's sex. We\nfurther generated 1,000 visual documentations for various other diagnoses to\ncreate a binary classification dataset. A model trained solely on this\nsynthetic data achieved an F1 score of 88% on 70 real-world documentations.\nThese results demonstrate the potential of LLM-generated synthetic data, which\nis particularly valuable for addressing data scarcity in rare diseases. While\nour methodology is tailored to the medical domain, the insights and techniques\ncan be adapted to other fields.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12866v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12866v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.439,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.39,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper uses LLMs to programmatically generate synthetic visual data and labels for training a model, which aligns with weak supervision by relying on noisy or imprecise sources rather than hand-labeled data. However, it does not explicitly frame this as weak supervision or explore its core techniques in depth, making it moderately relevant.",
      "diffusion_reasoning_justification": "The paper mentions techniques like chain-of-thought reasoning and guided decoding but does not involve diffusion models or iterative refinement processes for logical reasoning. There is no evidence of adapting diffusion for multi-step reasoning, so it does not fit this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper focuses on creating synthetic datasets (e.g., 1,000 visual documentations for diagnoses), analyzing their quality, and evaluating a model's performance on real-world data, which directly aligns with research on dataset creation, curation, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper investigates the use of large language models (LLMs) to generate synthetic visual data for diagnosing canine musculoskeletal conditions, addressing data scarcity by mapping colored strokes on dog body maps to textual descriptions of over 200 anatomical regions. Employing techniques such as guided decoding, chain-of-thought reasoning, and few-shot prompting, the authors generated 1,000 synthetic documentations for patellar luxation, which proved sensitive to diagnosis specifics, and another 1,000 for various diagnoses to train a binary classification model that achieved an 88% F1 score on real-world data, demonstrating the potential of LLMs for enhancing AI performance in specialized medical domains.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting LLMs to generate synthetic visual data for a niche veterinary application through a new anatomical mapping, though it builds on existing LLM techniques for data generation. This clever combination addresses data scarcity in a specific domain but does not introduce a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI for veterinary medicine and synthetic data generation, as it offers practical techniques for data-scarce areas. However, its influence may remain limited to specialized applications rather than broadly affecting mainstream research or commercial AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a valuable and practical contribution by showing how LLMs can generate effective synthetic data for rare medical diagnoses, making it essential for researchers in AI and veterinary fields. While insightful, it is not groundbreaking enough to be a must-read for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3c5b6ce086424eeee607867d29c0917f436baee2",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 1,
      "average_h_index": 0.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Martin Thißen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2306004211"
        },
        {
          "name": "Thi Ngoc Diep Tran",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2320822636"
        },
        {
          "name": "Barbara Esteve Ratsch",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2163255816"
        },
        {
          "name": "Ben Joel Schonbein",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380609818"
        },
        {
          "name": "Ute Trapp",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2306008340"
        },
        {
          "name": "Beate Egner",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2305945814"
        },
        {
          "name": "Romana Piat",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2305999955"
        },
        {
          "name": "Elke Hergenrother",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380609355"
        }
      ]
    },
    {
      "id": "2509.12867",
      "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use",
      "authors": [
        "Yabo Zhang",
        "Yihan Zeng",
        "Qingyun Li",
        "Zhen Hu",
        "Kavin Han",
        "Wangmeng Zuo"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12867v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12867v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.472,
      "weak_supervision_score": 0.42,
      "diffusion_reasoning_score": 0.468,
      "distributed_training_score": 0.369,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with an outcome-based reward function derived from automated LLM judgments and code execution success, not human-ranked data or a separate reward model trained on human feedback. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "The paper involves programmatically generated rewards from LLM-based judgments and code execution, which could be seen as noisy sources, but it primarily focuses on reinforcement learning rather than weak supervision techniques for label generation.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for reasoning; it focuses on reinforcement learning for tool use via code generation, with no mention of treating reasoning paths as entities for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12871",
      "title": "Cumulative Consensus Score: Label-Free and Model-Agnostic Evaluation of\n  Object Detectors in Deployment",
      "authors": [
        "Avinaash Manoharan",
        "Xiangyu Yin",
        "Domenik Helm",
        "Chih-Hong Cheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Evaluating object detection models in deployment is challenging because\nground-truth annotations are rarely available. We introduce the Cumulative\nConsensus Score (CCS), a label-free metric that enables continuous monitoring\nand comparison of detectors in real-world settings. CCS applies test-time data\naugmentation to each image, collects predicted bounding boxes across augmented\nviews, and computes overlaps using Intersection over Union. Maximum overlaps\nare normalized and averaged across augmentation pairs, yielding a measure of\nspatial consistency that serves as a proxy for reliability without annotations.\nIn controlled experiments on Open Images and KITTI, CCS achieved over 90%\ncongruence with F1-score, Probabilistic Detection Quality, and Optimal\nCorrection Cost. The method is model-agnostic, working across single-stage and\ntwo-stage detectors, and operates at the case level to highlight\nunder-performing scenarios. Altogether, CCS provides a robust foundation for\nDevOps-style monitoring of object detectors.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12871v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12871v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.339,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a label-free metric (CCS) for evaluating object detectors in deployment by assessing prediction consistency through data augmentation, without involving the training of models. Weak supervision specifically refers to techniques for generating training labels from noisy or imprecise sources, which this paper does not address, as it focuses on post-training evaluation rather than label creation for model training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12875",
      "title": "LTA-thinker: Latent Thought-Augmented Training Framework for Large\n  Language Models on Complex Reasoning",
      "authors": [
        "Jiaqi Wang",
        "Binquan Ji",
        "Haibo Luo",
        "Yiyang Qi",
        "Ruiting Li",
        "Huiyan Wang",
        "Yuantao Han",
        "Cangyi Yang",
        "jiaxu Zhang",
        "Feiliang Ren"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Complex Reasoning in Large Language Models can be dynamically optimized using\nTest-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut,\nSoftCoT and its variant are effective in continuous latent space inference, the\ncore bottleneck still lies in the efficient generation and utilization of\nhigh-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger\nvariance in the generated Latent Thought distribution more closely approximates\nthe golden truth distribution, we propose a Latent Thought-Augmented Training\nFramework--LTA-Thinker, which improves distributional variance and enhances\nreasoning performance from two perspectives. First, LTA-Thinker constructs a\nLatent Thought generation architecture based on a learnable prior. This\narchitecture aims to increase the variance distribution of generated Latent\nThought Vectors in order to simplify the overall structure and raise the\nperformance ceiling. Second, LTA-Thinker introduces a distribution-based\ndirectional optimization paradigm that jointly constrains both distribution\nlocality and distribution scale. This mechanism improves information efficiency\nand computational cost through a multi-objective co-training strategy, which\ncombines standard Supervised Fine-Tuning (SFT) loss with two novel losses:\nSemantic Alignment Loss, which utilizes KL divergence to ensure that the Latent\nThought is highly relevant to the semantics of the question; Reasoning Focus\nLoss, which utilizes a contrastive learning mechanism to guide the model to\nfocus on the most critical reasoning steps. Experiments show that LTA-thinker\nachieves state-of-the-art (SOTA) performance among various baselines and\ndemonstrates a higher performance ceiling and better scaling effects.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12875v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12875v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.572,
      "distributed_training_score": 0.417,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Latent Thought-Augmented Training Framework for LLMs, emphasizing optimization of Latent Thought distributions in a continuous space to improve reasoning. While it discusses variance in distributions (e.g., referencing SoftCoT++), it does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a holistically corrected entity over multiple steps. There is no clear component adapting diffusion mechanics for multi-step logical reasoning, making it unrelated to this topic.",
      "distributed_training_justification": "The paper's main contribution is a training framework for enhancing LLM reasoning through Latent Thought generation and multi-objective loss functions, without any discussion of distributed training, parallel computing, or partitioning data/computation across multiple nodes or processors. It describes a single-model setup with a frozen backbone LLM and a lightweight assistant, which does not address acceleration via distributed systems or algorithms.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12878",
      "title": "Few to Big: Prototype Expansion Network via Diffusion Learner for Point\n  Cloud Few-shot Semantic Segmentation",
      "authors": [
        "Qianguang Zhao",
        "Dongli Wang",
        "Yan Zhou",
        "Jianxun Li",
        "Richard Irampa"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Few-shot 3D point cloud semantic segmentation aims to segment novel\ncategories using a minimal number of annotated support samples. While existing\nprototype-based methods have shown promise, they are constrained by two\ncritical challenges: (1) Intra-class Diversity, where a prototype's limited\nrepresentational capacity fails to cover a class's full variations, and (2)\nInter-set Inconsistency, where prototypes derived from the support set are\nmisaligned with the query feature space. Motivated by the powerful generative\ncapability of diffusion model, we re-purpose its pre-trained conditional\nencoder to provide a novel source of generalizable features for expanding the\nprototype's representational range. Under this setup, we introduce the\nPrototype Expansion Network (PENet), a framework that constructs big-capacity\nprototypes from two complementary feature sources. PENet employs a dual-stream\nlearner architecture: it retains a conventional fully supervised Intrinsic\nLearner (IL) to distill representative features, while introducing a novel\nDiffusion Learner (DL) to provide rich generalizable features. The resulting\ndual prototypes are then processed by a Prototype Assimilation Module (PAM),\nwhich adopts a novel push-pull cross-guidance attention block to iteratively\nalign the prototypes with the query space. Furthermore, a Prototype Calibration\nMechanism (PCM) regularizes the final big capacity prototype to prevent\nsemantic drift. Extensive experiments on the S3DIS and ScanNet datasets\ndemonstrate that PENet significantly outperforms state-of-the-art methods\nacross various few-shot settings.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12878v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12878v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.411,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion model's pre-trained encoder to extract generalizable features for point cloud segmentation, but it does not adapt the iterative refinement process of diffusion models for multi-step logical reasoning or Chain-of-Thought tasks. Instead, it focuses on generative features for enhancing prototypes in a computer vision context, lacking any component for holistic reasoning path correction.",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or multi-node machine learning techniques. It concentrates on the Prototype Expansion Network for few-shot segmentation, with no mention of partitioning data, architecture, or computation across processors or nodes for training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12883",
      "title": "Lego-Edit: A General Image Editing Framework with Model-Level Bricks and\n  MLLM Builder",
      "authors": [
        "Qifei Jia",
        "Yu Liu",
        "Yajie Chai",
        "Xintong Yao",
        "Qiming Lu",
        "Yasen Zhang",
        "Runyu Shi",
        "Ying Huang",
        "Guoquan Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Instruction-based image editing has garnered significant attention due to its\ndirect interaction with users. However, real-world user instructions are\nimmensely diverse, and existing methods often fail to generalize effectively to\ninstructions outside their training domain, limiting their practical\napplication. To address this, we propose Lego-Edit, which leverages the\ngeneralization capability of Multi-modal Large Language Model (MLLM) to\norganize a suite of model-level editing tools to tackle this challenge.\nLego-Edit incorporates two key designs: (1) a model-level toolkit comprising\ndiverse models efficiently trained on limited data and several image\nmanipulation functions, enabling fine-grained composition of editing actions by\nthe MLLM; and (2) a three-stage progressive reinforcement learning approach\nthat uses feedback on unannotated, open-domain instructions to train the MLLM,\nequipping it with generalized reasoning capabilities for handling real-world\ninstructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art\nperformance on GEdit-Bench and ImgBench. It exhibits robust reasoning\ncapabilities for open-domain instructions and can utilize newly introduced\nediting tools without additional fine-tuning.\n  Code is available: https://github.com/xiaomi-research/lego-edit.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12883v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12883v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.307,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a framework called Lego-Edit, which uses Multi-modal Large Language Models (MLLMs) and reinforcement learning to organize image editing tools for handling diverse instructions. It does not involve diffusion models or adapt the iterative refinement process of diffusion for multi-step logical reasoning. The core contributions are in MLLM training and tool composition, with no mention of treating a Chain-of-Thought as a holistic entity via diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12886",
      "title": "The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via\n  Hidden Representations",
      "authors": [
        "Yubo Zhu",
        "Dongrui Liu",
        "Zecheng Lin",
        "Wei Tong",
        "Sheng Zhong",
        "Jing Shao"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Estimating the difficulty of input questions as perceived by large language\nmodels (LLMs) is essential for accurate performance evaluation and adaptive\ninference. Existing methods typically rely on repeated response sampling,\nauxiliary models, or fine-tuning the target model itself, which may incur\nsubstantial computational costs or compromise generality. In this paper, we\npropose a novel approach for difficulty estimation that leverages only the\nhidden representations produced by the target LLM. We model the token-level\ngeneration process as a Markov chain and define a value function to estimate\nthe expected output quality given any hidden state. This allows for efficient\nand accurate difficulty estimation based solely on the initial hidden state,\nwithout generating any output tokens. Extensive experiments across both textual\nand multimodal tasks demonstrate that our method consistently outperforms\nexisting baselines in difficulty estimation. Moreover, we apply our difficulty\nestimates to guide adaptive reasoning strategies, including Self-Consistency,\nBest-of-N, and Self-Refine, achieving higher inference efficiency with fewer\ngenerated tokens.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12886v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12886v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.428,
      "diffusion_reasoning_score": 0.52,
      "distributed_training_score": 0.386,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is estimating question difficulty using hidden representations and a Markov chain with a value function, without involving human feedback, reward models, or reinforcement learning for model alignment. While a value function is an RL concept, it is used here for estimation, not for aligning models with human preferences.",
      "weak_supervision_justification": "The paper focuses on inference-time difficulty estimation from LLM hidden states and does not involve training models with programmatically generated, noisy, or imprecise labels, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "The paper models token generation as a Markov chain for difficulty estimation but does not use diffusion models, iterative refinement of reasoning paths, or any components specific to diffusion-based approaches for multi-step logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12888",
      "title": "Runge-Kutta Approximation and Decoupled Attention for Rectified Flow\n  Inversion and Semantic Editing",
      "authors": [
        "Weiming Chen",
        "Zhihan Zhu",
        "Yijia Wang",
        "Zhihai He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Rectified flow (RF) models have recently demonstrated superior generative\nperformance compared to DDIM-based diffusion models. However, in real-world\napplications, they suffer from two major challenges: (1) low inversion accuracy\nthat hinders the consistency with the source image, and (2) entangled\nmultimodal attention in diffusion transformers, which hinders precise attention\ncontrol. To address the first challenge, we propose an efficient high-order\ninversion method for rectified flow models based on the Runge-Kutta solver of\ndifferential equations. To tackle the second challenge, we introduce Decoupled\nDiffusion Transformer Attention (DDTA), a novel mechanism that disentangles\ntext and image attention inside the multimodal diffusion transformers, enabling\nmore precise semantic control. Extensive experiments on image reconstruction\nand text-guided editing tasks demonstrate that our method achieves\nstate-of-the-art performance in terms of fidelity and editability. Code is\navailable at https://github.com/wmchen/RKSovler_DDTA.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12888v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12888v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.536,
      "distributed_training_score": 0.333,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contributions involve enhancing Rectified Flow models for image inversion, reconstruction, and text-guided editing using Runge-Kutta methods and decoupled attention. It focuses on generative tasks for visual data, such as improving fidelity in image processing, and does not address adapting diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12892",
      "title": "Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings",
      "authors": [
        "Shiyu Li",
        "Yang Tang",
        "Ruijie Liu",
        "Shi-Zhe Chen",
        "Xi Chen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have recently demonstrated excellent performance\nin text embedding tasks. Previous work usually use LoRA to fine-tune existing\nLLMs, which are limited by the data and training gap between LLMs and embedding\nmodels. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM\ntrained from scratch and fine-tuned as a text embedder. First, we add news data\nand multilingual pairs for LLM pretraining to bridge the data gap. Based on\nthis, we propose a cross-lingual retrieval dataset that enables the LLM to\nbetter integrate embeddings across different languages. Second, whereas LLMs\nuse a causal mask with token-level loss, embedding models use a bidirectional\nmask with sentence-level loss. This training gap makes full fine-tuning less\neffective than LoRA. We introduce a soft-masking mechanism to gradually\ntransition between these two types of masks, enabling the model to learn more\ncomprehensive representations. Based on this, we propose a dynamic hard\nnegative mining method that exposes the model to more difficult negative\nexamples throughout the training process. Being intuitive and effective, with\nonly approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA\nperformance on both the Massive Text Embedding Benchmark (MTEB) and Chinese\nMTEB (May 19, 2025).",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12892v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12892v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.384,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12893",
      "title": "MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and\n  Intra-Task Joint Optimization",
      "authors": [
        "Yiyi Zhang",
        "Yuchen Yuan",
        "Ying Zheng",
        "Jialun Pei",
        "Jinpeng Li",
        "Zheng Li",
        "Pheng-Ann Heng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Surgical triplet recognition, which involves identifying instrument, verb,\ntarget, and their combinations, is a complex surgical scene understanding\nchallenge plagued by long-tailed data distribution. The mainstream multi-task\nlearning paradigm benefiting from cross-task collaborative promotion has shown\npromising performance in identifying triples, but two key challenges remain: 1)\ninter-task optimization conflicts caused by entangling task-generic and\ntask-specific representations; 2) intra-task optimization conflicts due to\nclass-imbalanced training data. To overcome these difficulties, we propose the\nMLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- and\nintra-task optimization for surgical triplet recognition. For inter-task\noptimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learning\nscheme that decomposes representations into task-shared and task-specific\ncomponents. To enhance task-shared representations, we construct a Multimodal\nLarge Language Model (MLLM) powered probabilistic prompt pool to dynamically\naugment visual features with expert-level semantic cues. Additionally,\ncomprehensive task-specific cues are modeled via distinct task prompts covering\nthe temporal-spatial dimensions, effectively mitigating inter-task ambiguities.\nTo tackle intra-task optimization conflicts, we develop a Coordinated Gradient\nLearning (CGL) strategy, which dissects and rebalances the positive-negative\ngradients originating from head and tail classes for more coordinated learning\nbehaviors. Extensive experiments on the CholecT45 and CholecT50 datasets\ndemonstrate the superiority of our proposed framework, validating its\neffectiveness in handling optimization conflicts.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12893v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12893v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.407,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper employs weakly-supervised localization cues, such as class activation maps (CAM) or bounding boxes for instruments, to guide model training without relying on fully hand-labeled data. Additionally, it uses Multimodal Large Language Models (MLLMs) to generate fine-grained knowledge, which serves as a noisy or programmatic source of supervision. While these elements align with weak supervision principles, the paper's main focus is on inter- and intra-task optimization rather than weak supervision as a core methodology.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors. Its contributions center on algorithmic improvements for multi-task learning in surgical triplet recognition, with no mention of scaling training processes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper proposes the MEJO framework to address inter-task and intra-task optimization conflicts in surgical triplet recognition, which involves identifying instrument, verb, and target from surgical videos. It introduces Shared-Specific-Disentangled (S²D) learning enhanced by Multimodal Large Language Models for inter-task optimization and Coordinated Gradient Learning (CGL) for intra-task imbalances due to long-tailed data, demonstrating state-of-the-art performance on the CholecT45 and CholecT50 datasets.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel MEJO framework that combines MLLM-driven feature enhancement with S²D learning and CGL strategies, significantly advancing multi-task learning for surgical scene understanding by effectively resolving inter- and intra-task conflicts.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in AI for surgical applications due to its innovative handling of optimization conflicts, though its influence may remain confined to specialized subfields like computer vision in medicine.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a high-quality, innovative approach with strong experimental results that advance surgical AI, making it essential for researchers in the field to be aware of and consider for future work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/eb1a140d103a7bb6ea2a74f6bc2c7c108d8d2c7a",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 9,
      "average_h_index": 2.857142857142857,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yiyi Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2342360586"
        },
        {
          "name": "Yuchen Yuan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2256744062"
        },
        {
          "name": "Ying Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381068474"
        },
        {
          "name": "Jialun Pei",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/29928742"
        },
        {
          "name": "Jinpeng Li",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2254280871"
        },
        {
          "name": "Zheng Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381063650"
        },
        {
          "name": "Pheng-Ann Heng",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2066137540"
        }
      ]
    },
    {
      "id": "2509.12894",
      "title": "DialNav: Multi-turn Dialog Navigation with a Remote Guide",
      "authors": [
        "Leekyeung Han",
        "Hyunji Min",
        "Gyeom Hwangbo",
        "Jonghyun Choi",
        "Paul Hongsuck Seo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce DialNav, a novel collaborative embodied dialog task, where a\nnavigation agent (Navigator) and a remote guide (Guide) engage in multi-turn\ndialog to reach a goal location. Unlike prior work, DialNav aims for holistic\nevaluation and requires the Guide to infer the Navigator's location, making\ncommunication essential for task success. To support this task, we collect and\nrelease the Remote Assistance in Navigation (RAIN) dataset, human-human dialog\npaired with navigation trajectories in photorealistic environments. We design a\ncomprehensive benchmark to evaluate both navigation and dialog, and conduct\nextensive experiments analyzing the impact of different Navigator and Guide\nmodels. We highlight key challenges and publicly release the dataset, code, and\nevaluation framework to foster future research in embodied dialog.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12894v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12894v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.353,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes introducing and releasing the Remote Assistance in Navigation (RAIN) dataset, which is a new dataset for embodied dialog and navigation tasks. It details the dataset creation process, including human-human dialog collection in photorealistic environments, and provides a benchmark for evaluation, directly aligning with research on creating, analyzing, and benchmarking datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces DialNav, a novel collaborative embodied dialog task where a Navigator and a remote Guide engage in multi-turn communication to navigate photorealistic environments, with the Guide inferring the Navigator's location to make dialog essential. The authors collect and release the Remote Assistance in Navigation (RAIN) dataset of human-human dialogs paired with trajectories, develop a comprehensive benchmark for evaluating navigation and dialog, and conduct experiments to analyze model performances, highlighting challenges and providing resources for future research in embodied AI.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new problem by creating a realistic non-omniscient guide setup in embodied dialog navigation, significantly advancing the state-of-the-art by emphasizing meaningful communication. This innovative task and dataset address key limitations in prior work, fostering more effective human-AI interactions.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in embodied AI, dialog systems, and navigation by providing a new dataset and benchmark that could lead to advancements in real-world applications like assistive robotics. Its public release of resources ensures it will be built upon and cited extensively within the computer vision and AI communities.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to embodied AI research through its innovative task and dataset, making it essential for researchers in computer vision and dialog systems to be aware of for advancing their work. While not groundbreaking for all audiences, it offers practical insights and resources that warrant attention.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bf017eefddc06019d95defdbf75e0fd90471b480",
      "total_authors": 5,
      "authors_found": 4,
      "highest_h_index": 1,
      "average_h_index": 0.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Leekyeung Han",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Hyunji Min",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380607589"
        },
        {
          "name": "Gyeom Hwangbo",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2362631135"
        },
        {
          "name": "Jonghyun Choi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381473280"
        },
        {
          "name": "Paul Hongsuck Seo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380610776"
        }
      ]
    },
    {
      "id": "2509.12897",
      "title": "Cross-Layer Vision Smoothing: Enhancing Visual Understanding via\n  Sustained Focus on Key Objects in Large Vision-Language Models",
      "authors": [
        "Jianfei Zhao",
        "Feng Zhang",
        "Xin Sun",
        "Lingxing Kong",
        "Zhixing Tan",
        "Chong Feng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) can accurately locate key objects in\nimages, yet their attention to these objects tends to be very brief. Motivated\nby the hypothesis that sustained focus on key objects can improve LVLMs' visual\ncapabilities, we propose Cross-Layer Vision Smoothing (CLVS). The core idea of\nCLVS is to incorporate a vision memory that smooths the attention distribution\nacross layers. Specifically, we initialize this vision memory with\nposition-unbiased visual attention in the first layer. In subsequent layers,\nthe model's visual attention jointly considers the vision memory from previous\nlayers, while the memory is updated iteratively, thereby maintaining smooth\nattention on key objects. Given that visual understanding primarily occurs in\nthe early and middle layers of the model, we use uncertainty as an indicator of\ncompleted visual understanding and terminate the smoothing process accordingly.\nExperiments on four benchmarks across three LVLMs confirm the effectiveness and\ngeneralizability of our method. CLVS achieves state-of-the-art performance on a\nvariety of visual understanding tasks, with particularly significant\nimprovements in relation and attribute understanding.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12897v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12897v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.366,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving visual attention in Large Vision-Language Models through a cross-layer smoothing technique, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses sustaining focus on key objects in images via attention smoothing across layers, but it does not involve diffusion models, iterative refinement for logical reasoning, or treating a chain-of-thought as an entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12901",
      "title": "MSGFusion: Multimodal Scene Graph-Guided Infrared and Visible Image\n  Fusion",
      "authors": [
        "Guihui Li",
        "Bowei Dong",
        "Kaizhi Dong",
        "Jiayi Li",
        "Haiyong Zheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Infrared and visible image fusion has garnered considerable attention owing\nto the strong complementarity of these two modalities in complex, harsh\nenvironments. While deep learning-based fusion methods have made remarkable\nadvances in feature extraction, alignment, fusion, and reconstruction, they\nstill depend largely on low-level visual cues, such as texture and contrast,\nand struggle to capture the high-level semantic information embedded in images.\nRecent attempts to incorporate text as a source of semantic guidance have\nrelied on unstructured descriptions that neither explicitly model entities,\nattributes, and relationships nor provide spatial localization, thereby\nlimiting fine-grained fusion performance. To overcome these challenges, we\nintroduce MSGFusion, a multimodal scene graph-guided fusion framework for\ninfrared and visible imagery. By deeply coupling structured scene graphs\nderived from text and vision, MSGFusion explicitly represents entities,\nattributes, and spatial relations, and then synchronously refines high-level\nsemantics and low-level details through successive modules for scene graph\nrepresentation, hierarchical aggregation, and graph-driven fusion. Extensive\nexperiments on multiple public benchmarks show that MSGFusion significantly\noutperforms state-of-the-art approaches, particularly in detail preservation\nand structural clarity, and delivers superior semantic consistency and\ngeneralizability in downstream tasks such as low-light object detection,\nsemantic segmentation, and medical image fusion.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12901v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12901v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.456,
      "distributed_training_score": 0.298,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for infrared and visible image fusion using multimodal scene graphs, focusing on semantic representation, hierarchical aggregation, and graph-driven fusion. It does not involve diffusion models, iterative refinement processes, or any mechanism for multi-step logical reasoning on tasks like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12905",
      "title": "AREPAS: Anomaly Detection in Fine-Grained Anatomy with\n  Reconstruction-Based Semantic Patch-Scoring",
      "authors": [
        "Branko Mitic",
        "Philipp Seeböck",
        "Helmut Prosch",
        "Georg Langs"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Early detection of newly emerging diseases, lesion severity assessment,\ndifferentiation of medical conditions and automated screening are examples for\nthe wide applicability and importance of anomaly detection (AD) and\nunsupervised segmentation in medicine. Normal fine-grained tissue variability\nsuch as present in pulmonary anatomy is a major challenge for existing\ngenerative AD methods. Here, we propose a novel generative AD approach\naddressing this issue. It consists of an image-to-image translation for\nanomaly-free reconstruction and a subsequent patch similarity scoring between\nobserved and generated image-pairs for precise anomaly localization. We\nvalidate the new method on chest computed tomography (CT) scans for the\ndetection and segmentation of infectious disease lesions. To assess\ngeneralizability, we evaluate the method on an ischemic stroke lesion\nsegmentation task in T1-weighted brain MRI. Results show improved pixel-level\nanomaly segmentation in both chest CTs and brain MRIs, with relative DICE score\nimprovements of +1.9% and +4.4%, respectively, compared to other\nstate-of-the-art reconstruction-based methods.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12905v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12905v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.269,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.3,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12908",
      "title": "All Roads Lead to Rome: Graph-Based Confidence Estimation for Large\n  Language Model Reasoning",
      "authors": [
        "Caiqi Zhang",
        "Chang Shu",
        "Ehsan Shareghi",
        "Nigel Collier"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Confidence estimation is essential for the reliable deployment of large\nlanguage models (LLMs). Existing methods are primarily designed for factual QA\ntasks and often fail to generalize to reasoning tasks. To address this gap, we\npropose a set of training-free, graph-based confidence estimation methods\ntailored to reasoning tasks. Our approach models reasoning paths as directed\ngraphs and estimates confidence by exploiting graph properties such as\ncentrality, path convergence, and path weighting. Experiments with two LLMs on\nthree reasoning datasets demonstrate improved confidence estimation and\nenhanced performance on two downstream tasks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12908v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12908v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.533,
      "distributed_training_score": 0.355,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on graph-based confidence estimation for LLMs in reasoning tasks, without any mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces graph-based methods for confidence estimation in reasoning tasks, but it does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12913",
      "title": "T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and\n  Efficient UAV Tracking",
      "authors": [
        "Hojat Ardi",
        "Amir Jahanshahi",
        "Ali Diba"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Aerial object tracking remains a challenging task due to scale variations,\ndynamic backgrounds, clutter, and frequent occlusions. While most existing\ntrackers emphasize spatial cues, they often overlook temporal dependencies,\nresulting in limited robustness in long-term tracking and under occlusion.\nFurthermore, correlation-based Siamese trackers are inherently constrained by\nthe linear nature of correlation operations, making them ineffective against\ncomplex, non-linear appearance changes. To address these limitations, we\nintroduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends\nthe SiamTPN architecture with explicit temporal modeling. Our approach\nincorporates temporal feature fusion and attention-based interactions,\nstrengthening temporal consistency and enabling richer feature representations.\nThese enhancements yield significant improvements over the baseline and achieve\nperformance competitive with state-of-the-art trackers. Crucially, despite the\nadded temporal modules, T-SiamTPN preserves computational efficiency. Deployed\non the resource-constrained Jetson Nano, the tracker runs in real time at 7.1\nFPS, demonstrating its suitability for real-world embedded applications without\nnotable runtime overhead. Experimental results highlight substantial gains:\ncompared to the baseline, T-SiamTPN improves success rate by 13.7% and\nprecision by 14.7%. These findings underscore the importance of temporal\nmodeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and\nefficient solution for aerial object tracking. Code is available at:\nhttps://github.com/to/be/released",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12913v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12913v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.371,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12914",
      "title": "Stochastic Streets: A Walk Through Random LLM Address Generation in four\n  European Cities",
      "authors": [
        "Tairan Fu",
        "David Campo-Nazareno",
        "Javier Coronado-Blázquez",
        "Javier Conde",
        "Pedro Reviriego",
        "Fabrizio Lombardi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are capable of solving complex math problems or\nanswer difficult questions on almost any topic, but can they generate random\nstreet addresses for European cities?",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12914v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12914v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.296,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12918",
      "title": "A Novel Compression Framework for YOLOv8: Achieving Real-Time Aerial\n  Object Detection on Edge Devices via Structured Pruning and Channel-Wise\n  Distillation",
      "authors": [
        "Melika Sabaghian",
        "Mohammad Ali Keyvanrad",
        "Seyyedeh Mahila Moghadami"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Efficient deployment of deep learning models for aerial object detection on\nresource-constrained devices requires significant compression without\ncom-promising performance. In this study, we propose a novel three-stage\ncompression pipeline for the YOLOv8 object detection model, integrating\nsparsity-aware training, structured channel pruning, and Channel-Wise Knowledge\nDistillation (CWD). First, sparsity-aware training introduces dynamic sparsity\nduring model optimization, effectively balancing parameter reduction and\ndetection accuracy. Second, we apply structured channel pruning by leveraging\nbatch normalization scaling factors to eliminate redundant channels,\nsignificantly reducing model size and computational complexity. Finally, to\nmitigate the accuracy drop caused by pruning, we employ CWD to transfer\nknowledge from the original model, using an adjustable temperature and loss\nweighting scheme tailored for small and medium object detection. Extensive\nexperiments on the VisDrone dataset demonstrate the effectiveness of our\napproach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model\nparameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to\n13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The\nresulting compressed model achieves 47.9 AP50 and boosts inference speed from\n26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge\ndevices. We further apply TensorRT as a lightweight optimization step. While\nthis introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly\nimproves inference speed from 45 to 68 FPS, demonstrating the practicality of\nour approach for high-throughput, re-source-constrained scenarios.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12918v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12918v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.407,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on model compression techniques, such as sparsity-aware training, structured pruning, and knowledge distillation, to optimize YOLOv8 for edge devices. It does not involve distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation to accelerate training, making it unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12923",
      "title": "A Graph-Based Approach to Alert Contextualisation in Security Operations\n  Centres",
      "authors": [
        "Magnus Wiik Eckhoff",
        "Peter Marius Flydal",
        "Siem Peters",
        "Martin Eian",
        "Jonas Halvorsen",
        "Vasileios Mavroeidis",
        "Gudmund Grov"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Interpreting the massive volume of security alerts is a significant challenge\nin Security Operations Centres (SOCs). Effective contextualisation is\nimportant, enabling quick distinction between genuine threats and benign\nactivity to prioritise what needs further analysis. This paper proposes a\ngraph-based approach to enhance alert contextualisation in a SOC by aggregating\nalerts into graph-based alert groups, where nodes represent alerts and edges\ndenote relationships within defined time-windows. By grouping related alerts,\nwe enable analysis at a higher abstraction level, capturing attack steps more\neffectively than individual alerts. Furthermore, to show that our format is\nwell suited for downstream machine learning methods, we employ Graph Matching\nNetworks (GMNs) to correlate incoming alert groups with historical incidents,\nproviding analysts with additional insights.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12923v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12923v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.31,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12924",
      "title": "MATTER: Multiscale Attention for Registration Error Regression",
      "authors": [
        "Shipeng Liu",
        "Ziliang Xiong",
        "Khac-Hoang Ngo",
        "Per-Erik Forssén"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Point cloud registration (PCR) is crucial for many downstream tasks, such as\nsimultaneous localization and mapping (SLAM) and object tracking. This makes\ndetecting and quantifying registration misalignment, i.e., PCR quality\nvalidation, an important task. All existing methods treat validation as a\nclassification task, aiming to assign the PCR quality to a few classes. In this\nwork, we instead use regression for PCR validation, allowing for a more\nfine-grained quantification of the registration quality. We also extend\npreviously used misalignment-related features by using multiscale extraction\nand attention-based aggregation. This leads to accurate and robust registration\nerror estimation on diverse datasets, especially for point clouds with\nheterogeneous spatial densities. Furthermore, when used to guide a mapping\ndownstream task, our method significantly improves the mapping quality for a\ngiven amount of re-registered frames, compared to the state-of-the-art\nclassification-based method.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12924v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12924v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.35,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12926",
      "title": "Population Estimation using Deep Learning over Gandhinagar Urban Area",
      "authors": [
        "Jai Singla",
        "Peal Jotania",
        "Keivalya Pandya"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Population estimation is crucial for various applications, from resource\nallocation to urban planning. Traditional methods such as surveys and censuses\nare expensive, time-consuming and also heavily dependent on human resources,\nrequiring significant manpower for data collection and processing. In this\nstudy a deep learning solution is proposed to estimate population using high\nresolution (0.3 m) satellite imagery, Digital Elevation Models (DEM) of 0.5m\nresolution and vector boundaries. Proposed method combines Convolution Neural\nNetwork (CNN) architecture for classification task to classify buildings as\nresidential and non-residential and Artificial Neural Network (ANN)\narchitecture to estimate the population. Approx. 48k building footprints over\nGandhinagar urban area are utilized containing both residential and\nnon-residential, with residential categories further used for building-level\npopulation estimation. Experimental results on a large-scale dataset\ndemonstrate the effectiveness of our model, achieving an impressive overall\nF1-score of 0.9936. The proposed system employs advanced geospatial analysis\nwith high spatial resolution to estimate Gandhinagar population at 278,954. By\nintegrating real-time data updates, standardized metrics, and infrastructure\nplanning capabilities, this automated approach addresses critical limitations\nof conventional census-based methodologies. The framework provides\nmunicipalities with a scalable and replicable tool for optimized resource\nmanagement in rapidly urbanizing cities, showcasing the efficiency of AI-driven\ngeospatial analytics in enhancing data-driven urban governance.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12926v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12926v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.278,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.288,
      "distributed_training_score": 0.318,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12927",
      "title": "HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic\n  Decision-Making",
      "authors": [
        "Xingxing Hong",
        "Yungong Wang",
        "Dexin Jin",
        "Ye Yuan",
        "Ximing Huang",
        "Zijian Wu",
        "Wenxin Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.GT (Computer Science and Game Theory)",
        "cs.LG (Machine Learning)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Benchmarks are crucial for assessing multi-agent reinforcement learning\n(MARL) algorithms. While StarCraft II-related environments have driven\nsignificant advances in MARL, existing benchmarks like SMAC focus primarily on\nmicromanagement, limiting comprehensive evaluation of high-level strategic\nintelligence. To address this, we introduce HLSMAC, a new cooperative MARL\nbenchmark with 12 carefully designed StarCraft II scenarios based on classical\nstratagems from the Thirty-Six Stratagems. Each scenario corresponds to a\nspecific stratagem and is designed to challenge agents with diverse strategic\nelements, including tactical maneuvering, timing coordination, and deception,\nthereby opening up avenues for evaluating high-level strategic decision-making\ncapabilities. We also propose novel metrics across multiple dimensions beyond\nconventional win rate, such as ability utilization and advancement efficiency,\nto assess agents' overall performance within the HLSMAC environment. We\nintegrate state-of-the-art MARL algorithms and LLM-based agents with our\nbenchmark and conduct comprehensive experiments. The results demonstrate that\nHLSMAC serves as a robust testbed for advancing multi-agent strategic\ndecision-making.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12927v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12927v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.31,
      "distributed_training_score": 0.353,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the introduction of HLSMAC, a benchmark for evaluating multi-agent reinforcement learning (MARL) algorithms on high-level strategic decision-making in StarCraft II scenarios. It draws inspiration from human strategic wisdom (e.g., Thirty-Six Stratagems) for benchmark design but does not involve training models with human feedback, such as using human-ranked data to create a reward model for fine-tuning. Therefore, it does not align with RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12931",
      "title": "4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D\n  Radar",
      "authors": [
        "Xiao Tang",
        "Guirong Zhuo",
        "Cong Wang",
        "Boyuan Zheng",
        "Minqing Huang",
        "Lianqing Zheng",
        "Long Chen",
        "Shouyi Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D reconstruction and novel view synthesis are critical for validating\nautonomous driving systems and training advanced perception models. Recent\nself-supervised methods have gained significant attention due to their\ncost-effectiveness and enhanced generalization in scenarios where annotated\nbounding boxes are unavailable. However, existing approaches, which often rely\non frequency-domain decoupling or optical flow, struggle to accurately\nreconstruct dynamic objects due to imprecise motion estimation and weak\ntemporal consistency, resulting in incomplete or distorted representations of\ndynamic scene elements. To address these challenges, we propose 4DRadar-GS, a\n4D Radar-augmented self-supervised 3D reconstruction framework tailored for\ndynamic driving scenes. Specifically, we first present a 4D Radar-assisted\nGaussian initialization scheme that leverages 4D Radar's velocity and spatial\ninformation to segment dynamic objects and recover monocular depth scale,\ngenerating accurate Gaussian point representations. In addition, we propose a\nVelocity-guided PointTrack (VGPT) model, which is jointly trained with the\nreconstruction pipeline under scene flow supervision, to track fine-grained\ndynamic trajectories and construct temporally consistent representations.\nEvaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art\nperformance in dynamic driving scene 3D reconstruction.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12931v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12931v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.338,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12934",
      "title": "The Anatomy of Alignment: Decomposing Preference Optimization by\n  Steering Sparse Features",
      "authors": [
        "Jeremias Ferrao",
        "Matthijs van der Lende",
        "Ilija Lichkovski",
        "Clement Neo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Prevailing alignment methods induce opaque parameter changes, making it\ndifficult to audit what the model truly learns. To address this, we introduce\nFeature Steering with Reinforcement Learning (FSRL), a framework that trains a\nlightweight adapter to steer model behavior by modulating interpretable sparse\nfeatures. First, we theoretically show that this mechanism is principled and\nexpressive enough to approximate the behavioral shifts of post-training\nprocesses. Then, we apply this framework to the task of preference optimization\nand perform a causal analysis of the learned policy. We find that the model\nrelies on stylistic presentation as a proxy for quality, disproportionately\nsteering features related to style and formatting over those tied to alignment\nconcepts like honesty. Despite exploiting this heuristic, FSRL proves to be an\neffective alignment method, achieving a substantial reduction in preference\nloss. Overall, FSRL offers an interpretable control interface and a practical\nway to diagnose how preference optimization pressures manifest at the feature\nlevel.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12934v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12934v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.601,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.378,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution, FSRL, uses reinforcement learning to optimize model alignment with human preferences, similar to RLHF. It applies this to preference optimization tasks like those in RLHF, such as using datasets like UltraFeedback, and trains an adapter based on reward signals, making it a direct extension of RLHF principles.",
      "weak_supervision_justification": "The paper focuses on feature steering with reinforcement learning and sparse autoencoders for model alignment, without any mention of programmatically generating labels from noisy sources or using weak supervision techniques for training.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for reasoning, or multi-step logical processes; it centers on alignment through feature steering and reinforcement learning, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Feature Steering with Reinforcement Learning (FSRL), a framework designed to make large language model alignment more transparent by using Sparse Autoencoders (SAEs) to identify interpretable sparse features and training a lightweight adapter to steer these features via reinforcement learning. The authors theoretically demonstrate that FSRL is expressive enough to approximate existing fine-tuning methods like LoRA, empirically show its effectiveness in reducing preference loss on datasets such as UltraFeedback while preserving model capabilities, and reveal through causal analysis that the framework prioritizes stylistic features over core alignment concepts like honesty, providing a practical tool for interpretable alignment and diagnosis.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing techniques like Sparse Autoencoders and reinforcement learning to create an interpretable alignment framework, solving the known problem of model opacity in a new way. However, it builds on established concepts rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in interpretable AI and model alignment by offering a transparent method for auditing and steering behaviors, potentially leading to citations and developments within subfields like AI safety. Nonetheless, its applicability may be limited to specific areas of machine learning rather than broadly across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides valuable insights into making alignment processes more interpretable and effective, making it a significant contribution for researchers in AI ethics and model interpretability. While not essential for all readers, it offers practical tools and findings that warrant attention from those working in related areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a868d9e302737cfb032ea60d35de954db6d4eaae",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "J. Ferrao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2357086134"
        },
        {
          "name": "Matthijs van der Lende",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2357083408"
        },
        {
          "name": "Ilija Lichkovski",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380604554"
        },
        {
          "name": "Clement Neo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380604836"
        }
      ]
    },
    {
      "id": "2509.12937",
      "title": "Jailbreaking Large Language Models Through Content Concretization",
      "authors": [
        "Johan Wahréus",
        "Ahmed Hussain",
        "Panos Papadimitratos"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed for task automation\nand content generation, yet their safety mechanisms remain vulnerable to\ncircumvention through different jailbreaking techniques. In this paper, we\nintroduce \\textit{Content Concretization} (CC), a novel jailbreaking technique\nthat iteratively transforms abstract malicious requests into concrete,\nexecutable implementations. CC is a two-stage process: first, generating\ninitial LLM responses using lower-tier, less constrained safety filters models,\nthen refining them through higher-tier models that process both the preliminary\noutput and original prompt. We evaluate our technique using 350\ncybersecurity-specific prompts, demonstrating substantial improvements in\njailbreak Success Rates (SRs), increasing from 7\\% (no refinements) to 62\\%\nafter three refinement iterations, while maintaining a cost of 7.5\\textcent~per\nprompt. Comparative A/B testing across nine different LLM evaluators confirms\nthat outputs from additional refinement steps are consistently rated as more\nmalicious and technically superior. Moreover, manual code analysis reveals that\ngenerated outputs execute with minimal modification, although optimal\ndeployment typically requires target-specific fine-tuning. With eventual\nimproved harmful code generation, these results highlight critical\nvulnerabilities in current LLM safety frameworks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12937v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12937v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.333,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a jailbreaking technique for LLMs through iterative refinement of prompts, without any mention of training models using human feedback, reward models, or reinforcement learning. It deals solely with exploiting existing LLMs, not aligning them via RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an iterative refinement process for generating code from prompts using LLMs, but it does not involve diffusion models or adapt the diffusion process for multi-step logical reasoning. The iterations are general LLM-based refinements, not a holistic Chain-of-Thought correction as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12938",
      "title": "Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian\n  Splatting and Bag of Embeddings",
      "authors": [
        "Abdalla Arafa",
        "Didier Stricker"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Novel view synthesis has seen significant advancements with 3D Gaussian\nSplatting (3DGS), enabling real-time photorealistic rendering. However, the\ninherent fuzziness of Gaussian Splatting presents challenges for 3D scene\nunderstanding, restricting its broader applications in AR/VR and robotics.\nWhile recent works attempt to learn semantics via 2D foundation model\ndistillation, they inherit fundamental limitations: alpha blending averages\nsemantics across objects, making 3D-level understanding impossible. We propose\na paradigm-shifting alternative that bypasses differentiable rendering for\nsemantics entirely. Our key insight is to leverage predecomposed object-level\nGaussians and represent each object through multiview CLIP feature aggregation,\ncreating comprehensive \"bags of embeddings\" that holistically describe objects.\nThis allows: (1) accurate open-vocabulary object retrieval by comparing text\nqueries to object-level (not Gaussian-level) embeddings, and (2) seamless task\nadaptation: propagating object IDs to pixels for 2D segmentation or to\nGaussians for 3D extraction. Experiments demonstrate that our method\neffectively overcomes the challenges of 3D open-vocabulary object extraction\nwhile remaining comparable to state-of-the-art performance in 2D\nopen-vocabulary segmentation, ensuring minimal compromise.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12938v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12938v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.263,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.302,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on 3D scene understanding using Gaussian Splatting and CLIP embeddings for semantic extraction, without any involvement of diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not address adapting diffusion for complex logical tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12939",
      "title": "Sy-FAR: Symmetry-based Fair Adversarial Robustness",
      "authors": [
        "Haneen Najjar",
        "Eyal Ronen",
        "Mahmood Sharif"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Security-critical machine-learning (ML) systems, such as face-recognition\nsystems, are susceptible to adversarial examples, including real-world\nphysically realizable attacks. Various means to boost ML's adversarial\nrobustness have been proposed; however, they typically induce unfair\nrobustness: It is often easier to attack from certain classes or groups than\nfrom others. Several techniques have been developed to improve adversarial\nrobustness while seeking perfect fairness between classes. Yet, prior work has\nfocused on settings where security and fairness are less critical. Our insight\nis that achieving perfect parity in realistic fairness-critical tasks, such as\nface recognition, is often infeasible -- some classes may be highly similar,\nleading to more misclassifications between them. Instead, we suggest that\nseeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful\nas from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable\nbecause class resemblance is a symmetric relation in most domains.\nAdditionally, as we prove theoretically, symmetry between individuals induces\nsymmetry between any set of sub-groups, in contrast to other fairness notions\nwhere group-fairness is often elusive. We develop Sy-FAR, a technique to\nencourage symmetry while also optimizing adversarial robustness and extensively\nevaluate it using five datasets, with three model architectures, including\nagainst targeted and untargeted realistic attacks. The results show Sy-FAR\nsignificantly improves fair adversarial robustness compared to state-of-the-art\nmethods. Moreover, we find that Sy-FAR is faster and more consistent across\nruns. Notably, Sy-FAR also ameliorates another type of unfairness we discover\nin this work -- target classes that adversarial examples are likely to be\nclassified into become significantly less vulnerable after inducing symmetry.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12939v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12939v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.367,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12945",
      "title": "FusionMAE: large-scale pretrained model to optimize and simplify\n  diagnostic and control of fusion plasma",
      "authors": [
        "Zongyu Yang",
        "Zhenghao Yang",
        "Wenjing Tian",
        "Jiyuan Li",
        "Xiang Sun",
        "Guohui Zheng",
        "Songfen Liu",
        "Niannian Wu",
        "Rongpeng Li",
        "Zhaohe Xu",
        "Bo Li",
        "Zhongbing Shi",
        "Zhe Gao",
        "Wei Chen",
        "Xiaoquan Ji",
        "Min Xu",
        "Wulyu Zhong"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In magnetically confined fusion device, the complex, multiscale, and\nnonlinear dynamics of plasmas necessitate the integration of extensive\ndiagnostic systems to effectively monitor and control plasma behaviour. The\ncomplexity and uncertainty arising from these extensive systems and their\ntangled interrelations has long posed a significant obstacle to the\nacceleration of fusion energy development. In this work, a large-scale model,\nfusion masked auto-encoder (FusionMAE) is pre-trained to compress the\ninformation from 88 diagnostic signals into a concrete embedding, to provide a\nunified interface between diagnostic systems and control actuators. Two\nmechanisms are proposed to ensure a meaningful embedding: compression-reduction\nand missing-signal reconstruction. Upon completion of pre-training, the model\nacquires the capability for 'virtual backup diagnosis', enabling the inference\nof missing diagnostic data with 96.7% reliability. Furthermore, the model\ndemonstrates three emergent capabilities: automatic data analysis, universal\ncontrol-diagnosis interface, and enhancement of control performance on multiple\ntasks. This work pioneers large-scale AI model integration in fusion energy,\ndemonstrating how pre-trained embeddings can simplify the system interface,\nreducing necessary diagnostic systems and optimize operation performance for\nfuture fusion reactors.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12945v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12945v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.426,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a masked auto-encoder for compressing diagnostic signals in fusion plasma, emphasizing data reconstruction and control interfaces. There is no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "The paper discusses the development and application of a pretrained model for fusion plasma diagnostics but does not address distributed training techniques, parallel computing, or multi-node strategies for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12951",
      "title": "Black-box Model Merging for Language-Model-as-a-Service with Massive\n  Model Repositories",
      "authors": [
        "Shilian Chen",
        "Jie Zhou",
        "Tianyu Huai",
        "Yujiang Lu",
        "Junsong Li",
        "Bihao Zhan",
        "Qianjun Pan",
        "Yutao Yang",
        "Xin Li",
        "Qin Chen",
        "Hang Yan",
        "Liang He"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Model merging refers to the process of integrating multiple distinct models\ninto a unified model that preserves and combines the strengths and capabilities\nof the individual models. Most existing approaches rely on task vectors to\ncombine models, typically under the assumption that model parameters are\naccessible. However, for extremely large language models (LLMs) such as GPT-4,\nwhich are often provided solely as black-box services through API interfaces\n(Language-Model-as-a-Service), model weights are not available to end users.\nThis presents a significant challenge, which we refer to as black-box model\nmerging (BMM) with massive LLMs. To address this challenge, we propose a\nderivative-free optimization framework based on the evolutionary algorithm\n(Evo-Merging) that enables effective model merging using only inference-time\nAPI queries. Our method consists of two key components: (1) sparsity-based\ndenoising, designed to identify and filter out irrelevant or redundant\ninformation across models, and (2) sign-aware scaling, which dynamically\ncomputes optimal combination weights for the relevant models based on their\nperformance. We also provide a formal justification, along with a theoretical\nanalysis, for our asymmetric sparsification. Extensive experimental evaluations\ndemonstrate that our approach achieves state-of-the-art results on a range of\ntasks, significantly outperforming existing strong baselines.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12951v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12951v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.453,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on black-box model merging using evolutionary algorithms to combine language models via API queries, with components like sparsity-based denoising and sign-aware scaling. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic.",
      "distributed_training_justification": "The paper addresses model merging for pre-existing language models without parameter access, relying on API-based optimization. It does not discuss distributed training techniques, parallel computing for accelerating model training, or partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12958",
      "title": "Forget What's Sensitive, Remember What Matters: Token-Level Differential\n  Privacy in Memory Sculpting for Continual Learning",
      "authors": [
        "Bihao Zhan",
        "Jie Zhou",
        "Junsong Li",
        "Yutao Yang",
        "Shilian Chen",
        "Qianjun Pan",
        "Xin Li",
        "Wen Wu",
        "Xingjiao Wu",
        "Qin Chen",
        "Hang Yan",
        "Liang He"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Continual Learning (CL) models, while adept at sequential knowledge\nacquisition, face significant and often overlooked privacy challenges due to\naccumulating diverse information. Traditional privacy methods, like a uniform\nDifferential Privacy (DP) budget, indiscriminately protect all data, leading to\nsubstantial model utility degradation and hindering CL deployment in\nprivacy-sensitive areas. To overcome this, we propose a privacy-enhanced\ncontinual learning (PeCL) framework that forgets what's sensitive and remembers\nwhat matters. Our approach first introduces a token-level dynamic Differential\nPrivacy strategy that adaptively allocates privacy budgets based on the\nsemantic sensitivity of individual tokens. This ensures robust protection for\nprivate entities while minimizing noise injection for non-sensitive, general\nknowledge. Second, we integrate a privacy-guided memory sculpting module. This\nmodule leverages the sensitivity analysis from our dynamic DP mechanism to\nintelligently forget sensitive information from the model's memory and\nparameters, while explicitly preserving the task-invariant historical knowledge\ncrucial for mitigating catastrophic forgetting. Extensive experiments show that\nPeCL achieves a superior balance between privacy preserving and model utility,\noutperforming baseline models by maintaining high accuracy on previous tasks\nwhile ensuring robust privacy.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12958v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12958v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.424,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on privacy mechanisms in continual learning, specifically token-level differential privacy and memory sculpting, without any mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses continual learning with differential privacy and does not involve diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes.",
      "distributed_training_justification": "The paper's contributions are centered on privacy enhancements in continual learning, with no discussion of distributed training, parallel computing, data partitioning across nodes, or multi-processor acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12959",
      "title": "Time-step Mixup for Efficient Spiking Knowledge Transfer from Appearance\n  to Event Domain",
      "authors": [
        "Yuqi Xie",
        "Shuhan Ye",
        "Chong Wang",
        "Jiazhen Xu",
        "Le Shen",
        "Yuanbin Qian",
        "Jiangbo Qian"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The integration of event cameras and spiking neural networks holds great\npromise for energy-efficient visual processing. However, the limited\navailability of event data and the sparse nature of DVS outputs pose challenges\nfor effective training. Although some prior work has attempted to transfer\nsemantic knowledge from RGB datasets to DVS, they often overlook the\nsignificant distribution gap between the two modalities. In this paper, we\npropose Time-step Mixup knowledge transfer (TMKT), a novel fine-grained mixing\nstrategy that exploits the asynchronous nature of SNNs by interpolating RGB and\nDVS inputs at various time-steps. To enable label mixing in cross-modal\nscenarios, we further introduce modality-aware auxiliary learning objectives.\nThese objectives support the time-step mixup process and enhance the model's\nability to discriminate effectively across different modalities. Our approach\nenables smoother knowledge transfer, alleviates modality shift during training,\nand achieves superior performance in spiking image classification tasks.\nExtensive experiments demonstrate the effectiveness of our method across\nmultiple datasets. The code will be released after the double-blind review\nprocess.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12959v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12959v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.392,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves developing a Time-step Mixup strategy for knowledge transfer in spiking neural networks between RGB and event domains, focusing on data mixing and auxiliary learning objectives. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12960",
      "title": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language\n  Models",
      "authors": [
        "Yuval Weiss",
        "David Demitri Africa",
        "Paula Buttery",
        "Richard Diehl Martinez"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Parameter-efficient methods like LoRA have revolutionised large language\nmodel (LLM) fine-tuning. ReLoRA extends this idea to pretraining by repeatedly\nmerging and reinitialising low-rank adapters, increasing cumulative rank while\nkeeping updates cheap. This aligns well with observations that high-capacity\nmodels learn through locally low-rank trajectories that expand over time. By\ncontrast, recent work suggests that small language models (SLMs) exhibit rank\ndeficiencies and under-utilise their available dimensionality. This raises a\nnatural question: can ReLoRA's rank-expanding update rule \\textit{steer} SLMs\ntoward healthier learning dynamics, mitigating rank bottlenecks in a\ncapacity-constrained regime? We argue SLMs are an ideal testbed: they train\nquickly, enable controlled ablations, and make rank phenomena more measurable.\nWe present the first systematic study of ReLoRA in SLMs (11M-66M parameters),\nevaluating both performance and learning dynamics. Across loss, Paloma\nperplexity, and BLiMP, we find that ReLoRA underperforms full-rank training,\nwith gaps widening at larger scales. Analysis of proportional effective rank\nand condition numbers shows that ReLoRA amplifies existing rank deficiencies\nand induces ill-conditioned updates early in training. Our results suggest that\nwhile ReLoRA's merge-and-restart strategy can expand ranks in larger models, it\ndoes not straightforwardly translate to capacity-limited SLMs, motivating\nadaptive-rank or hybrid-rank approaches for low-compute pretraining.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12960v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12960v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.399,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating ReLoRA, a parameter-efficient method for training small language models, emphasizing low-rank adaptations, learning dynamics, and rank deficiencies. It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI models with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12963",
      "title": "MMMS: Multi-Modal Multi-Surface Interactive Segmentation",
      "authors": [
        "Robin Schön",
        "Julian Lorenz",
        "Katja Ludwig",
        "Daniel Kienzle",
        "Rainer Lienhart"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In this paper, we present a method to interactively create segmentation masks\non the basis of user clicks. We pay particular attention to the segmentation of\nmultiple surfaces that are simultaneously present in the same image. Since\nthese surfaces may be heavily entangled and adjacent, we also present a novel\nextended evaluation metric that accounts for the challenges of this scenario.\nAdditionally, the presented method is able to use multi-modal inputs to\nfacilitate the segmentation task. At the center of this method is a network\narchitecture which takes as input an RGB image, a number of non-RGB modalities,\nan erroneous mask, and encoded clicks. Based on this input, the network\npredicts an improved segmentation mask. We design our architecture such that it\nadheres to two conditions: (1) The RGB backbone is only available as a\nblack-box. (2) To reduce the response time, we want our model to integrate the\ninteraction-specific information after the image feature extraction and the\nmulti-modal fusion. We refer to the overall task as Multi-Modal Multi-Surface\ninteractive segmentation (MMMS). We are able to show the effectiveness of our\nmulti-modal fusion strategy. Using additional modalities, our system reduces\nthe NoC@90 by up to 1.28 clicks per surface on average on DeLiVER and up to\n1.19 on MFNet. On top of this, we are able to show that our RGB-only baseline\nachieves competitive, and in some cases even superior performance when tested\nin a classical, single-mask interactive segmentation scenario.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12963v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12963v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.322,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12965",
      "title": "ICDAR 2025 Competition on FEw-Shot Text line segmentation of ancient\n  handwritten documents (FEST)",
      "authors": [
        "Silvia Zottin",
        "Axel De Nardin",
        "Giuseppe Branca",
        "Claudio Piciarelli",
        "Gian Luca Foresti"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text line segmentation is a critical step in handwritten document image\nanalysis. Segmenting text lines in historical handwritten documents, however,\npresents unique challenges due to irregular handwriting, faded ink, and complex\nlayouts with overlapping lines and non-linear text flow. Furthermore, the\nscarcity of large annotated datasets renders fully supervised learning\napproaches impractical for such materials. To address these challenges, we\nintroduce the Few-Shot Text Line Segmentation of Ancient Handwritten Documents\n(FEST) Competition. Participants are tasked with developing systems capable of\nsegmenting text lines in U-DIADS-TL dataset, using only three annotated images\nper manuscript for training. The competition dataset features a diverse\ncollection of ancient manuscripts exhibiting a wide range of layouts,\ndegradation levels, and non-standard formatting, closely reflecting real-world\nconditions. By emphasizing few-shot learning, FEST competition aims to promote\nthe development of robust and adaptable methods that can be employed by\nhumanities scholars with minimal manual annotation effort, thus fostering\nbroader adoption of automated document analysis tools in historical research.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12965v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12965v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.275,
      "distributed_training_score": 0.314,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12976",
      "title": "SHREC 2025: Protein surface shape retrieval including electrostatic\n  potential",
      "authors": [
        "Taher Yacoub",
        "Camille Depenveiller",
        "Atsushi Tatsuma",
        "Tin Barisin",
        "Eugen Rusakov",
        "Udo Gobel",
        "Yuxu Peng",
        "Shiqiang Deng",
        "Yuki Kagaya",
        "Joon Hong Park",
        "Daisuke Kihara",
        "Marco Guerra",
        "Giorgio Palmieri",
        "Andrea Ranieri",
        "Ulderico Fugacci",
        "Silvia Biasotti",
        "Ruiwen He",
        "Halim Benhabiles",
        "Adnane Cabani",
        "Karim Hammoudi",
        "Haotian Li",
        "Hao Huang",
        "Chunyan Li",
        "Alireza Tehrani",
        "Fanwang Meng",
        "Farnaz Heidar-Zadeh",
        "Tuan-Anh Yang",
        "Matthieu Montes"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This SHREC 2025 track dedicated to protein surface shape retrieval involved 9\nparticipating teams. We evaluated the performance in retrieval of 15 proposed\nmethods on a large dataset of 11,555 protein surfaces with calculated\nelectrostatic potential (a key molecular surface descriptor). The performance\nin retrieval of the proposed methods was evaluated through different metrics\n(Accuracy, Balanced accuracy, F1 score, Precision and Recall). The best\nretrieval performance was achieved by the proposed methods that used the\nelectrostatic potential complementary to molecular surface shape. This\nobservation was also valid for classes with limited data which highlights the\nimportance of taking into account additional molecular surface descriptors.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12976v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12976v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.238,
      "diffusion_reasoning_score": 0.255,
      "distributed_training_score": 0.3,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12980",
      "title": "Improving Accuracy and Efficiency of Implicit Neural Representations:\n  Making SIREN a WINNER",
      "authors": [
        "Hemanth Chandravamsi",
        "Dhanush V. Shenoy",
        "Steven H. Frankel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We identify and address a fundamental limitation of sinusoidal representation\nnetworks (SIRENs), a class of implicit neural representations. SIRENs Sitzmann\net al. (2020), when not initialized appropriately, can struggle at fitting\nsignals that fall outside their frequency support. In extreme cases, when the\nnetwork's frequency support misaligns with the target spectrum, a 'spectral\nbottleneck' phenomenon is observed, where the model yields to a near-zero\noutput and fails to recover even the frequency components that are within its\nrepresentational capacity. To overcome this, we propose WINNER - Weight\nInitialization with Noise for Neural Representations. WINNER perturbs uniformly\ninitialized weights of base SIREN with Gaussian noise - whose noise scales are\nadaptively determined by the spectral centroid of the target signal. Similar to\nrandom Fourier embeddings, this mitigates 'spectral bias' but without\nintroducing additional trainable parameters. Our method achieves\nstate-of-the-art audio fitting and significant gains in image and 3D shape\nfitting tasks over base SIREN. Beyond signal fitting, WINNER suggests new\navenues in adaptive, target-aware initialization strategies for optimizing deep\nneural network training. For code and data visit\ncfdlabtechnion.github.io/siren_square/.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12980v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12980v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.347,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12982",
      "title": "Out of Distribution Detection in Self-adaptive Robots with AI-powered\n  Digital Twins",
      "authors": [
        "Erblin Isaku",
        "Hassan Sartaj",
        "Shaukat Ali",
        "Beatriz Sanguino",
        "Tongtong Wang",
        "Guoyuan Li",
        "Houxiang Zhang",
        "Thomas Peyrucain"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Self-adaptive robots (SARs) in complex, uncertain environments must\nproactively detect and address abnormal behaviors, including\nout-of-distribution (OOD) cases. To this end, digital twins offer a valuable\nsolution for OOD detection. Thus, we present a digital twin-based approach for\nOOD detection (ODiSAR) in SARs. ODiSAR uses a Transformer-based digital twin to\nforecast SAR states and employs reconstruction error and Monte Carlo dropout\nfor uncertainty quantification. By combining reconstruction error with\npredictive variance, the digital twin effectively detects OOD behaviors, even\nin previously unseen conditions. The digital twin also includes an\nexplainability layer that links potential OOD to specific SAR states, offering\ninsights for self-adaptation. We evaluated ODiSAR by creating digital twins of\ntwo industrial robots: one navigating an office environment, and another\nperforming maritime ship navigation. In both cases, ODiSAR forecasts SAR\nbehaviors (i.e., robot trajectories and vessel motion) and proactively detects\nOOD events. Our results showed that ODiSAR achieved high detection performance\n-- up to 98\\% AUROC, 96\\% TNR@TPR95, and 95\\% F1-score -- while providing\ninterpretable insights to support self-adaptation.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12982v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12982v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.383,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a digital twin-based approach for out-of-distribution (OOD) detection in self-adaptive robots, focusing on forecasting robot states, uncertainty quantification, and explainability. It does not involve reinforcement learning, human feedback, reward models, or fine-tuning based on human-ranked data, which are core to RLHF. Therefore, there is no connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12987",
      "title": "Toward PDDL Planning Copilot",
      "authors": [
        "Yarin Benyamin",
        "Argaman Mordoch",
        "Shahaf S. Shperberg",
        "Roni Stern"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being used as autonomous agents\ncapable of performing complicated tasks. However, they lack the ability to\nperform reliable long-horizon planning on their own. This paper bridges this\ngap by introducing the Planning Copilot, a chatbot that integrates multiple\nplanning tools and allows users to invoke them through instructions in natural\nlanguage. The Planning Copilot leverages the Model Context Protocol (MCP), a\nrecently developed standard for connecting LLMs with external tools and\nsystems. This approach allows using any LLM that supports MCP without\ndomain-specific fine-tuning. Our Planning Copilot supports common planning\ntasks such as checking the syntax of planning problems, selecting an\nappropriate planner, calling it, validating the plan it generates, and\nsimulating their execution. We empirically evaluate the ability of our Planning\nCopilot to perform these tasks using three open-source LLMs. The results show\nthat the Planning Copilot highly outperforms using the same LLMs without the\nplanning tools. We also conducted a limited qualitative comparison of our tool\nagainst Chat GPT-5, a very recent commercial LLM. Our results shows that our\nPlanning Copilot significantly outperforms GPT-5 despite relying on a much\nsmaller LLM. This suggests dedicated planning tools may be an effective way to\nenable LLMs to perform planning tasks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12987v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12987v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.333,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on integrating LLMs with external planning tools via the Model Context Protocol to enhance planning capabilities, without any mention of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a system for LLMs to perform planning tasks using tools like PDDL planners, but it does not involve diffusion models, iterative refinement processes for reasoning, or multi-step logical reasoning based on diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12989",
      "title": "PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era",
      "authors": [
        "Xu Zheng",
        "Chenfei Liao",
        "Ziqiao Weng",
        "Kaiyu Lei",
        "Zihao Dongfang",
        "Haocong He",
        "Yuanhuiyi Lyu",
        "Lutao Jiang",
        "Lu Qi",
        "Li Chen",
        "Danda Pani Paudel",
        "Kailun Yang",
        "Linfeng Zhang",
        "Luc Van Gool",
        "Xuming Hu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Omnidirectional vision, using 360-degree vision to understand the\nenvironment, has become increasingly critical across domains like robotics,\nindustrial inspection, and environmental monitoring. Compared to traditional\npinhole vision, omnidirectional vision provides holistic environmental\nawareness, significantly enhancing the completeness of scene perception and the\nreliability of decision-making. However, foundational research in this area has\nhistorically lagged behind traditional pinhole vision. This talk presents an\nemerging trend in the embodied AI era: the rapid development of omnidirectional\nvision, driven by growing industrial demand and academic interest. We highlight\nrecent breakthroughs in omnidirectional generation, omnidirectional perception,\nomnidirectional understanding, and related datasets. Drawing on insights from\nboth academia and industry, we propose an ideal panoramic system architecture\nin the embodied AI era, PANORAMA, which consists of four key subsystems.\nMoreover, we offer in-depth opinions related to emerging trends and\ncross-community impacts at the intersection of panoramic vision and embodied\nAI, along with the future roadmap and open challenges. This overview\nsynthesizes state-of-the-art advancements and outlines challenges and\nopportunities for future research in building robust, general-purpose\nomnidirectional AI systems in the embodied AI era.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12989v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12989v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.35,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12990",
      "title": "Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection",
      "authors": [
        "Boyu Han",
        "Qianqian Xu",
        "Shilong Bao",
        "Zhiyong Yang",
        "Sicong Li",
        "Qingming Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In this report, we address the problem of determining whether a user performs\nan action incorrectly from egocentric video data. To handle the challenges\nposed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted\nMixture-of-Experts (DR-MoE) framework. In the first stage, features are\nextracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are\ncombined through a feature-level expert module. In the second stage, three\nclassifiers are trained with different objectives: reweighted cross-entropy to\nmitigate class imbalance, AUC loss to improve ranking under skewed\ndistributions, and label-aware loss with sharpness-aware minimization to\nenhance calibration and generalization. Their predictions are fused using a\nclassification-level expert module. The proposed method achieves strong\nperformance, particularly in identifying rare and ambiguous mistake instances.\nThe code is available at https://github.com/boyuh/DR-MoE.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12990v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12990v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.343,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12991",
      "title": "Bridging Performance Gaps for Foundation Models: A Post-Training\n  Strategy for ECGFounder",
      "authors": [
        "Ya Zhou",
        "Yujie Yang",
        "Xiaohan Fan",
        "Wei Zhao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.AP (Applications)"
      ],
      "abstract": "ECG foundation models are increasingly popular due to their adaptability\nacross various tasks. However, their clinical applicability is often limited by\nperformance gaps compared to task-specific models, even after pre-training on\nlarge ECG datasets and fine-tuning on target data. This limitation is likely\ndue to the lack of an effective post-training strategy. In this paper, we\npropose a simple yet effective post-training approach to enhance ECGFounder, a\nstate-of-the-art ECG foundation model pre-trained on over 7 million ECG\nrecordings. Experiments on the PTB-XL benchmark show that our approach improves\nthe baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% in\nmacro AUPRC. Additionally, our method outperforms several recent\nstate-of-the-art approaches, including task-specific and advanced\narchitectures. Further evaluation reveals that our method is more stable and\nsample-efficient compared to the baseline, achieving a 9.1% improvement in\nmacro AUROC and a 34.9% improvement in macro AUPRC using just 10% of the\ntraining data. Ablation studies identify key components, such as stochastic\ndepth and preview linear probing, that contribute to the enhanced performance.\nThese findings underscore the potential of post-training strategies to improve\nECG foundation models, and we hope this work will contribute to the continued\ndevelopment of foundation models in the ECG domain.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12991v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12991v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.417,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a post-training strategy to enhance the performance of an ECG foundation model, focusing on techniques like stochastic depth and preview linear probing. It discusses model fine-tuning and performance gains on benchmarks, but there is no mention of distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors. Thus, it does not address or relate to the topic of distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12995",
      "title": "Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized\n  Detectors on In-the-Wild AI Image Detection",
      "authors": [
        "Yue Zhou",
        "Xinan He",
        "Kaiqing Lin",
        "Bing Fan",
        "Feng Ding",
        "Jinhua Zeng",
        "Bin Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While specialized detectors for AI-generated images excel on curated\nbenchmarks, they fail catastrophically in real-world scenarios, as evidenced by\ntheir critically high false-negative rates on `in-the-wild' benchmarks. Instead\nof crafting another specialized `knife' for this problem, we bring a `gun' to\nthe fight: a simple linear classifier on a modern Vision Foundation Model\n(VFM). Trained on identical data, this baseline decisively `outguns' bespoke\ndetectors, boosting in-the-wild accuracy by a striking margin of over 20\\%.\n  Our analysis pinpoints the source of the VFM's `firepower': First, by probing\ntext-image similarities, we find that recent VLMs (e.g., Perception Encoder,\nMeta CLIP2) have learned to align synthetic images with forgery-related\nconcepts (e.g., `AI-generated'), unlike previous versions. Second, we speculate\nthat this is due to data exposure, as both this alignment and overall accuracy\nplummet on a novel dataset scraped after the VFM's pre-training cut-off date,\nensuring it was unseen during pre-training. Our findings yield two critical\nconclusions: 1) For the real-world `gunfight' of AI-generated image detection,\nthe raw `firepower' of an updated VFM is far more effective than the\n`craftsmanship' of a static detector. 2) True generalization evaluation\nrequires test data to be independent of the model's entire training history,\nincluding pre-training.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12995v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12995v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.353,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on AI-generated image detection using Vision Foundation Models and compares their performance to specialized detectors, emphasizing data exposure and pre-training effects. It does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses image detection methods and the effectiveness of Vision Foundation Models, but it does not adapt diffusion processes for multi-step logical reasoning or treat reasoning paths as entities for iterative refinement. There is no mention of diffusion models in the context of logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12997",
      "title": "Drone Detection Using a Low-Power Neuromorphic Virtual Tripwire",
      "authors": [
        "Anton Eldeborg Lundin",
        "Rasmus Winzell",
        "Hanna Hamrell",
        "David Gustafsson",
        "Hannes Ovrén"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Small drones are an increasing threat to both military personnel and civilian\ninfrastructure, making early and automated detection crucial. In this work we\ndevelop a system that uses spiking neural networks and neuromorphic cameras\n(event cameras) to detect drones. The detection model is deployed on a\nneuromorphic chip making this a fully neuromorphic system. Multiple detection\nunits can be deployed to create a virtual tripwire which detects when and where\ndrones enter a restricted zone. We show that our neuromorphic solution is\nseveral orders of magnitude more energy efficient than a reference solution\ndeployed on an edge GPU, allowing the system to run for over a year on battery\npower. We investigate how synthetically generated data can be used for\ntraining, and show that our model most likely relies on the shape of the drone\nrather than the temporal characteristics of its propellers. The small size and\nlow power consumption allows easy deployment in contested areas or locations\nthat lack power infrastructure.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12997v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12997v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.356,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12999",
      "title": "Data-driven Methods of Extracting Text Structure and Information\n  Transfer",
      "authors": [
        "Shinichi Honna",
        "Taichi Murayama",
        "Akira Matsui"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The Anna Karenina Principle (AKP) holds that success requires satisfying a\nsmall set of essential conditions, whereas failure takes diverse forms. We test\nAKP, its reverse, and two further patterns described as ordered and noisy\nacross novels, online encyclopedias, research papers, and movies. Texts are\nrepresented as sequences of functional blocks, and convergence is assessed in\ntransition order and position. Results show that structural principles vary by\nmedium: novels follow reverse AKP in order, Wikipedia combines AKP with ordered\npatterns, academic papers display reverse AKP in order but remain noisy in\nposition, and movies diverge by genre. Success therefore depends on structural\nconstraints that are specific to each medium, while failure assumes different\nshapes across domains.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.12999v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12999v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.311,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on data-driven analysis of text structures across domains like novels and research papers, testing principles such as the Anna Karenina Principle using functional blocks and convergence metrics. It does not involve diffusion models, iterative refinement processes, or any adaptation for solving complex logical tasks. There is no mention of treating a 'Chain-of-Thought' as an entity for holistic correction, making the paper entirely unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13011",
      "title": "A Visualized Framework for Event Cooperation with Generative Agents",
      "authors": [
        "Yuyang Tian",
        "Shunqiang Mao",
        "Wenchang Gao",
        "Lanlan Qiu",
        "Tianxing He"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionized the simulation of agent\nsocieties, enabling autonomous planning, memory formation, and social\ninteractions. However, existing frameworks often overlook systematic\nevaluations for event organization and lack visualized integration with\nphysically grounded environments, limiting agents' ability to navigate spaces\nand interact with items realistically. We develop MiniAgentPro, a visualization\nplatform featuring an intuitive map editor for customizing environments and a\nsimulation player with smooth animations. Based on this tool, we introduce a\ncomprehensive test set comprising eight diverse event scenarios with basic and\nhard variants to assess agents' ability. Evaluations using GPT-4o demonstrate\nstrong performance in basic settings but highlight coordination challenges in\nhard variants.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13011v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13011v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.346,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a visualization platform for generative agents using Large Language Models (LLMs) for event simulation and coordination, with evaluations on agent interactions in physical environments. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13013",
      "title": "Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single\n  Image",
      "authors": [
        "Gaofeng Liu",
        "Hengsen Li",
        "Ruoyu Gao",
        "Xuetong Li",
        "Zhiyuan Ma",
        "Tao Fang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With the rapid advancement of 3D representation techniques and generative\nmodels, substantial progress has been made in reconstructing full-body 3D\navatars from a single image. However, this task remains fundamentally\nill-posedness due to the limited information available from monocular input,\nmaking it difficult to control the geometry and texture of occluded regions\nduring generation. To address these challenges, we redesign the reconstruction\npipeline and propose Dream3DAvatar, an efficient and text-controllable\ntwo-stage framework for 3D avatar generation. In the first stage, we develop a\nlightweight, adapter-enhanced multi-view generation model. Specifically, we\nintroduce the Pose-Adapter to inject SMPL-X renderings and skeletal information\ninto SDXL, enforcing geometric and pose consistency across views. To preserve\nfacial identity, we incorporate ID-Adapter-G, which injects high-resolution\nfacial features into the generation process. Additionally, we leverage BLIP2 to\ngenerate high-quality textual descriptions of the multi-view images, enhancing\ntext-driven controllability in occluded regions. In the second stage, we design\na feedforward Transformer model equipped with a multi-view feature fusion\nmodule to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS)\nfrom the generated images. Furthermore, we introduce ID-Adapter-R, which\nutilizes a gating mechanism to effectively fuse facial features into the\nreconstruction process, improving high-frequency detail recovery. Extensive\nexperiments demonstrate that our method can generate realistic, animation-ready\n3D avatars without any post-processing and consistently outperforms existing\nbaselines across multiple evaluation metrics.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13013v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13013v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.319,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes SDXL, a diffusion-based model, for generating multi-view images in 3D avatar reconstruction. However, it does not involve adapting the diffusion process for multi-step logical reasoning, such as treating a Chain-of-Thought as a single entity for holistic correction. The focus is on visual generation and reconstruction tasks, not on solving complex logical problems, so it lacks the required components for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13021",
      "title": "xOffense: An AI-driven autonomous penetration testing framework with\n  offensive knowledge-enhanced LLMs and multi agent systems",
      "authors": [
        "Phung Duc Luong",
        "Le Tran Gia Bao",
        "Nguyen Vu Khai Tam",
        "Dong Huu Nguyen Khoa",
        "Nguyen Huu Quyen",
        "Van-Hau Pham",
        "Phan The Duy"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This work introduces xOffense, an AI-driven, multi-agent penetration testing\nframework that shifts the process from labor-intensive, expert-driven manual\nefforts to fully automated, machine-executable workflows capable of scaling\nseamlessly with computational infrastructure. At its core, xOffense leverages a\nfine-tuned, mid-scale open-source LLM (Qwen3-32B) to drive reasoning and\ndecision-making in penetration testing. The framework assigns specialized\nagents to reconnaissance, vulnerability scanning, and exploitation, with an\norchestration layer ensuring seamless coordination across phases. Fine-tuning\non Chain-of-Thought penetration testing data further enables the model to\ngenerate precise tool commands and perform consistent multi-step reasoning. We\nevaluate xOffense on two rigorous benchmarks: AutoPenBench and\nAI-Pentest-Benchmark. The results demonstrate that xOffense consistently\noutperforms contemporary methods, achieving a sub-task completion rate of\n79.17%, decisively surpassing leading systems such as VulnBot and PentestGPT.\nThese findings highlight the potential of domain-adapted mid-scale LLMs, when\nembedded within structured multi-agent orchestration, to deliver superior,\ncost-efficient, and reproducible solutions for autonomous penetration testing.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13021v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13021v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.381,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on fine-tuning an LLM (Qwen3-32B) using Chain-of-Thought penetration testing data, which appears to be supervised fine-tuning rather than RLHF. There is no mention of training a reward model with human-ranked data or using reinforcement learning based on human feedback to align the model. While the introduction references general RL approaches in penetration testing, xOffense itself does not incorporate RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes Chain-of-Thought prompting for multi-step reasoning in the LLM, but it does not involve diffusion models or an iterative refinement process typical of diffusion-based reasoning. There is no description of treating the reasoning path as a holistic entity for correction over steps, as required for this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13023",
      "title": "Validating Solidity Code Defects using Symbolic and Concrete Execution\n  powered by Large Language Models",
      "authors": [
        "Ştefan-Claudiu Susan",
        "Andrei Arusoaie",
        "Dorel Lucanu"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The high rate of false alarms from static analysis tools and Large Language\nModels (LLMs) complicates vulnerability detection in Solidity Smart Contracts,\ndemanding methods that can formally or empirically prove the presence of\ndefects. This paper introduces a novel detection pipeline that integrates\ncustom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is\ndesigned to reliably detect defects and generate proofs. We currently perform\nexperiments with promising results for seven types of critical defects. We\ndemonstrate the pipeline's efficacy by presenting our findings for three\nvulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control\nPolicies -- that are challenging for current verification solutions, which\noften generate false alarms or fail to detect them entirely. We highlight the\npotential of either symbolic or concrete execution in correctly classifying\nsuch code faults. By chaining these instruments, our method effectively\nvalidates true positives, significantly reducing the manual verification\nburden. Although we identify potential limitations, such as the inconsistency\nand the cost of LLMs, our findings establish a robust framework for combining\nheuristic analysis with formal verification to achieve more reliable and\nautomated smart contract auditing.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13023v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13023v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.327,
      "distributed_training_score": 0.312,
      "datasets_score": 0.262,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13025",
      "title": "GView: A Survey of Binary Forensics via Visual, Semantic, and\n  AI-Enhanced Analysis",
      "authors": [
        "Raul Zaharia",
        "Dragoş Gavriluţ",
        "Gheorghiţă Mutu"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Cybersecurity threats continue to become more sophisticated and diverse in\ntheir artifacts, boosting both their volume and complexity. To overcome those\nchallenges, we present GView, an open-source forensic analysis framework with\nvisual and AI-enhanced reasoning. It started with focus on the practical\ncybersecurity industry. It has evolved significantly, incorporating large\nlanguage models (LLMs) to dynamically enhance reasoning and ease the forensic\nworkflows. This paper surveys both the current state of GView with its\npublished papers alongside those that are in the publishing process. It also\nincludes its innovative use of logical inference through predicates and\ninference rules for both the analyzed documents and the user's actions for\nbetter suggestions. We highlight the extensible architecture, showcasing its\npotential as a bridge between the practical forensics worlds with the academic\nresearch.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13025v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13025v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.305,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on GView, a framework for binary forensics that incorporates large language models (LLMs) for reasoning, logical inference via predicates and rules, and visual analysis. However, it does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The main contributions involve automated extraction, correlation, visual reasoning, and expert guidance, which are unrelated to the specific topic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13031",
      "title": "Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual\n  Reasoning in Vision-Language Models",
      "authors": [
        "Yan Chen",
        "Long Li",
        "Teng Xi",
        "Long Zeng",
        "Jingdong Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reinforcement learning (RL) has proven highly effective in eliciting the\nreasoning capabilities of large language models (LLMs). Inspired by this\nsuccess, recent studies have explored applying similar techniques to\nvision-language models (VLMs), aiming to enhance their reasoning performance.\nHowever, directly transplanting RL methods from LLMs to VLMs is suboptimal, as\nthe tasks faced by VLMs are inherently more complex. Specifically, VLMs must\nfirst accurately perceive and understand visual inputs before reasoning can be\neffectively performed. To address this challenge, we propose a two-stage\nreinforcement learning framework designed to jointly enhance both the\nperceptual and reasoning capabilities of VLMs. To mitigate the vanishing\nadvantage issue commonly observed in RL training, we first perform\ndataset-level sampling to selectively strengthen specific capabilities using\ndistinct data sources. During training, the first stage focuses on improving\nthe model's visual perception through coarse- and fine-grained visual\nunderstanding, while the second stage targets the enhancement of reasoning\nabilities. After the proposed two-stage reinforcement learning process, we\nobtain PeBR-R1, a vision-language model with significantly enhanced perceptual\nand reasoning capabilities. Experimental results on seven benchmark datasets\ndemonstrate the effectiveness of our approach and validate the superior\nperformance of PeBR-R1 across diverse visual reasoning tasks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13031v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13031v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.454,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.526,
      "distributed_training_score": 0.34,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a reinforcement learning framework using automated reward signals (e.g., from FGCLIP and rule-based methods) to enhance VLMs, but it does not involve human-ranked data or a separate reward model trained on human feedback, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a two-stage reinforcement learning approach for improving perception and reasoning in VLMs, with no mention of diffusion models, iterative refinement processes, or treating reasoning as a holistically corrected chain-of-thought.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13032",
      "title": "Introducing the A2AJ's Canadian Legal Data: An open-source alternative\n  to CanLII for the era of computational law",
      "authors": [
        "Simon Wallace",
        "Sean Rehaag"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The Access to Algorithmic Justice project (A2AJ) is an open-source\nalternative to the Canadian Legal Information Institute (CanLII). At a moment\nwhen technology promises to enable new ways of working with law, CanLII is\nbecoming an impediment to the free access of law and access to justice\nmovements because it restricts bulk and programmatic access to Canadian legal\ndata. This means that Canada is staring down a digital divide: well-resourced\nactors have the best new technological tools and, because CanLII has disclaimed\nleadership, the public only gets second-rate tools. This article puts CanLII in\nits larger historical context and shows how long and deep efforts to\ndemocratize access to Canadian legal data are, and how often they are thwarted\nby private industry. We introduce the A2AJ's Canadian Legal Data project, which\nprovides open access to over 116,000 court decisions and 5,000 statutes through\nmultiple channels including APIs, machine learning datasets, and AI integration\nprotocols. Through concrete examples, we demonstrate how open legal data\nenables courts to conduct evidence-based assessments and allows developers to\ncreate tools for practitioners serving low-income communities.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13032v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13032v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.248,
      "distributed_training_score": 0.227,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13046",
      "title": "MIA-EPT: Membership Inference Attack via Error Prediction for Tabular\n  Data",
      "authors": [
        "Eyal German",
        "Daniel Samira",
        "Yuval Elovici",
        "Asaf Shabtai"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Synthetic data generation plays an important role in enabling data sharing,\nparticularly in sensitive domains like healthcare and finance. Recent advances\nin diffusion models have made it possible to generate realistic, high-quality\ntabular data, but they may also memorize training records and leak sensitive\ninformation. Membership inference attacks (MIAs) exploit this vulnerability by\ndetermining whether a record was used in training. While MIAs have been studied\nin images and text, their use against tabular diffusion models remains\nunderexplored despite the unique risks of structured attributes and limited\nrecord diversity. In this paper, we introduce MIAEPT, Membership Inference\nAttack via Error Prediction for Tabular Data, a novel black-box attack\nspecifically designed to target tabular diffusion models. MIA-EPT constructs\nerrorbased feature vectors by masking and reconstructing attributes of target\nrecords, disclosing membership signals based on how well these attributes are\npredicted. MIA-EPT operates without access to the internal components of the\ngenerative model, relying only on its synthetic data output, and was shown to\ngeneralize across multiple state-of-the-art diffusion models. We validate\nMIA-EPT on three diffusion-based synthesizers, achieving AUC-ROC scores of up\nto 0.599 and TPR@10% FPR values of 22.0% in our internal tests. Under the MIDST\n2025 competition conditions, MIA-EPT achieved second place in the Black-box\nMulti-Table track (TPR@10% FPR = 20.0%). These results demonstrate that our\nmethod can uncover substantial membership leakage in synthetic tabular data,\nchallenging the assumption that synthetic data is inherently\nprivacy-preserving. Our code is publicly available at\nhttps://github.com/eyalgerman/MIA-EPT.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13046v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13046v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.314,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a membership inference attack on diffusion models for generating synthetic tabular data, focusing on privacy risks and error prediction. It does not involve adapting diffusion models for solving complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity for iterative correction, as required by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13047",
      "title": "Multi-Model Synthetic Training for Mission-Critical Small Language\n  Models",
      "authors": [
        "Nolan Platt",
        "Pragyansmita Nayak"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their application to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing overfitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models -- when fine\ntuned properly -- can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expanding research in the growing field of specialized small\nlanguage models, our approach has immediate applications in maritime safety,\nsecurity operations, and vessel traffic management systems in various\nindustries.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13047v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13047v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.468,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.433,
      "datasets_score": 0.414,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves generating synthetic question-answer pairs using LLMs as a high-level source for labels, which aligns directly with weak supervision. This approach programmatically creates large quantities of training data from noisy or imprecise sources (e.g., LLMs like GPT-4o), bypassing the need for hand-labeled data, and is used to train smaller models effectively.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion-based models or iterative refinement processes for reasoning. It focuses on synthetic data generation using standard LLMs (GPT-4o and o3-mini) and multi-model strategies to prevent overfitting, without any mention of diffusion techniques for logical tasks.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node setups. It describes fine-tuning on a single GPU (e.g., NVIDIA H100) and focuses on data generation and model fine-tuning, with no discussion of partitioning data or computation across multiple processors.",
      "datasets_justification": "The paper centers on creating and utilizing a synthetic dataset (21,543 question-answer pairs from AIS data) for training specialized language models, which fits research on dataset generation, curation, and evaluation. It introduces a new dataset, discusses methodologies to ensure diversity and accuracy, and benchmarks its effectiveness for maritime AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper presents a novel method for training small language models (SLMs) for maritime intelligence by using large language models like GPT-4o and o3-mini to generate synthetic question-answer pairs from 3.2 billion Automatic Identification System (AIS) records, achieving a 261x cost reduction and 75% accuracy with the fine-tuned Qwen2.5-7B model. The core objectives are to address the scarcity of domain-specific data and reduce inference costs, employing a multi-model generation strategy to prevent overfitting, while the key findings demonstrate that properly fine-tuned smaller models can match the performance of larger ones in specialized tasks, with applications in maritime safety and security.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining multi-model synthetic data generation to prevent overfitting in specialized domains, though it builds on existing techniques for synthetic data creation rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like synthetic data generation for specialized AI applications, such as maritime intelligence, due to its cost-effective framework, but its influence may remain limited to niche areas rather than broadly affecting general research or commercial sectors.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a strong, practical contribution to cost-efficient AI for domain-specific tasks, making it valuable for researchers in AI and specialized applications, though it is not essential for those outside these areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/540b91b2d440a8de6d2c6fa2700f774c50e25b88",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Nolan Platt",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380611060"
        },
        {
          "name": "Pragyansmita Nayak",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380611166"
        }
      ]
    },
    {
      "id": "2509.13067",
      "title": "HERO: Rethinking Visual Token Early Dropping in High-Resolution Large\n  Vision-Language Models",
      "authors": [
        "Xu Li",
        "Yuxuan Liang",
        "Xiaolei Chen",
        "Yi Zheng",
        "Haotian Chen",
        "Bin Li",
        "Xiangyang Xue"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "By cropping high-resolution images into local tiles and encoding them\nindependently, High-Resolution Large Vision-Language Models (HR-LVLMs) have\ndemonstrated remarkable fine-grained visual understanding capabilities.\nHowever, this divide-and-conquer paradigm significantly increases the number of\nvisual tokens, resulting in substantial computational and memory overhead. To\nbetter understand and address this challenge, we empirically investigate visual\ntoken utilization in HR-LVLMs and uncover three key findings: (1) the local\ntiles have varying importance, jointly determined by visual saliency and task\nrelevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage\nattention pattern across layers, with each stage attending to different types\nof visual tokens; (3) the visual tokens emphasized at different stages encode\ninformation at varying levels of granularity, playing complementary roles\nwithin LVLMs. Building on these insights, we propose HERO, a High-resolution\nvisual token early dropping framework that integrates content-adaptive token\nbudget allocation with function-aware token selection. By accurately estimating\ntile-level importance and selectively retaining visual tokens with\ncomplementary roles, HERO achieves superior efficiency-accuracy trade-offs\nacross diverse benchmarks and model scales, all in a training-free manner. This\nstudy provides both empirical insights and practical solutions toward efficient\ninference in HR-LVLMs.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13067v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13067v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.395,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of HERO, a framework for efficient visual token processing in high-resolution vision-language models, focusing on inference optimization through token early dropping. It involves empirical analysis and techniques like token budget allocation, but does not mention reinforcement learning, human feedback, reward models, or any mechanisms for aligning models with human preferences. Thus, it has no connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13070",
      "title": "TFANet: Three-Stage Image-Text Feature Alignment Network for Robust\n  Referring Image Segmentation",
      "authors": [
        "Qianqi Lu",
        "Yuxiang Xie",
        "Jing Zhang",
        "Shiwei Zou",
        "Yan Chen",
        "Xidao Luan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Referring Image Segmentation (RIS) is a task that segments image regions\nbased on language expressions, requiring fine-grained alignment between two\nmodalities. However, existing methods often struggle with multimodal\nmisalignment and language semantic loss, especially in complex scenes\ncontaining multiple visually similar objects, where uniquely described targets\nare frequently mislocalized or incompletely segmented. To tackle these\nchallenges, this paper proposes TFANet, a Three-stage Image-Text Feature\nAlignment Network that systematically enhances multimodal alignment through a\nhierarchical framework comprising three stages: Knowledge Plus Stage (KPS),\nKnowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the\nfirst stage, we design the Multiscale Linear Cross-Attention Module (MLAM),\nwhich facilitates bidirectional semantic exchange between visual features and\ntextual representations across multiple scales. This establishes rich and\nefficient alignment between image regions and different granularities of\nlinguistic descriptions. Subsequently, the KFS further strengthens feature\nalignment through the Cross-modal Feature Scanning Module (CFSM), which applies\nmultimodal selective scanning to capture long-range dependencies and construct\na unified multimodal representation. This is essential for modeling long-range\ncross-modal dependencies and enhancing alignment accuracy in complex scenes.\nFinally, in the KIS, we propose the Word-level Linguistic Feature-guided\nSemantic Deepening Module (WFDM) to compensate for semantic degradation\nintroduced in earlier stages.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13070v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13070v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.364,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes TFANet, a network for referring image segmentation that uses stages like KPS, KFS, and KIS with modules for feature alignment and iterative refinement in mask generation. However, it does not involve diffusion models, iterative refinement for complex logical tasks, or treating a 'Chain-of-Thought' as a single entity for holistic correction. The focus is on multimodal alignment in vision-language tasks, not multi-step logical reasoning via diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13077",
      "title": "A Design Co-Pilot for Task-Tailored Manipulators",
      "authors": [
        "Jonathan Külz",
        "Sehoon Ha",
        "Matthias Althoff"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Although robotic manipulators are used in an ever-growing range of\napplications, robot manufacturers typically follow a ``one-fits-all''\nphilosophy, employing identical manipulators in various settings. This often\nleads to suboptimal performance, as general-purpose designs fail to exploit\nparticularities of tasks. The development of custom, task-tailored robots is\nhindered by long, cost-intensive development cycles and the high cost of\ncustomized hardware. Recently, various computational design methods have been\ndevised to overcome the bottleneck of human engineering. In addition, a surge\nof modular robots allows quick and economical adaptation to changing industrial\nsettings. This work proposes an approach to automatically designing and\noptimizing robot morphologies tailored to a specific environment. To this end,\nwe learn the inverse kinematics for a wide range of different manipulators. A\nfully differentiable framework realizes gradient-based fine-tuning of designed\nrobots and inverse kinematics solutions. Our generative approach accelerates\nthe generation of specialized designs from hours with optimization-based\nmethods to seconds, serving as a design co-pilot that enables instant\nadaptation and effective human-AI collaboration. Numerical experiments show\nthat our approach finds robots that can navigate cluttered environments,\nmanipulators that perform well across a specified workspace, and can be adapted\nto different hardware constraints. Finally, we demonstrate the real-world\napplicability of our method by setting up a modular robot designed in\nsimulation that successfully moves through an obstacle course.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13077v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13077v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.354,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13081",
      "title": "Shaping Explanations: Semantic Reward Modeling with Encoder-Only\n  Transformers for GRPO",
      "authors": [
        "Francesco Pappone",
        "Ruggero Marino Lazzaroni",
        "Federico Califano",
        "Niccolò Gentile",
        "Roberto Marras"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While Large Language Models (LLMs) excel at generating human-like text,\naligning their outputs with complex, qualitative goals like pedagogical\nsoundness remains a significant challenge. Standard reinforcement learning\ntechniques often rely on slow and expensive LLM-as-a-judge evaluations or on\nbrittle, keyword-based metrics like ROUGE, which fail to capture the semantic\nessence of a high-quality explanation. In this work, we introduce a novel\napproach to reward shaping within the Group Relative Policy Optimisation (GRPO)\nframework. Our central contribution is the use of a small, efficient\nencoder-only transformer as a semantic reward model. This model provides a\ndense, semantically rich reward signal based on the cosine similarity between a\ngenerated explanation and a ground-truth reference, guiding the policy towards\nexplanations that are not just factually correct but also structurally and\nconceptually aligned with expert reasoning. We apply this method to the task of\ntraining a model for the Italian medical-school entrance examinations,\nfollowing standard domain-adaptive continued pre-training (CPT) and supervised\nfine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic\nreward significantly improves explanation faithfulness and clarity over a\nstrong SFT baseline, showcasing the power of using lightweight encoder models\nfor nuanced reward shaping in complex generation tasks",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13081v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13081v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.475,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.483,
      "distributed_training_score": 0.347,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning (specifically GRPO) with a reward model for aligning model outputs, which shares conceptual similarities with RLHF. However, it does not involve training a reward model on human-ranked data; instead, it relies on semantic similarity to ground-truth references, making it not true RLHF as defined.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reward shaping with encoder-only transformers in GRPO for generating explanations, with no mention of diffusion models, iterative refinement processes, or treating chain-of-thought as a holistically corrected entity over multiple steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13083",
      "title": "Using KL-Divergence to Focus Frequency Information in Low-Light Image\n  Enhancement",
      "authors": [
        "Yan Xingyang",
        "Huang Xiaohong",
        "Zhang Zhao",
        "You Tian",
        "Xu Ziheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In the Fourier domain, luminance information is primarily encoded in the\namplitude spectrum, while spatial structures are captured in the phase\ncomponents. The traditional Fourier Frequency information fitting employs\npixel-wise loss functions, which tend to focus excessively on local information\nand may lead to global information loss. In this paper, we present LLFDisc, a\nU-shaped deep enhancement network that integrates cross-attention and gating\nmechanisms tailored for frequency-aware enhancement. We propose a novel\ndistribution-aware loss that directly fits the Fourier-domain information and\nminimizes their divergence using a closed-form KL-Divergence objective. This\nenables the model to align Fourier-domain information more robustly than with\nconventional MSE-based losses. Furthermore, we enhance the perceptual loss\nbased on VGG by embedding KL-Divergence on extracted deep features, enabling\nbetter structural fidelity. Extensive experiments across multiple benchmarks\ndemonstrate that LLFDisc achieves state-of-the-art performance in both\nqualitative and quantitative evaluations. Our code will be released at:\nhttps://github.com/YanXY000/LLFDisc",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13083v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13083v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.334,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13084",
      "title": "Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation\n  with Uncertainty-Guided Pseudo-Labeling",
      "authors": [
        "Yunyao Lu",
        "Yihang Wu",
        "Ahmad Chaddad",
        "Tareef Daqqaq",
        "Reem Kateb"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite the remarkable performance of supervised medical image segmentation\nmodels, relying on a large amount of labeled data is impractical in real-world\nsituations. Semi-supervised learning approaches aim to alleviate this challenge\nusing unlabeled data through pseudo-label generation. Yet, existing\nsemi-supervised segmentation methods still suffer from noisy pseudo-labels and\ninsufficient supervision within the feature space. To solve these challenges,\nthis paper proposes a novel semi-supervised 3D medical image segmentation\nframework based on a dual-network architecture. Specifically, we investigate a\nCross Consistency Enhancement module using both cross pseudo and\nentropy-filtered supervision to reduce the noisy pseudo-labels, while we design\na dynamic weighting strategy to adjust the contributions of pseudo-labels using\nan uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In\naddition, we use a self-supervised contrastive learning mechanism to align\nuncertain voxel features with reliable class prototypes by effectively\ndifferentiating between trustworthy and uncertain predictions, thus reducing\nprediction uncertainty. Extensive experiments are conducted on three 3D\nsegmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed\napproach consistently exhibits superior performance across various settings\n(e.g., 89.95\\% Dice score on left Atrial with 10\\% labeled data) compared to\nthe state-of-the-art methods. Furthermore, the usefulness of the proposed\nmodules is further validated via ablation experiments.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13084v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13084v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.473,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.334,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using pseudo-labels generated from model predictions to train on unlabeled data, which directly aligns with weak supervision. It programmatically creates noisy or imprecise labels (e.g., via Cross Consistency Enhancement and uncertainty-guided mechanisms) to supplement limited labeled data, reducing reliance on hand-labeled annotations and addressing the core principles of weak supervision in medical image segmentation.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper proposes a novel semi-supervised framework for 3D medical image segmentation that addresses noisy pseudo-labels and insufficient feature-space supervision using a dual-network architecture. It incorporates a Cross Consistency Enhancement module with cross pseudo supervision and entropy-filtered techniques, an uncertainty-aware weighting strategy based on Kullback-Leibler divergence to dynamically adjust pseudo-label contributions, and a self-supervised contrastive learning mechanism to align uncertain voxel features with reliable class prototypes. Experimental results on datasets such as Left Atrial, NIH Pancreas, and BraTS-2019 demonstrate superior performance, including a 89.95% Dice score on Left Atrial with only 10% labeled data, outperforming state-of-the-art methods and validating the proposed modules through ablation studies.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing techniques like cross pseudo supervision, entropy filtering, and contrastive learning in a dual-network framework for medical image segmentation, offering a clever integration that advances handling of noisy labels, though it does not introduce entirely new problems or architectures.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in semi-supervised medical image segmentation by providing effective strategies for managing noisy pseudo-labels and enhancing feature learning, potentially leading to citations and applications within the subfield of computer vision for medical imaging.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a high-quality contribution with practical advancements in semi-supervised learning for medical image segmentation, making it valuable for researchers in the field to understand and build upon, though it is not essential for those outside this specific area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6f2d31b84167d8d292cff1ddbd72bba956529b72",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 6,
      "average_h_index": 3.4,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yunyao Lu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362998310"
        },
        {
          "name": "Yihang Wu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2260821946"
        },
        {
          "name": "Ahmad Chaddad",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2275606211"
        },
        {
          "name": "T. Daqqaq",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/1456253780"
        },
        {
          "name": "Reem Kateb",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/31850798"
        }
      ]
    },
    {
      "id": "2509.13089",
      "title": "A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual\n  Assembly Control",
      "authors": [
        "Jonas Werheid",
        "Shengjie He",
        "Aymen Gannouni",
        "Anas Abdelrazeq",
        "Robert H. Schmitt"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Quality control of assembly processes is essential in manufacturing to ensure\nnot only the quality of individual components but also their proper integration\ninto the final product. To assist in this matter, automated assembly control\nusing computer vision methods has been widely implemented. However, the costs\nassociated with image acquisition, annotation, and training of computer vision\nalgorithms pose challenges for integration, especially for small- and\nmedium-sized enterprises (SMEs), which often lack the resources for extensive\ntraining, data collection, and manual image annotation. Synthetic data offers\nthe potential to reduce manual data collection and labeling. Nevertheless, its\npractical application in the context of assembly quality remains limited. In\nthis work, we present a novel approach for easily integrable and data-efficient\nvisual assembly control. Our approach leverages simulated scene generation\nbased on computer-aided design (CAD) data and object detection algorithms. The\nresults demonstrate a time-saving pipeline for generating image data in\nmanufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95)\nup to 99,5% for correctly identifying instances of synthetic planetary gear\nsystem components within our simulated training data, and up to 93% when\ntransferred to real-world camera-captured testing data. This research\nhighlights the effectiveness of synthetic data generation within an adaptable\npipeline and underscores its potential to support SMEs in implementing\nresource-efficient visual assembly control solutions.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13089v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13089v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.331,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves generating synthetic data and annotations programmatically using CAD models and simulation tools like Blender and BlenderProc, which aligns closely with weak supervision. By automating label creation for training object detection models, it reduces reliance on manual annotation and provides large quantities of labels from simulated sources, potentially noisy due to the Sim2Real gap. This directly supports weak supervision's goal of using high-level, imprecise sources for training data, making it highly relevant for resource-constrained SMEs.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper presents a synthetic data pipeline aimed at helping small- and medium-sized enterprises (SMEs) implement visual assembly control in manufacturing by reducing the need for extensive real-world data collection and annotation. Utilizing computer-aided design (CAD) data, Blender, and BlenderProc for simulated scene generation, the pipeline integrates with object detection algorithms to train models that achieve up to 99.5% mean Average Precision (mAP) on synthetic data and 93% on real-world data for detecting planetary gear components, demonstrating its effectiveness and potential for resource-efficient adoption.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper combines existing tools like Blender and BlenderProc with object detection for a tailored pipeline in visual assembly control, offering a notable improvement in accessibility for SMEs without introducing entirely new techniques. While it addresses a practical gap, it builds on prior work rather than pioneering a fundamentally new problem or method.",
      "impact_score": "Moderate",
      "impact_justification": "The work could influence resource-constrained manufacturing sectors by promoting synthetic data use in visual control, potentially leading to wider adoption in SMEs and related subfields. However, its impact is likely confined to specific applications in computer vision and robotics, rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a practical and innovative pipeline that addresses real-world challenges for SMEs, making it valuable for researchers and practitioners in manufacturing and computer vision. While not essential for all, it provides actionable insights that could inspire further developments in the area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/901c34c3f0ce761830ff6c45f4d936931b4e9d5c",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 6,
      "average_h_index": 2.6,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Jonas Werheid",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2238960924"
        },
        {
          "name": "Shengjie He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2301105500"
        },
        {
          "name": "Aymen Gannouni",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2020710550"
        },
        {
          "name": "Anas Abdelrazeq",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2238960686"
        },
        {
          "name": "Robert H. Schmitt",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2238591843"
        }
      ]
    },
    {
      "id": "2509.13107",
      "title": "Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery\n  Detection -- The 2024 Global Deepfake Image Detection Challenge",
      "authors": [
        "Kohou Wang",
        "Huan Hu",
        "Xiang Liu",
        "Zezhou Chen",
        "Ping Chen",
        "Zhaoxiang Liu",
        "Shiguo Lian"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The proliferation of sophisticated deepfake technology poses significant\nchallenges to digital security and authenticity. Detecting these forgeries,\nespecially across a wide spectrum of manipulation techniques, requires robust\nand generalized models. This paper introduces the Hierarchical Deep Fusion\nFramework (HDFF), an ensemble-based deep learning architecture designed for\nhigh-performance facial forgery detection. Our framework integrates four\ndiverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT,\nwhich are meticulously fine-tuned through a multi-stage process on the\nMultiFFDI dataset. By concatenating the feature representations from these\nspecialized models and training a final classifier layer, HDFF effectively\nleverages their collective strengths. This approach achieved a final score of\n0.96852 on the competition's private leaderboard, securing the 20th position\nout of 184 teams, demonstrating the efficacy of hierarchical fusion for complex\nimage classification tasks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13107v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13107v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.388,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a framework for facial forgery detection using an ensemble of pre-trained models, focusing on image classification and feature fusion. It does not involve diffusion models, iterative refinement processes, or any multi-step logical reasoning tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13116",
      "title": "Weakly and Self-Supervised Class-Agnostic Motion Prediction for\n  Autonomous Driving",
      "authors": [
        "Ruibo Li",
        "Hanyu Shi",
        "Zhe Wang",
        "Guosheng Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Understanding motion in dynamic environments is critical for autonomous\ndriving, thereby motivating research on class-agnostic motion prediction. In\nthis work, we investigate weakly and self-supervised class-agnostic motion\nprediction from LiDAR point clouds. Outdoor scenes typically consist of mobile\nforegrounds and static backgrounds, allowing motion understanding to be\nassociated with scene parsing. Based on this observation, we propose a novel\nweakly supervised paradigm that replaces motion annotations with fully or\npartially annotated (1%, 0.1%) foreground/background masks for supervision. To\nthis end, we develop a weakly supervised approach utilizing\nforeground/background cues to guide the self-supervised learning of motion\nprediction models. Since foreground motion generally occurs in non-ground\nregions, non-ground/ground masks can serve as an alternative to\nforeground/background masks, further reducing annotation effort. Leveraging\nnon-ground/ground cues, we propose two additional approaches: a weakly\nsupervised method requiring fewer (0.01%) foreground/background annotations,\nand a self-supervised method without annotations. Furthermore, we design a\nRobust Consistency-aware Chamfer Distance loss that incorporates multi-frame\ninformation and robust penalty functions to suppress outliers in\nself-supervised learning. Experiments show that our weakly and self-supervised\nmodels outperform existing self-supervised counterparts, and our weakly\nsupervised models even rival some supervised ones. This demonstrates that our\napproaches effectively balance annotation effort and performance.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13116v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13116v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.463,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.346,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves developing weakly supervised methods for motion prediction in autonomous driving, such as WeakMotion-FB and WeakMotion-NG, which use partially annotated foreground/background or non-ground/ground masks (e.g., 1%, 0.1%, or 0.01% annotations) instead of full motion labels. This directly aligns with the definition of weak supervision, as it relies on high-level, noisy, or imprecise sources (e.g., masks) to generate training signals, reducing the need for perfectly hand-labeled data. The paper demonstrates effective performance with minimal annotations, making it a clear and central application of weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper presents weakly and self-supervised methods for class-agnostic motion prediction in autonomous driving using LiDAR point clouds, aiming to reduce the reliance on expensive motion annotations by leveraging partially annotated foreground/background masks or non-ground/ground cues. The authors propose novel approaches, including WeakMotion-FB and WeakMotion-NG for weak supervision, and SelfMotion-NG for full self-supervision, along with a new Robust Consistency-aware Chamfer Distance loss to handle outliers, demonstrating that these methods outperform existing self-supervised techniques and achieve performance comparable to some fully supervised models, effectively balancing annotation effort and accuracy.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly novel weakly supervised paradigm for class-agnostic motion prediction, which is the first of its kind, and proposes new methods and a robust loss function that significantly advance self-supervised learning in this domain.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in autonomous driving and computer vision by providing practical ways to reduce annotation needs, though its impact may be confined to specific subfields like motion prediction from LiDAR data.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers high-quality innovations in weakly supervised learning for motion prediction, making it a valuable contribution that researchers in autonomous driving and computer vision should be aware of for its practical advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b95b761f44c1fbcc76a11463bca31169eaad4aa5",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 12,
      "average_h_index": 4.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Ruibo Li",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2150924478"
        },
        {
          "name": "Hanyu Shi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2273646186"
        },
        {
          "name": "Zhe Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2274086585"
        },
        {
          "name": "Guosheng Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363912564"
        }
      ]
    },
    {
      "id": "2509.13131",
      "title": "Reasoning with Preference Constraints: A Benchmark for Language Models\n  in Many-to-One Matching Markets",
      "authors": [
        "Marylou Fauchard",
        "Florian Carichon",
        "Margarida Carvalho",
        "Golnoosh Farnadi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in reasoning with large language models (LLMs) have\ndemonstrated strong performance on complex mathematical tasks, including\ncombinatorial optimization. Techniques such as Chain-of-Thought and In-Context\nLearning have further enhanced this capability, making LLMs both powerful and\naccessible tools for a wide range of users, including non-experts. However,\napplying LLMs to matching problems, which require reasoning under preferential\nand structural constraints, remains underexplored. To address this gap, we\nintroduce a novel benchmark of 369 instances of the College Admission Problem,\na canonical example of a matching problem with preferences, to evaluate LLMs\nacross key dimensions: feasibility, stability, and optimality. We employ this\nbenchmark to assess the performance of several open-weight LLMs. Our results\nfirst reveal that while LLMs can satisfy certain constraints, they struggle to\nmeet all evaluation criteria consistently. They also show that reasoning LLMs,\nlike QwQ and GPT-oss, significantly outperform traditional models such as\nLlama, Qwen or Mistral, defined here as models used without any dedicated\nreasoning mechanisms. Moreover, we observed that LLMs reacted differently to\nthe various prompting strategies tested, which include Chain-of-Thought,\nIn-Context Learning and role-based prompting, with no prompt consistently\noffering the best performance. Finally, we report the performances from\niterative prompting with auto-generated feedback and show that they are not\nmonotonic; they can peak early and then significantly decline in later\nattempts. Overall, this work offers a new perspective on model reasoning\nperformance and the effectiveness of prompting strategies in combinatorial\noptimization problems with preferential constraints.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13131v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13131v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.472,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.367,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates existing LLMs on a benchmark for matching problems using various prompting strategies, but it does not involve training or fine-tuning models with human feedback, a reward model, or reinforcement learning techniques. The mention of iterative prompting with auto-generated feedback is not equivalent to RLHF, as it lacks human involvement.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses prompting strategies like Chain-of-Thought and iterative prompting for reasoning in matching problems, but it does not adapt or use diffusion models for iterative refinement of reasoning paths. There is no evidence of a diffusion-based process for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13132",
      "title": "An Uncertainty-Weighted Decision Transformer for Navigation in Dense,\n  Complex Driving Scenarios",
      "authors": [
        "Zhihao Zhang",
        "Chengyang Peng",
        "Minghao Zhu",
        "Ekim Yurtsever",
        "Keith A. Redmill"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autonomous driving in dense, dynamic environments requires decision-making\nsystems that can exploit both spatial structure and long-horizon temporal\ndependencies while remaining robust to uncertainty. This work presents a novel\nframework that integrates multi-channel bird's-eye-view occupancy grids with\ntransformer-based sequence modeling for tactical driving in complex roundabout\nscenarios. To address the imbalance between frequent low-risk states and rare\nsafety-critical decisions, we propose the Uncertainty-Weighted Decision\nTransformer (UWDT). UWDT employs a frozen teacher transformer to estimate\nper-token predictive entropy, which is then used as a weight in the student\nmodel's loss function. This mechanism amplifies learning from uncertain,\nhigh-impact states while maintaining stability across common low-risk\ntransitions. Experiments in a roundabout simulator, across varying traffic\ndensities, show that UWDT consistently outperforms other baselines in terms of\nreward, collision rate, and behavioral stability. The results demonstrate that\nuncertainty-aware, spatial-temporal transformers can deliver safer and more\nefficient decision-making for autonomous driving in complex traffic\nenvironments.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13132v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13132v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.385,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on transformer-based models for autonomous driving, specifically using Decision Transformers with uncertainty weighting to handle decision-making in complex scenarios. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought generation. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13133",
      "title": "Advancing Real-World Parking Slot Detection with Large-Scale Dataset and\n  Semi-Supervised Baseline",
      "authors": [
        "Zhihao Zhang",
        "Chunyu Lin",
        "Lang Nie",
        "Jiyuan Wang",
        "Yao Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "As automatic parking systems evolve, the accurate detection of parking slots\nhas become increasingly critical. This study focuses on parking slot detection\nusing surround-view cameras, which offer a comprehensive bird's-eye view of the\nparking environment. However, the current datasets are limited in scale, and\nthe scenes they contain are seldom disrupted by real-world noise (e.g., light,\nocclusion, etc.). Moreover, manual data annotation is prone to errors and\nomissions due to the complexity of real-world conditions, significantly\nincreasing the cost of annotating large-scale datasets. To address these\nissues, we first construct a large-scale parking slot detection dataset (named\nCRPS-D), which includes various lighting distributions, diverse weather\nconditions, and challenging parking slot variants. Compared with existing\ndatasets, the proposed dataset boasts the largest data scale and consists of a\nhigher density of parking slots, particularly featuring more slanted parking\nslots. Additionally, we develop a semi-supervised baseline for parking slot\ndetection, termed SS-PSD, to further improve performance by exploiting\nunlabeled data. To our knowledge, this is the first semi-supervised approach in\nparking slot detection, which is built on the teacher-student model with\nconfidence-guided mask consistency and adaptive feature perturbation.\nExperimental results demonstrate the superiority of SS-PSD over the existing\nstate-of-the-art (SoTA) solutions on both the proposed dataset and the existing\ndataset. Particularly, the more unlabeled data there is, the more significant\nthe gains brought by our semi-supervised scheme. The relevant source codes and\nthe dataset have been made publicly available at\nhttps://github.com/zzh362/CRPS-D.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13133v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13133v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.429,
      "diffusion_reasoning_score": 0.307,
      "distributed_training_score": 0.387,
      "datasets_score": 0.424,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces a semi-supervised baseline (SS-PSD) for parking slot detection, which leverages unlabeled data through a teacher-student framework, confidence-guided mask consistency, and adaptive feature perturbation. This directly aligns with weak supervision by programmatically generating and refining labels from noisy or imprecise sources, reducing the need for extensive manual annotations.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution is the creation and analysis of a new large-scale dataset (CRPS-D) for parking slot detection, including details on its scale, diversity, benchmarking against existing datasets, and evaluation of algorithm performance. This fits squarely within research on dataset curation, introduction, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of existing parking slot detection datasets and methods by introducing CRPS-D, a large-scale dataset featuring diverse real-world conditions such as various lighting, weather, and complex scenes with a high density of parking slots, and proposing SS-PSD, a novel semi-supervised baseline using a teacher-student model with confidence-guided mask consistency and adaptive feature perturbation to leverage unlabeled data. The methodology involves dataset collection and annotation, along with the development of the SS-PSD framework, and key findings show that it outperforms state-of-the-art methods on both the new and existing datasets, with performance gains increasing with more unlabeled data, making the code and dataset publicly available.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new large-scale dataset (CRPS-D) and the first semi-supervised approach (SS-PSD) for parking slot detection, significantly advancing the field by addressing real-world challenges and reducing annotation needs. This represents a truly novel contribution, as it tackles a new problem variant and provides a fresh technique not previously explored in this domain.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in parking slot detection and autonomous driving by providing a new benchmark dataset and method, potentially leading to citations and improvements in related subfields. However, its impact may be confined to computer vision applications in vehicle systems rather than broader domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with a new dataset and innovative semi-supervised method that advances parking slot detection, making it essential for researchers in autonomous driving and computer vision to be aware of. While not universally groundbreaking, its practical implications and public resources justify reading for relevant specialists.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2628690617dcd966556dc7706196c37be2854d91",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 16,
      "average_h_index": 9.4,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Zhihao Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380627978"
        },
        {
          "name": "Chunyu Lin",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/49043799"
        },
        {
          "name": "Lang Nie",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/2034846820"
        },
        {
          "name": "Jiyuan Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2257129326"
        },
        {
          "name": "Yao Zhao",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/2129511552"
        }
      ]
    },
    {
      "id": "2509.13137",
      "title": "Agentic AI for Financial Crime Compliance",
      "authors": [
        "Henrik Axelsen",
        "Valdemar Licht",
        "Jan Damsgaard"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "The cost and complexity of financial crime compliance (FCC) continue to rise,\noften without measurable improvements in effectiveness. While AI offers\npotential, most solutions remain opaque and poorly aligned with regulatory\nexpectations. This paper presents the design and deployment of an agentic AI\nsystem for FCC in digitally native financial platforms. Developed through an\nAction Design Research (ADR) process with a fintech firm and regulatory\nstakeholders, the system automates onboarding, monitoring, investigation, and\nreporting, emphasizing explainability, traceability, and compliance-by-design.\nUsing artifact-centric modeling, it assigns clearly bounded roles to autonomous\nagents and enables task-specific model routing and audit logging. The\ncontribution includes a reference architecture, a real-world prototype, and\ninsights into how Agentic AI can reconfigure FCC workflows under regulatory\nconstraints. Our findings extend IS literature on AI-enabled compliance by\ndemonstrating how automation, when embedded within accountable governance\nstructures, can support transparency and institutional trust in high-stakes,\nregulated environments.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13137v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13137v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.268,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the design and deployment of an agentic AI system for financial crime compliance, focusing on automation, explainability, and regulatory alignment through Action Design Research. It does not involve reinforcement learning, human feedback for model training, or any elements of RLHF, such as using human-ranked data to fine-tune models.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13149",
      "title": "MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation",
      "authors": [
        "Minqing Huang",
        "Shouyi Lu",
        "Boyuan Zheng",
        "Ziyao Li",
        "Xiao Tang",
        "Guirong Zhuo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "4D radar super-resolution, which aims to reconstruct sparse and noisy point\nclouds into dense and geometrically consistent representations, is a\nfoundational problem in autonomous perception. However, existing methods often\nsuffer from high training cost or rely on complex diffusion-based sampling,\nresulting in high inference latency and poor generalization, making it\ndifficult to balance accuracy and efficiency. To address these limitations, we\npropose MSDNet, a multi-stage distillation framework that efficiently transfers\ndense LiDAR priors to 4D radar features to achieve both high reconstruction\nquality and computational efficiency. The first stage performs\nreconstruction-guided feature distillation, aligning and densifying the\nstudent's features through feature reconstruction. In the second stage, we\npropose diffusion-guided feature distillation, which treats the stage-one\ndistilled features as a noisy version of the teacher's representations and\nrefines them via a lightweight diffusion network. Furthermore, we introduce a\nnoise adapter that adaptively aligns the noise level of the feature with a\npredefined diffusion timestep, enabling a more precise denoising. Extensive\nexperiments on the VoD and in-house datasets demonstrate that MSDNet achieves\nboth high-fidelity reconstruction and low-latency inference in the task of 4D\nradar point cloud super-resolution, and consistently improves performance on\ndownstream tasks. The code will be publicly available upon publication.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13149v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13149v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.397,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for feature refinement in 4D radar super-resolution, specifically for iterative denoising to enhance point clouds. While it involves multi-step processes, these are applied to perceptual and generative tasks in computer vision, not to solving complex logical tasks or Chain-of-Thought reasoning as defined in the topic. There is no component for logical reasoning or holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13151",
      "title": "TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual\n  Document Images",
      "authors": [
        "Rohan Kumar",
        "Jyothi Swaroopa Jinka",
        "Ravi Kiran Sarvadevabhatla"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recognizing textual attributes such as bold, italic, underline and strikeout\nis essential for understanding text semantics, structure, and visual\npresentation. These attributes highlight key information, making them crucial\nfor document analysis. Existing methods struggle with computational efficiency\nor adaptability in noisy, multilingual settings. To address this, we introduce\nTexTAR, a multi-task, context-aware Transformer for Textual Attribute\nRecognition (TAR). Our novel data selection pipeline enhances context\nawareness, and our architecture employs a 2D RoPE (Rotary Positional\nEmbedding)-style mechanism to incorporate input context for more accurate\nattribute predictions. We also introduce MMTAD, a diverse, multilingual,\nmulti-domain dataset annotated with text attributes across real-world documents\nsuch as legal records, notices, and textbooks. Extensive evaluations show\nTexTAR outperforms existing methods, demonstrating that contextual awareness\ncontributes to state-of-the-art TAR performance.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13151v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13151v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.317,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13160",
      "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
      "authors": [
        "Liang Hu",
        "Jianpeng Jiao",
        "Jiashuo Liu",
        "Yanle Ren",
        "Zhoufutu Wen",
        "Kaiyuan Zhang",
        "Xuanliang Zhang",
        "Xiang Gao",
        "Tianci He",
        "Fei Hu",
        "Yali Liao",
        "Zaiyuan Wang",
        "Chenghao Yang",
        "Qianyu Yang",
        "Mingren Yin",
        "Zhiyuan Zeng",
        "Ge Zhang",
        "Xinyi Zhang",
        "Xiying Zhao",
        "Zhenwei Zhu",
        "Hongseok Namkoong",
        "Wenhao Huang",
        "Yuwen Tang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13160v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13160v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.343,
      "datasets_score": 0.397,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13161",
      "title": "Enhancing Video Large Language Models with Structured Multi-Video\n  Collaborative Reasoning (early version)",
      "authors": [
        "Zhihao He",
        "Tianyao He",
        "Tieyuan Chen",
        "Yun Xu",
        "Huabin Liu",
        "Chaofan Gan",
        "Gui Zou",
        "Weiyao Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite the prosperity of the video language model, the current pursuit of\ncomprehensive video reasoning is thwarted by the inherent spatio-temporal\nincompleteness within individual videos, resulting in hallucinations and\ninaccuracies. A promising solution is to augment the reasoning performance with\nmultiple related videos. However, video tokens are numerous and contain\nredundant information, so directly feeding the relevant video data into a large\nlanguage model to enhance responses could be counterproductive. To address this\nchallenge, we propose a multi-video collaborative framework for video language\nmodels. For efficient and flexible video representation, we establish a Video\nStructuring Module to represent the video's knowledge as a spatio-temporal\ngraph. Based on the structured video representation, we design the Graph Fusion\nModule to fuse the structured knowledge and valuable information from related\nvideos into the augmented graph node tokens. Finally, we construct an elaborate\nmulti-video structured prompt to integrate the graph, visual, and textual\ntokens as the input to the large language model. Extensive experiments\nsubstantiate the effectiveness of our framework, showcasing its potential as a\npromising avenue for advancing video language models.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13161v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13161v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.506,
      "distributed_training_score": 0.38,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on enhancing video language models through structured multi-video collaboration using graph-based representations and fusion modules, with no mention of human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs graph attention networks and cross-graph attention for multi-video reasoning, but it does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a single entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13165",
      "title": "On the Correlation between Individual Fairness and Predictive Accuracy\n  in Probabilistic Models",
      "authors": [
        "Alessandro Antonucci",
        "Eric Rossetto",
        "Ivan Duvnjak"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We investigate individual fairness in generative probabilistic classifiers by\nanalysing the robustness of posterior inferences to perturbations in private\nfeatures. Building on established results in robustness analysis, we\nhypothesise a correlation between robustness and predictive accuracy,\nspecifically, instances exhibiting greater robustness are more likely to be\nclassified accurately. We empirically assess this hypothesis using a benchmark\nof fourteen datasets with fairness concerns, employing Bayesian networks as the\nunderlying generative models. To address the computational complexity\nassociated with robustness analysis over multiple private features with\nBayesian networks, we reformulate the problem as a most probable explanation\ntask in an auxiliary Markov random field. Our experiments confirm the\nhypothesis about the correlation, suggesting novel directions to mitigate the\ntraditional trade-off between fairness and accuracy.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13165v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13165v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.347,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is on individual fairness and robustness in generative probabilistic models, specifically using Bayesian networks to analyze correlations between fairness and predictive accuracy. It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI models with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13172",
      "title": "WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory",
      "authors": [
        "Ruifei Ding",
        "Zhe Chen",
        "Wen Fan",
        "Chen Long",
        "Huijuan Xiao",
        "Yelu Zeng",
        "Zhen Dong",
        "Bisheng Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Street trees are vital to urban livability, providing ecological and social\nbenefits. Establishing a detailed, accurate, and dynamically updated street\ntree inventory has become essential for optimizing these multifunctional assets\nwithin space-constrained urban environments. Given that traditional field\nsurveys are time-consuming and labor-intensive, automated surveys utilizing\nMobile Mapping Systems (MMS) offer a more efficient solution. However, existing\nMMS-acquired tree datasets are limited by small-scale scene, limited\nannotation, or single modality, restricting their utility for comprehensive\nanalysis. To address these limitations, we introduce WHU-STree, a cross-city,\nrichly annotated, and multi-modal urban street tree dataset. Collected across\ntwo distinct cities, WHU-STree integrates synchronized point clouds and\nhigh-resolution images, encompassing 21,007 annotated tree instances across 50\nspecies and 2 morphological parameters. Leveraging the unique characteristics,\nWHU-STree concurrently supports over 10 tasks related to street tree inventory.\nWe benchmark representative baselines for two key tasks--tree species\nclassification and individual tree segmentation. Extensive experiments and\nin-depth analysis demonstrate the significant potential of multi-modal data\nfusion and underscore cross-domain applicability as a critical prerequisite for\npractical algorithm deployment. In particular, we identify key challenges and\noutline potential future works for fully exploiting WHU-STree, encompassing\nmulti-modal fusion, multi-task collaboration, cross-domain generalization,\nspatial pattern learning, and Multi-modal Large Language Model for street tree\nasset management. The WHU-STree dataset is accessible at:\nhttps://github.com/WHU-USI3DV/WHU-STree.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13172v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13172v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.265,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.263,
      "distributed_training_score": 0.3,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13175",
      "title": "More performant and scalable: Rethinking contrastive vision-language\n  pre-training of radiology in the LLM era",
      "authors": [
        "Yingtai Li",
        "Haoran Lai",
        "Xiaoqian Zhou",
        "Shuai Ming",
        "Wenxin Ma",
        "Wei Wei",
        "Shaohua Kevin Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The emergence of Large Language Models (LLMs) presents unprecedented\nopportunities to revolutionize medical contrastive vision-language\npre-training. In this paper, we show how LLMs can facilitate large-scale\nsupervised pre-training, thereby advancing vision-language alignment. We begin\nby demonstrate that modern LLMs can automatically extract diagnostic labels\nfrom radiology reports with remarkable precision (>96\\% AUC in our experiments)\nwithout complex prompt engineering, enabling the creation of large-scale\n\"silver-standard\" datasets at a minimal cost (~\\$3 for 50k CT image-report\npairs). Further, we find that vision encoder trained on this \"silver-standard\"\ndataset achieves performance comparable to those trained on labels extracted by\nspecialized BERT-based models, thereby democratizing the access to large-scale\nsupervised pre-training. Building on this foundation, we proceed to reveal that\nsupervised pre-training fundamentally improves contrastive vision-language\nalignment. Our approach achieves state-of-the-art performance using only a 3D\nResNet-18 with vanilla CLIP training, including 83.8\\% AUC for zero-shot\ndiagnosis on CT-RATE, 77.3\\% AUC on RAD-ChestCT, and substantial improvements\nin cross-modal retrieval (MAP@50=53.7\\% for image-image, Recall@100=52.2\\% for\nreport-image). These results demonstrate the potential of utilizing LLMs to\nfacilitate {\\bf more performant and scalable} medical AI systems. Our code is\navaiable at https://github.com/SadVoxel/More-performant-and-scalable.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13175v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13175v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.393,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using LLMs for label extraction and supervised pre-training in vision-language models, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper's core contribution involves using LLMs to programmatically generate \"silver-standard\" labels from radiology reports, which aligns directly with weak supervision by creating large-scale training data from noisy, imprecise sources without manual annotation.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical reasoning, or any multi-step reasoning processes; it centers on label extraction via LLMs and vision-language pre-training.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates the use of Large Language Models (LLMs) to extract diagnostic labels from radiology reports, enabling the creation of large-scale, cost-effective \"silver-standard\" datasets for supervised pre-training in medical AI. The authors demonstrate that vision encoders trained on these LLM-extracted labels achieve performance on par with those using expensive specialized methods, leading to significant improvements in contrastive vision-language alignment, as evidenced by state-of-the-art results in zero-shot diagnosis (e.g., 83.8% AUC on CT-RATE) and cross-modal retrieval tasks using a simple 3D ResNet-18 model.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel application of LLMs for automatic, high-precision label extraction from radiology reports, enabling scalable supervised pre-training that advances state-of-the-art vision-language alignment in medical AI. This represents a significant innovation by combining existing technologies in a new way to address longstanding challenges in data curation and model training.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications in medical AI by democratizing access to large-scale datasets and improving model performance with minimal costs. Its demonstrated improvements in zero-shot tasks and scalability could lead to broader adoption in clinical diagnostics and related fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality, innovative contribution that advances medical computer vision through practical and scalable methods, making it essential for researchers interested in AI for radiology. While impactful, it builds on existing techniques, so it is valuable but not absolutely groundbreaking.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ad1e7fc24850d5ddf6cb562962796d9b6cf8bc4b",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 3,
      "average_h_index": 0.8571428571428571,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yingtai Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380626826"
        },
        {
          "name": "Haoran Lai",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2268676215"
        },
        {
          "name": "Xiaoqian Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380631268"
        },
        {
          "name": "Shuai Ming",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2159773785"
        },
        {
          "name": "Wenxin Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380631392"
        },
        {
          "name": "Wei Wei",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2326924928"
        },
        {
          "name": "Shaohua Kevin Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379971020"
        }
      ]
    },
    {
      "id": "2509.13181",
      "title": "Road Obstacle Video Segmentation",
      "authors": [
        "Shyam Nandan Rai",
        "Shyamgopal Karthik",
        "Mariana-Iuliana Georgescu",
        "Barbara Caputo",
        "Carlo Masone",
        "Zeynep Akata"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With the growing deployment of autonomous driving agents, the detection and\nsegmentation of road obstacles have become critical to ensure safe autonomous\nnavigation. However, existing road-obstacle segmentation methods are applied on\nindividual frames, overlooking the temporal nature of the problem, leading to\ninconsistent prediction maps between consecutive frames. In this work, we\ndemonstrate that the road-obstacle segmentation task is inherently temporal,\nsince the segmentation maps for consecutive frames are strongly correlated. To\naddress this, we curate and adapt four evaluation benchmarks for road-obstacle\nvideo segmentation and evaluate 11 state-of-the-art image- and video-based\nsegmentation methods on these benchmarks. Moreover, we introduce two strong\nbaseline methods based on vision foundation models. Our approach establishes a\nnew state-of-the-art in road-obstacle video segmentation for long-range video\nsequences, providing valuable insights and direction for future research.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13181v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13181v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.309,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13185",
      "title": "Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification\n  with Limited Entropy",
      "authors": [
        "Yunchuan Guan",
        "Yu Liu",
        "Ke Zhou",
        "Zhiqi Shen",
        "Jenq-Neng Hwang",
        "Serge Belongie",
        "Lei Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Meta-learning is a powerful paradigm for tackling few-shot tasks. However,\nrecent studies indicate that models trained with the whole-class training\nstrategy can achieve comparable performance to those trained with meta-learning\nin few-shot classification tasks. To demonstrate the value of meta-learning, we\nestablish an entropy-limited supervised setting for fair comparisons. Through\nboth theoretical analysis and experimental validation, we establish that\nmeta-learning has a tighter generalization bound compared to whole-class\ntraining. We unravel that meta-learning is more efficient with limited entropy\nand is more robust to label noise and heterogeneous tasks, making it\nwell-suited for unsupervised tasks. Based on these insights, We propose MINO, a\nmeta-learning framework designed to enhance unsupervised performance. MINO\nutilizes the adaptive clustering algorithm DBSCAN with a dynamic head for\nunsupervised task construction and a stability-based meta-scaler for robustness\nagainst label noise. Extensive experiments confirm its effectiveness in\nmultiple unsupervised few-shot and zero-shot tasks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13185v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13185v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.462,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.379,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper discusses meta-learning's robustness to label noise in unsupervised few-shot tasks and proposes methods like a stability-based meta-scaler to handle such noise, which aligns with weak supervision's focus on training with noisy or imprecise labels. However, the primary contribution is on comparing meta-learning to whole-class training and enhancing meta-learning frameworks, rather than directly generating labels from high-level sources.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper reevaluates the effectiveness of meta-learning compared to whole-class training in few-shot classification by introducing an entropy-limited supervised setting, demonstrating through theoretical analysis using uniform stability theory and experimental validation that meta-learning offers a tighter generalization error bound and is more efficient with limited entropy, while also being robust to label noise and heterogeneous tasks. The authors propose MINO, a novel meta-learning framework that employs DBSCAN for adaptive task construction and a dynamic head with a meta-scaler for enhanced performance in unsupervised few-shot and zero-shot classification tasks, with extensive experiments confirming its superiority.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing an entropy-limited setting and a new framework (MINO) that combines existing techniques like DBSCAN in a clever way to address unsupervised few-shot classification, though it builds on established meta-learning concepts rather than introducing a entirely new problem.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in meta-learning for unsupervised settings by providing theoretical insights and a practical framework, potentially leading to citations and developments within the subfield of few-shot learning.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable theoretical and practical contributions to meta-learning, making it a significant read for researchers in machine learning and AI focused on few-shot and unsupervised tasks, though it may not be essential for those outside this niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/afd0f42cdc50d4b524340877fa513c7783f9eae4",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 7,
      "average_h_index": 2.4285714285714284,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yun-ping Guan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/14240413"
        },
        {
          "name": "Yu Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2256807084"
        },
        {
          "name": "Ke Zhou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2288592967"
        },
        {
          "name": "Zhiqi Shen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2325243095"
        },
        {
          "name": "Jenq-Neng Hwang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2238945143"
        },
        {
          "name": "Serge Belongie",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2343636950"
        },
        {
          "name": "Lei Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2343640294"
        }
      ]
    },
    {
      "id": "2509.13202",
      "title": "B-TGAT: A Bi-directional Temporal Graph Attention Transformer for\n  Clustering Multivariate Spatiotemporal Data",
      "authors": [
        "Francis Ndikum Nji",
        "Vandana Janaja",
        "Jianwu Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Clustering high-dimensional multivariate spatiotemporal climate data is\nchallenging due to complex temporal dependencies, evolving spatial\ninteractions, and non-stationary dynamics. Conventional clustering methods,\nincluding recurrent and convolutional models, often struggle to capture both\nlocal and global temporal relationships while preserving spatial context. We\npresent a time-distributed hybrid U-Net autoencoder that integrates a\nBi-directional Temporal Graph Attention Transformer (B-TGAT) to guide efficient\ntemporal clustering of multidimensional spatiotemporal climate datasets. The\nencoder and decoder are equipped with ConvLSTM2D modules that extract joint\nspatial--temporal features by modeling localized dynamics and spatial\ncorrelations over time, and skip connections that preserve multiscale spatial\ndetails during feature compression and reconstruction. At the bottleneck,\nB-TGAT integrates graph-based spatial modeling with attention-driven temporal\nencoding, enabling adaptive weighting of temporal neighbors and capturing both\nshort and long-range dependencies across regions. This architecture produces\ndiscriminative latent embeddings optimized for clustering. Experiments on three\ndistinct spatiotemporal climate datasets demonstrate superior cluster\nseparability, temporal stability, and alignment with known climate transitions\ncompared to state-of-the-art baselines. The integration of ConvLSTM2D, U-Net\nskip connections, and B-TGAT enhances temporal clustering performance while\nproviding interpretable insights into complex spatiotemporal variability,\nadvancing both methodological development and climate science applications.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13202v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13202v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.251,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.368,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13203",
      "title": "G-CSEA: A Graph-Based Conflict Set Extraction Algorithm for Identifying\n  Infeasibility in Pseudo-Boolean Models",
      "authors": [
        "Kanishk Garg",
        "Saranya D.",
        "Sanal Kumar",
        "Saurabh Singh",
        "Anupam Purwar"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Workforce scheduling involves a variety of rule-based constraints-such as\nshift limits, staffing policies, working hour restrictions, and many similar\nscheduling rules-which can interact in conflicting ways, leading to infeasible\nmodels. Identifying the underlying causes of such infeasibility is critical for\nresolving scheduling issues and restoring feasibility. A common diagnostic\napproach is to compute Irreducible Infeasible Subsets (IISs): minimal sets of\nconstraints that are jointly infeasible but become feasible when any one is\nremoved. We consider models formulated using pseudo-Boolean constraints with\ninequality relations over binary variables, which naturally encode scheduling\nlogic. Existing IIS extraction methods such as Additive Deletion and\nQuickXplain rely on repeated feasibility checks, often incurring large numbers\nof solver calls. Dual ray analysis, while effective for LP-based models, may\nfail when the relaxed problem is feasible but the underlying pseudo-Boolean\nmodel is not. To address these limitations, we propose Graph-based Conflict Set\nExtraction Algorithm (G-CSEA) to extract a conflict set, an approach inspired\nby Conflict-Driven Clause Learning (CDCL) in SAT solvers. Our method constructs\nan implication graph during constraint propagation and, upon detecting a\nconflict, traces all contributing constraints across both decision branches.\nThe resulting conflict set can optionally be minimized using QuickXplain to\nproduce an IIS.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13203v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13203v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.283,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13210",
      "title": "Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection\n  in Public Surveillance",
      "authors": [
        "Ligang Chang",
        "Shengkai Xu",
        "Liangchang Shen",
        "Binhan Xu",
        "Junqiao Wang",
        "Tianyu Shi",
        "Yanhui Du"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Violence detection in public surveillance is critical for public safety. This\nstudy addresses challenges such as small-scale targets, complex environments,\nand real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal\nframework that integrates an enhanced YOLOv8 with a Temporal Segment Network\n(TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as\na lightweight backbone, an exponential moving average (EMA) attention\nmechanism, and pruning to reduce computational cost while maintaining accuracy.\nYOLOv8 and TSN are trained separately on pedestrian and violence datasets,\nwhere YOLOv8 extracts human regions and TSN performs binary classification of\nviolent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE\nachieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming\nexisting methods in both accuracy and efficiency, demonstrating its\neffectiveness for public safety surveillance. Code is available at\nhttps://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13210v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13210v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.345,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13214",
      "title": "End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting\n  Detection",
      "authors": [
        "Fei Wang",
        "Xuecheng Wu",
        "Zheng Zhang",
        "Danlei Huang",
        "Yuheng Huang",
        "Bo Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The powerful generative capabilities of diffusion models have significantly\nadvanced the field of image synthesis, enhancing both full image generation and\ninpainting-based image editing. Despite their remarkable advancements,\ndiffusion models also raise concerns about potential misuse for malicious\npurposes. However, existing approaches struggle to identify images generated by\ndiffusion-based inpainting models, even when similar inpainted images are\nincluded in their training data. To address this challenge, we propose a novel\ndetection method based on End-to-end denoising diffusion (End4). Specifically,\nEnd4 designs a denoising reconstruction model to improve the alignment degree\nbetween the latent spaces of the reconstruction and detection processes, thus\nreconstructing features that are more conducive to detection. Meanwhile, it\nleverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local\nimage features under the guidance of attention pyramid layers at different\nscales, enhancing feature discriminability. Additionally, to evaluate detection\nperformance on inpainted images, we establish a comprehensive benchmark\ncomprising images generated from five distinct masked regions. Extensive\nexperiments demonstrate that our End4 effectively generalizes to unseen masking\npatterns and remains robust under various perturbations. Our code and dataset\nwill be released soon.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13214v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13214v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.511,
      "distributed_training_score": 0.318,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for detecting forgeries in inpainted images, specifically through end-to-end denoising and feature refinement for image processing tasks. It does not adapt the iterative refinement process of diffusion models to solve complex logical tasks, such as treating a Chain-of-Thought as a single entity for multi-step reasoning. Instead, it applies diffusion for visual reconstruction and detection, lacking any component for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13227",
      "title": "Rich Vehicle Routing Problem in Disaster Management enabling\n  Temporally-causal Transhipments across Multi-Modal Transportation Network",
      "authors": [
        "Santanu Banerjee",
        "Goutam Sen",
        "Siddhartha Mukhopadhyay"
      ],
      "categories": [
        "math.OC (Optimization and Control)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "A rich vehicle routing problem is considered, allowing multiple trips of\nheterogeneous vehicles stationed at geographically distributed vehicle depots\nhaving access to different modes of transportation. The problem arises from the\nreal-world requirement of optimizing the disaster response time by minimizing\nthe makespan of vehicular routes. Multiple diversely-functional vertices are\nconsidered, including Transhipment Ports as inter-modal resource transfer\nstations. Both simultaneous and split pickup and delivery are considered, for\nmultiple cargo types, along with Vehicle-Cargo and Transhipment Port-Cargo\ncompatibilities. The superiority of the proposed cascaded minimization approach\nis demonstrated over the existing makespan minimization approaches through our\ndeveloped Mixed-Integer Linear Programming formulation. To solve the problem\nquickly for practical implementation in a Disaster Management-specific Decision\nSupport System, an extensive Heuristic Algorithm is devised which utilizes\nDecision Tree based structuring of possible routes; the Decision Tree approach\nhelps to inherently capture the compatibility issues, while also explore the\nsolution space through stochastic weights. Preferential generation of small\nroute elements is performed, which are integrated into route clusters; we\nconsider multiple different logical integration approaches, as well as\nshuffling the logics to simultaneously produce multiple independent solutions.\nFinally, perturbations of the different solutions are done to find better\nneighbouring solutions. The computational performance of the PSR-GIP Heuristic,\non our created novel datasets, indicates that it is able to give good solutions\nswiftly for practical problems involving large integer instances that the MILP\nis unable to solve.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13227v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13227v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.259,
      "weak_supervision_score": 0.249,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.304,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13229",
      "title": "Curriculum Multi-Task Self-Supervision Improves Lightweight\n  Architectures for Onboard Satellite Hyperspectral Image Segmentation",
      "authors": [
        "Hugo Carlesso",
        "Josiane Mothe",
        "Radu Tudor Ionescu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Hyperspectral imaging (HSI) captures detailed spectral signatures across\nhundreds of contiguous bands per pixel, being indispensable for remote sensing\napplications such as land-cover classification, change detection, and\nenvironmental monitoring. Due to the high dimensionality of HSI data and the\nslow rate of data transfer in satellite-based systems, compact and efficient\nmodels are required to support onboard processing and minimize the transmission\nof redundant or low-value data, e.g. cloud-covered areas. To this end, we\nintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)\nframework designed for lightweight architectures for HSI analysis. CMTSSL\nintegrates masked image modeling with decoupled spatial and spectral jigsaw\npuzzle solving, guided by a curriculum learning strategy that progressively\nincreases data complexity during self-supervision. This enables the encoder to\njointly capture fine-grained spectral continuity, spatial structure, and global\nsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously\naddresses spatial and spectral reasoning within a unified and computationally\nefficient design, being particularly suitable for training lightweight models\nfor onboard satellite deployment. We validate our approach on four public\nbenchmark datasets, demonstrating consistent gains in downstream segmentation\ntasks, using architectures that are over 16,000x lighter than some\nstate-of-the-art models. These results highlight the potential of CMTSSL in\ngeneralizable representation learning with lightweight architectures for\nreal-world HSI applications. Our code is publicly available at\nhttps://github.com/hugocarlesso/CMTSSL.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13229v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13229v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.436,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.406,
      "datasets_score": 0.391,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on self-supervised learning (SSL) for hyperspectral image segmentation, where supervisory signals are derived directly from the data through pretext tasks like masked image modeling and jigsaw puzzles. It does not involve programmatically generating labels from high-level, noisy, or imprecise sources, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper emphasizes developing lightweight architectures for onboard satellite processing and does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13232",
      "title": "Single-stream Policy Optimization",
      "authors": [
        "Zhongwen Xu",
        "Zihan Ding"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13232v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13232v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.457,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.418,
      "datasets_score": 0.261,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses reinforcement learning (RL) for LLMs, including methods like RLVR, which involve reward-based optimization. However, it does not explicitly mention human feedback, a reward model trained on human-ranked data, or alignment with human preferences, making it only loosely connected to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on policy optimization techniques for RL in LLMs, such as SPO and GRPO, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It does not adapt diffusion principles for reasoning tasks.",
      "distributed_training_justification": "The paper directly addresses distributed training challenges, such as synchronization barriers in group-based methods like GRPO, and proposes SPO as a scalable alternative that eliminates these bottlenecks, achieving significant throughput improvements (e.g., 4.35x speedup) in distributed settings.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Single-stream Policy Optimization (SPO), a novel approach to policy-gradient optimization for Large Language Models (LLMs) that addresses the limitations of group-based methods like GRPO by eliminating degenerate groups and synchronization barriers. SPO employs a persistent, KL-adaptive value tracker for baseline estimation, global advantage normalization for stable learning signals, and adaptive curriculum via prioritized sampling, resulting in smoother convergence, higher accuracy on benchmarks like math datasets (e.g., +3.4 percentage points in average maj@32), and improved scalability in variable-time settings, as demonstrated with Qwen3-8B experiments.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces SPO as a truly innovative technique that reverts to a single-stream paradigm while incorporating new components like a persistent value tracker and global normalization, significantly advancing state-of-the-art by resolving key flaws in existing group-based methods. This represents a fundamental shift that could redefine efficient policy optimization for LLMs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of reinforcement learning for LLMs due to its demonstrated improvements in efficiency and performance on specific benchmarks. However, its influence may be limited to specialized applications rather than broadly across AI or commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with empirical evidence of superior performance and scalability, making it valuable for researchers in AI and machine learning focused on LLM optimization. It provides insightful principles that could guide future work, though it may not be essential for all readers.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/37d7e1b913710dd74c54772067b88527e05bde73",
      "total_authors": 2,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhongwen Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380631153"
        },
        {
          "name": "Zihan Ding",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2509.13234",
      "title": "Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in\n  Diabetic Retinopathy",
      "authors": [
        "Nadim Barakat",
        "William Lotter"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI\nsystems can expand access to fundus photography screening. Current FDA-cleared\nsystems primarily provide binary referral outputs, where this minimal output\nmay limit clinical trust and utility. Yet, determining the most effective\noutput format to enhance clinician-AI performance is an empirical challenge\nthat is difficult to assess at scale. We evaluated multimodal large language\nmodels (MLLMs) for DR detection and their ability to simulate clinical AI\nassistance across different output types. Two models were tested on IDRiD and\nMessidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source\nmedical model. Experiments included: (1) baseline evaluation, (2) simulated AI\nassistance with synthetic predictions, and (3) actual AI-to-AI collaboration\nwhere GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at\nbaseline, achieving higher sensitivity and AUROC, while GPT-4o showed\nnear-perfect specificity but low sensitivity. Both models adjusted predictions\nbased on simulated AI inputs, but GPT-4o's performance collapsed with incorrect\nones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o\nachieved strong results when guided by MedGemma's descriptive outputs, even\nwithout direct image access (AUROC up to 0.96). These findings suggest MLLMs\nmay improve DR screening pipelines and serve as scalable simulators for\nstudying clinical AI assistance across varying output configurations. Open,\nlightweight models such as MedGemma may be especially valuable in low-resource\nsettings, while descriptive outputs could enhance explainability and clinician\ntrust in clinical workflows.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13234v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13234v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.379,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating multimodal LLMs for diabetic retinopathy detection and simulating AI assistance, without any mention of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for multi-step logical reasoning; it instead evaluates MLLMs for prediction and simulation in a clinical context, with no reference to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13235",
      "title": "A Scenario-Driven Cognitive Approach to Next-Generation AI Memory",
      "authors": [
        "Linyue Cai",
        "Yuyang Cheng",
        "Xiaoding Shao",
        "Huiming Wang",
        "Yong Zhao",
        "Wei Zhang",
        "Kang Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As artificial intelligence advances toward artificial general intelligence\n(AGI), the need for robust and human-like memory systems has become\nincreasingly evident. Current memory architectures often suffer from limited\nadaptability, insufficient multimodal integration, and an inability to support\ncontinuous learning. To address these limitations, we propose a scenario-driven\nmethodology that extracts essential functional requirements from representative\ncognitive scenarios, leading to a unified set of design principles for\nnext-generation AI memory systems. Based on this approach, we introduce the\n\\textbf{COgnitive Layered Memory Architecture (COLMA)}, a novel framework that\nintegrates cognitive scenarios, memory processes, and storage mechanisms into a\ncohesive design. COLMA provides a structured foundation for developing AI\nsystems capable of lifelong learning and human-like reasoning, thereby\ncontributing to the pragmatic development of AGI.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13235v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13235v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.443,
      "distributed_training_score": 0.326,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a cognitive memory architecture (COLMA) for AI systems, emphasizing adaptability, multimodal integration, and lifelong learning for AGI. It does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. As such, there is no component related to treating a 'Chain-of-Thought' as a single entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13236",
      "title": "Layout-Aware OCR for Black Digital Archives with Unsupervised Evaluation",
      "authors": [
        "Fitsum Sileshi Beyene",
        "Christopher L. Dancy"
      ],
      "categories": [
        "cs.DL (Digital Libraries)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite their cultural and historical significance, Black digital archives\ncontinue to be a structurally underrepresented area in AI research and\ninfrastructure. This is especially evident in efforts to digitize historical\nBlack newspapers, where inconsistent typography, visual degradation, and\nlimited annotated layout data hinder accurate transcription, despite the\navailability of various systems that claim to handle optical character\nrecognition (OCR) well. In this short paper, we present a layout-aware OCR\npipeline tailored for Black newspaper archives and introduce an unsupervised\nevaluation framework suited to low-resource archival contexts. Our approach\nintegrates synthetic layout generation, model pretraining on augmented data,\nand a fusion of state-of-the-art You Only Look Once (YOLO) detectors. We used\nthree annotation-free evaluation metrics, the Semantic Coherence Score (SCS),\nRegion Entropy (RE), and Textual Redundancy Score (TRS), which quantify\nlinguistic fluency, informational diversity, and redundancy across OCR regions.\nOur evaluation on a 400-page dataset from ten Black newspaper titles\ndemonstrates that layout-aware OCR improves structural diversity and reduces\nredundancy compared to full-page baselines, with modest trade-offs in\ncoherence. Our results highlight the importance of respecting cultural layout\nlogic in AI-driven document understanding and lay the foundation for future\ncommunity-driven and ethically grounded archival AI systems.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13236v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13236v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.329,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes curating a new 400-page benchmark dataset from historical Black newspapers, detailing its sources (e.g., titles like Freedom’s Journal), and using it for evaluation in OCR and layout analysis. This directly involves dataset creation, curation methodologies (e.g., selecting from digitized archives), and benchmark evaluation, aligning closely with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of digitizing historical Black newspaper archives by introducing a layout-aware OCR pipeline that incorporates synthetic layout generation, model pretraining on augmented data, and YOLO-based detectors to improve text recognition accuracy in the face of inconsistent typography and degradation. It also proposes an unsupervised evaluation framework using metrics like Semantic Coherence Score (SCS), Region Entropy (RE), and Textual Redundancy Score (TRS), demonstrating on a 400-page dataset that this approach enhances structural diversity and reduces redundancy compared to baselines, while emphasizing the cultural significance of layout in ethical AI for archives.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever adaptation of existing techniques like YOLO for a specific underrepresented domain and introduces new unsupervised evaluation metrics, offering a notable improvement rather than a entirely novel architecture. However, it builds on established methods without introducing a fundamentally new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in digital archives, AI ethics, and cultural heritage by providing tools for low-resource settings, potentially leading to citations and applications in subfields like digital humanities. Nonetheless, its impact may be confined to niche areas and not extend broadly to general AI or commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, valuable contribution by tackling an important and underrepresented topic in AI for cultural preservation, making it essential for researchers in related fields. While not groundbreaking for all audiences, its insights on ethical AI and practical methods warrant attention from those in digital libraries and archival studies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c86c0676ac8918ccf1894e68282bf9ee83036db2",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Fitsum Sileshi Beyene",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380605557"
        },
        {
          "name": "Christopher L. Dancy",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2257972930"
        }
      ]
    },
    {
      "id": "2509.13237",
      "title": "Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise\n  Behaviors",
      "authors": [
        "Aniket Didolkar",
        "Nicolas Ballas",
        "Sanjeev Arora",
        "Anirudh Goyal"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) now solve multi-step problems by emitting\nextended chains of thought. During the process, they often re-derive the same\nintermediate steps across problems, inflating token usage and latency. This\nsaturation of the context window leaves less capacity for exploration. We study\na simple mechanism that converts recurring reasoning fragments into concise,\nreusable \"behaviors\" (name + instruction) via the model's own metacognitive\nanalysis of prior traces. These behaviors are stored in a \"behavior handbook\"\nwhich supplies them to the model in-context at inference or distills them into\nparameters via supervised fine-tuning. This approach achieves improved\ntest-time reasoning across three different settings - 1) Behavior-conditioned\ninference: Providing the LLM relevant behaviors in-context during reasoning\nreduces number of reasoning tokens by up to 46% while matching or improving\nbaseline accuracy; 2) Behavior-guided self-improvement: Without any parameter\nupdates, the model improves its own future reasoning by leveraging behaviors\nfrom its own past problem solving attempts. This yields up to 10% higher\naccuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned\nSFT: SFT on behavior-conditioned reasoning traces is more effective at\nconverting non-reasoning models into reasoning models as compared to vanilla\nSFT. Together, these results indicate that turning slow derivations into fast\nprocedural hints enables LLMs to remember how to reason, not just what to\nconclude.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13237v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13237v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.518,
      "distributed_training_score": 0.346,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on metacognitive reuse in LLMs, involving self-reflection and supervised fine-tuning to extract and reuse reasoning behaviors, without any use of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for reasoning; it instead addresses extracting reusable behaviors from LLM traces to improve efficiency, with no mention of treating chains-of-thought as entities for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13250",
      "title": "Intelligent Vacuum Thermoforming Process",
      "authors": [
        "Andi Kuswoyo",
        "Christos Margadji",
        "Sebastian W. Pattinson"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Ensuring consistent quality in vacuum thermoforming presents challenges due\nto variations in material properties and tooling configurations. This research\nintroduces a vision-based quality control system to predict and optimise\nprocess parameters, thereby enhancing part quality with minimal data\nrequirements. A comprehensive dataset was developed using visual data from\nvacuum-formed samples subjected to various process parameters, supplemented by\nimage augmentation techniques to improve model training. A k-Nearest Neighbour\nalgorithm was subsequently employed to identify adjustments needed in process\nparameters by mapping low-quality parts to their high-quality counterparts. The\nmodel exhibited strong performance in adjusting heating power, heating time,\nand vacuum time to reduce defects and improve production efficiency.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13250v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13250v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.286,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13255",
      "title": "ResidualViT for Efficient Temporally Dense Video Encoding",
      "authors": [
        "Mattia Soldan",
        "Fabian Caba Heilbron",
        "Bernard Ghanem",
        "Josef Sivic",
        "Bryan Russell"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Several video understanding tasks, such as natural language temporal video\ngrounding, temporal activity localization, and audio description generation,\nrequire \"temporally dense\" reasoning over frames sampled at high temporal\nresolution. However, computing frame-level features for these tasks is\ncomputationally expensive given the temporal resolution requirements. In this\npaper, we make three contributions to reduce the cost of computing features for\ntemporally dense tasks. First, we introduce a vision transformer (ViT)\narchitecture, dubbed ResidualViT, that leverages the large temporal redundancy\nin videos to efficiently compute temporally dense frame-level features. Our\narchitecture incorporates (i) learnable residual connections that ensure\ntemporal consistency across consecutive frames and (ii) a token reduction\nmodule that enhances processing speed by selectively discarding temporally\nredundant information while reusing weights of a pretrained foundation model.\nSecond, we propose a lightweight distillation strategy to approximate the\nframe-level features of the original foundation model. Finally, we evaluate our\napproach across four tasks and five datasets, in both zero-shot and fully\nsupervised settings, demonstrating significant reductions in computational cost\n(up to 60%) and improvements in inference speed (up to 2.5x faster), all while\nclosely approximating the accuracy of the original foundation model.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13255v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13255v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.366,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13266",
      "title": "JANUS: A Dual-Constraint Generative Framework for Stealthy Node\n  Injection Attacks",
      "authors": [
        "Jiahao Zhang",
        "Xiaobing Pei",
        "Zhaokun Zhong",
        "Wenqiang Hao",
        "Zhenghao Tang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable performance across\nvarious applications, yet they are vulnerable to sophisticated adversarial\nattacks, particularly node injection attacks. The success of such attacks\nheavily relies on their stealthiness, the ability to blend in with the original\ngraph and evade detection. However, existing methods often achieve stealthiness\nby relying on indirect proxy metrics, lacking consideration for the fundamental\ncharacteristics of the injected content, or focusing only on imitating local\nstructures, which leads to the problem of local myopia. To overcome these\nlimitations, we propose a dual-constraint stealthy node injection framework,\ncalled Joint Alignment of Nodal and Universal Structures (JANUS). At the local\nlevel, we introduce a local feature manifold alignment strategy to achieve\ngeometric consistency in the feature space. At the global level, we incorporate\nstructured latent variables and maximize the mutual information with the\ngenerated structures, ensuring the injected structures are consistent with the\nsemantic patterns of the original graph. We model the injection attack as a\nsequential decision process, which is optimized by a reinforcement learning\nagent. Experiments on multiple standard datasets demonstrate that the JANUS\nframework significantly outperforms existing methods in terms of both attack\neffectiveness and stealthiness.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13266v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13266v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.293,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.314,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13270",
      "title": "RadGame: An AI-Powered Platform for Radiology Education",
      "authors": [
        "Mohammed Baharoon",
        "Siavash Raissi",
        "John S. Jun",
        "Thibault Heintz",
        "Mahmoud Alabbad",
        "Ali Alburkani",
        "Sung Eun Kim",
        "Kent Kleinschmidt",
        "Abdulrahman O. Alhumaydhi",
        "Mohannad Mohammed G. Alghamdi",
        "Jeremy Francis Palacio",
        "Mohammed Bukhaytan",
        "Noah Michael Prudlo",
        "Rithvik Akula",
        "Brady Chrisler",
        "Benjamin Galligos",
        "Mohammed O. Almutairi",
        "Mazeen Mohammed Alanazi",
        "Nasser M. Alrashdi",
        "Joel Jihwan Hwang",
        "Sri Sai Dinesh Jaliparthi",
        "Luke David Nelson",
        "Nathaniel Nguyen",
        "Sathvik Suryadevara",
        "Steven Kim",
        "Mohammed F. Mohammed",
        "Yevgeniy R. Semenov",
        "Kun-Hsing Yu",
        "Abdulrhman Aljouie",
        "Hassan AlOmaish",
        "Adam Rodman",
        "Pranav Rajpurkar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce RadGame, an AI-powered gamified platform for radiology education\nthat targets two core skills: localizing findings and generating reports.\nTraditional radiology training is based on passive exposure to cases or active\npractice with real-time input from supervising radiologists, limiting\nopportunities for immediate and scalable feedback. RadGame addresses this gap\nby combining gamification with large-scale public datasets and automated,\nAI-driven feedback that provides clear, structured guidance to human learners.\nIn RadGame Localize, players draw bounding boxes around abnormalities, which\nare automatically compared to radiologist-drawn annotations from public\ndatasets, and visual explanations are generated by vision-language models for\nuser missed findings. In RadGame Report, players compose findings given a chest\nX-ray, patient age and indication, and receive structured AI feedback based on\nradiology report generation metrics, highlighting errors and omissions compared\nto a radiologist's written ground truth report from public datasets, producing\na final performance and style score. In a prospective evaluation, participants\nusing RadGame achieved a 68% improvement in localization accuracy compared to\n17% with traditional passive methods and a 31% improvement in report-writing\naccuracy compared to 4% with traditional methods after seeing the same cases.\nRadGame highlights the potential of AI-driven gamification to deliver scalable,\nfeedback-rich radiology training and reimagines the application of medical AI\nresources in education.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13270v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13270v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.333,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13279",
      "title": "HARMONIC: A Content-Centric Cognitive Robotic Architecture",
      "authors": [
        "Sanjay Oruganti",
        "Sergei Nirenburg",
        "Marjorie McShane",
        "Jesse English",
        "Michael K. Roberts",
        "Christian Arndt",
        "Carlos Gonzalez",
        "Mingyo Seo",
        "Luis Sentis"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "This paper introduces HARMONIC, a cognitive-robotic architecture designed for\nrobots in human-robotic teams. HARMONIC supports semantic perception\ninterpretation, human-like decision-making, and intentional language\ncommunication. It addresses the issues of safety and quality of results; aims\nto solve problems of data scarcity, explainability, and safety; and promotes\ntransparency and trust. Two proof-of-concept HARMONIC-based robotic systems are\ndemonstrated, each implemented in both a high-fidelity simulation environment\nand on physical robotic platforms.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13279v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13279v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.486,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.355,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces the HARMONIC architecture focused on cognitive-robotic systems, semantic perception, and safety, but it does not mention reinforcement learning from human feedback. There is no reference to training a reward model on human-ranked data or using RL to fine-tune models based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses related work like GR00T, which incorporates diffusion-based visuomotor policies for reactive control, but HARMONIC's main contribution is a cognitive architecture for planning and execution, not multi-step logical reasoning via diffusion models. Diffusion is only mentioned peripherally, not as a core element.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13281",
      "title": "RepIt: Representing Isolated Targets to Steer Language Models",
      "authors": [
        "Vincent Siu",
        "Nathan W. Henry",
        "Nicholas Crispino",
        "Yang Liu",
        "Dawn Song",
        "Chenguang Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "While activation steering in large language models (LLMs) is a growing area\nof research, methods can often incur broader effects than desired. This\nmotivates isolation of purer concept vectors to enable targeted interventions\nand understand LLM behavior at a more granular level. We present RepIt, a\nsimple and data-efficient framework for isolating concept-specific\nrepresentations. Across five frontier LLMs, RepIt enables precise\ninterventions: it selectively suppresses refusal on targeted concepts while\npreserving refusal elsewhere, producing models that answer WMD-related\nquestions while still scoring as safe on standard benchmarks. We further show\nthat the corrective signal localizes to just 100-200 neurons and that robust\ntarget representations can be extracted from as few as a dozen examples on a\nsingle A6000. This efficiency raises a dual concern: manipulations can be\nperformed with modest compute and data to extend to underrepresented\ndata-scarce topics while evading existing benchmarks. By disentangling refusal\nvectors with RepIt, this work demonstrates that targeted interventions can\ncounteract overgeneralization, laying the foundation for more granular control\nof model behavior.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13281v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13281v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.479,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.374,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on inference-time activation steering for isolating concept vectors in LLMs, without any mention of training models using human feedback, reward models, or reinforcement learning. Its main contribution is about targeted interventions on existing models, not alignment through RLHF.",
      "weak_supervision_justification": "The paper uses a small number of examples (e.g., a dozen prompts) to extract concept-specific representations, which could loosely relate to weak supervision's use of noisy or limited data sources. However, it does not involve programmatically generating labels for training models, focusing instead on representation isolation rather than weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper's main contribution is on activation steering and isolating representations in LLMs for targeted behavior control, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13282",
      "title": "ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking\n  Guided Attention Refinement",
      "authors": [
        "Ali Salamatian",
        "Amirhossein Abaskohi",
        "Wan-Cyuan Fan",
        "Mir Rayat Imtiaz Hossain",
        "Leonid Sigal",
        "Giuseppe Carenini"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Charts are a crucial visual medium for communicating and representing\ninformation. While Large Vision-Language Models (LVLMs) have made progress on\nchart question answering (CQA), the task remains challenging, particularly when\nmodels attend to irrelevant regions of the chart. In this work, we present\nChartGaze, a new eye-tracking dataset that captures human gaze patterns during\nchart reasoning tasks. Through a systematic comparison of human and model\nattention, we find that LVLMs often diverge from human gaze, leading to reduced\ninterpretability and accuracy. To address this, we propose a gaze-guided\nattention refinement that aligns image-text attention with human fixations. Our\napproach improves both answer accuracy and attention alignment, yielding gains\nof up to 2.56 percentage points across multiple models. These results\ndemonstrate the promise of incorporating human gaze to enhance both the\nreasoning quality and interpretability of chart-focused LVLMs.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13282v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13282v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.311,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses human gaze data to supervise training via a gaze-guided loss function, which is a form of supervised learning to improve attention alignment in LVLMs. However, it does not involve training a separate reward model on human-ranked data or using reinforcement learning techniques to fine-tune the main model, which are core elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing chart understanding in LVLMs by refining attention with human eye-tracking data, without any mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13285",
      "title": "Contrastive timbre representations for musical instrument and\n  synthesizer retrieval",
      "authors": [
        "Gwendal Le Vaillant",
        "Yannick Molle"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Efficiently retrieving specific instrument timbres from audio mixtures\nremains a challenge in digital music production. This paper introduces a\ncontrastive learning framework for musical instrument retrieval, enabling\ndirect querying of instrument databases using a single model for both single-\nand multi-instrument sounds. We propose techniques to generate realistic\npositive/negative pairs of sounds for virtual musical instruments, such as\nsamplers and synthesizers, addressing limitations in common audio data\naugmentation methods.\n  The first experiment focuses on instrument retrieval from a dataset of 3,884\ninstruments, using single-instrument audio as input. Contrastive approaches are\ncompetitive with previous works based on classification pre-training. The\nsecond experiment considers multi-instrument retrieval with a mixture of\ninstruments as audio input. In this case, the proposed contrastive framework\noutperforms related works, achieving 81.7\\% top-1 and 95.7\\% top-5 accuracies\nfor three-instrument mixtures.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13285v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13285v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.277,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13288",
      "title": "Shapes of Cognition for Computational Cognitive Modeling",
      "authors": [
        "Marjorie McShane",
        "Sergei Nirenburg",
        "Sanjay Oruganti",
        "Jesse English"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Shapes of cognition is a new conceptual paradigm for the computational\ncognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are\nremembered constellations of sensory, linguistic, conceptual, episodic, and\nprocedural knowledge that allow agents to cut through the complexity of real\nlife the same way as people do: by expecting things to be typical, recognizing\npatterns, acting by habit, reasoning by analogy, satisficing, and generally\nminimizing cognitive load to the degree situations permit. Atypical outcomes\nare treated using shapes-based recovery methods, such as learning on the fly,\nasking a human partner for help, or seeking an actionable, even if imperfect,\nsituational understanding. Although shapes is an umbrella term, it is not\nvague: shapes-based modeling involves particular objectives, hypotheses,\nmodeling strategies, knowledge bases, and actual models of wide-ranging\nphenomena, all implemented within a particular cognitive architecture. Such\nspecificity is needed both to vet our hypotheses and to achieve our practical\naims of building useful agent systems that are explainable, extensible, and\nworthy of our trust, even in critical domains. However, although the LEIA\nexample of shapes-based modeling is specific, the principles can be applied\nmore broadly, giving new life to knowledge-based and hybrid AI.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13288v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13288v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.262,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces \"Shapes of Cognition,\" a paradigm for computational cognitive modeling focused on knowledge constellations for intelligent agents, emphasizing pattern recognition, analogy, and hybrid AI. It does not mention or involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a single entity for multi-step logical reasoning. As the core elements of diffusion-based reasoning are absent, the paper's contribution is unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13289",
      "title": "Image Realness Assessment and Localization with Multimodal Features",
      "authors": [
        "Lovish Kaushik",
        "Agnij Biswas",
        "Somdyuti Paul"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "A reliable method of quantifying the perceptual realness of AI-generated\nimages and identifying visually inconsistent regions is crucial for practical\nuse of AI-generated images and for improving photorealism of generative AI via\nrealness feedback during training. This paper introduces a framework that\naccomplishes both overall objective realness assessment and local inconsistency\nidentification of AI-generated images using textual descriptions of visual\ninconsistencies generated by vision-language models trained on large datasets\nthat serve as reliable substitutes for human annotations. Our results\ndemonstrate that the proposed multimodal approach improves objective realness\nprediction performance and produces dense realness maps that effectively\ndistinguish between realistic and unrealistic spatial regions.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13289v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13289v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.344,
      "datasets_score": 0.398,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on assessing the realness of AI-generated images using vision-language models as substitutes for human annotations, but it does not involve training a reward model or fine-tuning via reinforcement learning based on human-ranked data. There is no mention of RLHF elements, such as human feedback loops or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses evaluating images generated by diffusion-based models like Stable Diffusion, but it does not adapt diffusion processes for multi-step logical reasoning or treat a Chain-of-Thought as an entity for iterative refinement. Its contributions are centered on realness assessment and localization, not diffusion-based reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13298",
      "title": "QDFlow: A Python package for physics simulations of quantum dot devices",
      "authors": [
        "Donovan L. Buterakos",
        "Sandesh S. Kalantre",
        "Joshua Ziegler",
        "Jacob M Taylor",
        "Justyna P. Zwolak"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "quant-ph (Quantum Physics)"
      ],
      "abstract": "Recent advances in machine learning (ML) have accelerated progress in\ncalibrating and operating quantum dot (QD) devices. However, most ML approaches\nrely on access to large, high-quality labeled datasets for training,\nbenchmarking, and validation, with labels capturing key features in the data.\nObtaining such datasets experimentally is challenging due to limited data\navailability and the labor-intensive nature of labeling. QDFlow is an\nopen-source physics simulator for multi-QD arrays that generates realistic\nsynthetic data with ground-truth labels. QDFlow combines a self-consistent\nThomas-Fermi solver, a dynamic capacitance model, and flexible noise modules to\nproduce charge stability diagrams and ray-based data closely resembling\nexperiments. With extensive tunable parameters and customizable noise models,\nQDFlow supports the creation of large, diverse datasets for ML development,\nbenchmarking, and quantum device research.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13298v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13298v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.376,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13301",
      "title": "StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with\n  Texture-Geometry Dual Guidance",
      "authors": [
        "Zefan Qu",
        "Zhenwei Wang",
        "Haoyuan Wang",
        "Ke Xu",
        "Gerhard Hancke",
        "Rynson W. H. Lau"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Creating 3D assets that follow the texture and geometry style of existing\nones is often desirable or even inevitable in practical applications like video\ngaming and virtual reality. While impressive progress has been made in\ngenerating 3D objects from text or images, creating style-controllable 3D\nassets remains a complex and challenging problem. In this work, we propose\nStyleSculptor, a novel training-free approach for generating style-guided 3D\nassets from a content image and one or more style images. Unlike previous\nworks, StyleSculptor achieves style-guided 3D generation in a zero-shot manner,\nenabling fine-grained 3D style control that captures the texture, geometry, or\nboth styles of user-provided style images. At the core of StyleSculptor is a\nnovel Style Disentangled Attention (SD-Attn) module, which establishes a\ndynamic interaction between the input content image and style image for\nstyle-guided 3D asset generation via a cross-3D attention mechanism, enabling\nstable feature fusion and effective style-guided generation. To alleviate\nsemantic content leakage, we also introduce a style-disentangled feature\nselection strategy within the SD-Attn module, which leverages the variance of\n3D feature patches to disentangle style- and content-significant channels,\nallowing selective feature injection within the attention framework. With\nSD-Attn, the network can dynamically compute texture-, geometry-, or\nboth-guided features to steer the 3D generation process. Built upon this, we\nfurther propose the Style Guided Control (SGC) mechanism, which enables\nexclusive geometry- or texture-only stylization, as well as adjustable style\nintensity control. Extensive experiments demonstrate that StyleSculptor\noutperforms existing baseline methods in producing high-fidelity 3D assets.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13301v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13301v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.343,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13317",
      "title": "3D Aware Region Prompted Vision Language Model",
      "authors": [
        "An-Chieh Cheng",
        "Yang Fu",
        "Yukang Chen",
        "Zhijian Liu",
        "Xiaolong Li",
        "Subhashree Radhakrishnan",
        "Song Han",
        "Yao Lu",
        "Jan Kautz",
        "Pavlo Molchanov",
        "Hongxu Yin",
        "Xiaolong Wang",
        "Sifei Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present Spatial Region 3D (SR-3D) aware vision-language model that\nconnects single-view 2D images and multi-view 3D data through a shared visual\ntoken space. SR-3D supports flexible region prompting, allowing users to\nannotate regions with bounding boxes, segmentation masks on any frame, or\ndirectly in 3D, without the need for exhaustive multi-frame labeling. We\nachieve this by enriching 2D visual features with 3D positional embeddings,\nwhich allows the 3D model to draw upon strong 2D priors for more accurate\nspatial reasoning across frames, even when objects of interest do not co-occur\nwithin the same view. Extensive experiments on both general 2D vision language\nand specialized 3D spatial benchmarks demonstrate that SR-3D achieves\nstate-of-the-art performance, underscoring its effectiveness for unifying 2D\nand 3D representation space on scene understanding. Moreover, we observe\napplicability to in-the-wild videos without sensory 3D inputs or ground-truth\n3D annotations, where SR-3D accurately infers spatial relationships and metric\nmeasurements.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13317v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13317v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.343,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a 3D-aware vision-language model (SR-3D) that unifies 2D and 3D representations using positional embeddings and region prompting for spatial reasoning. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical tasks such as Chain-of-Thought reasoning. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13368",
      "title": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning\n  Automation",
      "authors": [
        "Yuan Wei",
        "Xiaohan Shan",
        "Ran Miao",
        "Jianmin Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reinforcement learning (RL) agent development traditionally requires\nsubstantial expertise and iterative effort, often leading to high failure rates\nand limited accessibility. This paper introduces Agent$^2$, an LLM-driven\nagent-generates-agent framework for fully automated RL agent design. Agent$^2$\nautonomously translates natural language task descriptions and environment code\ninto executable RL solutions without human intervention. The framework adopts a\ndual-agent architecture: a Generator Agent that analyzes tasks and designs\nagents, and a Target Agent that is automatically generated and executed. To\nbetter support automation, RL development is decomposed into two stages, MDP\nmodeling and algorithmic optimization, facilitating targeted and effective\nagent generation. Built on the Model Context Protocol, Agent$^2$ provides a\nunified framework for standardized agent creation across diverse environments\nand algorithms, incorporating adaptive training management and intelligent\nfeedback analysis for continuous refinement. Extensive experiments on\nbenchmarks including MuJoCo, MetaDrive, MPE, and SMAC show that Agent$^2$\noutperforms manually designed baselines across all tasks, achieving up to 55\\%\nperformance improvement with consistent average gains. By enabling a\nclosed-loop, end-to-end automation pipeline, this work advances a new paradigm\nin which agents can design and optimize other agents, underscoring the\npotential of agent-generates-agent systems for automated AI development.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13368v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13368v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.414,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.367,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an automated framework for generating RL agents using LLMs without any human involvement, such as feedback, preferences, or ranking data. There is no mention of training models with human-ranked data or aligning AI with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "The paper describes automating RL agent design through LLMs but does not involve programmatically generating training labels from noisy or imprecise sources. It lacks any discussion of weak supervision techniques, such as using high-level rules for labeling data, which is the essence of this topic.",
      "diffusion_reasoning_justification": "The paper utilizes LLMs for agent generation and reasoning but does not incorporate diffusion models or iterative refinement processes for multi-step logical tasks. There is no evidence of treating a 'Chain-of-Thought' as a holistic entity for correction, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13372",
      "title": "Generative AI Pipeline for Interactive Prompt-driven 2D-to-3D Vascular\n  Reconstruction for Fontan Geometries from Contrast-Enhanced X-Ray Fluoroscopy\n  Imaging",
      "authors": [
        "Prahlad G Menon"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "Fontan palliation for univentricular congenital heart disease progresses to\nhemodynamic failure with complex flow patterns poorly characterized by\nconventional 2D imaging. Current assessment relies on fluoroscopic angiography,\nproviding limited 3D geometric information essential for computational fluid\ndynamics (CFD) analysis and surgical planning.\n  A multi-step AI pipeline was developed utilizing Google's Gemini 2.5 Flash\n(2.5B parameters) for systematic, iterative processing of fluoroscopic\nangiograms through transformer-based neural architecture. The pipeline\nencompasses medical image preprocessing, vascular segmentation, contrast\nenhancement, artifact removal, and virtual hemodynamic flow visualization\nwithin 2D projections. Final views were processed through Tencent's\nHunyuan3D-2mini (384M parameters) for stereolithography file generation.\n  The pipeline successfully generated geometrically optimized 2D projections\nfrom single-view angiograms after 16 processing steps using a custom web\ninterface. Initial iterations contained hallucinated vascular features\nrequiring iterative refinement to achieve anatomically faithful\nrepresentations. Final projections demonstrated accurate preservation of\ncomplex Fontan geometry with enhanced contrast suitable for 3D conversion.\nAI-generated virtual flow visualization identified stagnation zones in central\nconnections and flow patterns in branch arteries. Complete processing required\nunder 15 minutes with second-level API response times.\n  This approach demonstrates clinical feasibility of generating CFD-suitable\ngeometries from routine angiographic data, enabling 3D generation and rapid\nvirtual flow visualization for cursory insights prior to full CFD simulation.\nWhile requiring refinement cycles for accuracy, this establishes foundation for\ndemocratizing advanced geometric and hemodynamic analysis using readily\navailable imaging data.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13372v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13372v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.347,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a multi-step AI pipeline for 2D-to-3D vascular reconstruction using models like Google's Gemini 2.5 Flash and Tencent's Hunyuan3D-2mini, emphasizing iterative refinement for image processing and geometric accuracy. While it involves iterative steps for refinement, these are applied to visual and generative tasks in medical imaging, not to diffusion-based models for complex logical reasoning or Chain-of-Thought processes as defined in the topic. There is no mention of diffusion models or their adaptation for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13375",
      "title": "An Empirical Analysis of VLM-based OOD Detection: Mechanisms,\n  Advantages, and Sensitivity",
      "authors": [
        "Yuxiao Lee",
        "Xiaofeng Cao",
        "Wei Ye",
        "Jiangchao Yao",
        "Jingkuan Song",
        "Heng Tao Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot out-of-distribution (OOD) detection capabilities, vital for reliable\nAI systems. Despite this promising capability, a comprehensive understanding of\n(1) why they work so effectively, (2) what advantages do they have over\nsingle-modal methods, and (3) how is their behavioral robustness -- remains\nnotably incomplete within the research community. This paper presents a\nsystematic empirical analysis of VLM-based OOD detection using in-distribution\n(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and\nformalize key operational properties within the VLM embedding space that\nfacilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the\nsuperiority of these models over established single-modal approaches,\nattributing this distinct advantage to the VLM's capacity to leverage rich\nsemantic novelty. (3) Sensitivity: We uncovers a significant and previously\nunder-explored asymmetry in their robustness profile: while exhibiting\nresilience to common image noise, these VLM-based methods are highly sensitive\nto prompt phrasing. Our findings contribute a more structured understanding of\nthe strengths and critical vulnerabilities inherent in VLM-based OOD detection,\noffering crucial, empirically-grounded guidance for developing more robust and\nreliable future designs.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13375v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13375v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.365,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on empirical analysis of Vision-Language Models for out-of-distribution detection, including mechanisms, advantages, and sensitivity, without any mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines VLM-based OOD detection through embedding spaces and prompts, but it does not involve diffusion models, iterative refinement processes, multi-step logical reasoning, or treating Chain-of-Thought as an entity for correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13379",
      "title": "The Art of Saying \"Maybe\": A Conformal Lens for Uncertainty Benchmarking\n  in VLMs",
      "authors": [
        "Asif Azad",
        "Mohammad Sadat Hossain",
        "MD Sadik Hossain Shanto",
        "M Saifur Rahman",
        "Md Rizwan Parvez"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-Language Models (VLMs) have achieved remarkable progress in complex\nvisual understanding across scientific and reasoning tasks. While performance\nbenchmarking has advanced our understanding of these capabilities, the critical\ndimension of uncertainty quantification has received insufficient attention.\nTherefore, unlike prior conformal prediction studies that focused on limited\nsettings, we conduct a comprehensive uncertainty benchmarking study, evaluating\n16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets\nwith 3 distinct scoring functions. Our findings demonstrate that larger models\nconsistently exhibit better uncertainty quantification; models that know more\nalso know better what they don't know. More certain models achieve higher\naccuracy, while mathematical and reasoning tasks elicit poorer uncertainty\nperformance across all models compared to other domains. This work establishes\na foundation for reliable uncertainty evaluation in multimodal systems.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13379v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13379v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.321,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on uncertainty benchmarking in VLMs using conformal prediction, evaluating model performance post-training, and does not involve training models with programmatically generated labels or noisy sources as defined in weak supervision.",
      "diffusion_reasoning_justification": "The paper examines uncertainty quantification in VLMs via conformal prediction and does not discuss or incorporate diffusion-based processes for multi-step logical reasoning or iterative refinement of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13380",
      "title": "ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy",
      "authors": [
        "Alejandro D. Mousist"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.MA (Multiagent Systems)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "This paper presents ASTREA, the first agentic system deployed on\nflight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using\nthermal control as a representative use case, we integrate a\nresource-constrained Large Language Model (LLM) agent with a reinforcement\nlearning controller in an asynchronous architecture tailored for\nspace-qualified platforms. Ground experiments show that LLM-guided supervision\nimproves thermal stability and reduces violations, confirming the feasibility\nof combining semantic reasoning with adaptive control under hardware\nconstraints. However, on-orbit validation aboard the International Space\nStation (ISS) reveals performance degradation caused by inference latency\nmismatched with the rapid thermal cycles characteristic of Low Earth Orbit\n(LEO) satellites. These results highlight both the opportunities and current\nlimitations of agentic LLM-based systems in real flight environments, providing\npractical design guidelines for future space autonomy.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13380v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13380v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.377,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the integration of a resource-constrained LLM with a reinforcement learning controller for autonomous thermal control in spacecraft, as demonstrated in ASTREA. While it uses reinforcement learning, there is no indication that human feedback, such as training a reward model on human-ranked data, was involved in fine-tuning the model. This lacks the core elements of RLHF, making the paper unrelated to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13385",
      "title": "Curvature as a tool for evaluating dimensionality reduction and\n  estimating intrinsic dimension",
      "authors": [
        "Charlotte Beylier",
        "Parvaneh Joharinad",
        "Jürgen Jost",
        "Nahid Torbati"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.DM (Discrete Mathematics)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Utilizing recently developed abstract notions of sectional curvature, we\nintroduce a method for constructing a curvature-based geometric profile of\ndiscrete metric spaces. The curvature concept that we use here captures the\nmetric relations between triples of points and other points. More\nsignificantly, based on this curvature profile, we introduce a quantitative\nmeasure to evaluate the effectiveness of data representations, such as those\nproduced by dimensionality reduction techniques. Furthermore, Our experiments\ndemonstrate that this curvature-based analysis can be employed to estimate the\nintrinsic dimensionality of datasets. We use this to explore the large-scale\ngeometry of empirical networks and to evaluate the effectiveness of\ndimensionality reduction techniques.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13385v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13385v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.254,
      "weak_supervision_score": 0.268,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.257,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13387",
      "title": "Uncovering AI Governance Themes in EU Policies using BERTopic and\n  Thematic Analysis",
      "authors": [
        "Delaram Golpayegani",
        "Marta Lasek-Markey",
        "Arjumand Younus",
        "Aphra Kerr",
        "Dave Lewis"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The upsurge of policies and guidelines that aim to ensure Artificial\nIntelligence (AI) systems are safe and trustworthy has led to a fragmented\nlandscape of AI governance. The European Union (EU) is a key actor in the\ndevelopment of such policies and guidelines. Its High-Level Expert Group (HLEG)\nissued an influential set of guidelines for trustworthy AI, followed in 2024 by\nthe adoption of the EU AI Act. While the EU policies and guidelines are\nexpected to be aligned, they may differ in their scope, areas of emphasis,\ndegrees of normativity, and priorities in relation to AI. To gain a broad\nunderstanding of AI governance from the EU perspective, we leverage qualitative\nthematic analysis approaches to uncover prevalent themes in key EU documents,\nincluding the AI Act and the HLEG Ethics Guidelines. We further employ\nquantitative topic modelling approaches, specifically through the use of the\nBERTopic model, to enhance the results and increase the document sample to\ninclude EU AI policy documents published post-2018. We present a novel\nperspective on EU policies, tracking the evolution of its approach to\naddressing AI governance.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13387v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13387v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.446,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.316,
      "datasets_score": 0.415,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on analyzing EU AI policies using BERTopic and thematic analysis to identify governance themes, with no mention of reinforcement learning, human feedback mechanisms, or techniques for aligning AI models with human preferences. It is solely about policy document analysis, not AI training methods.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses a collection of EU AI policy documents as a corpus for topic modeling with BERTopic, which involves handling and analyzing textual data. However, it does not focus on creating, benchmarking, evaluating, or curating datasets for AI applications; instead, the emphasis is on policy themes, making the dataset aspect incidental.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13388",
      "title": "Landcover classification and change detection using remote sensing and\n  machine learning: a case study of Western Fiji",
      "authors": [
        "Yadvendra Gurjar",
        "Ruoni Wan",
        "Ehsan Farahbakhsh",
        "Rohitash Chandra"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "stat.AP (Applications)"
      ],
      "abstract": "As a developing country, Fiji is facing rapid urbanisation, which is visible\nin the massive development projects that include housing, roads, and civil\nworks. In this study, we present machine learning and remote sensing frameworks\nto compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The\nultimate goal of this study is to provide technical support in land cover/land\nuse modelling and change detection. We used Landsat-8 satellite image for the\nstudy region and created our training dataset with labels for supervised\nmachine learning. We used Google Earth Engine and unsupervised machine learning\nvia k-means clustering to generate the land cover map. We used convolutional\nneural networks to classify the selected regions' land cover types. We present\na visualisation of change detection, highlighting urban area changes over time\nto monitor changes in the map.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13388v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13388v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.294,
      "distributed_training_score": 0.303,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13389",
      "title": "From Next Token Prediction to (STRIPS) World Models -- Preliminary\n  Results",
      "authors": [
        "Carlos Núñez-Molina",
        "Vicenç Gómez",
        "Hector Geffner"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We consider the problem of learning propositional STRIPS world models from\naction traces alone, using a deep learning architecture (transformers) and\ngradient descent. The task is cast as a supervised next token prediction\nproblem where the tokens are the actions, and an action $a$ may follow an\naction sequence if the hidden effects of the previous actions do not make an\naction precondition of $a$ false. We show that a suitable transformer\narchitecture can faithfully represent propositional STRIPS world models, and\nthat the models can be learned from sets of random valid (positive) and invalid\n(negative) action sequences alone. A number of experiments are reported.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13389v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13389v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.458,
      "distributed_training_score": 0.353,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on supervised learning using transformers for next token prediction to learn STRIPS world models from action traces, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs transformers for next token prediction and world model learning, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13390",
      "title": "A Domain Knowledge Informed Approach for Anomaly Detection of Electric\n  Vehicle Interior Sounds",
      "authors": [
        "Deepti Kunte",
        "Bram Cornelis",
        "Claudio Colangeli",
        "Karl Janssens",
        "Brecht Van Baelen",
        "Konstantinos Gryllias"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "The detection of anomalies in automotive cabin sounds is critical for\nensuring vehicle quality and maintaining passenger comfort. In many real-world\nsettings, this task is more appropriately framed as an unsupervised learning\nproblem rather than the supervised case due to the scarcity or complete absence\nof labeled faulty data. In such an unsupervised setting, the model is trained\nexclusively on healthy samples and detects anomalies as deviations from normal\nbehavior. However, in the absence of labeled faulty samples for validation and\nthe limited reliability of commonly used metrics, such as validation\nreconstruction error, effective model selection remains a significant\nchallenge. To overcome these limitations, a domain-knowledge-informed approach\nfor model selection is proposed, in which proxy-anomalies engineered through\nstructured perturbations of healthy spectrograms are used in the validation set\nto support model selection. The proposed methodology is evaluated on a\nhigh-fidelity electric vehicle dataset comprising healthy and faulty cabin\nsounds across five representative fault types viz., Imbalance, Modulation,\nWhine, Wind, and Pulse Width Modulation. This dataset, generated using advanced\nsound synthesis techniques, and validated via expert jury assessments, has been\nmade publicly available to facilitate further research. Experimental\nevaluations on the five fault cases demonstrate the selection of optimal models\nusing proxy-anomalies, significantly outperform conventional model selection\nstrategies.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13390v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13390v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.273,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13391",
      "title": "The Intercepted Self: How Generative AI Challenges the Dynamics of the\n  Relational Self",
      "authors": [
        "Sandrine R. Schiller",
        "Camilo Miguel Signorelli",
        "Filippos Stamatiou"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative AI is changing our way of interacting with technology, others, and\nourselves. Systems such as Microsoft copilot, Gemini and the expected Apple\nintelligence still awaits our prompt for action. Yet, it is likely that AI\nassistant systems will only become better at predicting our behaviour and\nacting on our behalf. Imagine new generations of generative and predictive AI\ndeciding what you might like best at a new restaurant, picking an outfit that\nincreases your chances on your date with a partner also chosen by the same or a\nsimilar system. Far from a science fiction scenario, the goal of several\nresearch programs is to build systems capable of assisting us in exactly this\nmanner. The prospect urges us to rethink human-technology relations, but it\nalso invites us to question how such systems might change the way we relate to\nourselves. Building on our conception of the relational self, we question the\npossible effects of generative AI with respect to what we call the sphere of\nexternalised output, the contextual sphere and the sphere of self-relating. In\nthis paper, we attempt to deepen the existential considerations accompanying\nthe AI revolution by outlining how generative AI enables the fulfilment of\ntasks and also increasingly anticipates, i.e. intercepts, our initiatives in\nthese different spheres.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13391v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13391v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.304,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper primarily explores the philosophical and existential impacts of generative AI on human self-relations, such as how AI might intercept actions in various spheres of life. It discusses AI systems like Microsoft Copilot and LLMs in general terms but does not address technical training methods, including Reinforcement Learning from Human Feedback (RLHF). There is no mention of using human feedback to train reward models or fine-tune AI via reinforcement learning, making the paper's content unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13395",
      "title": "TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech\n  Recognition Abilities of Large Multimodal Models",
      "authors": [
        "Haolong Zheng",
        "Yekaterina Yegorova",
        "Mark Hasegawa-Johnson"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Speech foundation models have recently demonstrated the ability to perform\nSpeech In-Context Learning (SICL). Selecting effective in-context examples is\ncrucial for SICL performance, yet selection methodologies remain underexplored.\nIn this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline\nthat uses semantic context to enhance off-the-shelf large multimodal models'\nspeech recognition ability without fine-tuning. Across challenging automatic\nspeech recognition tasks, including accented English, multilingual speech, and\nchildren's speech, our method enables models to surpass zero-shot performance\nwith up to 84.7% relative WER reduction. We conduct ablation studies to show\nthe robustness and efficiency of our method.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13395v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13395v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.322,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13396",
      "title": "Real-Time Detection and Tracking of Foreign Object Intrusions in Power\n  Systems via Feature-Based Edge Intelligence",
      "authors": [
        "Xinan Wang",
        "Di Shi",
        "Fengyu Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "This paper presents a novel three-stage framework for real-time foreign\nobject intrusion (FOI) detection and tracking in power transmission systems.\nThe framework integrates: (1) a YOLOv7 segmentation model for fast and robust\nobject localization, (2) a ConvNeXt-based feature extractor trained with\ntriplet loss to generate discriminative embeddings, and (3) a feature-assisted\nIoU tracker that ensures resilient multi-object tracking under occlusion and\nmotion. To enable scalable field deployment, the pipeline is optimized for\ndeployment on low-cost edge hardware using mixed-precision inference. The\nsystem supports incremental updates by adding embeddings from previously unseen\nobjects into a reference database without requiring model retraining. Extensive\nexperiments on real-world surveillance and drone video datasets demonstrate the\nframework's high accuracy and robustness across diverse FOI scenarios. In\naddition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's\npracticality and scalability for real-world edge applications.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13396v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13396v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.348,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13397",
      "title": "The threat of analytic flexibility in using large language models to\n  simulate human data: A call to attention",
      "authors": [
        "Jamie Cummins"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Social scientists are now using large language models to create \"silicon\nsamples\" - synthetic datasets intended to stand in for human respondents, aimed\nat revolutionising human subjects research. However, there are many analytic\nchoices which must be made to produce these samples. Though many of these\nchoices are defensible, their impact on sample quality is poorly understood. I\nmap out these analytic choices and demonstrate how a very small number of\ndecisions can dramatically change the correspondence between silicon samples\nand human data. Configurations (N = 252) varied substantially in their capacity\nto estimate (i) rank ordering of participants, (ii) response distributions, and\n(iii) between-scale correlations. Most critically, configurations were not\nconsistent in quality: those that performed well on one dimension often\nperformed poorly on another, implying that there is no \"one-size-fits-all\"\nconfiguration that optimises the accuracy of these samples. I call for greater\nattention to the threat of analytic flexibility in using silicon samples.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13397v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13397v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.452,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.346,
      "datasets_score": 0.422,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on the analytic flexibility and quality issues in generating synthetic datasets using large language models for social science research, but it does not involve training AI models with human feedback, reward models, or reinforcement learning techniques. There is no mention of aligning AI with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "The paper discusses using large language models to programmatically generate synthetic datasets as proxies for human data, which shares similarities with weak supervision's use of noisy or imprecise label sources. However, the paper's primary focus is on evaluating the impact of analytic choices on dataset quality for social science, rather than directly addressing weak supervision techniques for model training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and analyzing synthetic datasets (silicon samples) from large language models, examining their quality, correspondence to human data, and performance across configurations, which directly aligns with research on dataset creation, evaluation, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper investigates the risks of analytic flexibility in using large language models (LLMs) to generate synthetic datasets, or \"silicon samples,\" for social science research, aiming to highlight how various analytic choices affect sample quality. By examining 252 different configurations, the author demonstrates that minor decisions can significantly alter the accuracy of these samples in replicating human data patterns, such as rank ordering, response distributions, and correlations, and concludes that no single configuration optimizes all aspects, calling for increased scrutiny in this area.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by highlighting the underappreciated threat of analytic flexibility in silicon samples, combining existing AI techniques with a new focus on decision impacts to address a known problem in synthetic data generation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research practices in AI and social sciences by prompting better handling of analytic choices in synthetic data, though its applicability may remain limited to specific subfields like human subjects research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights into potential pitfalls of using LLMs for synthetic data, making it essential for researchers in AI ethics and social sciences to understand these limitations and improve their methodologies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/01426f02f0eb69d8055d8d157a9f105932c52457",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jamie Cummins",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685635"
        }
      ]
    },
    {
      "id": "2509.13399",
      "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable,\n  Fine-Grained Evaluation of Multi-Turn Editing",
      "authors": [
        "Tianyu Chen",
        "Yasi Zhang",
        "Zhi Zhang",
        "Peiyu Yu",
        "Shu Wang",
        "Zhendong Wang",
        "Kevin Lin",
        "Xiaofei Wang",
        "Zhengyuan Yang",
        "Linjie Li",
        "Chung-Ching Lin",
        "Jianwen Xie",
        "Oscar Leong",
        "Lijuan Wang",
        "Ying Nian Wu",
        "Mingyuan Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Instruction-based image editing has advanced rapidly, yet reliable and\ninterpretable evaluation remains a bottleneck. Current protocols either (i)\ndepend on paired reference images -- resulting in limited coverage and\ninheriting biases from prior generative models -- or (ii) rely solely on\nzero-shot vision-language models (VLMs), whose prompt-based assessments of\ninstruction following, content consistency, and visual quality are often\nimprecise.\n  To address this, we introduce EdiVal-Agent, an automated, scalable, and\nfine-grained evaluation framework for multi-turn instruction-based editing from\nan object-centric perspective, supported by a suite of expert tools. Given an\nimage, EdiVal-Agent first decomposes it into semantically meaningful objects,\nthen synthesizes diverse, context-aware editing instructions. For evaluation,\nit integrates VLMs with open-vocabulary object detectors to assess instruction\nfollowing, uses semantic-level feature extractors to evaluate content\nconsistency, and leverages human preference models to judge visual quality. We\nshow that combining VLMs with object detectors yields stronger agreement with\nhuman judgments in instruction-following evaluation compared to using VLMs\nalone and CLIP-based metrics. Furthermore, the pipeline's modular design allows\nfuture tools to be seamlessly integrated, enhancing evaluation accuracy over\ntime.\n  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing\nbenchmark covering 9 instruction types and 11 state-of-the-art editing models\nspanning autoregressive (AR) (including Nano Banana, GPT-Image-1),\nflow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be\nused to identify existing failure modes, thereby informing the development of\nthe next generation of editing models. Project page:\nhttps://tianyucodings.github.io/EdiVAL-page/.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13399v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13399v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.329,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses human preference models (e.g., HPSv3) for evaluating visual quality, which may have been trained using RLHF techniques, but the core contribution is an evaluation framework for image editing, not the development or application of RLHF for model alignment or fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses diffusion-based models in the context of image editing (e.g., evaluating editing paradigms), but it does not adapt diffusion for multi-step logical reasoning or chain-of-thought processes as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13400",
      "title": "Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer\n  Reviews",
      "authors": [
        "Sai Suresh Macharla Vasu",
        "Ivaxi Sheth",
        "Hui-Po Wang",
        "Ruta Binkyte",
        "Mario Fritz"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The adoption of large language models (LLMs) is transforming the peer review\nprocess, from assisting reviewers in writing more detailed evaluations to\ngenerating entire reviews automatically. While these capabilities offer\nexciting opportunities, they also raise critical concerns about fairness and\nreliability. In this paper, we investigate bias in LLM-generated peer reviews\nby conducting controlled experiments on sensitive metadata, including author\naffiliation and gender. Our analysis consistently shows affiliation bias\nfavoring institutions highly ranked on common academic rankings. Additionally,\nwe find some gender preferences, which, even though subtle in magnitude, have\nthe potential to compound over time. Notably, we uncover implicit biases that\nbecome more evident with token-based soft ratings.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13400v4",
      "pdf_url": "http://arxiv.org/pdf/2509.13400v4",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.464,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.341,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on detecting biases in LLM-generated peer reviews through experiments, but it does not involve training AI models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper examines biases in LLMs for peer reviews and uses controlled experiments, but it does not discuss training models with programmatically generated labels or weak supervision methods.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves experiments with metadata like author affiliations and gender, which may imply the use of datasets for testing biases, but its main contribution is analyzing LLM biases, not creating, analyzing, benchmarking, or evaluating datasets as a primary focus.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13414",
      "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction",
      "authors": [
        "Nikhil Keetha",
        "Norman Müller",
        "Johannes Schönberger",
        "Lorenzo Porzi",
        "Yuchen Zhang",
        "Tobias Fischer",
        "Arno Knapitsch",
        "Duncan Zauss",
        "Ethan Weber",
        "Nelson Antunes",
        "Jonathon Luiten",
        "Manuel Lopez-Antequera",
        "Samuel Rota Bulò",
        "Christian Richardt",
        "Deva Ramanan",
        "Sebastian Scherer",
        "Peter Kontschieder"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "We introduce MapAnything, a unified transformer-based feed-forward model that\ningests one or more images along with optional geometric inputs such as camera\nintrinsics, poses, depth, or partial reconstructions, and then directly\nregresses the metric 3D scene geometry and cameras. MapAnything leverages a\nfactored representation of multi-view scene geometry, i.e., a collection of\ndepth maps, local ray maps, camera poses, and a metric scale factor that\neffectively upgrades local reconstructions into a globally consistent metric\nframe. Standardizing the supervision and training across diverse datasets,\nalong with flexible input augmentation, enables MapAnything to address a broad\nrange of 3D vision tasks in a single feed-forward pass, including uncalibrated\nstructure-from-motion, calibrated multi-view stereo, monocular depth\nestimation, camera localization, depth completion, and more. We provide\nextensive experimental analyses and model ablations demonstrating that\nMapAnything outperforms or matches specialist feed-forward models while\noffering more efficient joint training behavior, thus paving the way toward a\nuniversal 3D reconstruction backbone.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13414v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13414v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.359,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13428",
      "title": "Autonomous Reporting of Normal Chest X-rays by Artificial Intelligence\n  in the United Kingdom. Can We Take the Human Out of the Loop?",
      "authors": [
        "Katrina Nash",
        "James Vaz",
        "Ahmed Maiter",
        "Christopher Johns",
        "Nicholas Woznitza",
        "Aditya Kale",
        "Abdala Espinosa Morgado",
        "Rhidian Bramley",
        "Mark Hall",
        "David Lowe",
        "Alex Novak",
        "Sarim Ather"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Chest X-rays (CXRs) are the most commonly performed imaging investigation. In\nthe UK, many centres experience reporting delays due to radiologist workforce\nshortages. Artificial intelligence (AI) tools capable of distinguishing normal\nfrom abnormal CXRs have emerged as a potential solution. If normal CXRs could\nbe safely identified and reported without human input, a substantial portion of\nradiology workload could be reduced.\n  This article examines the feasibility and implications of autonomous AI\nreporting of normal CXRs. Key issues include defining normal, ensuring\ngeneralisability across populations, and managing the sensitivity-specificity\ntrade-off. It also addresses legal and regulatory challenges, such as\ncompliance with IR(ME)R and GDPR, and the lack accountability frameworks for\nerrors. Further considerations include the impact on radiologists practice, the\nneed for robust post-market surveillance, and incorporation of patient\nperspectives. While the benefits are clear, adoption must be cautious.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13428v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13428v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.282,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13450",
      "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs",
      "authors": [
        "Vincent Siu",
        "Nicholas Crispino",
        "David Park",
        "Nathan W. Henry",
        "Zhun Wang",
        "Yang Liu",
        "Dawn Song",
        "Chenguang Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We introduce SteeringControl, a benchmark for evaluating representation\nsteering methods across core alignment objectives--bias, harmful generation,\nand hallucination--and their effects on secondary behaviors such as sycophancy\nand commonsense morality. While prior alignment work often highlights\ntruthfulness or reasoning ability to demonstrate the side effects of\nrepresentation steering, we find there are many unexplored tradeoffs not yet\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\nand secondary behaviors to evaluate steering effectiveness and behavioral\nentanglement centered around five popular steering methods. To enable this, we\ncraft a modular steering framework based on unique components that serve as the\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\nLlama-3.1-8B find that strong steering performance is dependent on the specific\ncombination of steering method, model, and targeted behavior, and that severe\nconcept entanglement can result from poor combinations of these three as well.\nWe release our code here:\nhttps://github.com/wang-research-lab/SteeringControl.git.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13450v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13450v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.48,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.357,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper discusses RLHF as a background technique for alignment in LLMs but does not focus on it as a main contribution. Instead, it evaluates representation steering methods as alternatives, mentioning RLHF only in the context of its limitations, without any direct implementation, analysis, or advancement of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the creation and evaluation of new datasets for safety-relevant behaviors in LLMs, such as those for bias, harmful generation, and hallucination, along with a framework for benchmarking. This directly aligns with research on dataset creation, curation, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces SteeringControl, a comprehensive benchmark for evaluating representation steering methods in large language models (LLMs) to address alignment objectives such as bias, harmful generation, and hallucination, while also examining their effects on secondary behaviors like sycophancy and commonsense morality. By developing a modular steering framework and collecting relevant datasets, the authors assess five popular steering methods on models like Qwen-2.5-7B and Llama-3.1-8B, revealing that steering effectiveness varies significantly based on the combination of method, model, and targeted behavior, and that poor combinations can lead to severe behavioral entanglement.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new benchmark and modular framework for systematically evaluating representation steering methods, addressing inconsistencies in prior evaluations through a clever combination of existing ideas. While it advances the understanding of behavioral tradeoffs, it does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of LLM alignment, as the benchmark and framework provide standardized tools for future evaluations of steering methods. However, its influence may be limited to specific research areas rather than broader commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by offering a practical benchmark and framework that could standardize alignment research, making it essential for researchers focused on LLM safety and steering to be aware of. While insightful, it is not groundbreaking enough to be considered must-read material.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/dc99e13f81558e1bd127af1b83e2e2021ea6335c",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 5,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Vincent Siu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2360172671"
        },
        {
          "name": "Nicholas Crispino",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2254275581"
        },
        {
          "name": "David Park",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381055694"
        },
        {
          "name": "Nathan W. Henry",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380607200"
        },
        {
          "name": "Zhun Wang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2316920439"
        },
        {
          "name": "Yang Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365077988"
        },
        {
          "name": "D. Song",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2255915277"
        },
        {
          "name": "Chenguang Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2360260857"
        }
      ]
    },
    {
      "id": "2509.13471",
      "title": "An LLM Agentic Approach for Legal-Critical Software: A Case Study for\n  Tax Prep Software",
      "authors": [
        "Sina Gogani-Khiabani",
        "Ashutosh Trivedi",
        "Diptikalyan Saha",
        "Saeid Tizpaz-Niari"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) show promise for translating natural-language\nstatutes into executable logic, but reliability in legally critical settings\nremains challenging due to ambiguity and hallucinations. We present an agentic\napproach for developing legal-critical software, using U.S. federal tax\npreparation as a case study. The key challenge is test-case generation under\nthe oracle problem, where correct outputs require interpreting law. Building on\nmetamorphic testing, we introduce higher-order metamorphic relations that\ncompare system outputs across structured shifts among similar individuals.\nBecause authoring such relations is tedious and error-prone, we use an\nLLM-driven, role-based framework to automate test generation and code\nsynthesis. We implement a multi-agent system that translates tax code into\nexecutable software and incorporates a metamorphic-testing agent that searches\nfor counterexamples. In experiments, our framework using a smaller model\n(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier\nmodels (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results\nsupport agentic LLM methodologies as a path to robust, trustworthy\nlegal-critical software from natural-language specifications.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13471v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13471v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.334,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an agentic LLM framework for legal-critical software development, emphasizing metamorphic testing and code synthesis, but does not involve training models with human feedback or using reinforcement learning to align AI with preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs LLMs and metamorphic testing for multi-step reasoning in software validation, but it does not adapt diffusion models or use iterative refinement processes for logical tasks as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13474",
      "title": "Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot\n  Localization",
      "authors": [
        "Yujia Lin",
        "Nicholas Evans"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Ensuring accurate localization of robots in environments without GPS\ncapability is a challenging task. Visual Place Recognition (VPR) techniques can\npotentially achieve this goal, but existing RGB-based methods are sensitive to\nchanges in illumination, weather, and other seasonal changes. Existing\ncross-modal localization methods leverage the geometric properties of RGB\nimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.\nCurrently, state-of-the-art methods struggle in complex scenes, fine-grained or\nhigh-resolution matching, and situations where changes can occur in viewpoint.\nIn this work, we introduce a framework we call Semantic-Enhanced Cross-Modal\nPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGB\nimages for robust localization in LiDAR maps. Our proposed method introduces: a\nVMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature\nFusion (SAFF) module for using both place descriptors and segmentation masks;\nLiDAR descriptors that incorporate both semantics and geometry; and a\ncross-modal semantic attention mechanism in NetVLAD to improve matching.\nIncorporating the semantic information also was instrumental in designing a\nMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in\na contrastive learning framework. Our experimental work on the KITTI and\nKITTI-360 datasets show that SCM-PR achieves state-of-the-art performance\ncompared to other cross-modal place recognition methods.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13474v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13474v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.321,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13482",
      "title": "Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice\n  Vector Quantization",
      "authors": [
        "Hao Xu",
        "Xiaolin Wu",
        "Xi Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its\nphotorealistic rendering quality and real-time performance, but it generates\nmassive amounts of data. Hence compressing 3DGS data is necessary for the cost\neffectiveness of 3DGS models. Recently, several anchor-based neural compression\nmethods have been proposed, achieving good 3DGS compression performance.\nHowever, they all rely on uniform scalar quantization (USQ) due to its\nsimplicity. A tantalizing question is whether more sophisticated quantizers can\nimprove the current 3DGS compression methods with very little extra overhead\nand minimal change to the system. The answer is yes by replacing USQ with\nlattice vector quantization (LVQ). To better capture scene-specific\ncharacteristics, we optimize the lattice basis for each scene, improving LVQ's\nadaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a\nbalance between the R-D efficiency of vector quantization and the low\ncomplexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS\ncompression architectures, enhancing their R-D performance with minimal\nmodifications and computational overhead. Moreover, by scaling the lattice\nbasis vectors, SALVQ can dynamically adjust lattice density, enabling a single\nmodel to accommodate multiple bit rate targets. This flexibility eliminates the\nneed to train separate models for different compression levels, significantly\nreducing training time and memory consumption.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13482v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13482v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.289,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.341,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13484",
      "title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes",
      "authors": [
        "Liu Liu",
        "Alexandra Kudaeva",
        "Marco Cipriano",
        "Fatimeh Al Ghannam",
        "Freya Tan",
        "Gerard de Melo",
        "Andres Sevtsuk"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13484v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13484v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.346,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13487",
      "title": "Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline\n  Generation",
      "authors": [
        "Abubakari Alidu",
        "Michele Ciavotta",
        "Flavio DePaoli"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Developing reliable data enrichment pipelines demands significant engineering\nexpertise. We present Prompt2DAG, a methodology that transforms natural\nlanguage descriptions into executable Apache Airflow DAGs. We evaluate four\ngeneration approaches -- Direct, LLM-only, Hybrid, and Template-based -- across\n260 experiments using thirteen LLMs and five case studies to identify optimal\nstrategies for production-grade automation. Performance is measured using a\npenalized scoring framework that combines reliability with code quality (SAT),\nstructural integrity (DST), and executability (PCT). The Hybrid approach\nemerges as the optimal generative method, achieving a 78.5% success rate with\nrobust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly\noutperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.\nOur findings show that reliability, not intrinsic code quality, is the primary\ndifferentiator. Cost-effectiveness analysis reveals the Hybrid method is over\ntwice as efficient as Direct prompting per successful DAG. We conclude that a\nstructured, hybrid approach is essential for balancing flexibility and\nreliability in automated workflow generation, offering a viable path to\ndemocratize data pipeline development.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13487v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13487v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.427,
      "weak_supervision_score": 0.449,
      "diffusion_reasoning_score": 0.482,
      "distributed_training_score": 0.401,
      "datasets_score": 0.422,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "The paper focuses on using LLMs for generating data pipelines from natural language descriptions, without any mention of training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper does not involve training machine learning models or generating labels from noisy sources; it centers on LLM-based code generation for workflows, not weak supervision methodologies.",
      "diffusion_reasoning_justification": "The paper discusses prompting strategies for LLMs to generate code, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "The paper is about automating data pipeline generation using LLMs, with no discussion of parallel computing, distributed algorithms, or accelerating model training across nodes.",
      "datasets_justification": "While the paper mentions experiments with case studies and LLMs, its main contribution is on methodology for code generation, not on creating, analyzing, benchmarking, or evaluating datasets themselves.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13496",
      "title": "BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden\n  Social Biases in Text-to-Image Generation",
      "authors": [
        "Rajatsubhra Chakraborty",
        "Xujun Che",
        "Depeng Xu",
        "Cori Faklaris",
        "Xi Niu",
        "Shuhan Yuan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Bias discovery is critical for black-box generative models, especiall\ntext-to-image (TTI) models. Existing works predominantly focus on output-level\ndemographic distributions, which do not necessarily guarantee concept\nrepresentations to be disentangled post-mitigation. We propose BiasMap, a\nmodel-agnostic framework for uncovering latent concept-level representational\nbiases in stable diffusion models. BiasMap leverages cross-attention\nattribution maps to reveal structural entanglements between demographics (e.g.,\ngender, race) and semantics (e.g., professions), going deeper into\nrepresentational bias during the image generation. Using attribution maps of\nthese concepts, we quantify the spatial demographics-semantics concept\nentanglement via Intersection over Union (IoU), offering a lens into bias that\nremains hidden in existing fairness discovery approaches. In addition, we\nfurther utilize BiasMap for bias mitigation through energy-guided diffusion\nsampling that directly modifies latent noise space and minimizes the expected\nSoftIoU during the denoising process. Our findings show that existing fairness\ninterventions may reduce the output distributional gap but often fail to\ndisentangle concept-level coupling, whereas our mitigation method can mitigate\nconcept entanglement in image generation while complementing distributional\nbias mitigation.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13496v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13496v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.522,
      "distributed_training_score": 0.363,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for bias detection and mitigation in text-to-image generation, specifically through cross-attention maps and energy-guided sampling. However, it does not adapt the iterative refinement process of diffusion for solving complex logical tasks or multi-step reasoning, such as treating a Chain-of-Thought as a single entity. Since there is no component for logical reasoning, the paper does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13499",
      "title": "Reproducible workflow for online AI in digital health",
      "authors": [
        "Susobhan Ghosh",
        "Bhanu T. Gulapalli",
        "Daiqi Gao",
        "Asim Gazi",
        "Anna Trella",
        "Ziping Xu",
        "Kelly Zhang",
        "Susan A. Murphy"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Online artificial intelligence (AI) algorithms are an important component of\ndigital health interventions. These online algorithms are designed to\ncontinually learn and improve their performance as streaming data is collected\non individuals. Deploying online AI presents a key challenge: balancing\nadaptability of online AI with reproducibility. Online AI in digital\ninterventions is a rapidly evolving area, driven by advances in algorithms,\nsensors, software, and devices. Digital health intervention development and\ndeployment is a continuous process, where implementation - including the AI\ndecision-making algorithm - is interspersed with cycles of re-development and\noptimization. Each deployment informs the next, making iterative deployment a\ndefining characteristic of this field. This iterative nature underscores the\nimportance of reproducibility: data collected across deployments must be\naccurately stored to have scientific utility, algorithm behavior must be\nauditable, and results must be comparable over time to facilitate scientific\ndiscovery and trustworthy refinement. This paper proposes a reproducible\nscientific workflow for developing, deploying, and analyzing online AI\ndecision-making algorithms in digital health interventions. Grounded in\npractical experience from multiple real-world deployments, this workflow\naddresses key challenges to reproducibility across all phases of the online AI\nalgorithm development life-cycle.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13499v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13499v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.446,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.378,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on reproducible workflows for online AI algorithms in digital health, emphasizing online learning from streaming data (e.g., passive sensing and self-reports) to make decisions in adaptive feedback loops. While it involves human data like self-reports, it does not describe training a separate reward model on human-ranked data or using reinforcement learning to fine-tune a main model based on human preferences, which are core to RLHF. Thus, the paper does not align with the specific definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13504",
      "title": "LivePyxel: Accelerating image annotations with a Python-integrated\n  webcam live streaming",
      "authors": [
        "Uriel Garcilazo-Cruz",
        "Joseph O. Okeme",
        "Rodrigo A. Vargas--Hernández"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The lack of flexible annotation tools has hindered the deployment of AI\nmodels in some scientific areas. Most existing image annotation software\nrequires users to upload a precollected dataset, which limits support for\non-demand pipelines and introduces unnecessary steps to acquire images. This\nconstraint is particularly problematic in laboratory environments, where\nreal-time data acquisition from instruments such as microscopes is increasingly\ncommon. In this work, we introduce \\texttt{LivePixel}, a Python-based graphical\nuser interface that integrates with imaging systems, such as webcams,\nmicroscopes, and others, to enable real-time image annotation. LivePyxel is\ndesigned to be easy to use through a simple interface that allows users to\nprecisely delimit areas for annotation using tools commonly found in commercial\ngraphics editing software. Of particular interest is the availability of\nB\\'ezier splines and binary masks, and the software's capacity to work with\nnon-destructive layers that enable high-performance editing. LivePyxel also\nintegrates a wide compatibility across video devices, and it's optimized for\nobject detection operations via the use of OpenCV in combination with\nhigh-performance libraries designed to handle matrix and linear algebra\noperations via Numpy effectively. LivePyxel facilitates seamless data\ncollection and labeling, accelerating the development of AI models in\nexperimental workflows. LivePyxel freely available at\nhttps://github.com/UGarCil/LivePyxel",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13504v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13504v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.356,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces LivePyxel, a tool for real-time image annotation to create high-quality, manually labeled datasets, which supports traditional supervised learning. While it facilitates data collection for AI models, it does not involve programmatically generating labels from noisy or imprecise sources, a core aspect of weak supervision. At most, it indirectly aids in preparing data that could be used in broader machine learning pipelines, including weak supervision, but this is not the paper's focus.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13506",
      "title": "DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised\n  H-Transform",
      "authors": [
        "Xingzi Xu",
        "Qi Li",
        "Shuwen Qiu",
        "Julien Han",
        "Karim Bouyarmane"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion models enable high-quality virtual try-on (VTO) with their\nestablished image synthesis abilities. Despite the extensive end-to-end\ntraining of large pre-trained models involved in current VTO methods,\nreal-world applications often prioritize limited training and inference,\nserving, and deployment budgets for VTO. To solve this obstacle, we apply\nDoob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained\nunconditional models for downstream image-conditioned VTO abilities. DEFT\nfreezes the pre-trained model's parameters and trains a small h-transform\nnetwork to learn a conditional h-transform. The h-transform network allows\ntraining only 1.42 percent of the frozen parameters, compared to a baseline of\n5.52 percent in traditional parameter-efficient fine-tuning (PEFT).\n  To further improve DEFT's performance and decrease existing models' inference\ntime, we additionally propose an adaptive consistency loss. Consistency\ntraining distills slow but high-performing diffusion models into a fast one\nwhile retaining performance by enforcing consistencies along the inference\npath. Inspired by constrained optimization, instead of distillation, we combine\nthe consistency loss and the denoising score matching loss in a data-adaptive\nmanner for fine-tuning existing VTO models at a low cost. Empirical results\nshow the proposed DEFT-VTON method achieves state-of-the-art performance on VTO\ntasks, with as few as 15 denoising steps, while maintaining competitive\nresults.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13506v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13506v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.481,
      "distributed_training_score": 0.372,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an efficient adaptation of diffusion models for virtual try-on image synthesis, focusing on conditional image generation and inference speed improvements. It does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks; instead, it applies diffusion iteratively for visual refinement, which does not align with the topic's requirements.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13507",
      "title": "Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian\n  Recognition in Autonomous Driving",
      "authors": [
        "Artem Savkin",
        "Thomas Lapotre",
        "Kevin Strauss",
        "Uzair Akbar",
        "Federico Tombari"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In the autonomous driving area synthetic data is crucial for cover specific\ntraffic scenarios which autonomous vehicle must handle. This data commonly\nintroduces domain gap between synthetic and real domains. In this paper we\ndeploy data augmentation to generate custom traffic scenarios with VRUs in\norder to improve pedestrian recognition. We provide a pipeline for augmentation\nof the Cityscapes dataset with virtual pedestrians. In order to improve\naugmentation realism of the pipeline we reveal a novel generative network\narchitecture for adversarial learning of the data-set lighting conditions. We\nalso evaluate our approach on the tasks of semantic and instance segmentation.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13507v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13507v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.346,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13508",
      "title": "FunKAN: Functional Kolmogorov-Arnold Network for Medical Image\n  Enhancement and Segmentation",
      "authors": [
        "Maksim Penkin",
        "Andrey Krylov"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical image enhancement and segmentation are critical yet challenging tasks\nin modern clinical practice, constrained by artifacts and complex anatomical\nvariations. Traditional deep learning approaches often rely on complex\narchitectures with limited interpretability. While Kolmogorov-Arnold networks\noffer interpretable solutions, their reliance on flattened feature\nrepresentations fundamentally disrupts the intrinsic spatial structure of\nimaging data. To address this issue we propose a Functional Kolmogorov-Arnold\nNetwork (FunKAN) -- a novel interpretable neural framework, designed\nspecifically for image processing, that formally generalizes the\nKolmogorov-Arnold representation theorem onto functional spaces and learns\ninner functions using Fourier decomposition over the basis Hermite functions.\nWe explore FunKAN on several medical image processing tasks, including Gibbs\nringing suppression in magnetic resonance images, benchmarking on IXI dataset.\nWe also propose U-FunKAN as state-of-the-art binary medical segmentation model\nwith benchmarks on three medical datasets: BUSI (ultrasound images), GlaS\n(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting\nbreast cancer, glands and polyps, respectively. Experiments on those diverse\ndatasets demonstrate that our approach outperforms other KAN-based backbones in\nboth medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work\nbridges the gap between theoretical function approximation and medical image\nanalysis, offering a robust, interpretable solution for clinical applications.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13508v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13508v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.299,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13515",
      "title": "Multimodal Hate Detection Using Dual-Stream Graph Neural Networks",
      "authors": [
        "Jiangbei Yue",
        "Shuonan Yang",
        "Tailin Chen",
        "Jianbo Jiao",
        "Zeyu Fu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Hateful videos present serious risks to online safety and real-world\nwell-being, necessitating effective detection methods. Although multimodal\nclassification approaches integrating information from several modalities\noutperform unimodal ones, they typically neglect that even minimal hateful\ncontent defines a video's category. Specifically, they generally treat all\ncontent uniformly, instead of emphasizing the hateful components. Additionally,\nexisting multimodal methods cannot systematically capture structured\ninformation in videos, limiting the effectiveness of multimodal fusion. To\naddress these limitations, we propose a novel multimodal dual-stream graph\nneural network model. It constructs an instance graph by separating the given\nvideo into several instances to extract instance-level features. Then, a\ncomplementary weight graph assigns importance weights to these features,\nhighlighting hateful instances. Importance weights and instance features are\ncombined to generate video labels. Our model employs a graph-based framework to\nsystematically model structured relationships within and across modalities.\nExtensive experiments on public datasets show that our model is\nstate-of-the-art in hateful video classification and has strong explainability.\nCode is available:\nhttps://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13515v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13515v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.334,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13525",
      "title": "ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using\n  Diffusion Priors",
      "authors": [
        "Romain Hardy",
        "Tyler Berzin",
        "Pranav Rajpurkar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Three-dimensional (3D) scene understanding in colonoscopy presents\nsignificant challenges that necessitate automated methods for accurate depth\nestimation. However, existing depth estimation models for endoscopy struggle\nwith temporal consistency across video sequences, limiting their applicability\nfor 3D reconstruction. We present ColonCrafter, a diffusion-based depth\nestimation model that generates temporally consistent depth maps from monocular\ncolonoscopy videos. Our approach learns robust geometric priors from synthetic\ncolonoscopy sequences to generate temporally consistent depth maps. We also\nintroduce a style transfer technique that preserves geometric structure while\nadapting real clinical videos to match our synthetic training domain.\nColonCrafter achieves state-of-the-art zero-shot performance on the C3VD\ndataset, outperforming both general-purpose and endoscopy-specific approaches.\nAlthough full trajectory 3D reconstruction remains a challenge, we demonstrate\nclinically relevant applications of ColonCrafter, including 3D point cloud\ngeneration and surface coverage assessment.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13525v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13525v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.332,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a diffusion-based model for depth estimation in colonoscopy videos, focusing on generating temporally consistent depth maps from visual data. However, it does not involve adapting diffusion for multi-step logical reasoning, such as treating a Chain-of-Thought as a single entity for holistic correction. The application is purely generative for computer vision tasks, lacking any component of complex logical tasks as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13536",
      "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM",
      "authors": [
        "Yinlong Bai",
        "Hongxin Zhang",
        "Sheng Zhong",
        "Junkai Niu",
        "Hai Li",
        "Yijia He",
        "Yi Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant\nimpact on rendering and reconstruction techniques. Current research\npredominantly focuses on improving rendering performance and reconstruction\nquality using high-performance desktop GPUs, largely overlooking applications\nfor embedded platforms like micro air vehicles (MAVs). These devices, with\ntheir limited computational resources and memory, often face a trade-off\nbetween system performance and reconstruction quality. In this paper, we\nimprove existing methods in terms of GPU memory usage while enhancing rendering\nquality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we\npropose merging them in voxel space based on geometric similarity. This reduces\nGPU memory usage without impacting system runtime performance. Furthermore,\nrendering quality is improved by initializing 3D Gaussian primitives via\nPatch-Grid (PG) point sampling, enabling more accurate modeling of the entire\nscene. Quantitative and qualitative evaluations on publicly available datasets\ndemonstrate the effectiveness of our improvements.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13536v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13536v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.252,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.335,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13541",
      "title": "Semantic 3D Reconstructions with SLAM for Central Airway Obstruction",
      "authors": [
        "Ayberk Acar",
        "Fangjie Li",
        "Hao Li",
        "Lidia Al-Zogbi",
        "Kanyifeechukwu Jane Oguine",
        "Susheela Sharma Stern",
        "Jesse F. d'Almeida",
        "Robert J. Webster III",
        "Ipek Oguz",
        "Jie Ying Wu"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Central airway obstruction (CAO) is a life-threatening condition with\nincreasing incidence, caused by tumors in and outside of the airway.\nTraditional treatment methods such as bronchoscopy and electrocautery can be\nused to remove the tumor completely; however, these methods carry a high risk\nof complications. Recent advances allow robotic interventions with lesser risk.\nThe combination of robot interventions with scene understanding and mapping\nalso opens up the possibilities for automation. We present a novel pipeline\nthat enables real-time, semantically informed 3D reconstructions of the central\nairway using monocular endoscopic video.\n  Our approach combines DROID-SLAM with a segmentation model trained to\nidentify obstructive tissues. The SLAM module reconstructs the 3D geometry of\nthe airway in real time, while the segmentation masks guide the annotation of\nobstruction regions within the reconstructed point cloud. To validate our\npipeline, we evaluate the reconstruction quality using ex vivo models.\n  Qualitative and quantitative results show high similarity between ground\ntruth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By\nintegrating segmentation directly into the SLAM workflow, our system produces\nannotated 3D maps that highlight clinically relevant regions in real time.\nHigh-speed capabilities of the pipeline allows quicker reconstructions compared\nto previous work, reflecting the surgical scene more accurately.\n  To the best of our knowledge, this is the first work to integrate semantic\nsegmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our\nframework is modular and can generalize to other anatomies or procedures with\nminimal changes, offering a promising step toward autonomous robotic\ninterventions.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13541v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.273,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13547",
      "title": "AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for\n  Enhanced Problem-Solving",
      "authors": [
        "Harper Reed",
        "Michael Sugimura",
        "Angelo Zangari"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "We investigate whether giving LLM agents the collaborative tools and autonomy\nthat humans naturally use for problem solving can improve their performance. We\nequip Claude Code agents with MCP-based social media and journaling tools and\nallow them to use these tools as they see fit. Across 34 Aider Polyglot Python\nprogramming challenges, collaborative tools substantially improve performance\non the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and\n12-38% faster completion than baseline agents. Effects on the full challenge\nset are mixed, suggesting these tools act as performance enhancers when\nadditional reasoning scaffolding is most needed. Surprisingly, Different models\nnaturally adopted distinct collaborative strategies without explicit\ninstruction. Sonnet 3.7 engaged broadly across tools and benefited from\narticulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,\nleaning on journal-based semantic search when problems were genuinely\ndifficult. This mirrors how human developers adjust collaboration based on\nexpertise and task complexity. Behavioral analysis shows agents prefer writing\nover reading by about 2-9x, indicating that structured articulation drives much\nof the improvement rather than information access alone. Overall, AI agents can\nsystematically benefit from human-inspired collaboration tools at the edge of\ntheir capabilities, pointing to adaptive collaborative interfaces as reasoning\nenhancers rather than universal efficiency boosts.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13547v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13547v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.449,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.36,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on equipping AI agents with collaborative tools like social media and journaling to enhance problem-solving, without any mention of human feedback, reward models, or reinforcement learning for model fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for logical tasks; it instead explores human-like collaborative tools for reasoning, such as journaling and social media, without any reference to Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13550",
      "title": "Complexity Bounds for Smooth Convex Multiobjective Optimization",
      "authors": [
        "Phillipe R. Sampaio"
      ],
      "categories": [
        "math.OC (Optimization and Control)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We study the oracle complexity of finding $\\varepsilon$-Pareto stationary\npoints in smooth multiobjective optimization with $m$ objectives. The progress\nmetric is the Pareto stationarity gap $\\mathcal{G}(x)$ (the norm of an optimal\nconvex combination of gradients). Our contributions are fourfold. (i) For\nstrongly convex objectives, any span first-order method (iterates lie in the\nspan of past gradients) exhibits linear convergence no faster than\n$\\exp(-\\Theta(T/\\sqrt{\\kappa}))$ after $T$ oracle calls, where $\\kappa$ is the\ncondition number, implying $\\Theta(\\sqrt{\\kappa}\\log(1/\\varepsilon))$\niterations; this matches classical accelerated upper bounds. (ii) For convex\nproblems and oblivious one-step methods (a fixed scalarization with\npre-scheduled step sizes), we prove a lower bound of order $1/T$ on the best\ngradient norm among the first $T$ iterates. (iii) Although accelerated gradient\ndescent is outside this restricted class, it is an oblivious span method and\nattains the same $1/T$ upper rate on a fixed scalarization. (iv) For convex\nproblems and general span methods with adaptive scalarizations, we establish a\nuniversal lower bound of order $1/T^{2}$ on the gradient norm of the final\niterate after $T$ steps, highlighting a gap between known upper bounds and\nworst-case guarantees. All bounds hold on non-degenerate instances with\ndistinct objectives and non-singleton Pareto fronts; rates are stated up to\nuniversal constants and natural problem scaling.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13550v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13550v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.263,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.274,
      "distributed_training_score": 0.319,
      "datasets_score": 0.206,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13570",
      "title": "Gen AI in Proof-based Math Courses: A Pilot Study",
      "authors": [
        "Hannah Klawa",
        "Shraddha Rajpal",
        "Cigole Thomas"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "math.HO (History and Overview)"
      ],
      "abstract": "With the rapid rise of generative AI in higher education and the\nunreliability of current AI detection tools, developing policies that encourage\nstudent learning and critical thinking has become increasingly important. This\nstudy examines student use and perceptions of generative AI across three\nproof-based undergraduate mathematics courses: a first-semester abstract\nalgebra course, a topology course and a second-semester abstract algebra\ncourse. In each case, course policy permitted some use of generative AI.\nDrawing on survey responses and student interviews, we analyze how students\nengaged with AI tools, their perceptions of generative AI's usefulness and\nlimitations, and what implications these perceptions hold for teaching\nproof-based mathematics. We conclude by discussing future considerations for\nintegrating generative AI into proof-based mathematics instruction.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13570v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13570v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.276,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13574",
      "title": "Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic\n  Policies: Mitigating Multi-Step Inference Degradation",
      "authors": [
        "Zidong Chen",
        "Zihao Guo",
        "Peng Wang",
        "ThankGod Itua Egbe",
        "Yan Lyu",
        "Chenghao Qian"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Flow matching has emerged as a competitive framework for learning\nhigh-quality generative policies in robotics; however, we find that\ngeneralisation arises and saturates early along the flow trajectory, in\naccordance with recent findings in the literature. We further observe that\nincreasing the number of Euler integration steps during inference\ncounter-intuitively and universally degrades policy performance. We attribute\nthis to (i) additional, uniformly spaced integration steps oversample the\nlate-time region, thereby constraining actions towards the training\ntrajectories and reducing generalisation; and (ii) the learned velocity field\nbecoming non-Lipschitz as integration time approaches 1, causing instability.\nTo address these issues, we propose a novel policy that utilises non-uniform\ntime scheduling (e.g., U-shaped) during training, which emphasises both early\nand late temporal stages to regularise policy training, and a dense-jump\nintegration schedule at inference, which uses a single-step integration to\nreplace the multi-step integration beyond a jump point, to avoid unstable areas\naround 1. Essentially, our policy is an efficient one-step learner that still\npushes forward performance through multi-step integration, yielding up to 23.7%\nperformance gains over state-of-the-art baselines across diverse robotic tasks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13574v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13574v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.372,
      "datasets_score": 0.263,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on flow matching for generative robotic policies, using techniques like non-uniform time scheduling and dense-jump integration to improve performance. It does not involve human feedback, reward modeling from human-ranked data, or fine-tuning via reinforcement learning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper mentions diffusion models briefly as related work for generative policies in robotics and contrasts them with flow matching, but its main contribution is on improving flow matching for action generation, not adapting diffusion for multi-step logical reasoning or holistic Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13576",
      "title": "Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for\n  Sparse-View CT",
      "authors": [
        "Haodong Li",
        "Shuo Han",
        "Haiyang Mao",
        "Yu Shi",
        "Changsheng Fang",
        "Jianjia Zhang",
        "Weiwen Wu",
        "Hengyong Yu"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Sparse-View CT (SVCT) reconstruction enhances temporal resolution and reduces\nradiation dose, yet its clinical use is hindered by artifacts due to view\nreduction and domain shifts from scanner, protocol, or anatomical variations,\nleading to performance degradation in out-of-distribution (OOD) scenarios. In\nthis work, we propose a Cross-Distribution Diffusion Priors-Driven Iterative\nReconstruction (CDPIR) framework to tackle the OOD problem in SVCT. CDPIR\nintegrates cross-distribution diffusion priors, derived from a Scalable\nInterpolant Transformer (SiT), with model-based iterative reconstruction\nmethods. Specifically, we train a SiT backbone, an extension of the Diffusion\nTransformer (DiT) architecture, to establish a unified stochastic interpolant\nframework, leveraging Classifier-Free Guidance (CFG) across multiple datasets.\nBy randomly dropping the conditioning with a null embedding during training,\nthe model learns both domain-specific and domain-invariant priors, enhancing\ngeneralizability. During sampling, the globally sensitive transformer-based\ndiffusion model exploits the cross-distribution prior within the unified\nstochastic interpolant framework, enabling flexible and stable control over\nmulti-distribution-to-noise interpolation paths and decoupled sampling\nstrategies, thereby improving adaptation to OOD reconstruction. By alternating\nbetween data fidelity and sampling updates, our model achieves state-of-the-art\nperformance with superior detail preservation in SVCT reconstructions.\nExtensive experiments demonstrate that CDPIR significantly outperforms existing\napproaches, particularly under OOD conditions, highlighting its robustness and\npotential clinical value in challenging imaging scenarios.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13576v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13576v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.484,
      "distributed_training_score": 0.397,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for Sparse-View CT image reconstruction using diffusion models to handle out-of-distribution scenarios and improve image quality. It employs iterative refinement for image generation and reconstruction, but this is applied to medical imaging tasks, not to solving complex logical tasks or treating a 'Chain-of-Thought' as an entity for multi-step reasoning. There is no component for logical reasoning, making it unrelated to the defined topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13577",
      "title": "Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for\n  Trajectory Prediction in Autonomous Vehicles",
      "authors": [
        "Tongfei Guo",
        "Lili Su"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Trajectory prediction is central to the safe and seamless operation of\nautonomous vehicles (AVs). In deployment, however, prediction models inevitably\nface distribution shifts between training data and real-world conditions, where\nrare or underrepresented traffic scenarios induce out-of-distribution (OOD)\ncases. While most prior OOD detection research in AVs has concentrated on\ncomputer vision tasks such as object detection and segmentation,\ntrajectory-level OOD detection remains largely underexplored. A recent study\nformulated this problem as a quickest change detection (QCD) task, providing\nformal guarantees on the trade-off between detection delay and false alarms\n[1]. Building on this foundation, we propose a new framework that introduces\nadaptive mechanisms to achieve robust detection in complex driving\nenvironments. Empirical analysis across multiple real-world datasets reveals\nthat prediction errors -- even on in-distribution samples -- exhibit\nmode-dependent distributions that evolve over time with dataset-specific\ndynamics. By explicitly modeling these error modes, our method achieves\nsubstantial improvements in both detection delay and false alarm rates.\nComprehensive experiments on established trajectory prediction benchmarks show\nthat our framework significantly outperforms prior UQ- and vision-based OOD\napproaches in both accuracy and computational efficiency, offering a practical\npath toward reliable, driving-aware autonomy.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13577v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13577v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.379,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on adaptive OOD detection for trajectory prediction in autonomous vehicles, using quickest change detection and error mode modeling. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13579",
      "title": "TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement\n  Learning",
      "authors": [
        "Momchil S. Tomov",
        "Sang Uk Lee",
        "Hansford Hendrago",
        "Jinwook Huh",
        "Teawon Han",
        "Forbes Howington",
        "Rafael da Silva",
        "Gianmarco Bernasconi",
        "Marc Heim",
        "Samuel Findler",
        "Xiaonan Ji",
        "Alexander Boule",
        "Michael Napoli",
        "Kuo Chen",
        "Jesse Miller",
        "Boaz Floor",
        "Yunqing Hu"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We present TreeIRL, a novel planner for autonomous driving that combines\nMonte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to\nachieve state-of-the-art performance in simulation and in real-world driving.\nThe core idea is to use MCTS to find a promising set of safe candidate\ntrajectories and a deep IRL scoring function to select the most human-like\namong them. We evaluate TreeIRL against both classical and state-of-the-art\nplanners in large-scale simulations and on 500+ miles of real-world autonomous\ndriving in the Las Vegas metropolitan area. Test scenarios include dense urban\ntraffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves\nthe best overall performance, striking a balance between safety, progress,\ncomfort, and human-likeness. To our knowledge, our work is the first\ndemonstration of MCTS-based planning on public roads and underscores the\nimportance of evaluating planners across a diverse set of metrics and in\nreal-world environments. TreeIRL is highly extensible and could be further\nimproved with reinforcement learning and imitation learning, providing a\nframework for exploring different combinations of classical and learning-based\napproaches to solve the planning bottleneck in autonomous driving.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13579v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13579v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.319,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Inverse Reinforcement Learning (IRL) to train a scoring function from expert human driving demonstrations, which involves learning from human-like data to evaluate trajectories. However, it does not align with RLHF's core definition, as RLHF specifically requires training a reward model on human-ranked preferences and then using it to fine-tune a model via reinforcement learning. Here, the IRL component is used for trajectory selection in a planning framework, without the explicit RL fine-tuning step, making it only loosely related.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13586",
      "title": "Annotating Satellite Images of Forests with Keywords from a Specialized\n  Corpus in the Context of Change Detection",
      "authors": [
        "Nathalie Neptune",
        "Josiane Mothe"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)",
        "cs.IR (Information Retrieval)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "The Amazon rain forest is a vital ecosystem that plays a crucial role in\nregulating the Earth's climate and providing habitat for countless species.\nDeforestation in the Amazon is a major concern as it has a significant impact\non global carbon emissions and biodiversity. In this paper, we present a method\nfor detecting deforestation in the Amazon using image pairs from Earth\nobservation satellites. Our method leverages deep learning techniques to\ncompare the images of the same area at different dates and identify changes in\nthe forest cover. We also propose a visual semantic model that automatically\nannotates the detected changes with relevant keywords. The candidate annotation\nfor images are extracted from scientific documents related to the Amazon\nregion. We evaluate our approach on a dataset of Amazon image pairs and\ndemonstrate its effectiveness in detecting deforestation and generating\nrelevant annotations. Our method provides a useful tool for monitoring and\nstudying the impact of deforestation in the Amazon. While we focus on\nenvironment applications of our work by using images of deforestation in the\nAmazon rain forest to demonstrate the effectiveness of our proposed approach,\nit is generic enough to be applied to other domains.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13586v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13586v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.322,
      "datasets_score": 0.41,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes introducing novel multimodal datasets comprising satellite image pairs and scientific text, specifically for evaluating visual semantic embedding (VSE) models in the context of deforestation detection. It also involves benchmarking these datasets by providing baseline performance using deep neural network-based models. This directly aligns with research on creating, analyzing, and evaluating datasets for machine learning and AI applications, making it a core focus of the paper.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a method for detecting deforestation in the Amazon using pairs of satellite images analyzed with deep learning techniques, while automatically annotating detected changes with relevant keywords extracted from scientific documents on the region. The approach builds on previous work by incorporating improvements such as attention mechanisms in the encoder-decoder model, pre-selecting candidate labels via keyword extraction, and evaluating across different datasets and corpora to demonstrate enhanced performance in change detection and annotation, ultimately providing a generic tool for environmental monitoring.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing deep learning techniques with keyword extraction for annotating satellite images in a new context like deforestation, building on prior work with enhancements such as attention blocks and different corpora.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like computer vision and environmental monitoring due to its practical applications in deforestation detection and generic adaptability, though its influence may remain confined to specialized areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper represents a strong contribution with practical methods for multimodal data integration in environmental contexts, making it valuable for researchers in AI and sustainability, though not essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1570db15b66b4ad39947be5c3c30e90de40afba7",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 6,
      "average_h_index": 4.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Nathalie Neptune",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/80103864"
        },
        {
          "name": "Josiane Mothe",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2124421744"
        }
      ]
    },
    {
      "id": "2509.13588",
      "title": "Programmable Cognitive Bias in Social Agents",
      "authors": [
        "Xuan Liu",
        "Haoyang Shang",
        "Haojian Jin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "This paper introduces CoBRA, a novel toolkit for systematically specifying\nagent behavior in LLM-based social simulation. We found that conventional\napproaches that specify agent behaviors through implicit natural language\ndescriptions cannot yield consistent behaviors across models, and the produced\nagent behaviors do not capture the nuances of the descriptions. In contrast,\nCoBRA presents a new approach to program agents' cognitive biases explicitly,\nby grounding agents' expected behaviors using classic social science\nexperiments. CoBRA has two components: (1) Cognitive Bias Index that measures\nthe cognitive bias of a social agent, by quantifying the agent's reactions in a\nset of validated classical social science experiments; (2) Behavioral\nRegulation Engine that aligns the agent's behavior to demonstrate controlled\ncognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and\ntechnical benchmarks. Our results suggest that CoBRA can precisely program the\ncognitive bias demonstrated in a social agent in a model-agnostic manner.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13588v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13588v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.284,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of CoBRA, a toolkit for programming cognitive biases in LLM-based social agents using social science experiments, involving measurement via a Cognitive Bias Index and adjustment through methods like prompt engineering, activation modifications, or fine-tuning. While fine-tuning is mentioned, there is no indication of using human feedback to train a reward model or applying reinforcement learning for alignment with human preferences. RLHF specifically requires human-ranked data and a reinforcement learning process, which is absent here, making the paper unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13590",
      "title": "Intelligent Healthcare Imaging Platform An VLM-Based Framework for\n  Automated Medical Image Analysis and Clinical Report Generation",
      "authors": [
        "Samer Al-Hamadani"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The rapid advancement of artificial intelligence (AI) in healthcare imaging\nhas revolutionized diagnostic medicine and clinical decision-making processes.\nThis work presents an intelligent multimodal framework for medical image\nanalysis that leverages Vision-Language Models (VLMs) in healthcare\ndiagnostics. The framework integrates Google Gemini 2.5 Flash for automated\ntumor detection and clinical report generation across multiple imaging\nmodalities including CT, MRI, X-ray, and Ultrasound. The system combines visual\nfeature extraction with natural language processing to enable contextual image\ninterpretation, incorporating coordinate verification mechanisms and\nprobabilistic Gaussian modeling for anomaly distribution. Multi-layered\nvisualization techniques generate detailed medical illustrations, overlay\ncomparisons, and statistical representations to enhance clinical confidence,\nwith location measurement achieving 80 pixels average deviation. Result\nprocessing utilizes precise prompt engineering and textual analysis to extract\nstructured clinical information while maintaining interpretability.\nExperimental evaluations demonstrated high performance in anomaly detection\nacross multiple modalities. The system features a user-friendly Gradio\ninterface for clinical workflow integration and demonstrates zero-shot learning\ncapabilities to reduce dependence on large datasets. This framework represents\na significant advancement in automated diagnostic support and radiological\nworkflow efficiency, though clinical validation and multi-center evaluation are\nnecessary prior to widespread adoption.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13590v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13590v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.325,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13591",
      "title": "Object Pose Estimation through Dexterous Touch",
      "authors": [
        "Amir-Hossein Shahidzadeh",
        "Jiyue Zhu",
        "Kezhou Chen",
        "Sha Yi",
        "Cornelia Fermüller",
        "Yiannis Aloimonos",
        "Xiaolong Wang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Robust object pose estimation is essential for manipulation and interaction\ntasks in robotics, particularly in scenarios where visual data is limited or\nsensitive to lighting, occlusions, and appearances. Tactile sensors often offer\nlimited and local contact information, making it challenging to reconstruct the\npose from partial data. Our approach uses sensorimotor exploration to actively\ncontrol a robot hand to interact with the object. We train with Reinforcement\nLearning (RL) to explore and collect tactile data. The collected 3D point\nclouds are used to iteratively refine the object's shape and pose. In our\nsetup, one hand holds the object steady while the other performs active\nexploration. We show that our method can actively explore an object's surface\nto identify critical pose features without prior knowledge of the object's\ngeometry. Supplementary material and more demonstrations will be provided at\nhttps://amirshahid.github.io/BimanualTactilePose .",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13591v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13591v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.298,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13597",
      "title": "Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents",
      "authors": [
        "Abhishek Goswami"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autonomous LLM agents can issue thousands of API calls per hour without human\noversight. OAuth 2.0 assumes deterministic clients, but in agentic settings\nstochastic reasoning, prompt injection, or multi-agent orchestration can\nsilently expand privileges.\n  We introduce Agentic JWT (A-JWT), a dual-faceted intent token that binds each\nagent's action to verifiable user intent and, optionally, to a specific\nworkflow step. A-JWT carries an agent's identity as a one-way checksum hash\nderived from its prompt, tools and configuration, and a chained delegation\nassertion to prove which downstream agent may execute a given task, and\nper-agent proof-of-possession keys to prevent replay and in-process\nimpersonation. We define a new authorization mechanism and add a lightweight\nclient shim library that self-verifies code at run time, mints intent tokens,\ntracks workflow steps and derives keys, thus enabling secure agent identity and\nseparation even within a single process.\n  We illustrate a comprehensive threat model for agentic applications,\nimplement a Python proof-of-concept and show functional blocking of\nscope-violating requests, replay, impersonation, and prompt-injection pathways\nwith sub-millisecond overhead on commodity hardware. The design aligns with\nongoing OAuth agent discussions and offers a drop-in path toward zero-trust\nguarantees for agentic applications. A comprehensive performance and security\nevaluation with experimental results will appear in our forthcoming journal\npublication",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.13597v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13597v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.333,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14274",
      "title": "Discovering New Theorems via LLMs with In-Context Proof Learning in Lean",
      "authors": [
        "Kazumi Kasaura",
        "Naoto Onda",
        "Yuta Oriike",
        "Masaya Taniguchi",
        "Akiyoshi Sannai",
        "Sho Sonoda"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "Large Language Models have demonstrated significant promise in formal theorem\nproving. However, previous works mainly focus on solving existing problems. In\nthis paper, we focus on the ability of LLMs to find novel theorems. We propose\nConjecturing-Proving Loop pipeline for automatically generating mathematical\nconjectures and proving them in Lean 4 format. A feature of our approach is\nthat we generate and prove further conjectures with context including\npreviously generated theorems and their proofs, which enables the generation of\nmore difficult proofs by in-context learning of proof strategies without\nchanging parameters of LLMs. We demonstrated that our framework rediscovered\ntheorems with verification, which were published in past mathematical papers\nand have not yet formalized. Moreover, at least one of these theorems could not\nbe proved by the LLM without in-context learning, even in natural language,\nwhich means that in-context learning was effective for neural theorem proving.\nThe source code is available at\nhttps://github.com/auto-res/ConjecturingProvingLoop.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.14274v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14274v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.315,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using LLMs in a Conjecturing-Proving Loop for generating and proving mathematical theorems, incorporating in-context learning from previous proofs. While this involves an iterative process, it does not adapt the iterative refinement process of diffusion models for multi-step logical reasoning or treat a Chain-of-Thought as a holistically corrected entity. There is no mention of diffusion-based techniques, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14275",
      "title": "FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated\n  LLMs in Mental Health",
      "authors": [
        "Nobin Sarwar",
        "Shubhashis Roy Dipta"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive\ndomains (e.g., mental health) requires balancing strict confidentiality with\nmodel utility and safety. We propose FedMentor, a federated fine-tuning\nframework that integrates Low-Rank Adaptation (LoRA) and domain-aware\nDifferential Privacy (DP) to meet per-domain privacy budgets while maintaining\nperformance. Each client (domain) applies a custom DP noise scale proportional\nto its data sensitivity, and the server adaptively reduces noise when utility\nfalls below a threshold. In experiments on three mental health datasets, we\nshow that FedMentor improves safety over standard Federated Learning without\nprivacy, raising safe output rates by up to three points and lowering toxicity,\nwhile maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the\nnon-private baseline and close to the centralized upper bound. The framework\nscales to backbones with up to 1.7B parameters on single-GPU clients, requiring\n< 173 MB of communication per round. FedMentor demonstrates a practical\napproach to privately fine-tune LLMs for safer deployments in healthcare and\nother sensitive fields.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.14275v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14275v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.467,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.454,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated fine-tuning with Differential Privacy for LLMs in mental health, using LoRA and domain-aware noise. It does not involve human feedback, reward models, or reinforcement learning techniques for aligning models with preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses privacy-preserving federated learning for LLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. It centers on fine-tuning for mental health applications.",
      "distributed_training_justification": "The paper's FedMentor framework employs federated learning, a form of distributed training, where clients perform local training on non-IID data and aggregate updates (e.g., via FedAvg) on a server. This directly relates to distributed and parallel computing for efficient model training across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "FedMentor is a federated fine-tuning framework designed for Large Language Models (LLMs) in sensitive domains like mental health, which integrates Low-Rank Adaptation (LoRA) and domain-aware Differential Privacy (DP) to enforce per-domain privacy budgets while preserving model utility and safety. The methodology involves clients applying custom DP noise scales based on data sensitivity, with the server adaptively reducing noise if utility drops below a threshold, and experiments on mental health datasets demonstrate improved safety metrics (e.g., higher safe output rates and lower toxicity) while maintaining performance close to non-private baselines, all with efficient communication under 173 MB per round for models up to 1.7B parameters.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining domain-aware Differential Privacy with federated LoRA fine-tuning for mental health LLMs, offering a clever adaptation of existing techniques to address privacy in heterogeneous settings, though it does not introduce an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and applications in privacy-preserving AI for healthcare by providing a practical framework for safer LLM deployments, but its impact may be limited to specific subfields like mental health chatbots rather than broader domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to privacy and AI in sensitive fields, making it essential for researchers in federated learning and healthcare AI to be aware of its methods and findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f1ab1182294964f6ad222e155043c2aeca6be798",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 5,
      "average_h_index": 3.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Nobin Sarwar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2379783899"
        },
        {
          "name": "Shubhashis Roy Dipta",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/50425406"
        }
      ]
    },
    {
      "id": "2509.14276",
      "title": "Constructive Conflict-Driven Multi-Agent Reinforcement Learning for\n  Strategic Diversity",
      "authors": [
        "Yuxiang Mai",
        "Qiyue Yin",
        "Wancheng Ni",
        "Pei Xu",
        "Kaiqi Huang"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In recent years, diversity has emerged as a useful mechanism to enhance the\nefficiency of multi-agent reinforcement learning (MARL). However, existing\nmethods predominantly focus on designing policies based on individual agent\ncharacteristics, often neglecting the interplay and mutual influence among\nagents during policy formation. To address this gap, we propose Competitive\nDiversity through Constructive Conflict (CoDiCon), a novel approach that\nincorporates competitive incentives into cooperative scenarios to encourage\npolicy exchange and foster strategic diversity among agents. Drawing\ninspiration from sociological research, which highlights the benefits of\nmoderate competition and constructive conflict in group decision-making, we\ndesign an intrinsic reward mechanism using ranking features to introduce\ncompetitive motivations. A centralized intrinsic reward module generates and\ndistributes varying reward values to agents, ensuring an effective balance\nbetween competition and cooperation. By optimizing the parameterized\ncentralized reward module to maximize environmental rewards, we reformulate the\nconstrained bilevel optimization problem to align with the original task\nobjectives. We evaluate our algorithm against state-of-the-art methods in the\nSMAC and GRF environments. Experimental results demonstrate that CoDiCon\nachieves superior performance, with competitive intrinsic rewards effectively\npromoting diverse and adaptive strategies among cooperative agents.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.14276v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14276v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.344,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on multi-agent reinforcement learning with intrinsic rewards derived from agent rankings and competition, inspired by sociological concepts, to enhance strategic diversity. It does not involve human feedback, human-ranked data, or a reward model trained on human preferences, which are core elements of RLHF. Therefore, there is no direct or indirect connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14278",
      "title": "Beyond Data Privacy: New Privacy Risks for Large Language Models",
      "authors": [
        "Yuntao Du",
        "Zitao Li",
        "Ninghui Li",
        "Bolin Ding"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable progress in natural\nlanguage understanding, reasoning, and autonomous decision-making. However,\nthese advancements have also come with significant privacy concerns. While\nsignificant research has focused on mitigating the data privacy risks of LLMs\nduring various stages of model training, less attention has been paid to new\nthreats emerging from their deployment. The integration of LLMs into widely\nused applications and the weaponization of their autonomous abilities have\ncreated new privacy vulnerabilities. These vulnerabilities provide\nopportunities for both inadvertent data leakage and malicious exfiltration from\nLLM-powered systems. Additionally, adversaries can exploit these systems to\nlaunch sophisticated, large-scale privacy attacks, threatening not only\nindividual privacy but also financial security and societal trust. In this\npaper, we systematically examine these emerging privacy risks of LLMs. We also\ndiscuss potential mitigation strategies and call for the research community to\nbroaden its focus beyond data privacy risks, developing new defenses to address\nthe evolving threats posed by increasingly powerful LLMs and LLM-powered\nsystems.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.14278v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14278v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.396,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on privacy risks in Large Language Models (LLMs), including data leakage, system vulnerabilities, and malicious exploitation, without any discussion of training methods like RLHF. There is no mention of using human feedback to train a reward model or fine-tune models via reinforcement learning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines privacy threats associated with LLMs and their applications, such as inadvertent data leakage and malicious attacks, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14279",
      "title": "Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and\n  Optimization",
      "authors": [
        "Robert Tjarko Lange",
        "Qi Sun",
        "Aaditya Prasad",
        "Maxence Faldor",
        "Yujin Tang",
        "David Ha"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advances in large language models (LLMs) demonstrate their\neffectiveness in scaling test-time compute for software engineering tasks.\nHowever, these approaches often focus on high-level solutions, with limited\nattention to optimizing low-level CUDA kernel implementations. Additionally,\nexisting kernel generation benchmarks suffer from exploitable loopholes and\ninsufficient diversity in testing conditions, hindering true generalization\nassessment. To address these limitations, we introduce robust-kbench, a new\nbenchmark for rigorous evaluation of kernel performance and correctness across\nvaried scenarios. Furthermore, we present a comprehensive agentic framework\nthat automates CUDA kernel discovery, verification, and optimization. This\npipeline enables frontier LLMs to translate torch code to CUDA kernels and\niteratively improve their runtime within our robust evaluation setting. Our\nsequential workflow first translates PyTorch code into equivalent CUDA kernels.\nIt then optimizes their runtime using a novel evolutionary meta-generation\nprocedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for\ncorrectness and efficient filtering. Evaluated on robust-kbench, our approach\nproduces CUDA kernels outperforming torch implementations for practical\napplications, including forward and backward passes. It can fuse operations and\ndeploy various runtime optimization strategies. The verifier workflow\naccurately classifies incorrect kernels, enhancing hardware verification\nefficiency.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.14279v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14279v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.489,
      "datasets_score": 0.398,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on LLM-driven automation for CUDA kernel optimization and verification, using evolutionary procedures and iterative refinement, but it does not involve human feedback, reward models, or fine-tuning via reinforcement learning based on human preferences. There is no mention of aligning models with human-ranked data.",
      "weak_supervision_justification": "The paper uses LLMs for programmatic verification and optimization of CUDA kernels, which could loosely relate to generating noisy or imprecise evaluations as in weak supervision. However, it does not primarily involve training models with programmatically labeled data, focusing instead on code optimization rather than weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper describes iterative optimization processes for CUDA kernels using evolutionary methods and LLMs, but it does not adapt diffusion models or processes for multi-step logical reasoning. There is no component involving diffusion-based iterative refinement of a Chain-of-Thought.",
      "distributed_training_justification": "The paper centers on low-level CUDA kernel benchmarking and optimization for GPUs, without addressing distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation in multi-node environments. It is focused on single-kernel performance.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14281",
      "title": "SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code\n  Problems",
      "authors": [
        "Xifeng Yao",
        "Dongyu Lang",
        "Wu Zhang",
        "Xintong Guo",
        "Huarui Xie",
        "Yinhao Ni",
        "Ping Liu",
        "Guang Shen",
        "Yi Bai",
        "Dandan Tu",
        "Changzheng Zhang"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Significant advancements have been made in the capabilities of code large\nlanguage models, leading to their rapid adoption and application across a wide\nrange of domains. However, their further advancements are often constrained by\nthe scarcity of real-world coding problems. To bridge this gap, we propose a\nnovel framework for synthesizing code problems that emulate authentic\nreal-world scenarios. This framework systematically integrates domain\nknowledge, domain skills, and coding skills, all of which are meticulously\nextracted from real-world programming-related datasets, including Stack\nOverflow and Kaggle. The extracted elements serve as the foundational building\nblocks for constructing code problems. To align the generated problems with\npractical applications, application scenarios are also mined from the\naforementioned datasets. These scenarios are then utilized to construct a\nscenario-centric graph that interconnects domain knowledge, domain skills, and\ncoding skills. Based on this structured representation, a sampling strategy on\nthe graph is designed, which effectively controls the generation of a code\nproblem with complexity and diversity, reflects real-world challenges.\nExperimental results demonstrate that the proposed method consistently achieves\nsuperior performance over state-of-the-art open-source large language models of\nvarying sizes and functionalities, including both coders and general-purpose\nmodels, across a diverse set of real-world benchmarks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.14281v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14281v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.434,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.393,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves synthesizing code problems from real-world datasets like Stack Overflow and Kaggle, which programmatically derives structured data from noisy sources, aligning somewhat with weak supervision concepts. However, it primarily focuses on data generation for code LLMs rather than directly training models with weak labels, making it only indirectly related.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning in the context of diffusion. It instead relies on graph-based synthesis, sampling strategies, and prompt engineering for generating code problems.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14284",
      "title": "The Sum Leaks More Than Its Parts: Compositional Privacy Risks and\n  Mitigations in Multi-Agent Collaboration",
      "authors": [
        "Vaidehi Patil",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "As large language models (LLMs) become integral to multi-agent systems, new\nprivacy risks emerge that extend beyond memorization, direct inference, or\nsingle-turn evaluations. In particular, seemingly innocuous responses, when\ncomposed across interactions, can cumulatively enable adversaries to recover\nsensitive information, a phenomenon we term compositional privacy leakage. We\npresent the first systematic study of such compositional privacy leaks and\npossible mitigation methods in multi-agent LLM systems. First, we develop a\nframework that models how auxiliary knowledge and agent interactions jointly\namplify privacy risks, even when each response is benign in isolation. Next, to\nmitigate this, we propose and evaluate two defense strategies: (1)\nTheory-of-Mind defense (ToM), where defender agents infer a questioner's intent\nby anticipating how their outputs may be exploited by adversaries, and (2)\nCollaborative Consensus Defense (CoDef), where responder agents collaborate\nwith peers who vote based on a shared aggregated state to restrict sensitive\ninformation spread. Crucially, we balance our evaluation across compositions\nthat expose sensitive information and compositions that yield benign\ninferences. Our experiments quantify how these defense strategies differ in\nbalancing the privacy-utility trade-off. We find that while chain-of-thought\nalone offers limited protection to leakage (~39% sensitive blocking rate), our\nToM defense substantially improves sensitive query blocking (up to 97%) but can\nreduce benign task success. CoDef achieves the best balance, yielding the\nhighest Balanced Outcome (79.8%), highlighting the benefit of combining\nexplicit reasoning with defender collaboration. Together, our results expose a\nnew class of risks in collaborative LLM deployments and provide actionable\ninsights for designing safeguards against compositional, context-driven privacy\nleakage.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.14284v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14284v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.421,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.371,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on compositional privacy risks and defenses in multi-agent LLM systems, including strategies like Theory-of-Mind and Collaborative Consensus Defense. It does not involve training AI models with human feedback, using a reward model, or applying reinforcement learning for alignment, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses Chain-of-Thought (CoT) reasoning in the context of privacy defenses but does not adapt the iterative refinement process of diffusion models for multi-step logical tasks. There is no mention of treating reasoning paths as entities for holistic correction via diffusion, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14287",
      "title": "Property-Isometric Variational Autoencoders for Sequence Modeling and\n  Design",
      "authors": [
        "Elham Sadeghi",
        "Xianqi Deng",
        "I-Hsin Lin",
        "Stacy M. Copp",
        "Petko Bogdanov"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Biological sequence design (DNA, RNA, or peptides) with desired functional\nproperties has applications in discovering novel nanomaterials, biosensors,\nantimicrobial drugs, and beyond. One common challenge is the ability to\noptimize complex high-dimensional properties such as target emission spectra of\nDNA-mediated fluorescent nanoparticles, photo and chemical stability, and\nantimicrobial activity of peptides across target microbes. Existing models rely\non simple binary labels (e.g., binding/non-binding) rather than\nhigh-dimensional complex properties. To address this gap, we propose a\ngeometry-preserving variational autoencoder framework, called PrIVAE, which\nlearns latent sequence embeddings that respect the geometry of their property\nspace. Specifically, we model the property space as a high-dimensional manifold\nthat can be locally approximated by a nearest neighbor graph, given an\nappropriately defined distance measure. We employ the property graph to guide\nthe sequence latent representations using (1) graph neural network encoder\nlayers and (2) an isometric regularizer. PrIVAE learns a property-organized\nlatent space that enables rational design of new sequences with desired\nproperties by employing the trained decoder. We evaluate the utility of our\nframework for two generative tasks: (1) design of DNA sequences that template\nfluorescent metal nanoclusters and (2) design of antimicrobial peptides. The\ntrained models retain high reconstruction accuracy while organizing the latent\nspace according to properties. Beyond in silico experiments, we also employ\nsampled sequences for wet lab design of DNA nanoclusters, resulting in up to\n16.1-fold enrichment of rare-property nanoclusters compared to their abundance\nin training data, demonstrating the practical utility of our framework.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.14287v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14287v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.322,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a geometry-preserving Variational Autoencoder (VAE) for biological sequence design, focusing on preserving property spaces using GNNs and isometric regularization. While it briefly mentions diffusion models in the introduction as existing approaches, it does not adapt or use diffusion processes for iterative refinement, multi-step logical reasoning, or chain-of-thought tasks. The main contribution is centered on VAEs, not diffusion-based methods, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14289",
      "title": "From Capabilities to Performance: Evaluating Key Functional Properties\n  of LLM Architectures in Penetration Testing",
      "authors": [
        "Lanxiao Huang",
        "Daksh Dave",
        "Ming Jin",
        "Tyler Cody",
        "Peter Beling"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large language models (LLMs) are increasingly used to automate or augment\npenetration testing, but their effectiveness and reliability across attack\nphases remain unclear. We present a comprehensive evaluation of multiple\nLLM-based agents, from single-agent to modular designs, across realistic\npenetration testing scenarios, measuring empirical performance and recurring\nfailure patterns. We also isolate the impact of five core functional\ncapabilities via targeted augmentations: Global Context Memory (GCM),\nInter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive\nPlanning (AP), and Real-Time Monitoring (RTM). These interventions support,\nrespectively: (i) context coherence and retention, (ii) inter-component\ncoordination and state management, (iii) tool use accuracy and selective\nexecution, (iv) multi-step strategic planning, error detection, and recovery,\nand (v) real-time dynamic responsiveness. Our results show that while some\narchitectures natively exhibit subsets of these properties, targeted\naugmentations substantially improve modular agent performance, especially in\ncomplex, multi-step, and real-time penetration testing tasks.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.14289v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14289v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.448,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.378,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates LLMs for penetration testing and focuses on their performance, failure patterns, and augmentations, but does not involve training or fine-tuning models using human feedback, a reward model, or reinforcement learning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses multi-step reasoning capabilities in LLMs for penetration testing tasks, such as adaptive planning, but does not adapt or use diffusion models for iterative refinement of reasoning paths or holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16241",
      "title": "REAMS: Reasoning Enhanced Algorithm for Maths Solving",
      "authors": [
        "Eishkaran Singh",
        "Tanav Singh Bajaj",
        "Siddharth Nayak"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.PL (Programming Languages)"
      ],
      "abstract": "The challenges of solving complex university-level mathematics problems,\nparticularly those from MIT, and Columbia University courses, and selected\ntasks from the MATH dataset, remain a significant obstacle in the field of\nartificial intelligence. Conventional methods have consistently fallen short in\nthis domain, highlighting the need for more advanced approaches. In this paper,\nwe introduce a language-based solution that leverages zero-shot learning and\nmathematical reasoning to effectively solve, explain, and generate solutions\nfor these advanced math problems. By integrating program synthesis, our method\nreduces reliance on large-scale training data while significantly improving\nproblem-solving accuracy. Our approach achieves an accuracy of 90.15%,\nrepresenting a substantial improvement over the previous benchmark of 81% and\nsetting a new standard in automated mathematical problem-solving. These\nfindings highlight the significant potential of advanced AI methodologies to\naddress and overcome the challenges presented by some of the most complex\nmathematical courses and datasets.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.16241v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16241v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.514,
      "distributed_training_score": 0.326,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces REAMS, which uses program synthesis, zero-shot learning, few-shot learning, and symbolic reasoning to solve math problems, but it does not involve diffusion models or any iterative refinement process for reasoning. There is no mention of adapting diffusion techniques for chain-of-thought or multi-step logical tasks, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18147",
      "title": "ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for\n  Convolutional Neural Networks",
      "authors": [
        "Xinyu Mu",
        "Hui Dou",
        "Furao Shen",
        "Jian Zhao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Concept-based interpretability for Convolutional Neural Networks (CNNs) aims\nto align internal model representations with high-level semantic concepts, but\nexisting approaches largely overlook the semantic roles of individual filters\nand the dynamic propagation of concepts across layers. To address these\nlimitations, we propose ConceptFlow, a concept-based interpretability framework\nthat simulates the internal \"thinking path\" of a model by tracing how concepts\nemerge and evolve across layers. ConceptFlow comprises two key components: (i)\nconcept attentions, which associate each filter with relevant high-level\nconcepts to enable localized semantic interpretation, and (ii) conceptual\npathways, derived from a concept transition matrix that quantifies how concepts\npropagate and transform between filters. Together, these components offer a\nunified and structured view of internal model reasoning. Experimental results\ndemonstrate that ConceptFlow yields semantically meaningful insights into model\nreasoning, validating the effectiveness of concept attentions and conceptual\npathways in explaining decision behavior. By modeling hierarchical conceptual\npathways, ConceptFlow provides deeper insight into the internal logic of CNNs\nand supports the generation of more faithful and human-aligned explanations.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.18147v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18147v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.499,
      "distributed_training_score": 0.288,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces ConceptFlow, a framework for interpreting CNNs by associating filters with concepts and tracing concept propagation across layers. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as it focuses solely on neural network interpretability without any elements of diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18148",
      "title": "Augmenting Limited and Biased RCTs through Pseudo-Sample Matching-Based\n  Observational Data Fusion Method",
      "authors": [
        "Kairong Han",
        "Weidong Huang",
        "Taiyang Zhou",
        "Peng Zhen",
        "Kun Kuang"
      ],
      "categories": [
        "stat.ME (Methodology)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "In the online ride-hailing pricing context, companies often conduct\nrandomized controlled trials (RCTs) and utilize uplift models to assess the\neffect of discounts on customer orders, which substantially influences\ncompetitive market outcomes. However, due to the high cost of RCTs, the\nproportion of trial data relative to observational data is small, which only\naccounts for 0.65\\% of total traffic in our context, resulting in significant\nbias when generalizing to the broader user base. Additionally, the complexity\nof industrial processes reduces the quality of RCT data, which is often subject\nto heterogeneity from potential interference and selection bias, making it\ndifficult to correct. Moreover, existing data fusion methods are challenging to\nimplement effectively in complex industrial settings due to the high\ndimensionality of features and the strict assumptions that are hard to verify\nwith real-world data. To address these issues, we propose an empirical data\nfusion method called pseudo-sample matching. By generating pseudo-samples from\nbiased, low-quality RCT data and matching them with the most similar samples\nfrom large-scale observational data, the method expands the RCT dataset while\nmitigating its heterogeneity. We validated the method through simulation\nexperiments, conducted offline and online tests using real-world data. In a\nweek-long online experiment, we achieved a 0.41\\% improvement in profit, which\nis a considerable gain when scaled to industrial scenarios with hundreds of\nmillions in revenue. In addition, we discuss the harm to model training,\noffline evaluation, and online economic benefits when the RCT data quality is\nnot high, and emphasize the importance of improving RCT data quality in\nindustrial scenarios. Further details of the simulation experiments can be\nfound in the GitHub repository https://github.com/Kairong-Han/Pseudo-Matching.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.18148v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18148v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.345,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18150",
      "title": "Sparse Training Scheme for Multimodal LLM",
      "authors": [
        "Kean Shi",
        "Liang Chen",
        "Haozhe Zhao",
        "Baobao Chang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated outstanding\nperformance across a variety of domains. However, training MLLMs is often\ninefficient due to the significantly longer input sequences introduced by\nmultimodal data and the low utilization of inter-layer computations. To address\nthis challenge, we shift the focus to the training process itself and propose a\nnovel training-efficient framework based on sparse representations, termed the\nSparse Training Scheme (STS). This scheme consists of two key components: the\nVisual Token Compressor, which reduces the information load by compressing\nvisual tokens, and the Layer Dynamic Skipper, which mitigates the computational\noverhead by dynamically skipping unnecessary layers in the language model\nduring both forward and backward passes. Our approach is broadly applicable to\ndiverse MLLM architectures and has been extensively evaluated on multiple\nbenchmarks, demonstrating its effectiveness and efficiency.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.18150v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18150v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.475,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on efficient training techniques for MLLMs through sparse representations and layer skipping, with no discussion of programmatically generating labels, using noisy sources, or any form of weak supervision. It relies on standard supervised training for MLLMs.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning. Its contributions are centered on compressing visual tokens and skipping layers for training efficiency, without any components related to holistic Chain-of-Thought correction or diffusion-based methods.",
      "distributed_training_justification": "The paper addresses training efficiency for MLLMs, which could indirectly support distributed setups by reducing computational demands (e.g., mentioning multi-GPU requirements), but it does not focus on distributed algorithms, parallel computing strategies, or partitioning data/models across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18151",
      "title": "HyperNAS: Enhancing Architecture Representation for NAS Predictor via\n  Hypernetwork",
      "authors": [
        "Jindi Lv",
        "Yuhao Zhou",
        "Yuxin Tian",
        "Qing Ye",
        "Wentao Feng",
        "Jiancheng Lv"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Time-intensive performance evaluations significantly impede progress in\nNeural Architecture Search (NAS). To address this, neural predictors leverage\nsurrogate models trained on proxy datasets, allowing for direct performance\npredictions for new architectures. However, these predictors often exhibit poor\ngeneralization due to their limited ability to capture intricate relationships\namong various architectures. In this paper, we propose HyperNAS, a novel neural\npredictor paradigm for enhancing architecture representation learning. HyperNAS\nconsists of two primary components: a global encoding scheme and a shared\nhypernetwork. The global encoding scheme is devised to capture the\ncomprehensive macro-structure information, while the shared hypernetwork serves\nas an auxiliary task to enhance the investigation of inter-architecture\npatterns. To ensure training stability, we further develop a dynamic adaptive\nmulti-task loss to facilitate personalized exploration on the Pareto front.\nExtensive experiments across five representative search spaces, including ViTs,\ndemonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For\ninstance, HyperNAS strikes new state-of-the-art results, with 97.60\\% top-1\naccuracy on CIFAR-10 and 82.4\\% top-1 accuracy on ImageNet, using at least\n5.0$\\times$ fewer samples.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.18151v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18151v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.429,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on improving Neural Architecture Search (NAS) predictors through techniques like global encoding and hypernetworks to enhance architecture representation and generalization. It does not address distributed training, parallel computing, multi-node machine learning, or any methods for partitioning data/computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18152",
      "title": "WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well\n  Geological Interpretation",
      "authors": [
        "Zhenyu Qi",
        "Qing Yu",
        "Jichen Wang",
        "Yun-Bo Zhao",
        "Zerui Li",
        "Wenjun Lv"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Well-log interpretation is fundamental for subsurface characterization but\nremains challenged by heterogeneous tool responses, noisy signals, and limited\nlabels. We propose WLFM, a foundation model pretrained on multi-curve logs from\n1200 wells, comprising three stages: tokenization of log patches into\ngeological tokens, self-supervised pretraining with masked-token modeling and\nstratigraphy-aware contrastive learning, and multi-task adaptation with\nfew-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,\nachieving 0.0041 MSE in porosity estimation and 74.13\\% accuracy in lithology\nclassification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\\%\naccuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,\nlearns a reusable geological vocabulary, and reconstructs masked curves with\nreasonable fidelity, though systematic offsets are observed in shallow and\nultra-deep intervals. Although boundary detection is not explicitly evaluated\nhere, clustering analyses suggest strong potential for future extension. These\nresults establish WLFM as a scalable, interpretable, and transferable backbone\nfor geological AI, with implications for multi-modal integration of logs,\nseismic, and textual data.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.18152v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18152v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.413,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.369,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's WLFM model uses self-supervised pretraining on unlabeled data and few-shot fine-tuning for adaptation, which involves limited labels and aligns with weak supervision by reducing reliance on extensive hand-labeled data. However, the primary focus is on developing a foundation model for geological tasks rather than explicitly generating or utilizing programmatically derived weak labels.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces WLFM, a foundation model for well-log interpretation that addresses challenges like noisy signals, heterogeneous data, and limited labels by tokenizing log patches into geological tokens, performing self-supervised pretraining with masked-token modeling and stratigraphy-aware contrastive learning, and enabling multi-task adaptation through few-shot fine-tuning. Pretrained on data from 1200 wells, WLFM outperforms baselines in tasks such as porosity estimation (achieving an MSE of 0.0041) and lithology classification (74.13% accuracy), while demonstrating emergent properties like layer-awareness, a reusable geological vocabulary, and potential for multi-modal integration, though it shows systematic offsets in certain depth intervals.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel vector-quantized tokenizer for multi-curve well-log data, creating a geological vocabulary that advances state-of-the-art in geological AI by enabling scalable representation learning and addressing heterogeneity in a way not previously explored in this domain.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in geological AI by providing a transferable backbone for multi-task interpretation and multi-modal data integration, though its applicability is primarily confined to the subsurface characterization subfield.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to AI in geoscience with its innovative approach and demonstrated performance improvements, making it essential for researchers focused on foundation models in Earth sciences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e2df0c8b50fde4f802c4b528b6347fd3b8050679",
      "total_authors": 6,
      "authors_found": 5,
      "highest_h_index": 15,
      "average_h_index": 3.6,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Zhenyu Qi",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Qing Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2351626622"
        },
        {
          "name": "Jichen Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2319386149"
        },
        {
          "name": "Yun-Bo Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381743692"
        },
        {
          "name": "Zerui Li",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/9247896"
        },
        {
          "name": "Wenjun Lv",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2315316986"
        }
      ]
    },
    {
      "id": "2509.18154",
      "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\n  Training Recipe",
      "authors": [
        "Tianyu Yu",
        "Zefan Wang",
        "Chongyi Wang",
        "Fuwei Huang",
        "Wenshuo Ma",
        "Zhihui He",
        "Tianchi Cai",
        "Weize Chen",
        "Yuxiang Huang",
        "Yuanqian Zhao",
        "Bokai Xu",
        "Junbo Cui",
        "Yingjing Xu",
        "Liqing Ruan",
        "Luoyuan Zhang",
        "Hanyu Liu",
        "Jingkun Tang",
        "Hongyuan Liu",
        "Qining Guo",
        "Wenhao Hu",
        "Bingxiang He",
        "Jie Zhou",
        "Jie Cai",
        "Ji Qi",
        "Zonghao Guo",
        "Chi Chen",
        "Guoyang Zeng",
        "Yuxuan Li",
        "Ganqu Cui",
        "Ning Ding",
        "Xu Han",
        "Yuan Yao",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.18154v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18154v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.44,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improvements in model architecture, data strategies, and a hybrid reinforcement learning approach for reasoning in MLLMs, but it does not involve diffusion models or their iterative refinement processes for logical tasks. There is no mention of treating Chain-of-Thought as a single entity for multi-step correction via diffusion, making this topic unrelated.",
      "distributed_training_justification": "The paper addresses efficiency in training and inference through architectural and methodological improvements, such as reducing GPU memory and inference time, but it does not discuss distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors. The contributions are centered on single-model optimizations rather than distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18156",
      "title": "Event Causality Identification with Synthetic Control",
      "authors": [
        "Haoyu Wang",
        "Fengze Liu",
        "Jiayao Zhang",
        "Dan Roth",
        "Kyle Richardson"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Event causality identification (ECI), a process that extracts causal\nrelations between events from text, is crucial for distinguishing causation\nfrom correlation. Traditional approaches to ECI have primarily utilized\nlinguistic patterns and multi-hop relational inference, risking false causality\nidentification due to informal usage of causality and specious graphical\ninference. In this paper, we adopt the Rubin Causal Model to identify event\ncausality: given two temporally ordered events, we see the first event as the\ntreatment and the second one as the observed outcome. Determining their\ncausality involves manipulating the treatment and estimating the resultant\nchange in the likelihood of the outcome. Given that it is only possible to\nimplement manipulation conceptually in the text domain, as a work-around, we\ntry to find a twin for the protagonist from existing corpora. This twin should\nhave identical life experiences with the protagonist before the treatment but\nundergoes an intervention of treatment. However, the practical difficulty of\nlocating such a match limits its feasibility. Addressing this issue, we use the\nsynthetic control method to generate such a twin' from relevant historical\ndata, leveraging text embedding synthesis and inversion techniques. This\napproach allows us to identify causal relations more robustly than previous\nmethods, including GPT-4, which is demonstrated on a causality benchmark,\nCOPES-hard.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.18156v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18156v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.269,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using synthetic control methods, text embeddings, and inversion techniques for event causality identification, drawing from the Rubin Causal Model. It does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning on a chain-of-thought. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18362",
      "title": "FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction",
      "authors": [
        "Yuxuan Cai",
        "Xiaozhuan Liang",
        "Xinghua Wang",
        "Jin Ma",
        "Haijin Liang",
        "Jinwen Luo",
        "Xinyu Zuo",
        "Lisheng Duan",
        "Yuyang Yin",
        "Xi Chen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As large language models (LLMs) become increasingly powerful, the sequential\nnature of autoregressive generation creates a fundamental throughput bottleneck\nthat limits the practical deployment. While Multi-Token Prediction (MTP) has\ndemonstrated remarkable benefits for model training efficiency and performance,\nits inherent potential for inference acceleration remains largely unexplored.\nThis paper introduces FastMTP, a simple yet effective method that improves\nmulti-step draft quality by aligning MTP training with its inference pattern,\nsignificantly enhancing speculative decoding performance. Our approach\nfine-tunes a single MTP head with position-shared weights on self-distilled\ndata, enabling it to capture dependencies among consecutive future tokens and\nmaintain high acceptance rates across multiple recursive draft steps. By\nintegrating language-aware dynamic vocabulary compression into the MTP head, we\nfurther reduce computational overhead in the drafting process. Experimental\nresults across seven diverse benchmarks demonstrate that FastMTP achieves an\naverage of 2.03x speedup compared to standard next token prediction with\nlossless output quality, outperforming vanilla MTP by 82%. FastMTP requires\nonly lightweight training and seamlessly integrates with existing inference\nframeworks, offering a practical and rapidly deployable solution for\naccelerating LLM inference.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.18362v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18362v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.47,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on accelerating LLM inference using Multi-Token Prediction (MTP) and speculative decoding, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity. It deals solely with autoregressive generation efficiency, not diffusion-based reasoning.",
      "distributed_training_justification": "The paper's main contribution is to enhance LLM inference speed through MTP fine-tuning and vocabulary compression, without addressing distributed training, parallel computing across nodes, or strategies for partitioning data/computation in multi-node environments. It pertains to inference optimization, not training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19339",
      "title": "Multi-population Ensemble Genetic Programming via Cooperative\n  Coevolution and Multi-view Learning for Classification",
      "authors": [
        "Mohammad Sadegh Khorshidi",
        "Navid Yazdanjue",
        "Hassan Gharoun",
        "Mohammad Reza Nikoo",
        "Fang Chen",
        "Amir H. Gandomi"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces Multi-population Ensemble Genetic Programming (MEGP), a\ncomputational intelligence framework that integrates cooperative coevolution\nand the multiview learning paradigm to address classification challenges in\nhigh-dimensional and heterogeneous feature spaces. MEGP decomposes the input\nspace into conditionally independent feature subsets, enabling multiple\nsubpopulations to evolve in parallel while interacting through a dynamic\nensemble-based fitness mechanism. Each individual encodes multiple genes whose\noutputs are aggregated via a differentiable softmax-based weighting layer,\nenhancing both model interpretability and adaptive decision fusion. A hybrid\nselection mechanism incorporating both isolated and ensemble-level fitness\npromotes inter-population cooperation while preserving intra-population\ndiversity. This dual-level evolutionary dynamic facilitates structured search\nexploration and reduces premature convergence. Experimental evaluations across\neight benchmark datasets demonstrate that MEGP consistently outperforms a\nbaseline GP model in terms of convergence behavior and generalization\nperformance. Comprehensive statistical analyses validate significant\nimprovements in Log-Loss, Precision, Recall, F1 score, and AUC. MEGP also\nexhibits robust diversity retention and accelerated fitness gains throughout\nevolution, highlighting its effectiveness for scalable, ensemble-driven\nevolutionary learning. By unifying population-based optimization, multi-view\nrepresentation learning, and cooperative coevolution, MEGP contributes a\nstructurally adaptive and interpretable framework that advances emerging\ndirections in evolutionary machine learning.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.19339v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19339v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.363,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19340",
      "title": "Joint Channel Estimation and Computation Offloading in Fluid\n  Antenna-assisted MEC Networks",
      "authors": [
        "Ying Ju",
        "Mingdong Li",
        "Haoyu Wang",
        "Lei Liu",
        "Youyang Qu",
        "Mianxiong Dong",
        "Victor C. M. Leung",
        "Chau Yuen"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.IT (Information Theory)",
        "cs.NI (Networking and Internet Architecture)",
        "math.IT (Information Theory)"
      ],
      "abstract": "With the emergence of fluid antenna (FA) in wireless communications, the\ncapability to dynamically adjust port positions offers substantial benefits in\nspatial diversity and spectrum efficiency, which are particularly valuable for\nmobile edge computing (MEC) systems. Therefore, we propose an FA-assisted MEC\noffloading framework to minimize system delay. This framework faces two severe\nchallenges, which are the complexity of channel estimation due to dynamic port\nconfiguration and the inherent non-convexity of the joint optimization problem.\nFirstly, we propose Information Bottleneck Metric-enhanced Channel Compressed\nSensing (IBM-CCS), which advances FA channel estimation by integrating\ninformation relevance into the sensing process and capturing key features of FA\nchannels effectively. Secondly, to address the non-convex and high-dimensional\noptimization problem in FA-assisted MEC systems, which includes FA port\nselection, beamforming, power control, and resource allocation, we propose a\ngame theory-assisted Hierarchical Twin-Dueling Multi-agent Algorithm (HiTDMA)\nbased offloading scheme, where the hierarchical structure effectively decouples\nand coordinates the optimization tasks between the user side and the base\nstation side. Crucially, the game theory effectively reduces the dimensionality\nof power control variables, allowing deep reinforcement learning (DRL) agents\nto achieve improved optimization efficiency. Numerical results confirm that the\nproposed scheme significantly reduces system delay and enhances offloading\nperformance, outperforming benchmarks. Additionally, the IBM-CCS channel\nestimation demonstrates superior accuracy and robustness under varying port\ndensities, contributing to efficient communication under imperfect CSI.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.19340v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19340v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.405,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on joint channel estimation and computation offloading in fluid antenna-assisted MEC networks, utilizing multi-agent deep reinforcement learning (MADRL) in the HiTDMA algorithm to coordinate decisions between users and base stations. While MADRL involves multiple agents that could imply distributed decision-making, the paper does not address distributed training techniques, such as partitioning data or models across nodes for accelerating machine learning training. Instead, it applies ML for optimization in a communication context, making the connection indirect.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19341",
      "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
      "authors": [
        "Yang Fu",
        "Peng Qin",
        "Yueyue Zhang",
        "Yifei Wang"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.19341v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19341v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.45,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on a distributed multi-agent reinforcement learning framework for optimizing AI model caching, migration, and broadcasting in edge networks, which involves cooperation among edge nodes. While this includes elements of distributed decision-making and parallel computing for policy learning, it does not address accelerating the training of AI models through data partitioning, model architecture distribution, or computation across nodes. Instead, it applies distributed techniques to operational optimization for inference, making it only loosely connected to the core topic of distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19343",
      "title": "Part-of-speech tagging for Nagamese Language using CRF",
      "authors": [
        "Alovi N Shohe",
        "Chonglio Khiamungam",
        "Teisovi Angami"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper investigates part-of-speech tagging, an important task in Natural\nLanguage Processing (NLP) for the Nagamese language. The Nagamese language,\na.k.a. Naga Pidgin, is an Assamese-lexified Creole language developed primarily\nas a means of communication in trade between the Nagas and people from Assam in\nnortheast India. A substantial amount of work in part-of-speech-tagging has\nbeen done for resource-rich languages like English, Hindi, etc. However, no\nwork has been done in the Nagamese language. To the best of our knowledge, this\nis the first attempt at part-of-speech tagging for the Nagamese Language. The\naim of this work is to identify the part-of-speech for a given sentence in the\nNagamese language. An annotated corpus of 16,112 tokens is created and applied\nmachine learning technique known as Conditional Random Fields (CRF). Using CRF,\nan overall tagging accuracy of 85.70%; precision, recall of 86%, and f1-score\nof 85% is achieved.\n  Keywords. Nagamese, NLP, part-of-speech, machine learning, CRF.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.19343v2",
      "pdf_url": "http://arxiv.org/pdf/2509.19343v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.267,
      "distributed_training_score": 0.229,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19345",
      "title": "SCORE: A Semantic Evaluation Framework for Generative Document Parsing",
      "authors": [
        "Renyu Li",
        "Antonio Jimeno Yepes",
        "Yao You",
        "Kamil Pluciński",
        "Maximilian Operlejn",
        "Crag Wolfe"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multi-modal generative document parsing systems challenge traditional\nevaluation: unlike deterministic OCR or layout models, they often produce\nsemantically correct yet structurally divergent outputs. Conventional\nmetrics-CER, WER, IoU, or TEDS-misclassify such diversity as error, penalizing\nvalid interpretations and obscuring system behavior.\n  We introduce SCORE (Structural and COntent Robust Evaluation), an\ninterpretation-agnostic framework that integrates (i) adjusted edit distance\nfor robust content fidelity, (ii) token-level diagnostics to distinguish\nhallucinations from omissions, (iii) table evaluation with spatial tolerance\nand semantic alignment, and (iv) hierarchy-aware consistency checks. Together,\nthese dimensions enable evaluation that embraces representational diversity\nwhile enforcing semantic rigor.\n  Across 1,114 pages spanning a holistic benchmark and a field dataset, SCORE\nconsistently revealed cross-dataset performance patterns missed by standard\nmetrics. In 2-5% of pages with ambiguous table structures, traditional metrics\npenalized systems by 12-25% on average, leading to distorted rankings. SCORE\ncorrected these cases, recovering equivalence between alternative but valid\ninterpretations. Moreover, by normalizing generative outputs into a\nformat-agnostic representation, SCORE reproduces traditional scores (e.g.,\ntable F1 up to 0.93) without requiring object-detection pipelines,\ndemonstrating that generative parsing alone suffices for comprehensive\nevaluation.\n  By exposing how interpretive diversity impacts evaluation outcomes and\nproviding multi-dimensional, interpretable diagnostics, SCORE establishes\nfoundational principles for semantically grounded, fair, and practical\nbenchmarking of modern document parsing systems.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.19345v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19345v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.334,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on SCORE, a framework for evaluating generative document parsing systems, emphasizing metrics like adjusted edit distance and semantic alignment for handling structural variations in outputs from VLMs. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.20368",
      "title": "LATTS: Locally Adaptive Test-Time Scaling",
      "authors": [
        "Theo Uscidda",
        "Matthew Trager",
        "Michael Kleinman",
        "Aditya Chattopadhyay",
        "Wei Xia",
        "Stefano Soatto"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "One common strategy for improving the performance of Large Language Models\n(LLMs) on downstream tasks involves using a \\emph{verifier model} to either\nselect the best answer from a pool of candidates or to steer the\nauto-regressive generation process towards better outputs. This class of\nmethods typically results in improved accuracy at the cost of increased\ncomputation at test-time, a paradigm known as \\emph{test-time scaling}.\nHowever, most existing approaches increase computation uniformly across all\nsamples and generation steps, without considering the complexity of individual\ninstances, leading to inefficient resource use. We address this limitation by\nproposing an approach, called \\emph{Locally Adaptive Test-Time Scaling\n(LATTS)}, that allocates variable compute across generation steps.\nSpecifically, at each generation step, LATTS employs a verifier-based\nacceptance criterion to decide whether to resample, backtrack, restart, or stop\nthe generation process. This criterion effectively adjusts the per-step\ncomputational effort based on a precise notion of \\emph{local difficulty}\nderived from the verifier model. Empirical results show that LATTS achieves\nsignificantly superior accuracy--compute tradeoffs compared to standard\nverifier-based methods.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.20368v1",
      "pdf_url": "http://arxiv.org/pdf/2509.20368v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.432,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces LATTS, a method for adaptive test-time scaling in LLMs using verifier models to adjust computation per generation step. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for holistic correction, as required for this topic.",
      "distributed_training_justification": "The paper focuses on inference-time techniques for LLMs, such as dynamic computation allocation during generation, and does not address distributed training, parallel computing, or multi-node strategies for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.21341",
      "title": "From Embeddings to Equations: Genetic-Programming Surrogates for\n  Interpretable Transformer Classification",
      "authors": [
        "Mohammad Sadegh Khorshidi",
        "Navid Yazdanjue",
        "Hassan Gharoun",
        "Mohammad Reza Nikoo",
        "Fang Chen",
        "Amir H. Gandomi"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We study symbolic surrogate modeling of frozen Transformer embeddings to\nobtain compact, auditable classifiers with calibrated probabilities. For five\nbenchmarks (SST2G, 20NG, MNIST, CIFAR10, MSC17), embeddings from ModernBERT,\nDINOv2, and SigLIP are partitioned on the training set into disjoint,\ninformation-preserving views via semantic-preserving feature partitioning\n(SPFP). A cooperative multi-population genetic program (MEGP) then learns\nadditive, closed-form logit programs over these views. Across 30 runs per\ndataset we report F1, AUC, log-loss, Brier, expected calibration error (ECE),\nand symbolic complexity; a canonical model is chosen by a one-standard-error\nrule on validation F1 with a parsimony tie-break. Temperature scaling fitted on\nvalidation yields substantial ECE reductions on test. The resulting surrogates\nachieve strong discrimination (up to F1 around 0.99 on MNIST, CIFAR10, MSC17;\naround 0.95 on SST2G), while 20NG remains most challenging. We provide\nreliability diagrams, dimension usage and overlap statistics,\ncontribution-based importances, and global effect profiles (PDP and ALE),\ndemonstrating faithful, cross-modal explanations grounded in explicit programs.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.21341v1",
      "pdf_url": "http://arxiv.org/pdf/2509.21341v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.379,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using genetic programming to create symbolic surrogates for Transformer embeddings, emphasizing interpretable classification and calibration. It does not involve diffusion models, iterative refinement for logical reasoning, or any adaptation of diffusion processes for Chain-of-Thought tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.21342",
      "title": "SGNNBench: A Holistic Evaluation of Spiking Graph Neural Network on\n  Large-scale Graph",
      "authors": [
        "Huizhe Zhang",
        "Jintang Li",
        "Yuchang Zhu",
        "Liang Chen",
        "Li Kuang"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Graph Neural Networks (GNNs) are exemplary deep models designed for graph\ndata. Message passing mechanism enables GNNs to effectively capture graph\ntopology and push the performance boundaries across various graph tasks.\nHowever, the trend of developing such complex machinery for graph\nrepresentation learning has become unsustainable on large-scale graphs. The\ncomputational and time overhead make it imperative to develop more\nenergy-efficient GNNs to cope with the explosive growth of real-world graphs.\nSpiking Graph Neural Networks (SGNNs), which integrate biologically plausible\nlearning via unique spike-based neurons, have emerged as a promising\nenergy-efficient alternative. Different layers communicate with sparse and\nbinary spikes, which facilitates computation and storage of intermediate graph\nrepresentations. Despite the proliferation of SGNNs proposed in recent years,\nthere is no systematic benchmark to explore the basic design principles of\nthese brain-inspired networks on the graph data. To bridge this gap, we present\nSGNNBench to quantify progress in the field of SGNNs. Specifically, SGNNBench\nconducts an in-depth investigation of SGNNs from multiple perspectives,\nincluding effectiveness, energy efficiency, and architectural design. We\ncomprehensively evaluate 9 state-of-the-art SGNNs across 18 datasets. Regarding\nefficiency, we empirically compare these baselines w.r.t model size, memory\nusage, and theoretical energy consumption to reveal the often-overlooked energy\nbottlenecks of SGNNs. Besides, we elaborately investigate the design space of\nSGNNs to promote the development of a general SGNN paradigm.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.21342v1",
      "pdf_url": "http://arxiv.org/pdf/2509.21342v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.39,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.21344",
      "title": "Towards mitigating information leakage when evaluating safety monitors",
      "authors": [
        "Gerard Boxo",
        "Aman Neelappa",
        "Shivam Raval"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "White box monitors that analyze model internals offer promising advantages\nfor detecting potentially harmful behaviors in large language models, including\nlower computational costs and integration into layered defense systems.However,\ntraining and evaluating these monitors requires response exemplars that exhibit\nthe target behaviors, typically elicited through prompting or fine-tuning. This\npresents a challenge when the information used to elicit behaviors inevitably\nleaks into the data that monitors ingest, inflating their effectiveness. We\npresent a systematic framework for evaluating a monitor's performance in terms\nof its ability to detect genuine model behavior rather than superficial\nelicitation artifacts. Furthermore, we propose three novel strategies to\nevaluate the monitor: content filtering (removing deception-related text from\ninputs), score filtering (aggregating only over task-relevant tokens), and\nprompt distilled fine-tuned model organisms (models trained to exhibit\ndeceptive behavior without explicit prompting). Using deception detection as a\nrepresentative case study, we identify two forms of leakage that inflate\nmonitor performance: elicitation leakage from prompts that explicitly request\nharmful behavior, and reasoning leakage from models that verbalize their\ndeceptive actions. Through experiments on multiple deception benchmarks, we\napply our proposed mitigation strategies and measure performance retention. Our\nevaluation of the monitors reveal three crucial findings: (1) Content filtering\nis a good mitigation strategy that allows for a smooth removal of elicitation\nsignal and can decrease probe AUROC by 30\\% (2) Score filtering was found to\nreduce AUROC by 15\\% but is not as straightforward to attribute to (3) A\nfinetuned model organism improves monitor evaluations but reduces their\nperformance by upto 40\\%, even when re-trained.",
      "published_date": "2025-09-16",
      "arxiv_url": "http://arxiv.org/abs/2509.21344v1",
      "pdf_url": "http://arxiv.org/pdf/2509.21344v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.466,
      "weak_supervision_score": 0.432,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.353,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper briefly mentions reinforcement learning as one of the training-based mitigation strategies for inducing deceptive behavior in models, such as directly training models without explicit instructions. However, it does not specifically involve human feedback, reward models, or alignment with human preferences, which are core to RLHF. Thus, while there is a minor overlap in training techniques, it is not a primary focus of the paper.",
      "weak_supervision_justification": "The paper focuses on evaluating and mitigating information leakage in safety monitors for AI models, including strategies like content filtering and fine-tuning, but it does not involve training models with programmatically generated labels from noisy sources. There is no discussion of weak supervision techniques, making this topic unrelated to the paper's main contribution.",
      "diffusion_reasoning_justification": "The paper discusses model reasoning and chain-of-thought in the context of detecting deceptive behaviors, but it does not adapt diffusion processes for iterative refinement in logical tasks. There is no mention of diffusion models or multi-step reasoning mechanisms, so this topic is not addressed in the paper.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 255,
  "date": "2025-09-16"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
