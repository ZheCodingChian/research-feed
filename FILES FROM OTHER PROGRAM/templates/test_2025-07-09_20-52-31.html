<!DOCTYPE html>
<html lang="en" data-bs-theme="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Papers</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- KaTeX CSS for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstbeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
    <style>
        body {
            background-color: #0f1011;
            color: #e0e0e0;
        }
        
        .full-width-header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding-top: 2rem;
            margin-bottom: 2rem;
            box-shadow: 0 4px 6px rgba(0,0,0,0.3);
        }
        
        .header-title-section {
            text-align: center;
            margin-bottom: 0;
            padding-bottom: 2rem;
        }
        
        .controls-section {
            background: rgba(0,0,0,0.2);
            border-top: 1px solid rgba(255,255,255,0.1);
            padding: 1rem 0;
            margin-top: 0;
        }
        
        .paper-card {
            margin-bottom: 1.5rem;
            border: 1px solid #404040;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.3);
            transition: transform 0.2s;
            background-color: #191a1b;
        }
        
        .paper-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.4);
        }
        
        .paper-header {
            padding: 1rem;
            border-bottom: 1px solid #404040;
            border-radius: 8px 8px 0 0;
            background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%);
            color: #ffffff;
        }
        
        .paper-title {
            color: #ffffff;
            font-size: 1.25rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-meta {
            color: #e0e0e0;
            font-size: 0.9rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-body {
            padding: 1rem;
            background-color: #191a1b;
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
            color: #d0d0d0;
        }
        
        .paper-categories {
            margin-bottom: 1rem;
        }
        
        .category-tag {
            display: inline-block;
            padding: 0.25rem 0.5rem;
            margin: 0.25rem;
            background-color: #404040;
            border-radius: 4px;
            font-size: 0.85rem;
            color: #e0e0e0;
        }
        
        .paper-scores-row {
            margin: 20px 0;
            text-align: left;
        }
        
        .score-tag {
            display: inline-block;
            padding: 0.4rem 0.6rem;
            margin: 0.25rem;
            background-color: #404040;
            border-radius: 4px;
            font-size: 1rem;
            color: #e0e0e0;
            font-weight: bold;
        }
        
        .score-tag.overall {
            background-color: #007bff;
            color: white;
        }
        
        /* Responsive: Stack scores on mobile */
        @media (max-width: 768px) {
            .paper-scores-row {
                font-size: 12px;
                line-height: 1.4;
                text-align: left;
            }
        }
        
        .similarity-scores {
            flex: 1;
        }
        
        .similarity-summary {
            background-color: rgba(255,255,255,0.05);
            border-radius: 8px;
            padding: 1rem;
            border: 1px solid rgba(255,255,255,0.1);
        }
        
        .similarity-scores-content {
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 0.5rem 1rem;
            align-items: center;
        }
        
        .similarity-label {
            font-weight: 600;
            color: #e0e0e0;
            white-space: nowrap;
        }
        
        .similarity-right-column {
            display: flex;
            align-items: center;
            gap: 0.75rem;
        }
        
        .similarity-bar {
            height: 8px;
            background-color: #404040;
            border-radius: 4px;
            overflow: hidden;
            width: 100px;
            flex-shrink: 0;
        }
        
        .similarity-value {
            color: #e0e0e0;
            font-weight: 600;
            min-width: 45px;
            flex-shrink: 0;
        }
        
        .similarity-bar-fill {
            height: 100%;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            border-radius: 4px;
            transition: width 0.3s ease;
        }
        
        .paper-link {
            color: #ffffff;
            text-decoration: none;
        }
        
        .paper-link:hover {
            text-decoration: underline;
            color: #f0f0f0;
        }
        
        .controls {
            display: flex;
            gap: 1rem;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
        }
        
        .control-group {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .form-select, .form-control {
            background-color: rgba(255,255,255,0.1);
            border: 1px solid rgba(255,255,255,0.3);
            color: #ffffff;
        }
        
        .form-select option {
            background-color: #2d2d2d;
            color: #ffffff;
        }
        
        .form-select:focus, .form-control:focus {
            background-color: rgba(255,255,255,0.2);
            border-color: #667eea;
            color: #ffffff;
            box-shadow: 0 0 0 0.25rem rgba(102, 126, 234, 0.25);
        }
        
        .btn-outline-light {
            border-color: rgba(255,255,255,0.5);
        }
        
        .btn-outline-light:hover {
            background-color: rgba(255,255,255,0.2);
            border-color: #ffffff;
        }
        
        .filter-count-section {
            text-align: center;
            margin-top: 1.5rem;
            padding-top: 1rem;
            border-top: 1px solid rgba(255,255,255,0.1);
        }
        
        .filter-count-display {
            font-size: 1.2rem;
            font-weight: 700;
            color: #ffffff;
            text-shadow: 0 1px 2px rgba(0,0,0,0.3);
        }
        
        .main-nav-link {
            color: #ffffff;
            text-decoration: none;
            padding: 0.75rem 1.5rem;
            border-radius: 8px;
            transition: all 0.2s;
            font-weight: 600;
            font-size: 1rem;
            background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%);
            border: 1px solid #404040;
            box-shadow: 0 2px 4px rgba(0,0,0,0.3);
        }
        
        .main-nav-link:hover {
            background: linear-gradient(135deg, #4A5568 0%, #2D3748 100%);
            text-decoration: none;
            color: #ffffff;
            transform: translateY(-1px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.4);
        }
        
        .paper-number {
            font-weight: bold;
            color: #ffffff;
            margin-right: 0.5rem;
        }
        
        .hidden {
            display: none !important;
        }
        
        .paper-metrics-row {
            display: flex;
            gap: 2rem;
            align-items: flex-start;
        }
        
        .similarity-scores {
            flex: 1;
            min-width: 0;
        }
        
        .llm-validation {
            flex: 1;
            min-width: 0;
        }
        
        .h-index-scores {
            flex: 1;
            min-width: 0;
        }
        
        .h-index-summary {
            background-color: rgba(255,255,255,0.05);
            border-radius: 8px;
            padding: 1rem;
            border: 1px solid rgba(255,255,255,0.1);
        }
        
        .h-index-metric {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.5rem;
        }
        
        .h-index-metric:last-child {
            margin-bottom: 0;
        }
        
        .h-index-label {
            font-weight: 600;
            color: #e0e0e0;
        }
        
        .h-index-value {
            color: #ffffff;
            font-weight: bold;
        }
        
        .h-index-expand {
            margin-top: 1rem;
            padding-top: 1rem;
            border-top: 1px solid rgba(255,255,255,0.1);
        }
        
        .h-index-toggle {
            font-size: 0.85rem;
            padding: 0.25rem 0.5rem;
        }
        
        .h-index-details {
            background-color: rgba(255,255,255,0.03);
            border-radius: 4px;
            padding: 0.75rem;
            border: 1px solid rgba(255,255,255,0.05);
        }
        
        .individual-h-index {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.25rem;
            font-size: 0.9rem;
        }
        
        .individual-h-index:last-child {
            margin-bottom: 0;
        }
        
        .author-name {
            color: #e0e0e0;
        }
        
        .author-name-link {
            color: #00d4aa;
            text-decoration: none;
        }
        
        .author-name-link:hover {
            color: #00ffcc;
            text-decoration: underline;
        }
        
        .author-h-value {
            color: #ffffff;
            font-weight: 600;
        }
        
        .paper-verification {
            text-align: center;
        }
        
        .paper-verification-link {
            color: #ffd700;
            text-decoration: none;
            font-size: 0.9rem;
            font-weight: 500;
        }
        
        .paper-verification-link:hover {
            color: #ffed4e;
            text-decoration: underline;
        }
        
        .llm-validation-summary {
            background-color: rgba(255,255,255,0.05);
            border-radius: 8px;
            padding: 1rem;
            border: 1px solid rgba(255,255,255,0.1);
        }
        
        .llm-validation-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.5rem;
        }
        
        .llm-validation-item:last-child {
            margin-bottom: 0;
        }
        
        .llm-topic-label {
            font-weight: 600;
            color: #e0e0e0;
        }
        
        .llm-status {
            font-weight: bold;
            font-size: 0.9rem;
        }
        
        .llm-yes {
            color: #28a745;
        }
        
        .llm-no {
            color: #dc3545;
        }
        
        .llm-disabled {
            color: #6c757d;
        }
        
        .llm-buttons-row {
            margin-top: 1rem;
            padding-top: 1rem;
            border-top: 1px solid rgba(255,255,255,0.1);
            display: flex;
            gap: 0.5rem;
            flex-wrap: wrap;
        }
        

        
        .llm-toggle {
            font-size: 0.85rem;
            padding: 0.25rem 0.5rem;
        }
        
        .llm-details {
            background-color: rgba(255,255,255,0.03);
            border-radius: 4px;
            padding: 0.75rem;
            border: 1px solid rgba(255,255,255,0.05);
        }
        
        .llm-justification {
            margin-bottom: 0.75rem;
            font-size: 0.9rem;
            line-height: 1.4;
        }
        
        .llm-justification:last-child {
            margin-bottom: 0;
        }
        
        /* New styles for topic toggles and separators */
        .similarity-toggle-section {
            margin-top: 1rem;
            padding-top: 1rem;
            border-top: 1px solid rgba(255,255,255,0.1);
            display: flex;
            gap: 0.5rem;
        }
        
        .similarity-toggle,
        .llm-validation-toggle,
        .similarity-normalize-toggle {
            font-size: 0.85rem;
            padding: 0.25rem 0.5rem;
        }
        
        /* Normalized similarity scores styling */
        .similarity-summary.normalized {
            background: linear-gradient(135deg, #cc5500 0%, #b8460e 100%);
            color: #ffffff;
            border: 2px solid #cc5500;
            box-shadow: 0 2px 8px rgba(204, 85, 0, 0.4);
        }
        
        .similarity-summary.normalized .similarity-label {
            color: #ffffff;
            font-weight: 600;
        }
        
        .similarity-summary.normalized .similarity-value {
            color: #ffffff;
            font-weight: bold;
        }
        
        .similarity-summary.normalized .similarity-bar {
            background-color: rgba(0, 0, 0, 0.3);
        }
        
        .similarity-summary.normalized .similarity-bar-fill {
            background: linear-gradient(90deg, #ffcc80 0%, #ff9800 100%);
        }
        
        .similarity-summary.normalized .topic-separator {
            border-color: #ffcc80;
            margin: 1rem 0;
            border-top-width: 2px;
        }
        
        .similarity-normalize-toggle.active {
            background: linear-gradient(135deg, #ff9800 0%, #ffcc80 100%);
            color: #1a1a1a;
            border-color: #ff9800;
            font-weight: 600;
            box-shadow: 0 2px 4px rgba(255, 152, 0, 0.4);
        }
        
        .similarity-normalize-toggle.active:hover {
            background: linear-gradient(135deg, #ffcc80 0%, #ff9800 100%);
            transform: translateY(-1px);
            box-shadow: 0 3px 6px rgba(255, 152, 0, 0.5);
        }
        
        .topic-separator {
            margin: 0.75rem 0;
            border-top: 2px solid rgba(255,255,255,0.2);
        }
        
        @media (max-width: 768px) {
            .paper-metrics-row {
                flex-direction: column;
                gap: 1rem;
            }
        }
    </style>
</head>
<body>
    <div class="full-width-header">
        <div class="container">
            <div style="position: absolute; top: 1rem; left: 1rem;">
                <a href="index.html" class="main-nav-link">‚Üê Back to Home</a>
            </div>
            <div class="header-title-section">
                <h1 class="mb-3">Test Papers</h1>
                
                <p class="lead mb-2">Test Mode - Specific Papers Analysis</p>
                <p class="text-muted mb-2">Paper dates: 2020-12-03, 2024-02-12, 2024-10-29, 2025-01-09, 2025-01-13, 2025-02-11, 2025-02-17, 2025-02-19, 2025-03-10, 2025-03-12, 2025-04-03, 2025-04-16, 2025-05-15, 2025-06-12</p>
                
                <p class="text-muted mb-0" id="paper-count">20 papers</p>
                
                <div class="models-info mt-2">
                    <small class="text-muted">
                        <strong>Models:</strong> 
                        Embedding: <span class="badge bg-secondary">openai-large</span>
                        
                        | LLM Validation: <span class="badge bg-secondary">x-ai/grok-3-mini</span>
                        
                        
                        | LLM Scoring: <span class="badge bg-secondary">gemini-flash</span>
                        
                    </small>
                </div>
                
            </div>
        </div>
        <div class="controls-section">
            <div class="container">
                <div class="controls">
                    <div class="control-group">
                        <label for="sortBy" class="form-label mb-0">Sort by:</label>
                        <select id="sortBy" class="form-select form-select-sm">
                            <option value="overall_score_desc">Overall Score (Descending)</option>
                            <option value="overall_score_asc">Overall Score (Ascending)</option>
                            <option value="similarity_desc">Similarity score (Descending)</option>
                            <option value="similarity_asc">Similarity score (Ascending)</option>
                            <option value="title">Title</option>
                            <option value="arxiv_id">arXiv ID</option>
                            <option value="max_h_index_desc">Max H-index (Descending)</option>
                            <option value="max_h_index_asc">Max H-index (Ascending)</option>
                            <option value="avg_h_index_desc">Avg H-index (Descending)</option>
                            <option value="avg_h_index_asc">Avg H-index (Ascending)</option>
                        </select>
                    </div>
                    
                    <div class="control-group">
                        <label class="form-label mb-0">Filter by topics:</label>
                        <div style="display: flex; gap: 0.5rem; flex-wrap: wrap;">
                            
                            <label class="form-check-label" style="display: flex; align-items: center; gap: 0.25rem; cursor: pointer;">
                                <input type="checkbox" class="form-check-input topic-checkbox" value="RLHF" style="margin: 0;">
                                <span style="font-size: 0.9rem; white-space: nowrap;">RLHF</span>
                            </label>
                            
                            <label class="form-check-label" style="display: flex; align-items: center; gap: 0.25rem; cursor: pointer;">
                                <input type="checkbox" class="form-check-input topic-checkbox" value="Weak_supervision" style="margin: 0;">
                                <span style="font-size: 0.9rem; white-space: nowrap;">Weak supervision</span>
                            </label>
                            
                            <label class="form-check-label" style="display: flex; align-items: center; gap: 0.25rem; cursor: pointer;">
                                <input type="checkbox" class="form-check-input topic-checkbox" value="Diffusion_reasoning" style="margin: 0;">
                                <span style="font-size: 0.9rem; white-space: nowrap;">Diffusion reasoning</span>
                            </label>
                            
                            <label class="form-check-label" style="display: flex; align-items: center; gap: 0.25rem; cursor: pointer;">
                                <input type="checkbox" class="form-check-input topic-checkbox" value="Distributed_training" style="margin: 0;">
                                <span style="font-size: 0.9rem; white-space: nowrap;">Distributed training</span>
                            </label>
                            
                        </div>
                    </div>
                    
                    <div class="control-group">
                        <label class="form-label mb-0">LLM Validation:</label>
                        <div style="display: flex; gap: 0.5rem; flex-wrap: wrap;">
                            <label class="form-check-label" style="display: flex; align-items: center; gap: 0.25rem; cursor: pointer;">
                                <input type="checkbox" class="form-check-input llm-filter-checkbox" value="yes" style="margin: 0;">
                                <span style="font-size: 0.9rem; white-space: nowrap;">Show YES</span>
                            </label>
                            <label class="form-check-label" style="display: flex; align-items: center; gap: 0.25rem; cursor: pointer;">
                                <input type="checkbox" class="form-check-input llm-filter-checkbox" value="no" style="margin: 0;">
                                <span style="font-size: 0.9rem; white-space: nowrap;">Show NO</span>
                            </label>
                            <label class="form-check-label" style="display: flex; align-items: center; gap: 0.25rem; cursor: pointer;">
                                <input type="checkbox" class="form-check-input llm-filter-checkbox" value="not_validated" style="margin: 0;">
                                <span style="font-size: 0.9rem; white-space: nowrap;">Show Not Validated</span>
                            </label>
                        </div>
                    </div>
                    
                    <div class="control-group">
                        <label class="form-label mb-0">H-Index Data:</label>
                        <div style="display: flex; gap: 0.5rem; flex-wrap: wrap;">
                            <label class="form-check-label" style="display: flex; align-items: center; gap: 0.25rem; cursor: pointer;">
                                <input type="checkbox" class="form-check-input h-index-filter-checkbox" value="full" style="margin: 0;">
                                <span style="font-size: 0.9rem; white-space: nowrap;">Full H-index</span>
                            </label>
                            <label class="form-check-label" style="display: flex; align-items: center; gap: 0.25rem; cursor: pointer;">
                                <input type="checkbox" class="form-check-input h-index-filter-checkbox" value="partial" style="margin: 0;">
                                <span style="font-size: 0.9rem; white-space: nowrap;">Partial H-index</span>
                            </label>
                            <label class="form-check-label" style="display: flex; align-items: center; gap: 0.25rem; cursor: pointer;">
                                <input type="checkbox" class="form-check-input h-index-filter-checkbox" value="none" style="margin: 0;">
                                <span style="font-size: 0.9rem; white-space: nowrap;">No H-index</span>
                            </label>
                        </div>
                    </div>
                    
                    <div class="control-group">
                        <label for="minScore" class="form-label mb-0">Min Score:</label>
                        <input type="number" id="minScore" class="form-control form-control-sm" 
                               step="0.01" min="0" max="1" value="0" style="width: 80px;">
                    </div>
                    
                    <div class="control-group">
                        <label for="maxScore" class="form-label mb-0">Max Score:</label>
                        <input type="number" id="maxScore" class="form-control form-control-sm" 
                               step="0.01" min="0" max="1" value="1" style="width: 80px;">
                    </div>
                    
                    <button id="resetFilters" class="btn btn-outline-light btn-sm">Reset</button>
                </div>
                
                <div class="filter-count-section">
                    <span id="filter-count" class="filter-count-display">
                        Showing 20/20 papers
                    </span>
                </div>
            </div>
        </div>
    </div>

    <div class="container">
        <div id="papers-container">
            
            <div class="paper-card" data-paper-index="0">
                <div class="paper-header">
                    <div class="paper-number">#1</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2502.13417v2" class="paper-link" target="_blank">
                            RLTHF: Targeted Human Feedback for LLM Alignment
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2502.13417v2 |
                        <strong>Published:</strong> 2025-02-19 |
                        <strong>Highest Score:</strong> 0.665 RLHF
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Fine-tuning large language models (LLMs) to align with user preferences is
challenging due to the high cost of quality human annotations in Reinforcement
Learning from Human Feedback (RLHF) and the generalizability limitations of AI
Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid
framework that combines LLM-based initial alignment with selective human
annotations to achieve full-human annotation alignment with minimal effort.
RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward
model&#x27;s reward distribution and iteratively enhances alignment by integrating
strategic human corrections while leveraging LLM&#x27;s correctly labeled samples.
Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human
annotation-level alignment with only 6-7% of the human annotation effort.
Furthermore, models trained on RLTHF&#x27;s curated datasets for downstream tasks
outperform those trained on fully human-annotated datasets, underscoring the
effectiveness of RLTHF&#x27;s strategic data curation.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.CL (Computation and Language)</span>
                        
                        <span class="category-tag">cs.AI (Artificial Intelligence)</span>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 9.0/10</span>
                        <span class="score-tag">Relevance: 9.5/10</span>
                        <span class="score-tag">Novelty: 8.0/10</span>
                        <span class="score-tag">Clarity: 9.0/10</span>
                        <span class="score-tag">Potential Impact: 9.0/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 66.5%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.665">0.665</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 39.1%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.391">0.391</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 31.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.317">0.317</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 29.8%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.298">0.298</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="0">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="0">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="0">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="0">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>RLHF:</strong> The paper's main contribution is RLTHF, a method that builds directly on RLHF by using human feedback to train a reward model and fine-tune LLMs via reinforcement learning, while selectively incorporating human annotations to address alignment challenges.
                                            </div>
                                            
                                        
                                            
                                        
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">14/14 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">14/14 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">6</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">2.6</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">1</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="0">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2268722396" target="_blank" class="author-name-link">
                                                    <span class="author-name">Yifei Xu:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/3431742" target="_blank" class="author-name-link">
                                                    <span class="author-name">Tusher Chakraborty:</span>
                                                </a>
                                                
                                                <span class="author-h-value">6</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2264962872" target="_blank" class="author-name-link">
                                                    <span class="author-name">Emre Kiciman:</span>
                                                </a>
                                                
                                                <span class="author-h-value">5</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2312460794" target="_blank" class="author-name-link">
                                                    <span class="author-name">Bibek Aryal:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2346116388" target="_blank" class="author-name-link">
                                                    <span class="author-name">Eduardo Rodrigues:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2346253357" target="_blank" class="author-name-link">
                                                    <span class="author-name">Srinagesh Sharma:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2256989289" target="_blank" class="author-name-link">
                                                    <span class="author-name">Roberto Estev√£o:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/34938986" target="_blank" class="author-name-link">
                                                    <span class="author-name">M. A. D. L. Balaguer:</span>
                                                </a>
                                                
                                                <span class="author-h-value">5</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2332536904" target="_blank" class="author-name-link">
                                                    <span class="author-name">Jessica Wolk:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2279548480" target="_blank" class="author-name-link">
                                                    <span class="author-name">Rafael Padilha:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2256989583" target="_blank" class="author-name-link">
                                                    <span class="author-name">Leonardo Nunes:</span>
                                                </a>
                                                
                                                <span class="author-h-value">3</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2346253640" target="_blank" class="author-name-link">
                                                    <span class="author-name">Shobana Balakrishnan:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2268727460" target="_blank" class="author-name-link">
                                                    <span class="author-name">Songwu Lu:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2256993742" target="_blank" class="author-name-link">
                                                    <span class="author-name">Ranveer Chandra:</span>
                                                </a>
                                                
                                                <span class="author-h-value">4</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/0152aafbec465a090684637e1da693d6deb98172" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="1">
                <div class="paper-header">
                    <div class="paper-number">#2</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2504.03784v4" class="paper-link" target="_blank">
                            Robust Reinforcement Learning from Human Feedback for Large Language
  Models Fine-Tuning
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2504.03784v4 |
                        <strong>Published:</strong> 2025-04-03 |
                        <strong>Highest Score:</strong> 0.617 RLHF
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Reinforcement learning from human feedback (RLHF) has emerged as a key
technique for aligning the output of large language models (LLMs) with human
preferences. To learn the reward function, most existing RLHF algorithms use
the Bradley-Terry model, which relies on assumptions about human preferences
that may not reflect the complexity and variability of real-world judgments. In
this paper, we propose a robust algorithm to enhance the performance of
existing approaches under such reward model misspecifications. Theoretically,
our algorithm reduces the variance of reward and policy estimators, leading to
improved regret bounds. Empirical evaluations on LLM benchmark datasets
demonstrate that the proposed algorithm consistently outperforms existing
methods, with 77-81% of responses being favored over baselines on the Anthropic
Helpful and Harmless dataset.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">stat.ML (Machine Learning)</span>
                        
                        <span class="category-tag">cs.AI (Artificial Intelligence)</span>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 9.0/10</span>
                        <span class="score-tag">Relevance: 9.5/10</span>
                        <span class="score-tag">Novelty: 8.5/10</span>
                        <span class="score-tag">Clarity: 8.0/10</span>
                        <span class="score-tag">Potential Impact: 9.0/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 61.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.617">0.617</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 36.8%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.368">0.368</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 28.0%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.280">0.280</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 27.6%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.276">0.276</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="1">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="1">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="1">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="1">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>RLHF:</strong> The paper directly addresses RLHF by proposing a robust algorithm to improve reward modeling from human feedback, used for fine-tuning LLMs, aligning with the definition of training a reward model on human-ranked data and applying reinforcement learning.
                                            </div>
                                            
                                        
                                            
                                        
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">5/5 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">5/5 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">2</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">1.2</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">0</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="1">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2354165813" target="_blank" class="author-name-link">
                                                    <span class="author-name">Kai Ye:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2354224430" target="_blank" class="author-name-link">
                                                    <span class="author-name">Hongyi Zhou:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2354195275" target="_blank" class="author-name-link">
                                                    <span class="author-name">Jin Zhu:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2354179582" target="_blank" class="author-name-link">
                                                    <span class="author-name">Francesco Quinzan:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2325201916" target="_blank" class="author-name-link">
                                                    <span class="author-name">Chengchun Shi:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/66c16a4eb1457f447a44fb1ea1968f8841ad5a2d" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="2">
                <div class="paper-header">
                    <div class="paper-number">#3</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2503.07025v1" class="paper-link" target="_blank">
                            Weak Supervision for Improved Precision in Search Systems
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2503.07025v1 |
                        <strong>Published:</strong> 2025-03-10 |
                        <strong>Highest Score:</strong> 0.552 Weak supervision
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Labeled datasets are essential for modern search engines, which increasingly
rely on supervised learning methods like Learning to Rank and massive amounts
of data to power deep learning models. However, creating these datasets is both
time-consuming and costly, leading to the common use of user click and activity
logs as proxies for relevance. In this paper, we present a weak supervision
approach to infer the quality of query-document pairs and apply it within a
Learning to Rank framework to enhance the precision of a large-scale search
system.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.IR (Information Retrieval)</span>
                        
                        <span class="category-tag">cs.AI (Artificial Intelligence)</span>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 9.0/10</span>
                        <span class="score-tag">Relevance: 9.5/10</span>
                        <span class="score-tag">Novelty: 8.0/10</span>
                        <span class="score-tag">Clarity: 9.0/10</span>
                        <span class="score-tag">Potential Impact: 9.0/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 40.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.407">0.407</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 55.2%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.552">0.552</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 28.1%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.281">0.281</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 33.4%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.334">0.334</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="2">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="2">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-no">NO</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                            
                                        
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="2">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="2">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>RLHF:</strong> The paper focuses on weak supervision for generating labels in search systems using heuristics and user logs, with no mention of reinforcement learning, reward models, or human feedback for fine-tuning.
                                            </div>
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Weak supervision:</strong> The paper's main contribution is a weak supervision approach that programmatically generates labels from noisy sources like user activity logs and heuristics to train models, aligning with the definition of weak supervision.
                                            </div>
                                            
                                        
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">1/1 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">1/1 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">1</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">1.0</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">0</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="2">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2305795476" target="_blank" class="author-name-link">
                                                    <span class="author-name">Sriram Vasudevan:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/625731ae2ed8fc00c87832a4019acf1730283a57" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="3">
                <div class="paper-header">
                    <div class="paper-number">#4</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2503.09025v1" class="paper-link" target="_blank">
                            Aligning to What? Limits to RLHF Based Alignment
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2503.09025v1 |
                        <strong>Published:</strong> 2025-03-12 |
                        <strong>Highest Score:</strong> 0.521 RLHF
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Reinforcement Learning from Human Feedback (RLHF) is increasingly used to
align large language models (LLMs) with human preferences. However, the
effectiveness of RLHF in addressing underlying biases remains unclear. This
study investigates the relationship between RLHF and both covert and overt
biases in LLMs, particularly focusing on biases against African Americans. We
applied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and
evaluated the covert and overt biases of the resulting models using
matched-guise probing and explicit bias testing. We performed additional tests
with DPO on different base models and datasets; among several implications, we
found that SFT before RLHF calcifies model biases. Additionally, we extend the
tools for measuring biases to multi-modal models. Through our experiments we
collect evidence that indicates that current alignment techniques are
inadequate for nebulous tasks such as mitigating covert biases, highlighting
the need for capable datasets, data curating techniques, or alignment tools.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.CL (Computation and Language)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 9.0/10</span>
                        <span class="score-tag">Relevance: 9.5/10</span>
                        <span class="score-tag">Novelty: 8.5/10</span>
                        <span class="score-tag">Clarity: 8.0/10</span>
                        <span class="score-tag">Potential Impact: 9.0/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 52.1%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.521">0.521</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 30.9%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.309">0.309</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 27.5%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.275">0.275</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 26.8%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.268">0.268</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="3">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="3">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="3">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="3">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>RLHF:</strong> The paper's main contribution focuses on evaluating the effectiveness of RLHF techniques (e.g., DPO, ORPO, RLOO) for aligning LLMs with human preferences, specifically in addressing biases, which directly matches the definition of RLHF.
                                            </div>
                                            
                                        
                                            
                                        
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">4/4 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">4/4 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">3</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">1.8</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">0</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="3">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2349646347" target="_blank" class="author-name-link">
                                                    <span class="author-name">Logan Barnhart:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2231396710" target="_blank" class="author-name-link">
                                                    <span class="author-name">Reza Akbarian Bafghi:</span>
                                                </a>
                                                
                                                <span class="author-h-value">3</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2349623568" target="_blank" class="author-name-link">
                                                    <span class="author-name">Stephen Becker:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2324389999" target="_blank" class="author-name-link">
                                                    <span class="author-name">Maziar Raissi:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/c9ab5fca0b7ec8c466daf0bbf1dd6c1f9b6b3a3d" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="4">
                <div class="paper-header">
                    <div class="paper-number">#5</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2506.10908v1" class="paper-link" target="_blank">
                            Probably Approximately Correct Labels
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2506.10908v1 |
                        <strong>Published:</strong> 2025-06-12 |
                        <strong>Highest Score:</strong> 0.443 Weak supervision
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Obtaining high-quality labeled datasets is often costly, requiring either
extensive human annotation or expensive experiments. We propose a method that
supplements such &quot;expert&quot; labels with AI predictions from pre-trained models to
construct labeled datasets more cost-effectively. Our approach results in
probably approximately correct labels: with high probability, the overall
labeling error is small. This solution enables rigorous yet efficient dataset
curation using modern AI models. We demonstrate the benefits of the methodology
through text annotation with large language models, image labeling with
pre-trained vision models, and protein folding analysis with AlphaFold.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">stat.ML (Machine Learning)</span>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 9.0/10</span>
                        <span class="score-tag">Relevance: 9.0/10</span>
                        <span class="score-tag">Novelty: 8.5/10</span>
                        <span class="score-tag">Clarity: 9.0/10</span>
                        <span class="score-tag">Potential Impact: 9.0/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 36.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.367">0.367</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 44.3%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.443">0.443</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 26.2%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.262">0.262</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 33.4%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.334">0.334</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="4">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="4">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="4">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="4">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Weak supervision:</strong> The paper's method uses AI predictions from pre-trained models as noisy, imprecise sources to generate labels, supplementing expert labels, which aligns with weak supervision's core idea of programmatically creating large quantities of labels without relying solely on hand-labeled data.
                                            </div>
                                            
                                        
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">3/3 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">3/3 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">15</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">6.7</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">1</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="4">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2283307289" target="_blank" class="author-name-link">
                                                    <span class="author-name">Emmanuel J. Candes:</span>
                                                </a>
                                                
                                                <span class="author-h-value">5</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2366565634" target="_blank" class="author-name-link">
                                                    <span class="author-name">Andrew Ilyas:</span>
                                                </a>
                                                
                                                <span class="author-h-value">0</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/7830023" target="_blank" class="author-name-link">
                                                    <span class="author-name">Tijana Zrnic:</span>
                                                </a>
                                                
                                                <span class="author-h-value">15</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/1cb28c5ca1eb9ebc3d29c0cf59fe4ef3fdd48fc6" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="5">
                <div class="paper-header">
                    <div class="paper-number">#6</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2402.07754v3" class="paper-link" target="_blank">
                            Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language
  Models
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2402.07754v3 |
                        <strong>Published:</strong> 2024-02-12 |
                        <strong>Highest Score:</strong> 0.673 Diffusion reasoning
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Recently, diffusion models have garnered significant interest in the field of
text processing due to their many potential advantages compared to conventional
autoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a
novel approach that integrates diffusion models with Chain-of-Thought, a
well-established technique for improving the reasoning ability of
autoregressive language models. In contrast to autoregressive language models
that make decisions in a left-to-right, token-by-token manner, DoT allows
reasoning steps to diffuse over time through a diffusion language model and
offers greater flexibility in trading-off computation for reasoning
performance. Our experimental results demonstrate the effectiveness of DoT in
multi-digit multiplication, boolean logic, and grade school math problems, with
a small diffusion model outperforming a much larger autoregressive model in
both efficiency and accuracy. In addition to that, DoT showcases promising
self-correction abilities and benefits from existing reasoning-enhancing
techniques like self-consistency decoding. Our findings contribute to the
understanding and development of reasoning with diffusion language models.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.CL (Computation and Language)</span>
                        
                        <span class="category-tag">cs.AI (Artificial Intelligence)</span>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 8.8/10</span>
                        <span class="score-tag">Relevance: 9.0/10</span>
                        <span class="score-tag">Novelty: 8.5/10</span>
                        <span class="score-tag">Clarity: 8.0/10</span>
                        <span class="score-tag">Potential Impact: 8.5/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 26.2%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.262">0.262</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 24.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.247">0.247</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 67.3%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.673">0.673</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 29.3%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.293">0.293</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="5">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="5">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="5">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="5">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Diffusion reasoning:</strong> The paper introduces Diffusion-of-Thought (DoT), which adapts the iterative refinement process of diffusion models to handle Chain-of-Thought reasoning for complex logical tasks, such as multi-digit multiplication and boolean logic, by holistically correcting and improving the reasoning path over multiple steps.
                                            </div>
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">10/10 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">10/10 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">16</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">7.9</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">8</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="5">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/65846898" target="_blank" class="author-name-link">
                                                    <span class="author-name">Jiacheng Ye:</span>
                                                </a>
                                                
                                                <span class="author-h-value">16</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2165001433" target="_blank" class="author-name-link">
                                                    <span class="author-name">Shansan Gong:</span>
                                                </a>
                                                
                                                <span class="author-h-value">10</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2283870581" target="_blank" class="author-name-link">
                                                    <span class="author-name">Liheng Chen:</span>
                                                </a>
                                                
                                                <span class="author-h-value">6</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/1633166807" target="_blank" class="author-name-link">
                                                    <span class="author-name">Lin Zheng:</span>
                                                </a>
                                                
                                                <span class="author-h-value">6</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/144407296" target="_blank" class="author-name-link">
                                                    <span class="author-name">Jiahui Gao:</span>
                                                </a>
                                                
                                                <span class="author-h-value">13</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2285182555" target="_blank" class="author-name-link">
                                                    <span class="author-name">Han Shi:</span>
                                                </a>
                                                
                                                <span class="author-h-value">6</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2284032895" target="_blank" class="author-name-link">
                                                    <span class="author-name">Chuan Wu:</span>
                                                </a>
                                                
                                                <span class="author-h-value">5</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2249755860" target="_blank" class="author-name-link">
                                                    <span class="author-name">Zhenguo Li:</span>
                                                </a>
                                                
                                                <span class="author-h-value">6</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2283842274" target="_blank" class="author-name-link">
                                                    <span class="author-name">Wei Bi:</span>
                                                </a>
                                                
                                                <span class="author-h-value">4</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2260528279" target="_blank" class="author-name-link">
                                                    <span class="author-name">Lingpeng Kong:</span>
                                                </a>
                                                
                                                <span class="author-h-value">7</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/f54aa5a594d054e9564413ed4c30d18f2e747bc7" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="6">
                <div class="paper-header">
                    <div class="paper-number">#7</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2505.10446v2" class="paper-link" target="_blank">
                            Reinforcing the Diffusion Chain of Lateral Thought with Diffusion
  Language Models
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2505.10446v2 |
                        <strong>Published:</strong> 2025-05-15 |
                        <strong>Highest Score:</strong> 0.649 Diffusion reasoning
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> We introduce the Diffusion Chain of Lateral Thought (DCoLT), a reasoning
framework for diffusion language models. DCoLT treats each intermediate step in
the reverse diffusion process as a latent &quot;thinking&quot; action and optimizes the
entire reasoning trajectory to maximize the reward on the correctness of the
final answer with outcome-based Reinforcement Learning (RL). Unlike traditional
Chain-of-Thought (CoT) methods that follow a causal, linear thinking process,
DCoLT allows bidirectional, non-linear reasoning with no strict rule on
grammatical correctness amid its intermediate steps of thought. We implement
DCoLT on two representative Diffusion Language Models (DLMs). First, we choose
SEDD as a representative continuous-time discrete diffusion model, where its
concrete score derives a probabilistic policy to maximize the RL reward over
the entire sequence of intermediate diffusion steps. We further consider the
discrete-time masked diffusion language model -- LLaDA, and find that the order
to predict and unmask tokens plays an essential role to optimize its RL action
resulting from the ranking-based Unmasking Policy Module (UPM) defined by the
Plackett-Luce model. Experiments on both math and code generation tasks show
that using only public data and 16 H800 GPUs, DCoLT-reinforced DLMs outperform
other DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA
boosts its reasoning accuracy by +9.8%, +5.7%, +11.4%, +19.5% on GSM8K, MATH,
MBPP, and HumanEval.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.CL (Computation and Language)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 8.8/10</span>
                        <span class="score-tag">Relevance: 9.0/10</span>
                        <span class="score-tag">Novelty: 8.5/10</span>
                        <span class="score-tag">Clarity: 8.0/10</span>
                        <span class="score-tag">Potential Impact: 8.5/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 39.0%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.390">0.390</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 30.8%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.308">0.308</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 64.9%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.649">0.649</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 30.5%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.305">0.305</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="6">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="6">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="6">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="6">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Diffusion reasoning:</strong> The paper proposes DCoLT, which adapts the iterative refinement process of diffusion models for complex logical tasks like math and code generation, treating the entire reasoning trajectory as a single entity optimized via reinforcement learning for multi-step reasoning.
                                            </div>
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">5/5 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">5/5 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">4</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">2.4</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">0</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="6">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2327176332" target="_blank" class="author-name-link">
                                                    <span class="author-name">Zemin Huang:</span>
                                                </a>
                                                
                                                <span class="author-h-value">3</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2315290492" target="_blank" class="author-name-link">
                                                    <span class="author-name">Zhiyang Chen:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2361504857" target="_blank" class="author-name-link">
                                                    <span class="author-name">Zijun Wang:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2333454859" target="_blank" class="author-name-link">
                                                    <span class="author-name">Tiancheng Li:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2327045558" target="_blank" class="author-name-link">
                                                    <span class="author-name">Guo-Jun Qi:</span>
                                                </a>
                                                
                                                <span class="author-h-value">4</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/c33a21e4215a5cd690ca8e2bb0d770b45adf5c89" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="7">
                <div class="paper-header">
                    <div class="paper-number">#8</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2504.12216v2" class="paper-link" target="_blank">
                            d1: Scaling Reasoning in Diffusion Large Language Models via
  Reinforcement Learning
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2504.12216v2 |
                        <strong>Published:</strong> 2025-04-16 |
                        <strong>Highest Score:</strong> 0.538 Diffusion reasoning
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Recent large language models (LLMs) have demonstrated strong reasoning
capabilities that benefits from online reinforcement learning (RL). These
capabilities have primarily been demonstrated within the left-to-right
autoregressive (AR) generation paradigm. In contrast, non-autoregressive
paradigms based on diffusion generate text in a coarse-to-fine manner. Although
recent diffusion-based large language models (dLLMs) have achieved competitive
language modeling performance compared to their AR counterparts, it remains
unclear if dLLMs can also leverage recent advances in LLM reasoning. To this
end, we propose d1, a framework to adapt pre-trained masked dLLMs into
reasoning models via a combination of supervised finetuning (SFT) and RL.
Specifically, we develop and extend techniques to improve reasoning in
pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge
and instill self-improvement behavior directly from existing datasets, and (b)
we introduce a novel critic-free, policy-gradient based RL algorithm called
diffu-GRPO, the first integration of policy gradient methods to masked dLLMs.
Through empirical studies, we investigate the performance of different
post-training recipes on multiple mathematical and planning benchmarks. We find
that d1 yields the best performance and significantly improves performance of a
state-of-the-art dLLM. Our code is released at
https://dllm-reasoning.github.io/.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.CL (Computation and Language)</span>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 8.8/10</span>
                        <span class="score-tag">Relevance: 9.0/10</span>
                        <span class="score-tag">Novelty: 8.5/10</span>
                        <span class="score-tag">Clarity: 8.0/10</span>
                        <span class="score-tag">Potential Impact: 8.5/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 41.5%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.415">0.415</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 31.1%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.311">0.311</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 53.8%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.538">0.538</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 33.2%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.332">0.332</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="7">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="7">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-no">NO</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="7">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="7">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>RLHF:</strong> The paper focuses on using RL (specifically diffu-GRPO) to enhance reasoning in diffusion LLMs, but it does not mention training a reward model on human-ranked data or incorporating human feedback, relying instead on SFT and benchmarks.
                                            </div>
                                            
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Diffusion reasoning:</strong> The paper adapts the iterative denoising process of diffusion models for multi-step reasoning tasks, such as math and planning, by treating reasoning paths holistically through dLLMs.
                                            </div>
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">4/4 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">4/4 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">10</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">5.0</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">2</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="7">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2260172378" target="_blank" class="author-name-link">
                                                    <span class="author-name">Siyan Zhao:</span>
                                                </a>
                                                
                                                <span class="author-h-value">6</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2357079322" target="_blank" class="author-name-link">
                                                    <span class="author-name">Devaansh Gupta:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2166847" target="_blank" class="author-name-link">
                                                    <span class="author-name">Qinqing Zheng:</span>
                                                </a>
                                                
                                                <span class="author-h-value">10</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2267723293" target="_blank" class="author-name-link">
                                                    <span class="author-name">Aditya Grover:</span>
                                                </a>
                                                
                                                <span class="author-h-value">3</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/8e3b1f5d8b6c165f64137cc1f7dea89cf6f622bd" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="8">
                <div class="paper-header">
                    <div class="paper-number">#9</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2501.07727v2" class="paper-link" target="_blank">
                            Stronger Than You Think: Benchmarking Weak Supervision on Realistic
  Tasks
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2501.07727v2 |
                        <strong>Published:</strong> 2025-01-13 |
                        <strong>Highest Score:</strong> 0.548 Weak supervision
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Weak supervision (WS) is a popular approach for label-efficient learning,
leveraging diverse sources of noisy but inexpensive weak labels to
automatically annotate training data. Despite its wide usage, WS and its
practical value are challenging to benchmark due to the many knobs in its
setup, including: data sources, labeling functions (LFs), aggregation
techniques (called label models), and end model pipelines. Existing evaluation
suites tend to be limited, focusing on particular components or specialized use
cases. Moreover, they often involve simplistic benchmark tasks or de-facto LF
sets that are suboptimally written, producing insights that may not generalize
to real-world settings. We address these limitations by introducing a new
benchmark, BOXWRENCH, designed to more accurately reflect real-world usages of
WS. This benchmark features tasks with (1) higher class cardinality and
imbalance, (2) notable domain expertise requirements, and (3) opportunities to
re-use LFs across parallel multilingual corpora. For all tasks, LFs are written
using a careful procedure aimed at mimicking real-world settings. In contrast
to existing WS benchmarks, we show that supervised learning requires
substantial amounts (1000+) of labeled examples to match WS in many settings.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 8.5/10</span>
                        <span class="score-tag">Relevance: 9.0/10</span>
                        <span class="score-tag">Novelty: 8.5/10</span>
                        <span class="score-tag">Clarity: 8.0/10</span>
                        <span class="score-tag">Potential Impact: 8.5/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 33.6%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.336">0.336</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 54.8%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.548">0.548</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 24.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.247">0.247</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 28.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.287">0.287</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="8">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="8">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="8">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="8">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Weak supervision:</strong> The paper's main contribution is to benchmark and evaluate weak supervision techniques, including labeling functions and label models for generating noisy labels, directly aligning with the definition of weak supervision.
                                            </div>
                                            
                                        
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">7/7 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">7/7 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">17</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">3.1</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">1</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="8">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2340048194" target="_blank" class="author-name-link">
                                                    <span class="author-name">Tianyi Zhang:</span>
                                                </a>
                                                
                                                <span class="author-h-value">0</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2340439703" target="_blank" class="author-name-link">
                                                    <span class="author-name">Linrong Cai:</span>
                                                </a>
                                                
                                                <span class="author-h-value">0</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2340172144" target="_blank" class="author-name-link">
                                                    <span class="author-name">Jeffrey Li:</span>
                                                </a>
                                                
                                                <span class="author-h-value">0</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2340014150" target="_blank" class="author-name-link">
                                                    <span class="author-name">Nicholas Roberts:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2820009" target="_blank" class="author-name-link">
                                                    <span class="author-name">Neel Guha:</span>
                                                </a>
                                                
                                                <span class="author-h-value">17</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2340362174" target="_blank" class="author-name-link">
                                                    <span class="author-name">Jinoh Lee:</span>
                                                </a>
                                                
                                                <span class="author-h-value">0</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2269049908" target="_blank" class="author-name-link">
                                                    <span class="author-name">Frederic Sala:</span>
                                                </a>
                                                
                                                <span class="author-h-value">3</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/6b690b63eb033c8db82577647bab9f743e922736" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="9">
                <div class="paper-header">
                    <div class="paper-number">#10</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2502.12366v1" class="paper-link" target="_blank">
                            ScriptoriumWS: A Code Generation Assistant for Weak Supervision
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2502.12366v1 |
                        <strong>Published:</strong> 2025-02-17 |
                        <strong>Highest Score:</strong> 0.540 Weak supervision
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Weak supervision is a popular framework for overcoming the labeled data
bottleneck: the need to obtain labels for training data. In weak supervision,
multiple noisy-but-cheap sources are used to provide guesses of the label and
are aggregated to produce high-quality pseudolabels. These sources are often
expressed as small programs written by domain experts -- and so are expensive
to obtain. Instead, we argue for using code-generation models to act as coding
assistants for crafting weak supervision sources. We study prompting strategies
to maximize the quality of the generated sources, settling on a multi-tier
strategy that incorporates multiple types of information. We explore how to
best combine hand-written and generated sources. Using these insights, we
introduce ScriptoriumWS, a weak supervision system that, when compared to
hand-crafted sources, maintains accuracy and greatly improves coverage.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 8.5/10</span>
                        <span class="score-tag">Relevance: 9.0/10</span>
                        <span class="score-tag">Novelty: 8.5/10</span>
                        <span class="score-tag">Clarity: 8.0/10</span>
                        <span class="score-tag">Potential Impact: 8.5/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 33.4%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.334">0.334</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 54.0%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.540">0.540</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 26.0%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.260">0.260</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 25.6%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.256">0.256</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="9">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="9">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="9">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="9">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Weak supervision:</strong> The paper's main contribution is developing a system to generate labeling functions for weak supervision using code-generation models, which aligns with weak supervision's focus on programmatically creating noisy labels from sources like heuristics and rules.
                                            </div>
                                            
                                        
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">6/6 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">6/6 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">8</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">3.7</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">1</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="9">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2116822245" target="_blank" class="author-name-link">
                                                    <span class="author-name">Tzu-Heng Huang:</span>
                                                </a>
                                                
                                                <span class="author-h-value">5</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2312271010" target="_blank" class="author-name-link">
                                                    <span class="author-name">Catherine Cao:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2136775815" target="_blank" class="author-name-link">
                                                    <span class="author-name">Spencer Schoenberg:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/27638084" target="_blank" class="author-name-link">
                                                    <span class="author-name">Harit Vishwakarma:</span>
                                                </a>
                                                
                                                <span class="author-h-value">8</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2340014150" target="_blank" class="author-name-link">
                                                    <span class="author-name">Nicholas Roberts:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2269049908" target="_blank" class="author-name-link">
                                                    <span class="author-name">Frederic Sala:</span>
                                                </a>
                                                
                                                <span class="author-h-value">3</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/199f126f64d28554217d391019a8f2f78dffeed3" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="10">
                <div class="paper-header">
                    <div class="paper-number">#11</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2506.10911v1" class="paper-link" target="_blank">
                            NoLoCo: No-all-reduce Low Communication Training Method for Large Models
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2506.10911v1 |
                        <strong>Published:</strong> 2025-06-12 |
                        <strong>Highest Score:</strong> 0.476 Distributed training
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Training large language models is generally done via optimization methods on
clusters containing tens of thousands of accelerators, communicating over a
high-bandwidth interconnect. Scaling up these clusters is expensive and can
become impractical, imposing limits on the size of models that can be trained.
Several recent studies have proposed training methods that are less
communication intensive, avoiding the need for a highly connected compute
cluster. These state-of-the-art low communication training methods still employ
a synchronization step for model parameters, which, when performed over all
model replicas, can become costly on a low-bandwidth network.
  In this work, we propose a novel optimization method, NoLoCo, that does not
explicitly synchronize all model parameters during training and, as a result,
does not require any collective communication. NoLoCo implicitly synchronizes
model weights via a novel variant of the Nesterov momentum optimizer by
partially averaging model weights with a randomly selected other one. We
provide both a theoretical convergence analysis for our proposed optimizer as
well as empirical results from language model training.
  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,
between 125M to 6.8B parameters. Our method requires significantly less
communication overhead than fully sharded data parallel training or even widely
used low communication training method, DiLoCo. The synchronization step itself
is estimated to be one magnitude faster than the all-reduce used in DiLoCo for
few hundred accelerators training over the internet. We also do not have any
global blocking communication that reduces accelerator idling time. Compared to
DiLoCo, we also observe up to $4\%$ faster convergence rate with wide range of
model sizes and accelerator counts.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 8.5/10</span>
                        <span class="score-tag">Relevance: 9.0/10</span>
                        <span class="score-tag">Novelty: 8.5/10</span>
                        <span class="score-tag">Clarity: 8.0/10</span>
                        <span class="score-tag">Potential Impact: 8.5/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 30.5%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.305">0.305</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 31.8%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.318">0.318</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 27.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.277">0.277</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 47.6%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.476">0.476</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="10">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="10">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="10">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="10">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                        
                                            
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Distributed training:</strong> The paper's main contribution is a novel optimization method for distributed training of large models, which reduces communication overhead by partitioning and synchronizing computations across multiple nodes, directly aligning with distributed training, parallel computing, and multi-node machine learning.
                                            </div>
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">5/5 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">5/5 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">22</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">5.2</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">1</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (title match)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="10">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2051209" target="_blank" class="author-name-link">
                                                    <span class="author-name">J. Kolehmainen:</span>
                                                </a>
                                                
                                                <span class="author-h-value">22</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2347536546" target="_blank" class="author-name-link">
                                                    <span class="author-name">Nikolay Blagoev:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2253812306" target="_blank" class="author-name-link">
                                                    <span class="author-name">John Donaghy:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2179107345" target="_blank" class="author-name-link">
                                                    <span class="author-name">Ouguzhan Ersoy:</span>
                                                </a>
                                                
                                                <span class="author-h-value">2</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2366564213" target="_blank" class="author-name-link">
                                                    <span class="author-name">Christopher Nies:</span>
                                                </a>
                                                
                                                <span class="author-h-value">0</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/17a1a0a9dc41fc779cd8e4a7a92ece4747ea439e" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="11">
                <div class="paper-header">
                    <div class="paper-number">#12</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2502.07750v2" class="paper-link" target="_blank">
                            PFedDST: Personalized Federated Learning with Decentralized Selection
  Training
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2502.07750v2 |
                        <strong>Published:</strong> 2025-02-11 |
                        <strong>Highest Score:</strong> 0.403 Distributed training
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Distributed Learning (DL) enables the training of machine learning models
across multiple devices, yet it faces challenges like non-IID data
distributions and device capability disparities, which can impede training
efficiency. Communication bottlenecks further complicate traditional Federated
Learning (FL) setups. To mitigate these issues, we introduce the Personalized
Federated Learning with Decentralized Selection Training (PFedDST) framework.
PFedDST enhances model training by allowing devices to strategically evaluate
and select peers based on a comprehensive communication score. This score
integrates loss, task similarity, and selection frequency, ensuring optimal
peer connections. This selection strategy is tailored to increase local
personalization and promote beneficial peer collaborations to strengthen the
stability and efficiency of the training process. Our experiments demonstrate
that PFedDST not only enhances model accuracy but also accelerates convergence.
This approach outperforms state-of-the-art methods in handling data
heterogeneity, delivering both faster and more effective training in diverse
and decentralized systems.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                        <span class="category-tag">cs.AI (Artificial Intelligence)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 8.5/10</span>
                        <span class="score-tag">Relevance: 9.0/10</span>
                        <span class="score-tag">Novelty: 7.5/10</span>
                        <span class="score-tag">Clarity: 8.0/10</span>
                        <span class="score-tag">Potential Impact: 8.5/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 33.4%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.334">0.334</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 25.1%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.251">0.251</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 26.3%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.263">0.263</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 40.3%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.403">0.403</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="11">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="11">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="11">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="11">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                        
                                            
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Distributed training:</strong> The paper introduces PFedDST, a framework for distributed training in federated learning, where model training is partitioned across multiple devices through peer selection and aggregation to accelerate convergence and handle data heterogeneity, aligning with distributed training and multi-node machine learning concepts.
                                            </div>
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                <p class="text-muted">No H-index data available</p>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="12">
                <div class="paper-header">
                    <div class="paper-number">#13</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2501.05323v1" class="paper-link" target="_blank">
                            Distributed Learning and Inference Systems: A Networking Perspective
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2501.05323v1 |
                        <strong>Published:</strong> 2025-01-09 |
                        <strong>Highest Score:</strong> 0.534 Distributed training
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Machine learning models have achieved, and in some cases surpassed,
human-level performance in various tasks, mainly through centralized training
of static models and the use of large models stored in centralized clouds for
inference. However, this centralized approach has several drawbacks, including
privacy concerns, high storage demands, a single point of failure, and
significant computing requirements. These challenges have driven interest in
developing alternative decentralized and distributed methods for AI training
and inference. Distribution introduces additional complexity, as it requires
managing multiple moving parts. To address these complexities and fill a gap in
the development of distributed AI systems, this work proposes a novel
framework, Data and Dynamics-Aware Inference and Training Networks (DA-ITN).
The different components of DA-ITN and their functions are explored, and the
associated challenges and research areas are highlighted.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                        <span class="category-tag">cs.NI (Networking and Internet Architecture)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 8.0/10</span>
                        <span class="score-tag">Relevance: 9.0/10</span>
                        <span class="score-tag">Novelty: 7.5/10</span>
                        <span class="score-tag">Clarity: 8.0/10</span>
                        <span class="score-tag">Potential Impact: 8.5/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 33.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.337">0.337</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 32.6%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.326">0.326</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 34.1%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.341">0.341</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 53.4%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.534">0.534</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="12">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="12">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="12">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="12">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                        
                                            
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Distributed training:</strong> The paper proposes a framework for distributed AI training and inference, including methods like federated learning and distributed learning, which involve partitioning data and computation across multiple nodes to accelerate training.
                                            </div>
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">4/4 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">4/4 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">7</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">3.0</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">1</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="12">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/14006040" target="_blank" class="author-name-link">
                                                    <span class="author-name">Hesham G. Moussa:</span>
                                                </a>
                                                
                                                <span class="author-h-value">7</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/66193246" target="_blank" class="author-name-link">
                                                    <span class="author-name">Arashmid Akhavain:</span>
                                                </a>
                                                
                                                <span class="author-h-value">3</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2340001346" target="_blank" class="author-name-link">
                                                    <span class="author-name">S. M. Hosseini:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/2256522006" target="_blank" class="author-name-link">
                                                    <span class="author-name">Bill McCormick:</span>
                                                </a>
                                                
                                                <span class="author-h-value">1</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/565c3a1b4c2b2f5635a171ec2d481870d311c6f2" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="13">
                <div class="paper-header">
                    <div class="paper-number">#14</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2012.01839v2" class="paper-link" target="_blank">
                            Distributed Training and Optimization Of Neural Networks
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2012.01839v2 |
                        <strong>Published:</strong> 2020-12-03 |
                        <strong>Highest Score:</strong> 0.625 Distributed training
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Deep learning models are yielding increasingly better performances thanks to
multiple factors. To be successful, model may have large number of parameters
or complex architectures and be trained on large dataset. This leads to large
requirements on computing resource and turn around time, even more so when
hyper-parameter optimization is done (e.g search over model architectures).
While this is a challenge that goes beyond particle physics, we review the
various ways to do the necessary computations in parallel, and put it in the
context of high energy physics.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                    </div>
                    
                    
                    <div class="paper-scores-row">
                        <span class="score-tag overall">Overall Score: 6.5/10</span>
                        <span class="score-tag">Relevance: 7.5/10</span>
                        <span class="score-tag">Novelty: 4.0/10</span>
                        <span class="score-tag">Clarity: 7.0/10</span>
                        <span class="score-tag">Potential Impact: 6.0/10</span>
                    </div>
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 29.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.297">0.297</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 28.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.287">0.287</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 27.2%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.272">0.272</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 62.5%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.625">0.625</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="13">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="13">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-yes">YES</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="13">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="13">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                        
                                            
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Distributed training:</strong> The paper's main contribution is a review of distributed training methods for neural networks, including parallel computing strategies like parameter, data, and model parallelism, which directly align with accelerating model training across multiple nodes.
                                            </div>
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                
                                <div class="h-index-summary">
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Data found:</span>
                                        <span class="h-index-value">2/2 authors</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">H-index available:</span>
                                        <span class="h-index-value">2/2 found</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Highest:</span>
                                        <span class="h-index-value">114</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Average:</span>
                                        <span class="h-index-value">66.0</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Notable (H>5):</span>
                                        <span class="h-index-value">2</span>
                                    </div>
                                    <div class="h-index-metric">
                                        <span class="h-index-label">Source:</span>
                                        <span class="h-index-value">Semantic Scholar (base arXiv ID)</span>
                                    </div>
                                    
                                    <div class="h-index-expand">
                                        <button class="btn btn-sm btn-outline-light h-index-toggle" 
                                                onclick="toggleHIndexDetails(this)" 
                                                data-paper-index="13">
                                            Show individual H-indices ‚ñº
                                        </button>
                                        <div class="h-index-details" style="display: none; margin-top: 0.5rem;">
                                            <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">Individual Author H-Indices:</h6>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/52630992" target="_blank" class="author-name-link">
                                                    <span class="author-name">J. Vlimant:</span>
                                                </a>
                                                
                                                <span class="author-h-value">114</span>
                                            </div>
                                            
                                            <div class="individual-h-index">
                                                
                                                <a href="https://www.semanticscholar.org/author/1779195" target="_blank" class="author-name-link">
                                                    <span class="author-name">Junqi Yin:</span>
                                                </a>
                                                
                                                <span class="author-h-value">18</span>
                                            </div>
                                            
                                            
                                            <div class="paper-verification" style="margin-top: 1rem; padding-top: 0.5rem; border-top: 1px solid #444;">
                                                <a href="https://www.semanticscholar.org/paper/3dba51f1c185cc7740515954ea2a83bf051bdaf6" target="_blank" class="paper-verification-link">
                                                    üîó Verify paper on Semantic Scholar
                                                </a>
                                            </div>
                                            
                                        </div>
                                    </div>
                                    
                                </div>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="14">
                <div class="paper-header">
                    <div class="paper-number">#15</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2410.21842v2" class="paper-link" target="_blank">
                            Diffusion as Reasoning: Enhancing Object Navigation via Diffusion Model
  Conditioned on LLM-based Object-Room Knowledge
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2410.21842v2 |
                        <strong>Published:</strong> 2024-10-29 |
                        <strong>Highest Score:</strong> 0.586 Diffusion reasoning
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> The Object Navigation (ObjectNav) task aims to guide an agent to locate
target objects in unseen environments using partial observations. Prior
approaches have employed location prediction paradigms to achieve long-term
goal reasoning, yet these methods often struggle to effectively integrate
contextual relation reasoning. Alternatively, map completion-based paradigms
predict long-term goals by generating semantic maps of unexplored areas.
However, existing methods in this category fail to fully leverage known
environmental information, resulting in suboptimal map quality that requires
further improvement. In this work, we propose a novel approach to enhancing the
ObjectNav task, by training a diffusion model to learn the statistical
distribution patterns of objects in semantic maps, and using the map of the
explored regions during navigation as the condition to generate the map of the
unknown regions, thereby realizing the long-term goal reasoning of the target
object, i.e., diffusion as reasoning (DAR). Meanwhile, we propose the Room
Guidance method, which leverages commonsense knowledge derived from large
language models (LLMs) to guide the diffusion model in generating room-aware
object distributions. Based on the generated map in the unknown region, the
agent sets the predicted location of the target as the goal and moves towards
it. Experiments on Gibson and MP3D show the effectiveness of our method.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.CV (Computer Vision and Pattern Recognition)</span>
                        
                        <span class="category-tag">cs.AI (Artificial Intelligence)</span>
                        
                    </div>
                    
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 33.2%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.332">0.332</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 28.1%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.281">0.281</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 58.6%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.586">0.586</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 27.5%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.275">0.275</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="14">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="14">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-no">NO</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="14">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="14">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Diffusion reasoning:</strong> The paper uses a diffusion model for generating semantic maps in object navigation, involving iterative refinement for spatial predictions, but it does not adapt this process to multi-step logical reasoning or treat a Chain-of-Thought as a single entity for holistic correction. It focuses on environmental mapping and object distribution, not complex logical tasks.
                                            </div>
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                <p class="text-muted">No H-index data available</p>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="15">
                <div class="paper-header">
                    <div class="paper-number">#16</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2506.10633v1" class="paper-link" target="_blank">
                            Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent
  Diffusion Models
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2506.10633v1 |
                        <strong>Published:</strong> 2025-06-12 |
                        <strong>Highest Score:</strong> 0.410 Diffusion reasoning
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Latent Diffusion Models have shown remarkable results in text-guided image
synthesis in recent years. In the domain of natural (RGB) images, recent works
have shown that such models can be adapted to various vision-language
downstream tasks with little to no supervision involved. On the contrary,
text-to-image Latent Diffusion Models remain relatively underexplored in the
field of medical imaging, primarily due to limited data availability (e.g., due
to privacy concerns). In this work, focusing on the chest X-ray modality, we
first demonstrate that a standard text-conditioned Latent Diffusion Model has
not learned to align clinically relevant information in free-text radiology
reports with the corresponding areas of the given scan. Then, to alleviate this
issue, we propose a fine-tuning framework to improve multi-modal alignment in a
pre-trained model such that it can be efficiently repurposed for downstream
tasks such as phrase grounding. Our method sets a new state-of-the-art on a
standard benchmark dataset (MS-CXR), while also exhibiting robust performance
on out-of-distribution data (VinDr-CXR). Our code will be made publicly
available.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.CV (Computer Vision and Pattern Recognition)</span>
                        
                    </div>
                    
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 31.9%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.319">0.319</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 35.0%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.350">0.350</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 41.0%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.410">0.410</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 23.3%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.233">0.233</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="15">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="15">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-no">NO</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="15">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="15">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                        
                                            
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>Diffusion reasoning:</strong> The paper discusses fine-tuning latent diffusion models for image-text alignment in medical imaging, specifically for tasks like phrase grounding in chest X-rays. It does not involve adapting the diffusion process for multi-step logical reasoning or treating a Chain-of-Thought as a single entity for holistic correction.
                                            </div>
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                <p class="text-muted">No H-index data available</p>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="16">
                <div class="paper-header">
                    <div class="paper-number">#17</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2506.10943v1" class="paper-link" target="_blank">
                            Self-Adapting Language Models
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2506.10943v1 |
                        <strong>Published:</strong> 2025-06-12 |
                        <strong>Highest Score:</strong> 0.402 RLHF
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Large language models (LLMs) are powerful but static; they lack mechanisms to
adapt their weights in response to new tasks, knowledge, or examples. We
introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to
self-adapt by generating their own finetuning data and update directives. Given
a new input, the model produces a self-edit-a generation that may restructure
the information in different ways, specify optimization hyperparameters, or
invoke tools for data augmentation and gradient-based updates. Through
supervised finetuning (SFT), these self-edits result in persistent weight
updates, enabling lasting adaptation. To train the model to produce effective
self-edits, we use a reinforcement learning loop with the downstream
performance of the updated model as the reward signal. Unlike prior approaches
that rely on separate adaptation modules or auxiliary networks, SEAL directly
uses the model&#x27;s own generation to control its adaptation process. Experiments
on knowledge incorporation and few-shot generalization show that SEAL is a
promising step toward language models capable of self-directed adaptation. Our
website and code is available at https://jyopari.github.io/posts/seal.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.LG (Machine Learning)</span>
                        
                    </div>
                    
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 40.2%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.402">0.402</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 38.0%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.380">0.380</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 35.1%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.351">0.351</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 30.2%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.302">0.302</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="16">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="16">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-no">NO</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                            
                                        
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="16">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                        <button class="btn btn-sm btn-outline-light llm-toggle" 
                                                onclick="toggleLLMDetails(this)" 
                                                data-paper-index="16">
                                            Show justifications ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                    <div class="llm-details" style="display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
                                        <h6 style="font-size: 0.9rem; margin-bottom: 0.5rem;">LLM Justifications:</h6>
                                        
                                            
                                            <div class="llm-justification">
                                                <strong>RLHF:</strong> The paper describes a reinforcement learning framework where the reward signal is based on downstream task performance, not human feedback. Since RLHF specifically requires human-ranked data and a separate reward model trained on human preferences, this does not qualify.
                                            </div>
                                            
                                        
                                            
                                        
                                            
                                        
                                            
                                        
                                    </div>
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                <p class="text-muted">No H-index data available</p>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="17">
                <div class="paper-header">
                    <div class="paper-number">#18</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2506.10820v1" class="paper-link" target="_blank">
                            A Combined Parallel-in-time Direct Inverse (ParaDIn)-Parareal Method for
  Nonlinear Differential Equations
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2506.10820v1 |
                        <strong>Published:</strong> 2025-06-12 |
                        <strong>Highest Score:</strong> 0.312 Distributed training
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> As has been shown in our previous work, the parallel-in-time direct inverse
(ParaDIn) method introduced by Yamaleev and Paudel in (arXiv: 2406.00878v1,
2024) imposes some constraint on the maximum number of time levels, $N_t$, that
can be integrated in parallel. To circumvent this problem and further increase
the speedup, we combine the ParaDIn method with the Parareal algorithm to
efficiently parallelize the first-order time derivative term in nonlinear
partial differential equations discretized by the method of lines. The main
idea of the proposed approach is to use a block-Jacobi preconditioner, so that
each block is solved by using the ParaDIn method. To accelerate the convergence
of Jacobi iterations, we use the Parareal method which can be interpreted as a
two-level multigrid method in time. In contrast to the conventional Parareal
algorithm whose coarse grid correction step is performed sequentially, both the
coarse- and fine-grid propagators in the proposed approach are implemented in
parallel by using the ParaDIn method, thus significantly increasing the
parallel performance of the combined algorithm. Numerical results show that the
new combined ParaDIn-Parareal method provides the speedup of up to 124 on 480
computing cores as compared with the sequential first-order implicit backward
difference (BDF1) scheme for the 2-D nonlinear heat and Burgers equations with
both smooth and discontinuous solutions.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">math.NA (Numerical Analysis)</span>
                        
                        <span class="category-tag">cs.NA (Numerical Analysis)</span>
                        
                    </div>
                    
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 12.7%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.127">0.127</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 12.8%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.128">0.128</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 27.3%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.273">0.273</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 31.2%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.312">0.312</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="17">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="17">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="17">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                <p class="text-muted">No H-index data available</p>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="18">
                <div class="paper-header">
                    <div class="paper-number">#19</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2506.10570v1" class="paper-link" target="_blank">
                            6G Infrastructures for Edge AI: An Analytical Perspective
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2506.10570v1 |
                        <strong>Published:</strong> 2025-06-12 |
                        <strong>Highest Score:</strong> 0.311 Distributed training
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> The convergence of Artificial Intelligence (AI) and the Internet of Things
has accelerated the development of distributed, network-sensitive applications,
necessitating ultra-low latency, high throughput, and real-time processing
capabilities. While 5G networks represent a significant technological
milestone, their ability to support AI-driven edge applications remains
constrained by performance gaps observed in real-world deployments. This paper
addresses these limitations and highlights critical advancements needed to
realize a robust and scalable 6G ecosystem optimized for AI applications.
Furthermore, we conduct an empirical evaluation of 5G network infrastructure in
central Europe, with latency measurements ranging from 61 ms to 110 ms across
different close geographical areas. These values exceed the requirements of
latency-critical AI applications by approximately 270%, revealing significant
shortcomings in current deployments. Building on these findings, we propose a
set of recommendations to bridge the gap between existing 5G performance and
the requirements of next-generation AI applications.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.DC (Distributed, Parallel, and Cluster Computing)</span>
                        
                    </div>
                    
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 22.8%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.228">0.228</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 21.5%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.215">0.215</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 21.8%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.218">0.218</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 31.1%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.311">0.311</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="18">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="18">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="18">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                <p class="text-muted">No H-index data available</p>
                            
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="paper-card" data-paper-index="19">
                <div class="paper-header">
                    <div class="paper-number">#20</div>
                    <h5 class="paper-title">
                        <a href="http://arxiv.org/abs/2506.10397v1" class="paper-link" target="_blank">
                            Bug Classification in Quantum Software: A Rule-Based Framework and Its
  Evaluation
                        </a>
                    </h5>
                    <div class="paper-meta">
                        <strong>arXiv ID:</strong> 2506.10397v1 |
                        <strong>Published:</strong> 2025-06-12 |
                        <strong>Highest Score:</strong> 0.220 Distributed training
                    </div>
                </div>
                
                <div class="paper-body">
                    <div class="paper-abstract">
                        <strong>Abstract:</strong> Accurate classification of software bugs is essential for improving software
quality. This paper presents a rule-based automated framework for classifying
issues in quantum software repositories by bug type, category, severity, and
impacted quality attributes, with additional focus on quantum-specific bug
types. The framework applies keyword and heuristic-based techniques tailored to
quantum computing. To assess its reliability, we manually classified a
stratified sample of 4,984 issues from a dataset of 12,910 issues across 36
Qiskit repositories. Automated classifications were compared with ground truth
using accuracy, precision, recall, and F1-score. The framework achieved up to
85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393
(quality attribute). Statistical validation via paired t-tests and Cohen&#x27;s
Kappa showed substantial to almost perfect agreement for bug type (k = 0.696),
category (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug
type (k = 0.712). Severity classification showed slight agreement (k = 0.162),
suggesting room for improvement. Large-scale analysis revealed that classical
bugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug
categories included compatibility, functional, and quantum-specific defects,
while usability, maintainability, and interoperability were the most impacted
quality attributes. Most issues (93.7%) were low severity; only 4.3% were
critical. A detailed review of 1,550 quantum-specific bugs showed that over
half involved quantum circuit-level problems, followed by gate errors and
hardware-related issues.
                    </div>
                    
                    <div class="paper-categories">
                        <strong>Categories:</strong>
                        
                        <span class="category-tag">cs.SE (Software Engineering)</span>
                        
                        <span class="category-tag">cs.CY (Computers and Society)</span>
                        
                        <span class="category-tag">cs.DC (Distributed, Parallel, and Cluster Computing)</span>
                        
                    </div>
                    
                    
                    
                    <div class="paper-metrics-row">
                        <div class="similarity-scores">
                            <h6>Similarity Scores:</h6>
                            
                                <div class="similarity-summary">
                                    <div class="similarity-scores-content">
                                        
                                        <span class="similarity-label" data-topic="RLHF">RLHF:</span>
                                        <div class="similarity-right-column" data-topic="RLHF">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 20.2%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.202">0.202</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Weak_supervision">Weak supervision:</span>
                                        <div class="similarity-right-column" data-topic="Weak_supervision">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 21.1%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.211">0.211</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Diffusion_reasoning">Diffusion reasoning:</span>
                                        <div class="similarity-right-column" data-topic="Diffusion_reasoning">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 21.9%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.219">0.219</span>
                                        </div>
                                        
                                        <span class="similarity-label" data-topic="Distributed_training">Distributed training:</span>
                                        <div class="similarity-right-column" data-topic="Distributed_training">
                                            <div class="similarity-bar">
                                                <div class="similarity-bar-fill" style="width: 22.0%"></div>
                                            </div>
                                            <span class="similarity-value" data-original-score="0.220">0.220</span>
                                        </div>
                                        
                                    </div>
                                    <div class="similarity-toggle-section">
                                        <button class="btn btn-sm btn-outline-light similarity-toggle" 
                                                onclick="toggleSimilarityDetails(this)" 
                                                data-paper-index="19">
                                            Show other topics ‚ñº
                                        </button>
                                        <button class="btn btn-sm btn-outline-light similarity-normalize-toggle" 
                                                onclick="toggleNormalizedScores(this)" 
                                                data-mode="raw"
                                                data-paper-index="19">
                                            Show normalized scores
                                        </button>
                                    </div>
                                </div>
                            
                        </div>
                        
                        <div class="llm-validation">
                            <h6>LLM Validation:</h6>
                            
                                
                                <div class="llm-validation-summary">
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="RLHF">
                                                <span class="llm-topic-label">RLHF:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Weak_supervision">
                                                <span class="llm-topic-label">Weak supervision:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Diffusion_reasoning">
                                                <span class="llm-topic-label">Diffusion reasoning:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                        
                                            
                                            <div class="llm-validation-item" data-topic="Distributed_training">
                                                <span class="llm-topic-label">Distributed training:</span>
                                                
                                                    
                                                        <span class="llm-status llm-disabled">Below threshold</span>
                                                    
                                                
                                            </div>
                                        
                                    
                                    
                                    
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                        
                                    
                                    
                                    <div class="llm-buttons-row">
                                        <button class="btn btn-sm btn-outline-light llm-validation-toggle" 
                                                onclick="toggleLLMValidationDetails(this)" 
                                                data-paper-index="19">
                                            Show other topics ‚ñº
                                        </button>
                                        
                                    </div>
                                    
                                    
                                </div>
                            
                        </div>
                        
                        <div class="h-index-scores">
                            <h6>Author H-Index:</h6>
                            
                                <p class="text-muted">No H-index data available</p>
                            
                        </div>
                    </div>
                </div>
            </div>
            
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script>
        // Papers data for JavaScript processing
        const papers = [{"abstract": "Fine-tuning large language models (LLMs) to align with user preferences is\nchallenging due to the high cost of quality human annotations in Reinforcement\nLearning from Human Feedback (RLHF) and the generalizability limitations of AI\nFeedback. To address these challenges, we propose RLTHF, a human-AI hybrid\nframework that combines LLM-based initial alignment with selective human\nannotations to achieve full-human annotation alignment with minimal effort.\nRLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward\nmodel\u0026#x27;s reward distribution and iteratively enhances alignment by integrating\nstrategic human corrections while leveraging LLM\u0026#x27;s correctly labeled samples.\nEvaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human\nannotation-level alignment with only 6-7% of the human annotation effort.\nFurthermore, models trained on RLTHF\u0026#x27;s curated datasets for downstream tasks\noutperform those trained on fully human-annotated datasets, underscoring the\neffectiveness of RLTHF\u0026#x27;s strategic data curation.", "arxiv_id": "2502.13417v2", "arxiv_url": "http://arxiv.org/abs/2502.13417v2", "author_h_indices": {"authors_found_count": 14, "authors_with_h_index_count": 14, "average_h_index": 2.6, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 6, "individual_h_indices": [{"h_index": 2, "name": "Yifei Xu", "profile_url": "https://www.semanticscholar.org/author/2268722396"}, {"h_index": 6, "name": "Tusher Chakraborty", "profile_url": "https://www.semanticscholar.org/author/3431742"}, {"h_index": 5, "name": "Emre Kiciman", "profile_url": "https://www.semanticscholar.org/author/2264962872"}, {"h_index": 1, "name": "Bibek Aryal", "profile_url": "https://www.semanticscholar.org/author/2312460794"}, {"h_index": 1, "name": "Eduardo Rodrigues", "profile_url": "https://www.semanticscholar.org/author/2346116388"}, {"h_index": 1, "name": "Srinagesh Sharma", "profile_url": "https://www.semanticscholar.org/author/2346253357"}, {"h_index": 2, "name": "Roberto Estev\u00e3o", "profile_url": "https://www.semanticscholar.org/author/2256989289"}, {"h_index": 5, "name": "M. A. D. L. Balaguer", "profile_url": "https://www.semanticscholar.org/author/34938986"}, {"h_index": 1, "name": "Jessica Wolk", "profile_url": "https://www.semanticscholar.org/author/2332536904"}, {"h_index": 2, "name": "Rafael Padilha", "profile_url": "https://www.semanticscholar.org/author/2279548480"}, {"h_index": 3, "name": "Leonardo Nunes", "profile_url": "https://www.semanticscholar.org/author/2256989583"}, {"h_index": 1, "name": "Shobana Balakrishnan", "profile_url": "https://www.semanticscholar.org/author/2346253640"}, {"h_index": 2, "name": "Songwu Lu", "profile_url": "https://www.semanticscholar.org/author/2268727460"}, {"h_index": 4, "name": "Ranveer Chandra", "profile_url": "https://www.semanticscholar.org/author/2256993742"}], "notable_authors_count": 1, "paper_url": "https://www.semanticscholar.org/paper/0152aafbec465a090684637e1da693d6deb98172", "success": true, "total_authors": 14}, "authors": ["Yifei Xu", "Tusher Chakraborty", "Emre K\u0131c\u0131man", "Bibek Aryal", "Eduardo Rodrigues", "Srinagesh Sharma", "Roberto Estevao", "Maria Angels de Luis Balaguer", "Jessica Wolk", "Rafael Padilha", "Leonardo Nunes", "Shobana Balakrishnan", "Songwu Lu", "Ranveer Chandra"], "categories": ["cs.CL (Computation and Language)", "cs.AI (Artificial Intelligence)", "cs.LG (Machine Learning)"], "embedding_model": "openai-large", "highest_score": 0.6653451676411026, "highest_score_topic": "RLHF", "introduction": {"content": "Introduction\n\n\\begin{figure*}[t]\n \\centering\n \\includegraphics[width=0.85\\textwidth]{figures/sargy_pipeline-6.pdf}\n \\caption{Overview of \\myname{} process. \\myname{} starts with coarse LLM alignment on the task. It then iteratively takes targeted human feedback to reach the complete human alignment, leveraging reward distribution of a reward model in its training dataset.}\n\n\\end{figure*}\n\nIn recent years, large language models (LLMs) have demonstrated remarkable advancements, unlocking new possibilities across a wide range of applications~. As these models become more powerful, the focus has shifted toward customization, i.e., fine-tuning base models to better serve specific tasks and user needs~. Companies are increasingly investing in solutions built upon fine-tuned models, recognizing the value of adapting LLMs to align with end-user preferences, including intent, style, grounding, and compliance requirements~. A key approach to achieving this alignment is Reinforcement Learning from Human Feedback (RLHF), which has emerged as a widely adopted technique in the literature for refining model behavior based on human feedback~.\n\nThe effectiveness of RLHF techniques heavily depends on high-quality human annotations, which are both costly and time-consuming to obtain~. To mitigate this challenge, Reinforcement Learning from AI Feedback (RLAIF) has been introduced, leveraging LLMs to replace human annotators in the feedback loop~. While RLAIF can approximate human judgment to some extent, it is sensitive to factors such as prompt optimization, task complexity, model bias, generator-discriminator gap, and the capability of the judge model, limiting its ability to fully replicate human annotations~. Our evaluation also provides evidence of these limitations. Furthermore, the samples that challenge a judge model are often the ones most critical for adapting base models to specialized fine-tuning tasks~. The cost of human annotation is further exacerbated by privacy and security constraints that restrict fine-tuning service providers\u0027 access to an entire customer data corpus. In such cases, only subject matter experts (SMEs) within the customer organization have full visibility into the data, making it particularly difficult to optimize prompts effectively across the entire corpus, especially for hard-to-annotate samples.\n\nTo address these challenges, we propose Reinforcement Learning from Targeted Human Feedback (\\myname{}), a human-AI hybrid solution that combines coarse initial alignment using general-purpose LLMs with the progressive integration of strategically selected human annotations to achieve annotation quality comparable to fully human-supervised approaches. \\myname{} begins with an initial alignment stage, where a general-purpose LLM labels unlabeled data based on high-level instructions. While this approach effectively captures broader human alignment for easier data points, it often struggles with fine-grained nuances, leading to incorrect labeling. \\myname{} automatically identifies these hard-to-annotate data points and directs human effort exclusively toward them. This targeted approach enables \\myname{} to achieve the quality of fully human-annotated data while reducing the majority of human annotation effort.\n\nTo enable this efficient human-in-the-loop approach for achieving comprehensive human alignment, \\myname{} introduces the following key technical contributions:\n\nFirst, we develop a concept that leverages the reward distribution of a reward model over its training dataset to capture the relative arrangement of samples based on rewarded features. This property allows us to detect plausible inaccuracies in annotations across the dataset. Specifically, we train a reward model on the LLM-labeled dataset to identify clusters of hard-to-annotate samples that are highly likely to be either mislabeled or correctly labeled by the LLM.\n\nBuilding on this concept, we propose an innovative iterative reward model training technique to achieve oracle-level human alignment in the dataset. In each iteration, \\myname{} identifies and rectifies highly probable mislabeled data points using human annotations. Simultaneously, it detects clusters of samples that are very likely to be correctly labeled by the LLM and incorporates them with human-annotated data to construct a high-quality training set for the next iteration of reward model training. Throughout this process, \\myname{} preserves data richness and maximizes the efficiency of human annotation investment through carefully controlled hyperparameters.\n\nFinally, we implement and evaluate \\myname{} on two distinct preference datasets: HH-RLHF and TL;DR. Our results demonstrate that \\myname{} achieves accuracy comparable to a fully human-annotated dataset while requiring only 6\u20137% of the total human annotations. Furthermore, we conduct a comparative study by training models on downstream tasks using DPO. Remarkably, models trained with \\myname{} even outperform those trained on fully human-annotated datasets, highlighting the impact of \\myname{}\u0027s meticulous data curation in enhancing model performance.", "extraction_method": "dedicated_file", "length": 5149, "success": true, "tex_filename": "1-intro.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.3174404831524335, "validated": false}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.29825050543183573, "validated": false}, "RLHF": {"justification": "The paper\u0027s main contribution is RLTHF, a method that builds directly on RLHF by using human feedback to train a reward model and fine-tune LLMs via reinforcement learning, while selectively incorporating human annotations to address alignment challenges.", "llm_relevant": "yes", "similarity_score": 0.6653451676411026, "validated": true}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.39089928827378306, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2502.13417v2", "published": "2025-02-19T04:25:11Z", "scores": {"Diffusion_reasoning": 0.3174404831524335, "Distributed_training": 0.29825050543183573, "RLHF": 0.6653451676411026, "Weak_supervision": 0.39089928827378306}, "scores_data": {"clarity": 9.0, "novelty": 8.0, "overall_priority": 9.0, "potential_impact": 9.0, "relevance": 9.5}, "title": "RLTHF: Targeted Human Feedback for LLM Alignment", "updated": "2025-02-21T02:51:18Z", "url": "http://arxiv.org/abs/2502.13417v2"}, {"abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a key\ntechnique for aligning the output of large language models (LLMs) with human\npreferences. To learn the reward function, most existing RLHF algorithms use\nthe Bradley-Terry model, which relies on assumptions about human preferences\nthat may not reflect the complexity and variability of real-world judgments. In\nthis paper, we propose a robust algorithm to enhance the performance of\nexisting approaches under such reward model misspecifications. Theoretically,\nour algorithm reduces the variance of reward and policy estimators, leading to\nimproved regret bounds. Empirical evaluations on LLM benchmark datasets\ndemonstrate that the proposed algorithm consistently outperforms existing\nmethods, with 77-81% of responses being favored over baselines on the Anthropic\nHelpful and Harmless dataset.", "arxiv_id": "2504.03784v4", "arxiv_url": "http://arxiv.org/abs/2504.03784v4", "author_h_indices": {"authors_found_count": 5, "authors_with_h_index_count": 5, "average_h_index": 1.2, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 2, "individual_h_indices": [{"h_index": 1, "name": "Kai Ye", "profile_url": "https://www.semanticscholar.org/author/2354165813"}, {"h_index": 1, "name": "Hongyi Zhou", "profile_url": "https://www.semanticscholar.org/author/2354224430"}, {"h_index": 1, "name": "Jin Zhu", "profile_url": "https://www.semanticscholar.org/author/2354195275"}, {"h_index": 1, "name": "Francesco Quinzan", "profile_url": "https://www.semanticscholar.org/author/2354179582"}, {"h_index": 2, "name": "Chengchun Shi", "profile_url": "https://www.semanticscholar.org/author/2325201916"}], "notable_authors_count": 0, "paper_url": "https://www.semanticscholar.org/paper/66c16a4eb1457f447a44fb1ea1968f8841ad5a2d", "success": true, "total_authors": 5}, "authors": ["Kai Ye", "Hongyi Zhou", "Jin Zhu", "Francesco Quinzan", "Chengchun Shi"], "categories": ["stat.ML (Machine Learning)", "cs.AI (Artificial Intelligence)", "cs.LG (Machine Learning)"], "embedding_model": "openai-large", "highest_score": 0.6172275807009072, "highest_score_topic": "RLHF", "introduction": {"content": "Reinforcement learning from human feedback (RLHF) has recently revolutionized the fine-tuning of large language models (LLMs), achieving remarkable success in aligning model behavior with human preferences \\citep{christiano2017deep,bai2022training, glaese_2022_improvingalignmentdialogueagents, ouyang2022training,GPT4TR}. Traditional reinforcement learning (RL) algorithms rely on explicitly defined reward functions \\citep{sutton2018reinforcement}, but specifying such functions for LLMs is notoriously challenging due to the subtlety and variability of human values \\citep{Bertrand_2023_Elo, munos2024nash}. RLHF addresses this limitation by leveraging direct human feedback, such as pairwise comparisons or rankings \u2014 which are easier to elicit and more aligned with human intuition. This approach enables LLMs to produce responses that better reflect nuanced human preferences. RLHF algorithms for LLMs fine-tuning typically require to specify a human preference model. Among those available, the most widely adopted is the Bradley-Terry (BT) model \\citep{bradley1952rank}, preferred for its computational tractability and theoretical convenience.\n\nModels like BT, however, rely on the reward-based preference condition, which entails various unrealistic assumptions on human preferences. One of these assumptions is transitivity -- human preferences are logically ordered ($A\\succ B\\succ C$ implies $A\\succ C$). However, empirical evidence consistently demonstrates that human preferences are inherently intransitive \\citep{May_1954_IntransitivityUA, Tversky1969IntransitivityOP, Gardener_1970_Mathematicalgames}. Another unrealistic assumption is context-independence, i.e., preferences between two responses are based solely on the prompts and responses themselves, overlooking the dynamic nature of human-AI interactions \\citep{Michaud2020UnderstandingLR, Milano_2021_Ethical, Lindner_2022_HumansAN}. A third assumption is the prefect rationality of the users providing feedback, since humans often provide inconsistent and stochastic preferences \\citep{Agranov_2015_StochasticCA}. For these reasons, reward-based preference models like BT may be misspecified in practice. Under these misspecifications, existing RLHF algorithms may produce suboptimal policies. A possible solution to this problem is to use more general preference models (see Section ). However, more complex models could increase the computational cost and slow the policy learning \\citep{zhang2024general}.\n\nTo address these challenges, we propose a robust fine-tuning framework to improve the sample efficiency of existing reward-based RLHF algorithms under model misspecification. Unlike previous approaches that aim to improve the preference model \\cite[e.g.,][]{zhang2024general}, we focus on scenarios where this model is inherently misspecified. Our approach is based on the insight that the reference policy, which generates the responses to be labeled, is typically known or it can be well-specified in practice. We leverage this information to enhance the sample efficiency of the estimated reward and policy under model misspecification.\n\n\\begin{figure}[t]\n \\centering\n \\includegraphics[width=1.0\\linewidth]{VRPO_flowchart.png}\n \\caption{VRPO incorporates an auxiliary preference model to reduce the variance of the estimated primary model. Left: The classic one-stage and two-stage optimization schemes in RLHF. Both approaches require fitting a reward model, either explicitly or implicitly, which may lead to model misspecification. Right: In contrast, VRPO employs an auxiliary reward-free preference model to better capture human preferences. It works jointly with the primary model for variance reduction and policy improvement.}\n\n\\end{figure}\n\nOur contribution:\n\\begin{enumerate}[leftmargin=*]\n \\item We propose variance-reduced preference optimization (VRPO), a flexible pipeline applicable to a variety of existing RLHF algorithms to enhance their sample efficiency under human preference model misspecification (see Figure for an overview and Section for implementation details).\n \\item We rigorously establish the statistical properties of VRPO (see Table ). Specifically, we prove that when the preference model is misspecified, compared to baseline RLHF algorithms, our method reduces both the variance and mean squared error (MSE) of the estimated parameters (see Theorem ) as well as the suboptimality gap of the resulting policy (see Theorem ).\n \\item We conduct comprehensive numerical experiments on several LLM datasets to demonstrate the superior performance of our algorithm. In particular, on the Anthropic Helpful and Harmless (HH) dataset \\citep{bai2022training}, we observe that 77-81% of the responses generated by our estimated policy are preferred over those produced by baselines (see Table ).\n\\end{enumerate}\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{small}\n\\begin{sc}\n\\begin{tabular}{lccc}\n\\toprule\n\\multirow{2}{*}{Model Setting} \u0026 variance of \u0026 MSE of \u0026 suboptimality\n\n\u0026estimator \u0026estimator \u0026 gap\n \\midrule\nMisspecified \u0026 \\tikz{\\draw[darkgreen, -latex] (0,-0.6) -- (0,-0.9);} \u0026\\quad\\tikz{\\draw[darkgreen, -latex] (0,-0.6) -- (0,-0.9);}\u0026 \\quad\\tikz{\\draw[darkgreen, -latex] (0,-0.6) -- (0,-0.9);}\n\nCorrectly specified \u0026 \\parbox[c]{1em}{\\tikz{\\draw[blue, -latex] (0,0.2) -- (0.3,0.2);}} \u0026 \\quad \\parbox[c]{1em}{\\tikz{\\draw[blue, -latex] (0,0.2) -- (0.3,0.2);}} \u0026\\quad \\parbox[c]{1em}{\\tikz{\\draw[blue, -latex] (0,0.2) -- (0.3,0.2);}}\n\n\\bottomrule\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\end{center}\n\\caption{Variance, MSE, and suboptimality gap: proposed VRPO vs. existing RLHF algorithms, where\n\\protect\\parbox[c]{1em}{\\protect\\hspace*{2pt}\\protect\\tikz{\\protect\\draw[darkgreen, -latex] (0,-0.6) -- (0,-0.9);}}\\hspace*{-4pt} represents a decrease and\n\\protect\\parbox[c]{1em}{\\protect\\tikz{\\protect\\draw[blue, -latex] (0,0.2) -- (0.3,0.2);}} indicates no asymptotic difference.}\n\\vskip -0.1in\n\\end{table}", "extraction_method": "main_file_section", "length": 5917, "success": true, "tex_filename": "main.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.27984474583894514, "validated": false}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.2755317207989169, "validated": false}, "RLHF": {"justification": "The paper directly addresses RLHF by proposing a robust algorithm to improve reward modeling from human feedback, used for fine-tuning LLMs, aligning with the definition of training a reward model on human-ranked data and applying reinforcement learning.", "llm_relevant": "yes", "similarity_score": 0.6172275807009072, "validated": true}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.3675680745915911, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2504.03784v4", "published": "2025-04-03T16:16:35Z", "scores": {"Diffusion_reasoning": 0.27984474583894514, "Distributed_training": 0.2755317207989169, "RLHF": 0.6172275807009072, "Weak_supervision": 0.3675680745915911}, "scores_data": {"clarity": 8.0, "novelty": 8.5, "overall_priority": 9.0, "potential_impact": 9.0, "relevance": 9.5}, "title": "Robust Reinforcement Learning from Human Feedback for Large Language\n  Models Fine-Tuning", "updated": "2025-06-23T18:51:33Z", "url": "http://arxiv.org/abs/2504.03784v4"}, {"abstract": "Labeled datasets are essential for modern search engines, which increasingly\nrely on supervised learning methods like Learning to Rank and massive amounts\nof data to power deep learning models. However, creating these datasets is both\ntime-consuming and costly, leading to the common use of user click and activity\nlogs as proxies for relevance. In this paper, we present a weak supervision\napproach to infer the quality of query-document pairs and apply it within a\nLearning to Rank framework to enhance the precision of a large-scale search\nsystem.", "arxiv_id": "2503.07025v1", "arxiv_url": "http://arxiv.org/abs/2503.07025v1", "author_h_indices": {"authors_found_count": 1, "authors_with_h_index_count": 1, "average_h_index": 1.0, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 1, "individual_h_indices": [{"h_index": 1, "name": "Sriram Vasudevan", "profile_url": "https://www.semanticscholar.org/author/2305795476"}], "notable_authors_count": 0, "paper_url": "https://www.semanticscholar.org/paper/625731ae2ed8fc00c87832a4019acf1730283a57", "success": true, "total_authors": 1}, "authors": ["Sriram Vasudevan"], "categories": ["cs.IR (Information Retrieval)", "cs.AI (Artificial Intelligence)", "cs.LG (Machine Learning)"], "embedding_model": "openai-large", "highest_score": 0.5523099040625784, "highest_score_topic": "Weak_supervision", "introduction": {"content": "Introduction\n\nIndustrial search systems that leverage supervised learning and deep learning techniques require large volumes of high-quality labeled data to produce relevant results. One of the key challenges in developing these systems is the significant time and cost involved in manually labeling massive datasets. This process often requires training Subject Matter Experts (SMEs), providing comprehensive guidelines, and waiting several months to curate a meaningful volume of graded relevance labels. Compounding this challenge is the fact that such data can quickly become outdated, necessitating repeated annotation efforts.\n\nTo circumvent the costs of creating ``golden\u0027\u0027 datasets, search and recommendation systems frequently rely on user activity logs as implicit labels for user-query-document interactions. These logs treat user actions on previously displayed results as feedback on relevance. While this approach helps address data scarcity, it often causes search engines to optimize for engagement rather than true relevance. Although engagement and relevance are correlated, models trained solely on activity logs may exhibit the Matthew Effect , amplify clickbait, and over-rely on activity-based features. This correlation can further break down in cases where user interface signals are ambiguous. For instance, a ``dismiss\u0027\u0027 button might indicate disinterest, a temporary lack of relevance, or simply a desire to clear viewed results. As a result, engagement-optimized models can suffer from reduced precision and recall.\n\nTo address these issues, the industry has increasingly explored weak supervision, a set of methods for generating noisy yet informative training labels efficiently and at scale. Early approaches utilized curated data sources or aggregated crowdsourced labels . More recently, Snorkel introduced the idea of SMEs authoring multiple heuristics, or labeling functions (LFs), with varying accuracies and coverage, which are then aggregated into a single label per data point . Snorkel Drybell extended this concept by incorporating organizational knowledge to refine heuristics and improving scalability through sampling-free aggregation techniques. The rise of Large Language Models (LLMs) further enhances weak supervision, with LLMs now being used as powerful heuristics themselves .\n\nHowever, a limitation of existing aggregation approaches is their focus on achieving consensus among heuristics without explicitly optimizing for label accuracy, often due to the absence of ground truth data. However, a more common scenario in industrial settings is the availability of a small dataset of ground truth labels obtained through human annotation, albeit insufficient to train Deep Neural Networks (DNNs) at scale. This scenario presents an opportunity to combine organizational knowledge with a limited ``golden labeled dataset\u0027\u0027 to simplify heuristic aggregation, thereby scaling up weak supervision while minimizing noise.\n\nIn this paper, we describe a distributed, scalable weak supervision solution that we successfully deployed in production to significantly improve the precision of a large-scale job search system. Building upon Snorkel\u0027s programmatic approach, we propose a novel technique that leverages SME-authored heuristics, enriched with a seed set of ground truth labels, to generate high-quality training data at scale.", "extraction_method": "dedicated_file", "length": 3378, "success": true, "tex_filename": "intro.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.2812212598084995, "validated": false}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.334232002961116, "validated": false}, "RLHF": {"justification": "The paper focuses on weak supervision for generating labels in search systems using heuristics and user logs, with no mention of reinforcement learning, reward models, or human feedback for fine-tuning.", "llm_relevant": "no", "similarity_score": 0.40691000720171966, "validated": true}, "Weak_supervision": {"justification": "The paper\u0027s main contribution is a weak supervision approach that programmatically generates labels from noisy sources like user activity logs and heuristics to train models, aligning with the definition of weak supervision.", "llm_relevant": "yes", "similarity_score": 0.5523099040625784, "validated": true}}, "pdf_url": "http://arxiv.org/pdf/2503.07025v1", "published": "2025-03-10T08:06:30Z", "scores": {"Diffusion_reasoning": 0.2812212598084995, "Distributed_training": 0.334232002961116, "RLHF": 0.40691000720171966, "Weak_supervision": 0.5523099040625784}, "scores_data": {"clarity": 9.0, "novelty": 8.0, "overall_priority": 9.0, "potential_impact": 9.0, "relevance": 9.5}, "title": "Weak Supervision for Improved Precision in Search Systems", "updated": "2025-03-10T08:06:30Z", "url": "http://arxiv.org/abs/2503.07025v1"}, {"abstract": "Reinforcement Learning from Human Feedback (RLHF) is increasingly used to\nalign large language models (LLMs) with human preferences. However, the\neffectiveness of RLHF in addressing underlying biases remains unclear. This\nstudy investigates the relationship between RLHF and both covert and overt\nbiases in LLMs, particularly focusing on biases against African Americans. We\napplied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and\nevaluated the covert and overt biases of the resulting models using\nmatched-guise probing and explicit bias testing. We performed additional tests\nwith DPO on different base models and datasets; among several implications, we\nfound that SFT before RLHF calcifies model biases. Additionally, we extend the\ntools for measuring biases to multi-modal models. Through our experiments we\ncollect evidence that indicates that current alignment techniques are\ninadequate for nebulous tasks such as mitigating covert biases, highlighting\nthe need for capable datasets, data curating techniques, or alignment tools.", "arxiv_id": "2503.09025v1", "arxiv_url": "http://arxiv.org/abs/2503.09025v1", "author_h_indices": {"authors_found_count": 4, "authors_with_h_index_count": 4, "average_h_index": 1.8, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 3, "individual_h_indices": [{"h_index": 1, "name": "Logan Barnhart", "profile_url": "https://www.semanticscholar.org/author/2349646347"}, {"h_index": 3, "name": "Reza Akbarian Bafghi", "profile_url": "https://www.semanticscholar.org/author/2231396710"}, {"h_index": 1, "name": "Stephen Becker", "profile_url": "https://www.semanticscholar.org/author/2349623568"}, {"h_index": 2, "name": "Maziar Raissi", "profile_url": "https://www.semanticscholar.org/author/2324389999"}], "notable_authors_count": 0, "paper_url": "https://www.semanticscholar.org/paper/c9ab5fca0b7ec8c466daf0bbf1dd6c1f9b6b3a3d", "success": true, "total_authors": 4}, "authors": ["Logan Barnhart", "Reza Akbarian Bafghi", "Stephen Becker", "Maziar Raissi"], "categories": ["cs.CL (Computation and Language)"], "embedding_model": "openai-large", "highest_score": 0.5206788131361925, "highest_score_topic": "RLHF", "introduction": {"content": "Introduction\n\nIncreasingly, training state-of-the-art large language models (LLMs) includes reinforcement learning from human feedback (RLHF) to align language models to human preferences such as understanding user intent, harmlessness, helpfulness, etc. \\citep{bai_training_2022,dubey_llama_2024, openai_gpt-4_2024, claude_report}. The process of collecting a meaningful amount of human feedback data requires the labor of many individuals \\citep{bai_training_2022} who may not agree on the quality of responses with respect to something like harmlessness, raising the question: is RLHF optimizing for the objective we want?\n\nPrevious work by \\citet{hofmann_dialect_2024} inspected off-the-shelf language models to evaluate their overt and covert racial biases. Surprisingly, they found that off-the-shelf models trained with RLHF appeared to hold the strongest covert biases \\citep{hofmann_dialect_2024}, but our review of the existing literature did not reveal any studies inspecting the relationship between RLHF and model biases. If covert biases represent --- or at least act as a proxy for --- a truer state of a model\u0027s `moral values,\u0027 then RLHF may not be adequately aligning LLMs to human preferences for more nuanced objectives such as harmlessness.\n\nOur goal is thus to analyze the relationship between post-training and model biases to conclude whether or not RLHF effectively aligns models to abstract goals such as harmlessness. We focus on examining the covert biases towards African Americans by examining a model\u0027s attitude towards speakers of two different dialects: African American English (AAE) and Standard American English (SAE).\n\nSpecifically, we train our own LLMs using alignment techniques to reduce harmful behavior, rather than depending on pre-trained models. After training, we use the methods from \\citet{hofmann_dialect_2024} to detect and monitor both explicit and implicit biases that may still be present in our model. A subset of the post-training and bias evaluations are repeated on Mistral \\citep{jiang_mistral_2023} to see both a different baseline for LLM biases and if RLHF influences different models uniquely. We also study the effects of extended post-training and the influence of different datasets on alignment. Although Llama 3 8B and Mistral are the only models to undergo post-training we conduct additional bias evaluations on Llama 3 Instruct, Llama 3.1 and its instruct tuned version, and Llama 3.2 and its instruct tuned version.\n\n\\iffalse\n\\begin{itemize}\n \\item Llama 3 Instruct,\n \\item Llama 3.1 and its instruct tuned version,\n \\item and Llama 3.2 and its instruct tuned version.\n\\end{itemize}\n\\fi\nAll instruct versions have undergone extensive post-training \\citep{dubey_llama_2024} beyond that of our experiments, and comparing Llama 3 - 3.2 will allow us to see how the base LLM biases have changed as models become more capable.\n\nWe also curate a new preference dataset containing only AAE text to see if the abundance of SAE text in pretraining is responsible for the biases, and whether or not further post-training on this dataset will meaningfully reduce bias. Finally, we extend current techniques limited to just language models to multimodal models to gather reduced-variance measurements of the models overt biases; this extension is performed on Llama 3.2 Vision 11B. Measuring overt biases in LLMs depends on explicit racial group names, whose limited availability leads to high variance measurements, but with VLMs explicit racial information is instead encoded into images of people.\n\nOur initial experiments seem to indicate that models are conditioned with covert biases after pre-training, and the overall nature of these biases are not influenced in any meaningful ways by RLHF regardless of post-training technique, dataset, or base model. Examining large-scale-post-trained models such as Llama 3-Instruct leads one to believe that with current techniques, to meaningfully alter a model\u0027s biases you need to introduce new ones. We also find that supervised fine-tuning prior to RLHF appears to calcify the model biases and make them more resistant to change. Additionally, and tangential to future research, our preliminary experiments on measuring multi-modal covert biases seems to indicate a model\u0027s overt and covert biases can be polar opposites of one another. In whole, these findings indicate that existing alignment techniques, such as RLHF, are inadequate for addressing complex tasks like reducing harmfulness or mitigating bias. This points to a potential limitation in current alignment strategies when dealing with subtler and more nebulous issues like model biases.\n\n\\begin{figure*}\n \\centering\n \\includegraphics[width=\\textwidth]{images/favorabilities.png}\n \\vspace{-8mm}\n \\caption{Average favorability scores for the top 5 personality traits most associated with AAE/SAE (covert, left) and African-Americans/Caucasians (overt, right). Red dotted lines represent the average favorability scores for African Americans from the Princeton trilogy studies and Bergsieker et al \\citep{katz_racial_1933, gilbert_stereotype_1951, karlins_fading_1969, bergsieker_stereotyping_2012}. Note that all models have negative favorability for African-Americans in the overt setting.}\n\n \\vspace{-4mm}\n\\end{figure*}", "extraction_method": "dedicated_file", "length": 5294, "success": true, "tex_filename": "latex/01-intro.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.27542694862971906, "validated": false}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.26798587747591046, "validated": false}, "RLHF": {"justification": "The paper\u0027s main contribution focuses on evaluating the effectiveness of RLHF techniques (e.g., DPO, ORPO, RLOO) for aligning LLMs with human preferences, specifically in addressing biases, which directly matches the definition of RLHF.", "llm_relevant": "yes", "similarity_score": 0.5206788131361925, "validated": true}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.30947074175033784, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2503.09025v1", "published": "2025-03-12T03:24:44Z", "scores": {"Diffusion_reasoning": 0.27542694862971906, "Distributed_training": 0.26798587747591046, "RLHF": 0.5206788131361925, "Weak_supervision": 0.30947074175033784}, "scores_data": {"clarity": 8.0, "novelty": 8.5, "overall_priority": 9.0, "potential_impact": 9.0, "relevance": 9.5}, "title": "Aligning to What? Limits to RLHF Based Alignment", "updated": "2025-03-12T03:24:44Z", "url": "http://arxiv.org/abs/2503.09025v1"}, {"abstract": "Obtaining high-quality labeled datasets is often costly, requiring either\nextensive human annotation or expensive experiments. We propose a method that\nsupplements such \u0026quot;expert\u0026quot; labels with AI predictions from pre-trained models to\nconstruct labeled datasets more cost-effectively. Our approach results in\nprobably approximately correct labels: with high probability, the overall\nlabeling error is small. This solution enables rigorous yet efficient dataset\ncuration using modern AI models. We demonstrate the benefits of the methodology\nthrough text annotation with large language models, image labeling with\npre-trained vision models, and protein folding analysis with AlphaFold.", "arxiv_id": "2506.10908v1", "arxiv_url": "http://arxiv.org/abs/2506.10908v1", "author_h_indices": {"authors_found_count": 3, "authors_with_h_index_count": 3, "average_h_index": 6.7, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 15, "individual_h_indices": [{"h_index": 5, "name": "Emmanuel J. Candes", "profile_url": "https://www.semanticscholar.org/author/2283307289"}, {"h_index": 0, "name": "Andrew Ilyas", "profile_url": "https://www.semanticscholar.org/author/2366565634"}, {"h_index": 15, "name": "Tijana Zrnic", "profile_url": "https://www.semanticscholar.org/author/7830023"}], "notable_authors_count": 1, "paper_url": "https://www.semanticscholar.org/paper/1cb28c5ca1eb9ebc3d29c0cf59fe4ef3fdd48fc6", "success": true, "total_authors": 3}, "authors": ["Emmanuel J. Cand\u00e8s", "Andrew Ilyas", "Tijana Zrnic"], "categories": ["stat.ML (Machine Learning)", "cs.LG (Machine Learning)"], "embedding_model": "openai-large", "highest_score": 0.44260462693969777, "highest_score_topic": "Weak_supervision", "introduction": {"content": "A key ingredient in machine learning and statistical pipelines alike\nis the availability of large amounts of high-quality labeled data.\nBreakthroughs in computer vision stem from the collection of millions\nof labeled images \\citep{deng2009imagenet};\nsocial science research relies on extensively labeled\ndatasets to understand human behavior and opinions \\citep{salganik2017bit}.\nWhile acquiring unlabeled\ndata (e.g., raw images or texts from the internet) can be relatively inexpensive,\nacquiring high-quality labels is typically an endeavor that requires significant\ntime and effort from human experts.\n\nGiven the expense of collecting high-quality labels,\nan enticing prospect is to use increasingly powerful AI models\nto predict labels for datasets, bypassing the need for human experts entirely.\nIndeed, recent works have demonstrated AI models\u0027 ability to predict protein structures \\citep{jumper2021highly},\nto evaluate language model responses \\citep{zheng2023judging}, and even to simulate human experimental\nsubjects \\citep{santurkar2023whose}.\nThese advances highlight the potential for AI to streamline data annotation,\nand to produce high-quality labels at a fraction of the cost of\nhuman experts.\n\nThe problem with such an approach is that AI models are not\nalways correct, and in particular come with no guarantees on\nhow well they will label a given dataset.\nThis makes it untenable to use\nAI-predicted labels as a direct substitute for human labels, particularly in\nsettings where label quality is critical\u2014for instance, in high-stakes\napplications like medical diagnosis, or when the downstream task is to draw\nconclusions that inform policy decisions.\n\nMotivated by this state of affairs, in this paper we ask:\n\\begin{center}\n Can we leverage powerful AI models to label data, while still guaranteeing quality?\n\\end{center}\nWe answer this question in the affirmative, and provide a method---which we call probably approximately correct (PAC) labeling---that\nautomatically combines cheap, non-expert labels (whether AI predictions,\ncrowd-sourced labels, or simple heuristics) with expensive, expert labels\nto produce a labeled dataset with small error.\nPAC labeling yields guarantees similar in flavor to that of its namesake in\nprobably approximately correct (PAC) learning \\citep{valiant1984theory}: given\nuser-specified constants $\\epsilon, \\alpha \u003e 0$, our procedure results in a labeled dataset\nwith error at most $\\epsilon$, with\nprobability at least~$1-\\alpha$.\n\nContributions\n\nWe give a brief overview of our contributions, beginning with the problem setup. Given an unlabeled dataset $X_1,\\dots,X_n \\in \\mathcal X$, with unknown expert labels $Y_1,\\dots,Y_n$, our goal is to return a labeled dataset $(X_1,\\tilde Y_1),\\dots,(X_n,\\tilde Y_n)$, such that we incur only a small amount of labeling errors:\n\\begin{equation}\n\n\\frac{1}{n} \\sum_{i=1}^n \\ell(Y_i,\\tilde Y_i)\\leq\\epsilon, with probability 1-\\alpha.\n\\end{equation}\nHere, $\\alpha$ and $\\epsilon$ are user-chosen error parameters and $\\ell$ is a relevant error metric. For example, if we want categorical labels to be accurate, we can choose the 0-1 loss: $\\ell(Y_i, \\tilde Y_i) = \\mathbf{1}\\{Y_i \\neq \\tilde Y_i\\}$. The guarantee \\eqref{eq:labeling_guarantee} then requires that at most an $\\epsilon$-fraction of the dataset is mislabeled, with high probability. In regression problems, one might choose the squared loss, $\\ell(Y_i,\\tilde Y_i) = (Y_i - \\tilde Y_i)^2$. We call $\\tilde Y_i$ that satisfy the criterion \\eqref{eq:labeling_guarantee} probably approximately correct (PAC) labels.\nTo avoid making strong assumptions, we treat the data as fixed; probabilities are taken only over the labeling algorithm.\n\nTo produce the label $\\tilde Y_i$, we are allowed to query an expert for $Y_i$, which is costly, or instead use a cheap AI prediction $\\hat Y_i = f(X_i)$, where $f$ is an AI model. The prediction $\\hat Y_i$ can depend on any feature information available for point $i$, as well as any source of randomness internal to $f$. We will consider two settings: a basic setting with a single AI model $f$, and a more complex setting that assumes access to $k$ different models $f_1,\\dots,f_k$.\n\nOf course, we can trivially achieve \\eqref{eq:labeling_guarantee} by collecting expert labels for all $n$ data points. The goal is to achieve the criterion while minimizing the cost of the labeling. We will consider two ways of measuring the cost. The basic one is to simply count the number of collected expert labels; the AI-predicted labels are assumed to essentially come at no cost. The second way of measuring the cost takes into account the costs $c_1,\\dots,c_k$ of querying the $k$ models, as well as the cost of an expert label $c_{\\mathrm{expert}}$. When $c_{\\mathrm{expert}}$ is much larger than $c_1,\\dots,c_k$, the second setting reduces to the first.\n\nOur main contribution is a method for producing PAC labels which, as we will show through a series of examples with different data modalities and AI models, allow for significant saves in labeling cost. The key feature that enables a cost reduction is access to a good measure of model uncertainty about the label, which allows focusing the expert budget on instances where the model is most uncertain. We provide refinements of the method that additionally learn to calibrate the uncertainty scores to make the saves in cost even more pronounced.\n\nRelated work\n\nOur work most closely relates to the literature on efficient dataset labeling from possibly noisy labels . Since we do not place distributional assumptions on the data but instead consider it fixed, our work particularly relates to the labeling problem known as transductive learning .\nThe distinguishing feature of our work is that we leverage pre-trained AI models, such as off-the-shelf language or vision models, and make no complexity assumptions on the expert labeling mechanism. Moreover, our procedure ensures rigorous labeling error guarantees.\n\nOn the statistical methodology side, our proposal relates in spirit to prediction-powered inference~, where the goal is to improve the power of statistical inferences given a small amount of expert-labeled data, a large amount of unlabeled data, and a good pre-trained model. We do not focus on statistical inference per se; rather, we aim to construct an accurately labeled dataset that can be used for any downstream task. At a technical level,\nour procedure resembles the construction of risk-controlling prediction sets and performing risk-limiting audits . Like the former, our procedure bounds a monotone loss function by tuning a one-dimensional threshold, though not for the purpose of predictive inference. Similarly to the latter, our procedure aims to collect sufficient expert labels so as to meet a pre-specified quality guarantee. Like \\citet{shekhar2023risk}, we consider adaptive sampling weights for improved sample efficiency.\n\nThe idea behind our method is to collect expert\nlabels where the AI model is most uncertain; in that sense, our method relates\nto active learning and active inference . Notably, there is a line of work in active learning that\nconsiders costs . Here, our goal is neither fitting a predictive model nor\nstatistical inference, but producing high-quality labeled data.\n\nFinally, our multi-model setting relates to a series of works trading off between multiple labelers with varying qualities and strengths . Our focus is on pre-trained AI models as labelers.", "extraction_method": "main_file_section", "length": 7469, "success": true, "tex_filename": "main.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.2622515113895617, "validated": false}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.33360941427485424, "validated": false}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.36745753013438004, "validated": false}, "Weak_supervision": {"justification": "The paper\u0027s method uses AI predictions from pre-trained models as noisy, imprecise sources to generate labels, supplementing expert labels, which aligns with weak supervision\u0027s core idea of programmatically creating large quantities of labels without relying solely on hand-labeled data.", "llm_relevant": "yes", "similarity_score": 0.44260462693969777, "validated": true}}, "pdf_url": "http://arxiv.org/pdf/2506.10908v1", "published": "2025-06-12T17:16:26Z", "scores": {"Diffusion_reasoning": 0.2622515113895617, "Distributed_training": 0.33360941427485424, "RLHF": 0.36745753013438004, "Weak_supervision": 0.44260462693969777}, "scores_data": {"clarity": 9.0, "novelty": 8.5, "overall_priority": 9.0, "potential_impact": 9.0, "relevance": 9.0}, "title": "Probably Approximately Correct Labels", "updated": "2025-06-12T17:16:26Z", "url": "http://arxiv.org/abs/2506.10908v1"}, {"abstract": "Recently, diffusion models have garnered significant interest in the field of\ntext processing due to their many potential advantages compared to conventional\nautoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a\nnovel approach that integrates diffusion models with Chain-of-Thought, a\nwell-established technique for improving the reasoning ability of\nautoregressive language models. In contrast to autoregressive language models\nthat make decisions in a left-to-right, token-by-token manner, DoT allows\nreasoning steps to diffuse over time through a diffusion language model and\noffers greater flexibility in trading-off computation for reasoning\nperformance. Our experimental results demonstrate the effectiveness of DoT in\nmulti-digit multiplication, boolean logic, and grade school math problems, with\na small diffusion model outperforming a much larger autoregressive model in\nboth efficiency and accuracy. In addition to that, DoT showcases promising\nself-correction abilities and benefits from existing reasoning-enhancing\ntechniques like self-consistency decoding. Our findings contribute to the\nunderstanding and development of reasoning with diffusion language models.", "arxiv_id": "2402.07754v3", "arxiv_url": "http://arxiv.org/abs/2402.07754v3", "author_h_indices": {"authors_found_count": 10, "authors_with_h_index_count": 10, "average_h_index": 7.9, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 16, "individual_h_indices": [{"h_index": 16, "name": "Jiacheng Ye", "profile_url": "https://www.semanticscholar.org/author/65846898"}, {"h_index": 10, "name": "Shansan Gong", "profile_url": "https://www.semanticscholar.org/author/2165001433"}, {"h_index": 6, "name": "Liheng Chen", "profile_url": "https://www.semanticscholar.org/author/2283870581"}, {"h_index": 6, "name": "Lin Zheng", "profile_url": "https://www.semanticscholar.org/author/1633166807"}, {"h_index": 13, "name": "Jiahui Gao", "profile_url": "https://www.semanticscholar.org/author/144407296"}, {"h_index": 6, "name": "Han Shi", "profile_url": "https://www.semanticscholar.org/author/2285182555"}, {"h_index": 5, "name": "Chuan Wu", "profile_url": "https://www.semanticscholar.org/author/2284032895"}, {"h_index": 6, "name": "Zhenguo Li", "profile_url": "https://www.semanticscholar.org/author/2249755860"}, {"h_index": 4, "name": "Wei Bi", "profile_url": "https://www.semanticscholar.org/author/2283842274"}, {"h_index": 7, "name": "Lingpeng Kong", "profile_url": "https://www.semanticscholar.org/author/2260528279"}], "notable_authors_count": 8, "paper_url": "https://www.semanticscholar.org/paper/f54aa5a594d054e9564413ed4c30d18f2e747bc7", "success": true, "total_authors": 10}, "authors": ["Jiacheng Ye", "Shansan Gong", "Liheng Chen", "Lin Zheng", "Jiahui Gao", "Han Shi", "Chuan Wu", "Xin Jiang", "Zhenguo Li", "Wei Bi", "Lingpeng Kong"], "categories": ["cs.CL (Computation and Language)", "cs.AI (Artificial Intelligence)", "cs.LG (Machine Learning)"], "embedding_model": "openai-large", "highest_score": 0.6732769141943762, "highest_score_topic": "Diffusion_reasoning", "introduction": {"content": "Introduction\n\nLarge language models (LLMs) have had a profound impact on the entire field of artificial intelligence~\\citep{gpt4,touvron2023llama}, transforming our approach to addressing classical problems in natural language processing and machine learning. Among the most notable aspects of LLMs is their remarkable reasoning ability, which many researchers consider to be a representative emergent capability brought about by LLMs~\\citep{wei2022emergent}.\nChain-of-thought prompting (CoT)~\\citep{Wei2022ChainOT}), which generates a series of intermediate reasoning steps in autoregressive (AR) way, has emerged as a central technique to support complex reasoning processes in LLMs.\nDespite advancements, errors in intermediate CoT steps can lead to inaccurate answers ~\\citep{lanham2023measuring}, posing self-correction difficulties ~\\citep{huang2023large}, and concerns about CoT\u0027s inefficiency have been highlighted in recent studies~\\citep{deng2023implicit}.\n\nRecently, diffusion models have attracted interest in text processing~\\citep{li2022diffusion, Zheng2023ARD,zou2023survey} as a result of success in the vision domain and distinctive modeling strengths over autoregressive models~, offering potential benefits including global planning ability~\\citep{zhang2023planner, ye2023diffusion}, self correction~\\citep{hoogeboom2021argmax} and efficiency~\\citep{lou2023discrete}. As part of the research community effort, pre-trained diffusion language models such as Plaid~\\citep{gulrajani2023likelihood} and SEDD~\\citep{lou2023discrete} have shown significant progress in text generation capabilities.\nAlthough they have not yet attained the scale and capabilities of existing proprietary autoregressive LLMs like GPT-4~\\citep{gpt4}, these models have demonstrated performance on par with GPT2~\\citep{Brown2020LanguageMA} and the scaling law~\\citep{kaplan2020scaling} in diffusion language models have been highlighted in Plaid.\nAs a result, it becomes pertinent to explore the following question:\ncan diffusion language models also leverage the CoT-style technique to gain enhanced complex reasoning abilities?\n\n\\begin{wrapfigure}{r}{0.5\\textwidth}\n \\vspace{-5pt}\n \\begin{center}\n \\includegraphics[width=0.9\\linewidth]{fig/motivation.pdf}\n \\end{center}\n \\caption{Illustration of reasoning approaches. (a) Answer-only and (b) CoT generate left-to-right tokens by prompting autoregressive language model. (c) Implicit CoT replaces horizontal reasoning (CoT) with vertical reasoning from shallow layer to deep layer~\\citep{deng2023implicit}. (d) DoT generates reasoning path along with the diffusion timesteps.}\n\n \\vspace{-10pt}\n\\end{wrapfigure}\n\nThis work presents a preliminary study on this question. We propose Diffusion of Thought (DoT), an inherent chain-of-thought method tailored for diffusion models. In essence, DoT progressively updates a sequence of latent variables representing thoughts in the hidden space, allowing reasoning steps to diffuse over time in parallel. We also introduce a multi-pass variant of DoT which focuses on generating one thought at a time to compensate for causal bias.\nTo condition on complex queries, instead of using gradient-based classifier guidance~\\citep{li2022diffusion,gulrajani2023likelihood}, DoT trains and samples from the denoising model using the classifier-free guidance as in~\\citet{gong2022diffuseq}, to provide more reliable controlling signals on exact tokens.\n\nFurthermore, to improve the self-correcting capability of the diffusion model, DoT integrates training-time sampling algorithms to learn to recover from errors originating from prior or current reasoning steps. This feature offers a fresh angle on the issue of error accumulation~\\citep{lanham2023measuring,huang2023large} inherent in autoregressive models.\nFinally, we adapt a conditional ODE Solver~\\citep{lu2022dpm++} for DoT during inference time to accelerate the inference of continuous diffusion models. We show DoT enjoys flexibility in trading off computation (reasoning time) and performance as more complex problems may necessitate increased computation in reasoning~\\citep{banino2021pondernet,Wei2022ChainOT}.\n\nFrom a methodological standpoint, DoT shares similarities with the recently proposed Implicit CoT approach~\\citep{deng2023implicit}, where the latter learns thoughts in hidden states across transformer layers to improve the time efficiency of autoregressive CoT generation.\nA schematic illustration of CoT, Implicit CoT, and DoT can be found in Figure~.\n\nThe main contributions of our paper are threefold:\n\\begin{enumerate}\n \\item We first introduce the reasoning technique for diffusion models (DoT), and showcase its advantages in simple reasoning tasks (digit multiplication and boolean logic) when compared to autoregressive CoT and Implicit CoT. DoT achieves up to $27\\times$ speed-up without performance drop (\\S).\n \\item We further adapt DoT to continuous and discrete diffusion base models, and introduce two training-time sampling algorithms to improve its self-correction ability. DoT exhibits superior performance compared to GPT2 with CoT on grade school math problems, enabling a small diffusion model to outperform a 4.6x larger autoregressive model, showing the potential of text diffusion models for complex reasoning (\\S).\n \\item Our analysis demonstrates the flexibility of DoT in the trade-off between reasoning time and performance (\\S), and showcases DoT\u0027s self-correction capability (\\S). We also find that self-consistency decoding can further improve DoT and its multi-pass variant (\\S).\n\\end{enumerate}\nAlthough it is challenging for current pre-trained diffusion language models to directly compete with LLMs that are hundreds of times larger in parameter size, our study emphasizes the possibility of their complex reasoning abilities and highlights the substantial potential in developing LLMs that go beyond the autoregressive paradigm.\nWe release all the codes at \\href{https://github.com/HKUNLP/diffusion-of-thoughts}{https://github.com/HKUNLP/diffusion-of-thoughts}.", "extraction_method": "dedicated_file", "length": 6048, "success": true, "tex_filename": "01-intro.tex"}, "llm_validation": {"Diffusion_reasoning": {"justification": "The paper introduces Diffusion-of-Thought (DoT), which adapts the iterative refinement process of diffusion models to handle Chain-of-Thought reasoning for complex logical tasks, such as multi-digit multiplication and boolean logic, by holistically correcting and improving the reasoning path over multiple steps.", "llm_relevant": "yes", "similarity_score": 0.6732769141943762, "validated": true}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.29311577355707724, "validated": false}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.26197856791327717, "validated": false}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.2471445123896864, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2402.07754v3", "published": "2024-02-12T16:23:28Z", "scores": {"Diffusion_reasoning": 0.6732769141943762, "Distributed_training": 0.29311577355707724, "RLHF": 0.26197856791327717, "Weak_supervision": 0.2471445123896864}, "scores_data": {"clarity": 8.0, "novelty": 8.5, "overall_priority": 8.8, "potential_impact": 8.5, "relevance": 9.0}, "title": "Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language\n  Models", "updated": "2024-12-05T06:49:06Z", "url": "http://arxiv.org/abs/2402.07754v3"}, {"abstract": "We introduce the Diffusion Chain of Lateral Thought (DCoLT), a reasoning\nframework for diffusion language models. DCoLT treats each intermediate step in\nthe reverse diffusion process as a latent \u0026quot;thinking\u0026quot; action and optimizes the\nentire reasoning trajectory to maximize the reward on the correctness of the\nfinal answer with outcome-based Reinforcement Learning (RL). Unlike traditional\nChain-of-Thought (CoT) methods that follow a causal, linear thinking process,\nDCoLT allows bidirectional, non-linear reasoning with no strict rule on\ngrammatical correctness amid its intermediate steps of thought. We implement\nDCoLT on two representative Diffusion Language Models (DLMs). First, we choose\nSEDD as a representative continuous-time discrete diffusion model, where its\nconcrete score derives a probabilistic policy to maximize the RL reward over\nthe entire sequence of intermediate diffusion steps. We further consider the\ndiscrete-time masked diffusion language model -- LLaDA, and find that the order\nto predict and unmask tokens plays an essential role to optimize its RL action\nresulting from the ranking-based Unmasking Policy Module (UPM) defined by the\nPlackett-Luce model. Experiments on both math and code generation tasks show\nthat using only public data and 16 H800 GPUs, DCoLT-reinforced DLMs outperform\nother DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA\nboosts its reasoning accuracy by +9.8%, +5.7%, +11.4%, +19.5% on GSM8K, MATH,\nMBPP, and HumanEval.", "arxiv_id": "2505.10446v2", "arxiv_url": "http://arxiv.org/abs/2505.10446v2", "author_h_indices": {"authors_found_count": 5, "authors_with_h_index_count": 5, "average_h_index": 2.4, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 4, "individual_h_indices": [{"h_index": 3, "name": "Zemin Huang", "profile_url": "https://www.semanticscholar.org/author/2327176332"}, {"h_index": 2, "name": "Zhiyang Chen", "profile_url": "https://www.semanticscholar.org/author/2315290492"}, {"h_index": 1, "name": "Zijun Wang", "profile_url": "https://www.semanticscholar.org/author/2361504857"}, {"h_index": 2, "name": "Tiancheng Li", "profile_url": "https://www.semanticscholar.org/author/2333454859"}, {"h_index": 4, "name": "Guo-Jun Qi", "profile_url": "https://www.semanticscholar.org/author/2327045558"}], "notable_authors_count": 0, "paper_url": "https://www.semanticscholar.org/paper/c33a21e4215a5cd690ca8e2bb0d770b45adf5c89", "success": true, "total_authors": 5}, "authors": ["Zemin Huang", "Zhiyang Chen", "Zijun Wang", "Tiancheng Li", "Guo-Jun Qi"], "categories": ["cs.CL (Computation and Language)"], "embedding_model": "openai-large", "highest_score": 0.6489167870394064, "highest_score_topic": "Diffusion_reasoning", "introduction": {"content": "Introduction\n\nTo enable complex reasoning, most large language models (LLMs) learn to decompose problems into simpler sub-steps and generate intermediate reasoning in natural language. Chain-of-Thought (CoT) first reveals that step-by-step reasoning facilitates language models, as the outputs from previous steps could be rationales for more accurate next step prediction.\nBased on that, OpenAI\u0027s PRM supervises these intermediate reasoning steps with progressive rewards, to ensure the correctness of each single step.\nMore recently, DeepSeek-R1 eliminates the need of the reward model and verifies only the correctness of the final answer, relaxing the constraints on the reasoning process. However, due to the causal nature of attention mechanisms, auto-regressive models are still forced to reason in a single, sequential direction.\n\nHowever, when developing ideas, human cognition does not always proceed through strictly sequential steps. At the beginning of thinking, human does not require an intact linguistic structure. Concepts, words, or ideas emerge spontaneously and independently first, and are gradually refined and organized over time to follow grammar rules.\n\nThis non-linear and creative mode of reasoning, known as lateral thinking , contrasts with the structured, step-by-step approach of vertical thinking.\n\\begin{figure}[t]\n\n \\hspace*{-0.6cm}\n \\includegraphics[width=1.04\\linewidth, trim=0 1 0 1, clip]{figures/DCoLT.pdf}\n \\vspace{-1.2em}\n \\caption{Comparison between CoT and DCoLT. (a) A typical CoT performs vertical thinking by following an auto-regressive convention that generates responses token by token from left to right in a linear way. (b) DCoLT performs lateral thinking that generates the responses in a non-linear way without following the auto-regressive order; moreover, at each step, it can generate multiple tokens at chosen positions. We focus on the lateral thinking in this paper by reinforcing the chain of such lateral thought {\\em as an entirety} in Diffusion Language Models (DLMs).\n }\n\n \\vspace{-1em}\n\\end{figure}\n\nContrary to auto-regressive models, Diffusion Language Models (DLMs) have also been adopted for text generation. The intermediate steps of the reverse diffusion process are naturally well-suited to emulate lateral thinking. Unlike auto-regressive models, diffusion models generate all tokens in parallel from a prior distribution. Each token can attend freely to all others under a non-causal mask in self attention, and intermediate reasoning steps are not required to conform to grammatical rules during multi-step generation, thus leading to more divergent thinking.\nIn this paper, we propose the Diffusion Chain of Lateral Thought (DCoLT) to reinforce the lateral reasoning in diffusion language models, as shown in Fig.~. Rather than providing explicit supervision for the thinking process, we employ outcome-based reinforcement learning, offering a rule-based reward that evaluates only the correctness of the final responses.\nThis reward encourages the model to explore diverse, creative, and non-linear thought trajectories that ultimately lead to correct answers.\n\nWe study two paradigms of diffusion language models to reinforce the DCoLT, continuous-time diffusion language models and discrete-time diffusion language models . For the continuous-time paradigm, we consider SEDD as a representative DLM. SEDD predicts the concrete score, allowing for a closed-form expression of the predicted diffused distribution of generated tokens at each step. This distribution can be viewed as a probabilistic policy for sampling tokens, which can be trained to optimize the reward on the final answers by reinforcing DCoLT.\n\nFor the discrete-time paradigm, we consider a masked DLM -- LLaDA .\nBesides the output distributions over discrete tokens, we note that the unmasking order plays an important role to decide which tokens ought to be kept to form the current step of lateral thought, and thus should be part of the learnable reasoning process.\nTo this end, we introduce a Plackett\u2013Luce model to define a ranking-based unmasking policy, where each masked token is assigned a predicted ranking score, and the unmasking policy selects the top-$K$ ranked tokens to retain in the output sequence at each diffusion step. The unmasking policy is trained together with the token generation policy to optimize the reward on the final answers.\n\nWe conduct experiments on both Math and code generation tasks to demonstrate the efficacy of DCoLT. After training the DCoLT on the SEDD 400M model, it achieves 96.2% and 57.0% in accuracy on Sudoku 4$\\times$4 and GSM8K-Aug tasks. On the LLaDA 8B model, DCoLT achieves the state-of-the-art performance among existing DLMs trained with SFT or RL or even both. Using only public data and 16 H800 GPUs, the DCoLT-reinforced LLaDA model achieves 88.1% on GSM8K , 44.6% on MATH , 51.6% on MBPP and 59.1% on HumanEval in the challenging zero-shot setting. Even compared with auto-regressive models that are trained with significantly more proprietary data and fully annotated CoT reasoning processes, it still demonstrates competitive performances.\n\n% \\end{itemize}", "extraction_method": "dedicated_file", "length": 5166, "success": true, "tex_filename": "sections/1_introduction.tex"}, "llm_validation": {"Diffusion_reasoning": {"justification": "The paper proposes DCoLT, which adapts the iterative refinement process of diffusion models for complex logical tasks like math and code generation, treating the entire reasoning trajectory as a single entity optimized via reinforcement learning for multi-step reasoning.", "llm_relevant": "yes", "similarity_score": 0.6489167870394064, "validated": true}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.3053145285105202, "validated": false}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.3896203272617523, "validated": false}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.3082824208357102, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2505.10446v2", "published": "2025-05-15T16:06:32Z", "scores": {"Diffusion_reasoning": 0.6489167870394064, "Distributed_training": 0.3053145285105202, "RLHF": 0.3896203272617523, "Weak_supervision": 0.3082824208357102}, "scores_data": {"clarity": 8.0, "novelty": 8.5, "overall_priority": 8.8, "potential_impact": 8.5, "relevance": 9.0}, "title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion\n  Language Models", "updated": "2025-05-21T01:44:47Z", "url": "http://arxiv.org/abs/2505.10446v2"}, {"abstract": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO, the first integration of policy gradient methods to masked dLLMs.\nThrough empirical studies, we investigate the performance of different\npost-training recipes on multiple mathematical and planning benchmarks. We find\nthat d1 yields the best performance and significantly improves performance of a\nstate-of-the-art dLLM. Our code is released at\nhttps://dllm-reasoning.github.io/.", "arxiv_id": "2504.12216v2", "arxiv_url": "http://arxiv.org/abs/2504.12216v2", "author_h_indices": {"authors_found_count": 4, "authors_with_h_index_count": 4, "average_h_index": 5.0, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 10, "individual_h_indices": [{"h_index": 6, "name": "Siyan Zhao", "profile_url": "https://www.semanticscholar.org/author/2260172378"}, {"h_index": 1, "name": "Devaansh Gupta", "profile_url": "https://www.semanticscholar.org/author/2357079322"}, {"h_index": 10, "name": "Qinqing Zheng", "profile_url": "https://www.semanticscholar.org/author/2166847"}, {"h_index": 3, "name": "Aditya Grover", "profile_url": "https://www.semanticscholar.org/author/2267723293"}], "notable_authors_count": 2, "paper_url": "https://www.semanticscholar.org/paper/8e3b1f5d8b6c165f64137cc1f7dea89cf6f622bd", "success": true, "total_authors": 4}, "authors": ["Siyan Zhao", "Devaansh Gupta", "Qinqing Zheng", "Aditya Grover"], "categories": ["cs.CL (Computation and Language)", "cs.LG (Machine Learning)"], "embedding_model": "openai-large", "highest_score": 0.5379839114946756, "highest_score_topic": "Diffusion_reasoning", "introduction": {"content": "Introduction\n\\begin{figure}[h]\n \\centering\n\\includegraphics[width=\\linewidth]{figs/separate_axes_chart.pdf}\n \\caption{Across four math and planning tasks, d1-LLaDA, which undergoes SFT followed by our proposed \\grpo{}, consistently outperforms the base LLaDA-8B-Instruct model. We report results using the best performing generation sequence length for each task and model, with complete sequence length results shown in \\cref{tab:performance_results}.}\n\n\\end{figure}\n\nRecent advances in large language models (LLMs) have demonstrated remarkable capabilities across diverse applications spanning chatbots, coding, summarization, and translation~\\citep{achiam2023gpt,dubey2024llama3herdmodels}. While these models typically scale through next-token prediction on vast corpora via computationally intensive pretraining, the finite availability of high-quality training data poses a fundamental scaling challenge. Reinforcement learning (RL) methods have emerged as a promising post-training method, enabling models to generate and explore with reward signals rather than relying solely on static datasets. This approach has yielded significant improvements on reasoning tasks in recent models, such as DeepSeek-R1~\\citep{guo2025deepseek} and Kimi K1.5~\\citep{team2025kimi}, demonstrating that applying RL directly to base models can achieve performance comparable to OpenAI\u0027s o1 model~\\citep{o1}. However, these advances in RL-based post-training have primarily been limited to autoregressive LLMs that operate through left-to-right, sequential inference.\n\nIn a parallel line of work, discrete diffusion large language models (dLLMs) \\citep{nie2025largelanguagediffusionmodels, gong2025scaling, nie2024scaling, dream2025} have emerged as promising non-autoregressive alternatives for language modeling. Unlike AR models that generate text token-by-token in a causal manner, dLLMs generate text through an iterative denoising process, refining sequences over multiple steps while leveraging both past and future context via bidirectional attention. Among them, open masked dLLMs such as LLaDA \\citep{nie2025largelanguagediffusionmodels} have demonstrated performance comparable to similarly sized AR models, and closed-source dLLMs such as Mercury~\\citep{inception2025mercury} further demonstrate excellent inference efficiency. However, leading open-source dLLMs have not undergone RL post-training, leaving this promising direction largely unexplored. This paradigm shift raises important questions about how RL post-training might be effectively realized in a non-autoregressive context.\n\nAdapting RL algorithms to masked dLLMs poses unique challenges because existing successful approaches for AR models, such as PPO~\\citep{schulman2017proximal} and GRPO~\\citep{shao2024deepseekmath}, rely on estimating and optimizing policy distributions through computing log-probabilities of generated sequences, which cannot be directly applied to dLLMs. While this computation is straightforward in AR models through sequential factorization, dLLMs lack this natural decomposition due to their iterative, non-sequential generation process.\n\nTo bridge this gap, we propose d1, a two-stage post-training framework for enhancing reasoning in masked dLLMs. In the first stage, the model undergoes supervised finetuning (SFT) on high-quality reasoning traces. In the RL stage, we introduce \\grpo{}, a novel policy gradient method for masked dLLMs that builds upon GRPO with our proposed efficient one-step estimation of log-probabilities. To the best of our knowledge, this represents the first application of policy gradient RL to masked dLLMs. Our estimator leverages random prompt masking, which acts a form of regularization for policy optimization, allowing us to scale the number of gradient updates per batch and reduces the number of online generations required by RL training. This substantially reduces the compute time.\n\nEmpirically, we instantiate d1 using LLaDA-8B-Instruct as our base model. We compare d1-LLaDA\u0027s performance with the base LLaDA model, as well as with LLaDA variants trained using SFT-only and \\grpo-only approaches. Our experiments demonstrate that d1 consistently outperforms the base model across four reasoning tasks in math and planning, as shown in \\cref{fig:pull}, with nearly doubled performance on planning tasks. Furthermore, d1 surpasses both the SFT-only and \\grpo-only methods. Additionally, we complement our primary findings with thorough ablation studies on algorithm design, qualitative analysis, and extensions of \\grpo{} to coding tasks, where we also observe consistent improvements.", "extraction_method": "dedicated_file", "length": 4619, "success": true, "tex_filename": "sections/intro.tex"}, "llm_validation": {"Diffusion_reasoning": {"justification": "The paper adapts the iterative denoising process of diffusion models for multi-step reasoning tasks, such as math and planning, by treating reasoning paths holistically through dLLMs.", "llm_relevant": "yes", "similarity_score": 0.5379839114946756, "validated": true}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.3324082946679241, "validated": false}, "RLHF": {"justification": "The paper focuses on using RL (specifically diffu-GRPO) to enhance reasoning in diffusion LLMs, but it does not mention training a reward model on human-ranked data or incorporating human feedback, relying instead on SFT and benchmarks.", "llm_relevant": "no", "similarity_score": 0.41517578303337366, "validated": true}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.3114651200459307, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2504.12216v2", "published": "2025-04-16T16:08:45Z", "scores": {"Diffusion_reasoning": 0.5379839114946756, "Distributed_training": 0.3324082946679241, "RLHF": 0.41517578303337366, "Weak_supervision": 0.3114651200459307}, "scores_data": {"clarity": 8.0, "novelty": 8.5, "overall_priority": 8.8, "potential_impact": 8.5, "relevance": 9.0}, "title": "d1: Scaling Reasoning in Diffusion Large Language Models via\n  Reinforcement Learning", "updated": "2025-06-03T17:02:25Z", "url": "http://arxiv.org/abs/2504.12216v2"}, {"abstract": "Weak supervision (WS) is a popular approach for label-efficient learning,\nleveraging diverse sources of noisy but inexpensive weak labels to\nautomatically annotate training data. Despite its wide usage, WS and its\npractical value are challenging to benchmark due to the many knobs in its\nsetup, including: data sources, labeling functions (LFs), aggregation\ntechniques (called label models), and end model pipelines. Existing evaluation\nsuites tend to be limited, focusing on particular components or specialized use\ncases. Moreover, they often involve simplistic benchmark tasks or de-facto LF\nsets that are suboptimally written, producing insights that may not generalize\nto real-world settings. We address these limitations by introducing a new\nbenchmark, BOXWRENCH, designed to more accurately reflect real-world usages of\nWS. This benchmark features tasks with (1) higher class cardinality and\nimbalance, (2) notable domain expertise requirements, and (3) opportunities to\nre-use LFs across parallel multilingual corpora. For all tasks, LFs are written\nusing a careful procedure aimed at mimicking real-world settings. In contrast\nto existing WS benchmarks, we show that supervised learning requires\nsubstantial amounts (1000+) of labeled examples to match WS in many settings.", "arxiv_id": "2501.07727v2", "arxiv_url": "http://arxiv.org/abs/2501.07727v2", "author_h_indices": {"authors_found_count": 7, "authors_with_h_index_count": 7, "average_h_index": 3.1, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 17, "individual_h_indices": [{"h_index": 0, "name": "Tianyi Zhang", "profile_url": "https://www.semanticscholar.org/author/2340048194"}, {"h_index": 0, "name": "Linrong Cai", "profile_url": "https://www.semanticscholar.org/author/2340439703"}, {"h_index": 0, "name": "Jeffrey Li", "profile_url": "https://www.semanticscholar.org/author/2340172144"}, {"h_index": 2, "name": "Nicholas Roberts", "profile_url": "https://www.semanticscholar.org/author/2340014150"}, {"h_index": 17, "name": "Neel Guha", "profile_url": "https://www.semanticscholar.org/author/2820009"}, {"h_index": 0, "name": "Jinoh Lee", "profile_url": "https://www.semanticscholar.org/author/2340362174"}, {"h_index": 3, "name": "Frederic Sala", "profile_url": "https://www.semanticscholar.org/author/2269049908"}], "notable_authors_count": 1, "paper_url": "https://www.semanticscholar.org/paper/6b690b63eb033c8db82577647bab9f743e922736", "success": true, "total_authors": 7}, "authors": ["Tianyi Zhang", "Linrong Cai", "Jeffrey Li", "Nicholas Roberts", "Neel Guha", "Jinoh Lee", "Frederic Sala"], "categories": ["cs.LG (Machine Learning)"], "embedding_model": "openai-large", "highest_score": 0.5479014739194858, "highest_score_topic": "Weak_supervision", "introduction": {"content": "Introduction\n\nWeak supervision (WS) aims to address the labeled data bottleneck for supervised machine learning.\n\nIt uses multiple weak but inexpensive sources of signal and combines them into high-quality pseudolabels that can be used for training downstream models \\citep{dataprogramming, Ratner2017SnorkelRT, shin2022universalizing}.\n\nThese weak sources can be diverse, including but not limited to: heuristic rules encoded into small programs, queries to knowledge bases, and pretrained models.\n\nFrameworks implementing WS are hugely popular and are widely applied in industry~ and academic settings~\\citep{aortic_fries, ukb}.\n\nWS frameworks typically have a simple three-stage approach.\n\nFirst, they formalize weak sources into labeling functions (LFs).\n\nIn contrast to manual labeling, these can be automatically applied to an entire unlabeled dataset.\n\nNext, since LFs are inherently noisy and may conflict with one another, a label model (LM) is used to estimate the quality of each source (typically without accessing ground truth labels) and then to aggregate their outputs into high-quality pseudolabels.\n\nFinally, these pseudolabels can be used to train a downstream model.\n\nA vast literature studies variations on this basic recipe, with diverse approaches to crafting LFs, creating LMs, and noise-aware training of end-models .\n\nFor practitioners, a key question is when is WS useful?\n\nWhile it is natural to produce benchmarks that answer this, surprisingly, there has been relatively little work doing so.\n\nOne reason for this may be the overall complexity of WS pipelines.\n\nThe performance of a WS system varies with (1) the underlying task and data, (2) the LFs, (3) the choice of LM, and (4) the choice of end model and training procedure.\n\nSeveral benchmarks predominantly focus on only one of these.\n\nFor example, WRENCH \\citep{zhang2021wrench} focuses primarily on evaluating (3), the LM, while AutoWS-Bench-101 \\citep{roberts2023autowsbench101} focuses on (2), the LFs, and specifically, techniques for automatically generating model-based LFs.\n\nRecently, \\citet{zhu2023weaker} tackle the goal of quantifying the value of WS.\nThey argue that the benefits of WS are often overestimated by showing that fine-tuning on only 50 ground-truth labels can achieve comparable---or better---results than certain WS approaches for many benchmark datasets.\n\nThey suggest that WS may not be broadly useful, as obtaining 50 ``clean\u0027\u0027 labels is rarely prohibitive, and data at this scale (or larger) may still be needed for tuning or evaluation even when using WS.\n\nIn this work, we show that these findings result from the simplicity of existing datasets rather than the inherent weakness of WS.\n\nIn particular, we identify key issues with the current WS benchmarks that led to this result and show that WS may be stronger than is thought in more realistic settings:\n\\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt]\n\n \\item Benchmark datasets usually have too few classes, are balanced, or aren\u0027t specialized enough to be representative of real-world datasets.\n\n \\item WS depends on the quality of LFs, and LFs from current benchmarks can be improved.\n\n \\item Previous benchmarks do not capture the adaptability of LFs across task specification, a key practical advantage of WS deployments compared to manual labeling.\n\n\\end{enumerate}\n\nWe introduce a new benchmark, \\benchname, that addresses these three challenges. It enables us to quantify the practical advantages of WS in a wide range of settings. Our findings indicate that even simple WS approaches often provide substantial value.\n\nWe address the issues we identified by:\n\n\\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]\n \\item Proposing new WS benchmarks based upon tasks that involve high-cardinality label spaces, imbalanced classes, and/or require specific domain knowledge.\n \\item Showing that by adhering to careful LFs design practices, we can write effective LFs for these tasks that can even improve upon existing benchmark LFs.\n \\item\n Using the MASSIVE dataset \\citep{fitzgerald2022massive}, we study simple but effective strategies for adapting existing LFs written for English data to parallel versions of the task in other languages.\n\\end{itemize}\n\nOur benchmark consists of five text-classification WS tasks that showcase the power of WS in a variety of challenging real-world scenarios.\n\nFor two of our tasks, we produce new LFs, while for one, we improve the existing LFs from WRENCH~\\citep{zhang2021wrench}.\n\nThe design of these LFs follows a rigorous procedure that we release as part of our benchmark, acting as guidance for LF design and for WS benchmarking overall.\n\nWe publicly release the code and other assets for our study at \\url{https://github.com/jeffreywpli/stronger-than-you-think}.", "extraction_method": "dedicated_file", "length": 4788, "success": true, "tex_filename": "sections/introduction.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.2466283809496461, "validated": false}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.28670305323418926, "validated": false}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.3362707570938119, "validated": false}, "Weak_supervision": {"justification": "The paper\u0027s main contribution is to benchmark and evaluate weak supervision techniques, including labeling functions and label models for generating noisy labels, directly aligning with the definition of weak supervision.", "llm_relevant": "yes", "similarity_score": 0.5479014739194858, "validated": true}}, "pdf_url": "http://arxiv.org/pdf/2501.07727v2", "published": "2025-01-13T22:29:31Z", "scores": {"Diffusion_reasoning": 0.2466283809496461, "Distributed_training": 0.28670305323418926, "RLHF": 0.3362707570938119, "Weak_supervision": 0.5479014739194858}, "scores_data": {"clarity": 8.0, "novelty": 8.5, "overall_priority": 8.5, "potential_impact": 8.5, "relevance": 9.0}, "title": "Stronger Than You Think: Benchmarking Weak Supervision on Realistic\n  Tasks", "updated": "2025-01-30T06:21:12Z", "url": "http://arxiv.org/abs/2501.07727v2"}, {"abstract": "Weak supervision is a popular framework for overcoming the labeled data\nbottleneck: the need to obtain labels for training data. In weak supervision,\nmultiple noisy-but-cheap sources are used to provide guesses of the label and\nare aggregated to produce high-quality pseudolabels. These sources are often\nexpressed as small programs written by domain experts -- and so are expensive\nto obtain. Instead, we argue for using code-generation models to act as coding\nassistants for crafting weak supervision sources. We study prompting strategies\nto maximize the quality of the generated sources, settling on a multi-tier\nstrategy that incorporates multiple types of information. We explore how to\nbest combine hand-written and generated sources. Using these insights, we\nintroduce ScriptoriumWS, a weak supervision system that, when compared to\nhand-crafted sources, maintains accuracy and greatly improves coverage.", "arxiv_id": "2502.12366v1", "arxiv_url": "http://arxiv.org/abs/2502.12366v1", "author_h_indices": {"authors_found_count": 6, "authors_with_h_index_count": 6, "average_h_index": 3.7, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 8, "individual_h_indices": [{"h_index": 5, "name": "Tzu-Heng Huang", "profile_url": "https://www.semanticscholar.org/author/2116822245"}, {"h_index": 2, "name": "Catherine Cao", "profile_url": "https://www.semanticscholar.org/author/2312271010"}, {"h_index": 2, "name": "Spencer Schoenberg", "profile_url": "https://www.semanticscholar.org/author/2136775815"}, {"h_index": 8, "name": "Harit Vishwakarma", "profile_url": "https://www.semanticscholar.org/author/27638084"}, {"h_index": 2, "name": "Nicholas Roberts", "profile_url": "https://www.semanticscholar.org/author/2340014150"}, {"h_index": 3, "name": "Frederic Sala", "profile_url": "https://www.semanticscholar.org/author/2269049908"}], "notable_authors_count": 1, "paper_url": "https://www.semanticscholar.org/paper/199f126f64d28554217d391019a8f2f78dffeed3", "success": true, "total_authors": 6}, "authors": ["Tzu-Heng Huang", "Catherine Cao", "Spencer Schoenberg", "Harit Vishwakarma", "Nicholas Roberts", "Frederic Sala"], "categories": ["cs.LG (Machine Learning)"], "embedding_model": "openai-large", "highest_score": 0.5397283254525923, "highest_score_topic": "Weak_supervision", "introduction": {"content": "Introduction\n\nAccess to substantial amounts of high-quality labeled data is a key ingredient for training performant machine learning models. Such data is usually produced by asking domain experts for ground-truth labels, making the process of dataset creation expensive, slow, and hard to scale. Programmatic weak supervision (PWS), a novel paradigm for generating labeled data , sidesteps these obstacles. The idea behind PWS is to leverage a combination of noisy label estimates obtained from domain knowledge, heuristic rules, and pattern matching. These sources act as noisy labeling functions (LFs), usually expressed as code. The outputs of these labeling functions are modeled and aggregated to annotate unlabeled data points .\n\nPWS has proven successful but remains expensive: users must painstakingly write small programs to act as LFs. Users, even domain experts, often need tedious experimentation to carefully set up proper thresholds, manually fine-tune heuristic rules to capture enough keywords, or debug regular expressions.\n\nTo tackle these challenges, recent approaches automatically produce\nLFs by using a minimal level of supervision (i.e. a few labeled data points) or access to powerful external models (like large language models) to prompt data labels . However, these approaches do not yield programmatic LFs, but rather model-generated noisy label estimates, and so lose the ability to debug and transfer, a key advantage of programmatic weak supervision.\n\nA best-of-both worlds approach is to have code-generation models write labeling functions. This neither requires domain experts to write code nor sacrifices the programmatic property of LFs.\n\nIndeed, such an approach is now plausible given advances in models that produce code, such as CodeT5 , Codex , and CodeGen ).\n\nAmong other benefits, LFs generated by such models can be edited and used as templates, providing programming assistance for users to design LFs more easily and efficiently.\n\nAdditionally, unlike human-designed LFs, synthesized LFs can be generated in large quantities.\n\nFinally, in contrast to using large language models to obtain the noisy labels estimates via prompting, which requires repeated inference calls, synthesized LFs can be stored and reused to label new data at zero cost.\n\nHowever, it is unclear whether code generation models can produce sufficiently high-quality LFs, and, when it is possible, what approach to take in order to do so. We ask the following fundamental questions we aim to answer in this work:\n\\begin{enumerate}\n \\item Prompt format: Prompts are highly sensitive. Small changes to prompt components lead to great variation in generated results. There is currently no consensus on the best way to generally prompt code-generation models, let alone specifically for labeling functions. Our first question is: what prompting strategy can yield high-quality LFs?\n \\item Capability of synthesized LFs: Next we ask: compared to human-designed LFs, what are the strengths and weaknesses of synthesized LFs? Additionally, what is the typical result when using these synthesized programs in programmatic weak supervision pipelines?\n \\item In-context few-shot settings: If we are allowed to include some heuristic rules or give several data examples into the prompt context, does this better guide the model in synthesizing high-quality LFs? What type of in-context information can add and influence the quality of synthesized LFs?\n\\end{enumerate}\n\nWe answer these questions and use the resulting insights to build a novel programmatic weak supervision system called ScriptoriumWS. A high-level view of ScriptoriumWS is illustrated in Figure . The system creates LFs by prompting code-generation models to synthesize programs and incorporates them into PWS pipelines. To validate ScriptoriumWS, we conduct experiments with OpenAI Codex , a state-of-the-art natural language-to-code system based on GPT-3 . We further propose a complementary approach to incorporate the strength of synthesized and human-designed LFs to improve the performance of the end model.\n\nWith the aid of ScriptoriumWS, we explore the advantages that synthesized LFs can bring to the weak supervision framework.\n\nWe study various prompting strategies to gain insight into how to best generate high-quality LFs.\n\nWe conduct experiments in diverse text domains and empirically demonstrate the effectiveness of ScriptoriumWS.\n\nExcitingly, we find that compared to the human-designed LFs in WRENCH, LFs generated using ScriptoriumWS achieve much higher coverage (the fraction of data points that receive labels) while maintaining high accuracy.\n\nFor example, using the WRENCH benchmark for comparison, we improve the coverage for the SMS dataset from 40.5% to 100% and for the Spouse dataset from 25.8% to 100%, while also improving downstream performance by 4.0% and 5.0% F1 points, respectively.\n\n\\begin{figure}[t!]\n \\includegraphics[width=\\linewidth]{figures/framework.png}\n \\centering\n \\caption{\\small Overview of the proposed ScriptoriumWS system. Code generation models are prompted to produce small programs that act as weak supervision labeling functions. These are used within a weak supervision pipeline to label an unlabeled dataset. A downstream end model is trained on the labeled data.}\n\n\\end{figure}", "extraction_method": "dedicated_file", "length": 5310, "success": true, "tex_filename": "sections/introduction.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.26042251358806573, "validated": false}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.2555468907779421, "validated": false}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.33385525422405093, "validated": false}, "Weak_supervision": {"justification": "The paper\u0027s main contribution is developing a system to generate labeling functions for weak supervision using code-generation models, which aligns with weak supervision\u0027s focus on programmatically creating noisy labels from sources like heuristics and rules.", "llm_relevant": "yes", "similarity_score": 0.5397283254525923, "validated": true}}, "pdf_url": "http://arxiv.org/pdf/2502.12366v1", "published": "2025-02-17T23:07:14Z", "scores": {"Diffusion_reasoning": 0.26042251358806573, "Distributed_training": 0.2555468907779421, "RLHF": 0.33385525422405093, "Weak_supervision": 0.5397283254525923}, "scores_data": {"clarity": 8.0, "novelty": 8.5, "overall_priority": 8.5, "potential_impact": 8.5, "relevance": 9.0}, "title": "ScriptoriumWS: A Code Generation Assistant for Weak Supervision", "updated": "2025-02-17T23:07:14Z", "url": "http://arxiv.org/abs/2502.12366v1"}, {"abstract": "Training large language models is generally done via optimization methods on\nclusters containing tens of thousands of accelerators, communicating over a\nhigh-bandwidth interconnect. Scaling up these clusters is expensive and can\nbecome impractical, imposing limits on the size of models that can be trained.\nSeveral recent studies have proposed training methods that are less\ncommunication intensive, avoiding the need for a highly connected compute\ncluster. These state-of-the-art low communication training methods still employ\na synchronization step for model parameters, which, when performed over all\nmodel replicas, can become costly on a low-bandwidth network.\n  In this work, we propose a novel optimization method, NoLoCo, that does not\nexplicitly synchronize all model parameters during training and, as a result,\ndoes not require any collective communication. NoLoCo implicitly synchronizes\nmodel weights via a novel variant of the Nesterov momentum optimizer by\npartially averaging model weights with a randomly selected other one. We\nprovide both a theoretical convergence analysis for our proposed optimizer as\nwell as empirical results from language model training.\n  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,\nbetween 125M to 6.8B parameters. Our method requires significantly less\ncommunication overhead than fully sharded data parallel training or even widely\nused low communication training method, DiLoCo. The synchronization step itself\nis estimated to be one magnitude faster than the all-reduce used in DiLoCo for\nfew hundred accelerators training over the internet. We also do not have any\nglobal blocking communication that reduces accelerator idling time. Compared to\nDiLoCo, we also observe up to $4\\%$ faster convergence rate with wide range of\nmodel sizes and accelerator counts.", "arxiv_id": "2506.10911v1", "arxiv_url": "http://arxiv.org/abs/2506.10911v1", "author_h_indices": {"authors_found_count": 5, "authors_with_h_index_count": 5, "average_h_index": 5.2, "data_source": "Semantic Scholar (title match)", "highest_h_index": 22, "individual_h_indices": [{"h_index": 22, "name": "J. Kolehmainen", "profile_url": "https://www.semanticscholar.org/author/2051209"}, {"h_index": 1, "name": "Nikolay Blagoev", "profile_url": "https://www.semanticscholar.org/author/2347536546"}, {"h_index": 1, "name": "John Donaghy", "profile_url": "https://www.semanticscholar.org/author/2253812306"}, {"h_index": 2, "name": "Ouguzhan Ersoy", "profile_url": "https://www.semanticscholar.org/author/2179107345"}, {"h_index": 0, "name": "Christopher Nies", "profile_url": "https://www.semanticscholar.org/author/2366564213"}], "notable_authors_count": 1, "paper_url": "https://www.semanticscholar.org/paper/17a1a0a9dc41fc779cd8e4a7a92ece4747ea439e", "success": true, "total_authors": 5}, "authors": ["Jari Kolehmainen", "Nikolay Blagoev", "John Donaghy", "O\u011fuzhan Ersoy", "Christopher Nies"], "categories": ["cs.LG (Machine Learning)"], "embedding_model": "openai-large", "highest_score": 0.476170051773984, "highest_score_topic": "Distributed_training", "introduction": {"content": "Large transformer models have recently shown impressive performance on a wide variety of tasks, including natural language understanding~; image related tasks~; or speech recognition and generation~. These large models are usually trained by a combination of different distributed training methods such as data parallelism~, pipeline parallelism~, and others~. The aforementioned training methods are bandwidth intensive and require a high-bandwidth interconnection fabric available between individual compute nodes that is generally only available in dedicated machine learning clusters~. This requirement increases the cost of training and poses a limit on the training scale as highly connected computer clusters cannot be scaled easily beyond a data center.\n\nRecently, a number of studies have aimed to address this issue by proposing algorithms that scale better than traditional distributed training algorithms in low-bandwidth and high latency network~. Most of these methods use an explicit step to synchronize the data parallel instances of the model during training, typically using all-reduce operations. This synchronization step can take several minutes in highly distributed network and lead to poor overall scaling efficiency of the training algorithm~.\n\nThe main contribution of this paper is to propose a novel optimization method, NoLoCo, for training large neural models that does not use any explicit all-to-all communication.\nNoLoCo is built upon the inner-outer optimizer paradigm together with the idea of epidemic learning where averaging is done among subset of accelerators instead of all.\nSpecifically, in NoLoCo, outer synchronization is done via only pairs of the accelerators, rather than all of them. Moreover, each inner optimizer step is done via random pipeline routing of accelerators which implicitly helps different pipelines to converge with less synchronisation.\nFurthermore, we modify the Nesterov momentum optimizer to prevent the weights of the same stage from diverging.\nWe also provide a theoretical convergence analysis on our modified optimizer step.\n\nWe test our method with a state of the art low communication method, DiLoCo, and with a traditional distributed model training in the language modeling task with two datasets (Pushshift Reddit and C4) and several model sizes (125M, 1.3B and 6.8B parameters).\nOur experimental results show that our method is more communication efficient and also converges up to $4%$ faster than DiLoCo for wide range worker counts, and model sizes. The speed-up from omitting the all-to-all communication increases with increasing number of workers and network latency variance.\n\nWe also study how the random pipeline routing between different model stages impacts the convergence. We show that while it can aid in load balancing across different workers~, it also hinders the convergence of validation loss slightly. This effect become less pronounced for larger models.\n\nThe paper is structured as follows. We first describe relevant work in Section . This is followed by Section presenting our method, NoLoCo. In Section we describe - for ease of reproducibility - the details of experiments. Results are presented and discussed in Section . Source code for running the experiments is available in \\href{https://github.com/gensyn-ai/noloco}{GitHub}.", "extraction_method": "main_file_section", "length": 3334, "success": true, "tex_filename": "noloco_arxiv.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.2773778494357561, "validated": false}, "Distributed_training": {"justification": "The paper\u0027s main contribution is a novel optimization method for distributed training of large models, which reduces communication overhead by partitioning and synchronizing computations across multiple nodes, directly aligning with distributed training, parallel computing, and multi-node machine learning.", "llm_relevant": "yes", "similarity_score": 0.476170051773984, "validated": true}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.3051652788554101, "validated": false}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.3182357109591522, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2506.10911v1", "published": "2025-06-12T17:23:23Z", "scores": {"Diffusion_reasoning": 0.2773778494357561, "Distributed_training": 0.476170051773984, "RLHF": 0.3051652788554101, "Weak_supervision": 0.3182357109591522}, "scores_data": {"clarity": 8.0, "novelty": 8.5, "overall_priority": 8.5, "potential_impact": 8.5, "relevance": 9.0}, "title": "NoLoCo: No-all-reduce Low Communication Training Method for Large Models", "updated": "2025-06-12T17:23:23Z", "url": "http://arxiv.org/abs/2506.10911v1"}, {"abstract": "Distributed Learning (DL) enables the training of machine learning models\nacross multiple devices, yet it faces challenges like non-IID data\ndistributions and device capability disparities, which can impede training\nefficiency. Communication bottlenecks further complicate traditional Federated\nLearning (FL) setups. To mitigate these issues, we introduce the Personalized\nFederated Learning with Decentralized Selection Training (PFedDST) framework.\nPFedDST enhances model training by allowing devices to strategically evaluate\nand select peers based on a comprehensive communication score. This score\nintegrates loss, task similarity, and selection frequency, ensuring optimal\npeer connections. This selection strategy is tailored to increase local\npersonalization and promote beneficial peer collaborations to strengthen the\nstability and efficiency of the training process. Our experiments demonstrate\nthat PFedDST not only enhances model accuracy but also accelerates convergence.\nThis approach outperforms state-of-the-art methods in handling data\nheterogeneity, delivering both faster and more effective training in diverse\nand decentralized systems.", "arxiv_id": "2502.07750v2", "arxiv_url": "http://arxiv.org/abs/2502.07750v2", "author_h_indices": {"authors_found_count": 0, "authors_with_h_index_count": 0, "average_h_index": null, "data_source": "None", "highest_h_index": null, "individual_h_indices": [], "notable_authors_count": 0, "paper_url": null, "success": false, "total_authors": 0}, "authors": ["Mengchen Fan", "Keren Li", "Tianyun Zhang", "Qing Tian", "Baocheng Geng"], "categories": ["cs.LG (Machine Learning)", "cs.AI (Artificial Intelligence)"], "embedding_model": "openai-large", "highest_score": 0.4033130341256313, "highest_score_topic": "Distributed_training", "introduction": {"content": "There has been growing interest in applying signal processing and machine learning (ML) to critical predictive tasks across various disciplines . The strategy of integrating data from multiple sources, such as sensors, enhances outcomes by providing multiple perspectives on a single phenomenon. Moreover, the global trend toward stricter data privacy laws and the growing implementation of regulations that restrict the sharing of sensitive data, such as health information, has accelerated advancements in distributed learning and decision-making processes that function without exchanging raw data .\n\nFederated learning (FL) has gained prominence for its ability to train models on decentralized devices . FL systems facilitate multi-client learning without centralizing raw data, addressing both privacy and communication challenges. However, heterogeneity in data distribution, resource allocation, task objectives, or network characteristics across nodes poses challenges to model accuracy and convergence . Personalized Federated Learning (PFL) addresses these issues by tailoring models to specific client needs, thereby enhancing the effectiveness beyond the conventional single-model approach (e.g., ) in FL.\n\nPFL is categorized into Centralized Personalized Federated Learning (CPFL) and Decentralized Personalized Federated Learning (DPFL) . CPFL can suffer from communication bottlenecks and server failures, leading to increased communication traffic and potential system crashes. In contrast, DPFL emphasizes peer-to-peer interactions among edge clients, reducing communication loads on local nodes and promoting faster convergence. In this topology, clients maintain an undirected and symmetric communication structure, facilitating model exchanges with peers.\n\nMost existing PFL approaches finely tune the interactions between global and personalized models to accommodate local data variations using methods such as regularization , knowledge distillation , multi-task learning , and clustering . These techniques aim to enhance personalized performance in the heterogeneous setting. For example, approaches like FedPer propose to capture personalization aspects in\nfederated learning by viewing deep learning models as base and personalization layers. And FedBABU utilize a single global feature representation coupled with multiple local classifiers, differing in how they manage the relationship between the shared representation and the individual linear components. FedFusion utilizes a representation method to fuse the batch information to solve the heterogeneity problem. Cho et al. provides theoretical convergence analysis for these algorithms under general non-convex conditions. DFedAvgM employs multiple local iterations with SGD and quantization techniques to reduce communication overhead. Dis-PFL designs personalized models and pruned masks for each client to personalized convergence. OSGP , DfedPGP , and AsyNG utilize the push-sum method to enhance training efficiency.\n\nDespite ongoing efforts, DPFL methodologies continue to face slow convergence rates during aggregations, a challenge compounded by heterogeneous data distributions among clients. Additionally, disparities in communication bandwidth and computational capabilities complicate these issues further, leading to unstable communication channels between clients. As a result, clients are compelled to selectively engage with only a limited subset of peers for communication.\n\nTo address these challenges, we introduce Personalized Federated Learning with\nDecentralized Selection Training (PFedDST), a decentralized selection training-based Personalized Federated Learning approach. This method ensures that each client maintains a model of the same dimensionality, facilitating efficient aggregation and strategic communication among clients. During each communication round, clients selectively engage with a subset of peers, chosen through a strategic scoring strategy for their relevance to the current learning context. They then aggregate their own model with those selected from their peers. After local updates, clients share their newly trained model parameters with the required peers, thereby enhancing the collective learning process and ensuring continuous improvement and relevance of the shared data.\nWe employ an innovative scoring scheme that evaluates potential peer clients based on three key factors: feature extraction capability, task heterogeneity, and communication frequency. Simulations in heterogeneous settings demonstrate that PFedDST not only increases the average test accuracy on local test data but also reduces the number of communication rounds required to achieve the same performance targets.\n\nWe summarize our contributions as following:\n\\begin{itemize} \\item We propose the PFedDST framework, a personalized federated learning approach where each client continuously learns from selected peers to update its feature extraction capabilities while maintaining a personalized prediction header. This integration of peer selection and partial model personalization enhances robust communication and accelerates convergence. \\item Strategic selection enables clients to enhance their feature extraction capabilities from the most informative and relevant neighbors. It also prioritizes communication with clients that have not recently interacted, thereby diversifying and refreshing the learning inputs. \\item Experimental results demonstrate that PFedDST outperforms various state-of-the-art baselines. It proves particularly effective in environments characterized by data heterogeneity and limited computational resources. \\end{itemize}\n\nIt should be noted that our strategy is different from traditional directed DFL methods such as Dis-PFL and AsyNG, which typically involve exchanging all parameters for a single consensus model or selecting communication targets randomly. Instead, our approach incorporates score-based neighbor selection, partial freeze training, and alternating optimization to accelerate convergence. This method not only ensures model robustness and enhances personalization but also optimizes communication efficiency.\n\n%", "extraction_method": "main_file_section", "length": 6186, "success": true, "tex_filename": "main.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.2632856348847062, "validated": false}, "Distributed_training": {"justification": "The paper introduces PFedDST, a framework for distributed training in federated learning, where model training is partitioned across multiple devices through peer selection and aggregation to accelerate convergence and handle data heterogeneity, aligning with distributed training and multi-node machine learning concepts.", "llm_relevant": "yes", "similarity_score": 0.4033130341256313, "validated": true}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.3341762001630327, "validated": false}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.2509316482169169, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2502.07750v2", "published": "2025-02-11T18:25:48Z", "scores": {"Diffusion_reasoning": 0.2632856348847062, "Distributed_training": 0.4033130341256313, "RLHF": 0.3341762001630327, "Weak_supervision": 0.2509316482169169}, "scores_data": {"clarity": 8.0, "novelty": 7.5, "overall_priority": 8.5, "potential_impact": 8.5, "relevance": 9.0}, "title": "PFedDST: Personalized Federated Learning with Decentralized Selection\n  Training", "updated": "2025-02-19T04:21:58Z", "url": "http://arxiv.org/abs/2502.07750v2"}, {"abstract": "Machine learning models have achieved, and in some cases surpassed,\nhuman-level performance in various tasks, mainly through centralized training\nof static models and the use of large models stored in centralized clouds for\ninference. However, this centralized approach has several drawbacks, including\nprivacy concerns, high storage demands, a single point of failure, and\nsignificant computing requirements. These challenges have driven interest in\ndeveloping alternative decentralized and distributed methods for AI training\nand inference. Distribution introduces additional complexity, as it requires\nmanaging multiple moving parts. To address these complexities and fill a gap in\nthe development of distributed AI systems, this work proposes a novel\nframework, Data and Dynamics-Aware Inference and Training Networks (DA-ITN).\nThe different components of DA-ITN and their functions are explored, and the\nassociated challenges and research areas are highlighted.", "arxiv_id": "2501.05323v1", "arxiv_url": "http://arxiv.org/abs/2501.05323v1", "author_h_indices": {"authors_found_count": 4, "authors_with_h_index_count": 4, "average_h_index": 3.0, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 7, "individual_h_indices": [{"h_index": 7, "name": "Hesham G. Moussa", "profile_url": "https://www.semanticscholar.org/author/14006040"}, {"h_index": 3, "name": "Arashmid Akhavain", "profile_url": "https://www.semanticscholar.org/author/66193246"}, {"h_index": 1, "name": "S. M. Hosseini", "profile_url": "https://www.semanticscholar.org/author/2340001346"}, {"h_index": 1, "name": "Bill McCormick", "profile_url": "https://www.semanticscholar.org/author/2256522006"}], "notable_authors_count": 1, "paper_url": "https://www.semanticscholar.org/paper/565c3a1b4c2b2f5635a171ec2d481870d311c6f2", "success": true, "total_authors": 4}, "authors": ["Hesham G. Moussa", "Arashmid Akhavain", "S. Maryam Hosseini", "Bill McCormick"], "categories": ["cs.LG (Machine Learning)", "cs.NI (Networking and Internet Architecture)"], "embedding_model": "openai-large", "highest_score": 0.5340102933505882, "highest_score_topic": "Distributed_training", "introduction": {"content": "\\IEEEPARstart{I}{n} recent years, human intelligence has been the reference for many researchers working in machine learning. Today we have models that exhibit similar or even surpass human capabilities in many fields including text (e.g., chatGPT), games (e.g., AlphaZero), and object recognition (e.g., YOLOv7) . While these results are impressive, the underlying model training paradigm relied mostly on centralization, where large volumes of data are collected and processed in a central cloud.\n\nHowever, centralization has become increasingly expensive, especially with the exponential growth in data volumes and model sizes . This cost is further aggravated in centralized life-long learning, which requires frequent initiation of costly re-training episodes to integrate new data without forgetting past learned tasks . Additionally, transferring data from distributed nodes poses significant privacy and security risks. Similar limitations also exist in centralized inference setups, especially when hosting large models on central servers which creates single points of failure, server congestion, and elevated computational cost. Furthermore, query and response data may carry sensitive information that, if exposed, may raise security and privacy concerns .\n\nTo address these limitations, decentralization has gained considerable attention from researchers in recent years . Decentralized AI finds its way into many applications such as healthcare, robotics, and mobile networks . As such, finding ways to enable this paradigm has driven many innovations. Many decentralized training methods like distributed learning, federated learning, gossip learning, split learning, and continual learning , along with decentralized inference approaches such as split inference, switched inference, collaborative inference, and multi-modal inference, have been proposed . These methods share the common objective of improving scalability, enhancing privacy, and reducing computational and storage demands . This is achieved by distributing data and models across several networked nodes and enabling extensive mobility of the various components to enable AI functionality \u2014 a concept known as the \"model-follow-data\" paradigm.\n\nThe model-follow-data paradigm views decentralized AI as a network of distributed, yet connected nodes. On the links between these nodes, models, data, and queries are optimally routed to facilitate AI training and inference. For instance, in the case of model-follow-data-based training, the objective is to enable dynamic interactions between data, compute, and models for optimal model training by creating adequate rendezvous points in the network. For example, in vanilla federated learning (FL), distributed data nodes collaborate to train a global model. Each data node receives a copy of the model from a central server, trains it on its local data, and returns the updated weights to the server. The server aggregates these updated weights and creates an updated global model that is ready for the next round of training . Under vanilla FL, data nodes act as the rendezvous points for model-data interaction.\n\nSimilarly, in inference, the goal is to provide cheaper and faster query responses by optimizing the placement of models across the network to which queries can be routed efficiently. For instance, in 6G-enabled edge computing, servers near base stations bring computing close to the users; hence, by deploying AI models at these edge servers, inference costs and response times can be significantly reduced . In this setup, model-hosting facilities are considered the rendezvous points to which queries are routed which necessitates careful deployment designs.\n\nAccordingly, under the model-follow-data paradigm, the success of training and inference relies on the careful design and optimization of several components. To address this requirement, we propose a novel network-inspired intelligent decentralized system that we refer to as Data and Dynamics aware Inference and Training Network (DA-ITN) which is the main contribution of this work. DA-ITN aims to fill a gap in the next generation of distributed AI systems. In this paper, we dive into the general framework of DA-ITN and describe its various components. To the best of the authors\u0027 knowledge, this is the first work that introduces such a framework and vision.\n\nThe article is organized as follows. Section 2 provides an overview of DA-ITN and its various components. Section 3 dives into a high-level example to show DA-ITN in action. Section 4 presents an envisioned forward-looking implementation of DA-ITN as an autonomous system. The paper is wrapped up in section 5 with a list of challenges and potential research areas.", "extraction_method": "main_file_section", "length": 4741, "success": true, "tex_filename": "main_manuscript.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.3411232099912031, "validated": false}, "Distributed_training": {"justification": "The paper proposes a framework for distributed AI training and inference, including methods like federated learning and distributed learning, which involve partitioning data and computation across multiple nodes to accelerate training.", "llm_relevant": "yes", "similarity_score": 0.5340102933505882, "validated": true}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.33687961378387643, "validated": false}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.32592072661402055, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2501.05323v1", "published": "2025-01-09T15:48:29Z", "scores": {"Diffusion_reasoning": 0.3411232099912031, "Distributed_training": 0.5340102933505882, "RLHF": 0.33687961378387643, "Weak_supervision": 0.32592072661402055}, "scores_data": {"clarity": 8.0, "novelty": 7.5, "overall_priority": 8.0, "potential_impact": 8.5, "relevance": 9.0}, "title": "Distributed Learning and Inference Systems: A Networking Perspective", "updated": "2025-01-09T15:48:29Z", "url": "http://arxiv.org/abs/2501.05323v1"}, {"abstract": "Deep learning models are yielding increasingly better performances thanks to\nmultiple factors. To be successful, model may have large number of parameters\nor complex architectures and be trained on large dataset. This leads to large\nrequirements on computing resource and turn around time, even more so when\nhyper-parameter optimization is done (e.g search over model architectures).\nWhile this is a challenge that goes beyond particle physics, we review the\nvarious ways to do the necessary computations in parallel, and put it in the\ncontext of high energy physics.", "arxiv_id": "2012.01839v2", "arxiv_url": "http://arxiv.org/abs/2012.01839v2", "author_h_indices": {"authors_found_count": 2, "authors_with_h_index_count": 2, "average_h_index": 66.0, "data_source": "Semantic Scholar (base arXiv ID)", "highest_h_index": 114, "individual_h_indices": [{"h_index": 114, "name": "J. Vlimant", "profile_url": "https://www.semanticscholar.org/author/52630992"}, {"h_index": 18, "name": "Junqi Yin", "profile_url": "https://www.semanticscholar.org/author/1779195"}], "notable_authors_count": 2, "paper_url": "https://www.semanticscholar.org/paper/3dba51f1c185cc7740515954ea2a83bf051bdaf6", "success": true, "total_authors": 2}, "authors": ["Jean-Roch Vlimant", "Junqi Yin"], "categories": ["cs.LG (Machine Learning)"], "embedding_model": "openai-large", "highest_score": 0.6248025954222479, "highest_score_topic": "Distributed_training", "introduction": {"content": "The main aspects of distributed training have been recently well reviewed in and we refer to it for a more in depth discussion on technical details.\nThere exists a rich literature on distributed training of neural networks and notably , recommended as supplementary reading.\n\nIt is commonly agreed that deep learning has shown great success over the last decade, thanks to the creation of large labeled datasets, advancement in model architectures, and increase in computation power --- in part due to general purpose graphical processing units (GPU).\nWith ever growing complexity of datasets and models, and despite the acceleration provided by GPU, training can still last for days and weeks on single device.\nBesides the training of a single model, it is often necessary to perform an optimization over some parameters, that are otherwise not learnable with gradient descent.\n\nWith the acceleration of the adoption of deep learning in high energy physics , it becomes necessary to look at ways to reduce the effective training time.\nWhile most simple neural network models and other classical machine learning methods can be trained in reasonable time, more advanced models like graph neural network \\CHAPTER{AIHEP:5.3} and generative adversarial network \\CHAPTER{AIHEP:3.1,AIHEP:3.2} can be hard to train~.\nComplex architectures that exhibit large training time per epoch are often just discarded solely due to the time it would take to bring them to converge --- let alone doing hyper-parameter tuning.\nImprovement of the time to solution is required to make the development of such models more amenable.\nDistributed training may reduce weeks of training down to days.\n\nIt should be noted that the challenge of accelerating the time to convergence is not specific to high energy physics (HEP).\nHowever, the specific computing and software environment of HEP might limit the possibilities otherwise available.\nFor example, due to budget constraints, it is not given that GPUs are available for training.\nFurthermore, the software options are limited by the requirement of affordable long term support.\n\n\\paragraph{}\n\nWe provide in this chapter a description of the key aspects of distributed training and optimization as a practical guide to developing large models with large amount of data.\nThis chapter is organized as follows.\nAfter introducing the formalism of training and optimizing neural network models in section , highlighting the possible strategies to parallelize computation, we start in section with the parameter distribution strategy, which was the first to be adopted as a way to speed up the training of models.\nWe then describe in section the data distributed strategy that seems to be widely adopted currently, thanks to its ease of use.\nWe go over model parallelism in section and recent development in generic deployment of this otherwise complicated method to implement.\nIn section , we get into the details of model search and optimization, key to the success of developing high-performance deep learning applications.\nWe conclude in section with an overview of advancements of software and outlooks on distributed training.", "extraction_method": "main_file_section", "length": 3153, "success": true, "tex_filename": "dist-train.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.27236900174052303, "validated": false}, "Distributed_training": {"justification": "The paper\u0027s main contribution is a review of distributed training methods for neural networks, including parallel computing strategies like parameter, data, and model parallelism, which directly align with accelerating model training across multiple nodes.", "llm_relevant": "yes", "similarity_score": 0.6248025954222479, "validated": true}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.2967608735960048, "validated": false}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.2870379248678242, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2012.01839v2", "published": "2020-12-03T11:18:46Z", "scores": {"Diffusion_reasoning": 0.27236900174052303, "Distributed_training": 0.6248025954222479, "RLHF": 0.2967608735960048, "Weak_supervision": 0.2870379248678242}, "scores_data": {"clarity": 7.0, "novelty": 4.0, "overall_priority": 6.5, "potential_impact": 6.0, "relevance": 7.5}, "title": "Distributed Training and Optimization Of Neural Networks", "updated": "2021-01-15T14:24:22Z", "url": "http://arxiv.org/abs/2012.01839v2"}, {"abstract": "The Object Navigation (ObjectNav) task aims to guide an agent to locate\ntarget objects in unseen environments using partial observations. Prior\napproaches have employed location prediction paradigms to achieve long-term\ngoal reasoning, yet these methods often struggle to effectively integrate\ncontextual relation reasoning. Alternatively, map completion-based paradigms\npredict long-term goals by generating semantic maps of unexplored areas.\nHowever, existing methods in this category fail to fully leverage known\nenvironmental information, resulting in suboptimal map quality that requires\nfurther improvement. In this work, we propose a novel approach to enhancing the\nObjectNav task, by training a diffusion model to learn the statistical\ndistribution patterns of objects in semantic maps, and using the map of the\nexplored regions during navigation as the condition to generate the map of the\nunknown regions, thereby realizing the long-term goal reasoning of the target\nobject, i.e., diffusion as reasoning (DAR). Meanwhile, we propose the Room\nGuidance method, which leverages commonsense knowledge derived from large\nlanguage models (LLMs) to guide the diffusion model in generating room-aware\nobject distributions. Based on the generated map in the unknown region, the\nagent sets the predicted location of the target as the goal and moves towards\nit. Experiments on Gibson and MP3D show the effectiveness of our method.", "arxiv_id": "2410.21842v2", "arxiv_url": "http://arxiv.org/abs/2410.21842v2", "author_h_indices": {"authors_found_count": 0, "authors_with_h_index_count": 0, "average_h_index": null, "highest_h_index": null, "individual_h_indices": [], "notable_authors_count": 0, "timestamp": "2025-07-09T20:50:41.559335", "total_authors": 7}, "authors": ["Yiming Ji", "Kaijie Yun", "Yang Liu", "Zhengpu Wang", "Boyu Ma", "Zongwu Xie", "Hong Liu"], "categories": ["cs.CV (Computer Vision and Pattern Recognition)", "cs.AI (Artificial Intelligence)"], "embedding_model": "openai-large", "highest_score": 0.5859894825670287, "highest_score_topic": "Diffusion_reasoning", "introduction": {"content": "Embodied navigation, which requires an agent to use visual sensing to actively interact with its environment and perform navigation tasks, has seen rapid development driven by large-scale photorealistic domestic scene datasets , , and advanced high-performance simulators , .\nAs a specific form of embodied navigation, Object Navigation (ObjectNav) is regarded as a key technology for enabling robotic intelligence.\nIn ObjectNav task, the agent is placed in an unknown environment and is required to navigate to a user-specified object category (e.g., sofa) based on visual observations. Since the environment is unseen, when the target object is not visible, the agent needs to infer the potential locations of the target.\n\nEmbodied navigation agents show intelligence through semantic understanding and reasoning.\nPrevious work explored learning object contextual relationships for ObjectNav systems.\nSome methods extract object/region/room relationships to enhance visual representation , .\nwhile other end-to-end approaches directly map vision to actions , .\nHowever, end-to-end methods implicitly embed semantic prior knowledge and simultaneously learn localization, mapping, and path planning in an implicit manner, which leads to the problems of high computational load and limited generalization in unseen environments.\n\nAlternatively, modular approaches typically address the ObjectNav problem by incorporating three modules: Mapping, Long-term Goal Policy, and Path Planning. Among these, the Long-term Goal Policy module plays a pivotal role. When the target object is absent from the currently constructed local map, a semantic reasoning algorithm is employed to infer an appropriate location as the long-term goal, which subsequently serves as input to the Path Planning module. Notably, the accuracy of the Long-term Goal Policy critically determines both the success rate and efficiency of the ObjectNav system.\n\nThe existing approaches for long-term goal inference can be broadly categorized into two classes: location prediction methods and map completion methods.\n\nLocation prediction methods refer to algorithms that directly predict the position of the long-term goal based on the semantic map constructed from explored regions and the target object category. Some methods perform location prediction along the frontiers of the local map, selecting the point closest to the target object as the long-term goal , or incorporating common-sense knowledge to enhance frontier-based predictions .\nOther methods extend beyond frontier regions, instead predicting the target object\u2019s location globally and designating it as the long-term goal . Additionally, Xinyao Yu et al. proposed a trajectory prediction approach , which leverages the agent\u2019s historical observations and the target object as conditions to predict a sequence of long-term goals. This method, in essence, also falls under the category of location prediction.\n\nHowever, location prediction methods suffer from two key limitations: (1) their reliance on pre-constructed datasets, and (2) their individual learning of target-object relations, which may vary significantly across different room layouts. These limitations are effectively addressed by map completion methods, which adopt a self-supervised learning paradigm to jointly model inter-object relationships (referred to as contextual object relations).\nRecent advances in map completion have employed encoder-decoder architectures to model these contextual object relations.\n\nSeveral approaches employ CNN-based models (e.g., U-Net ) to predict unobserved regions based on single-timestamp top-down maps . Furthermore, SGM adopts a masked modeling strategy to train a transformer-based model (ViT ), which integrates visual observations with LLM knowledge (e.g., GPT-4 , ChatGLM ). This framework operates by first patchifying the input map and then predicting missing patch values conditioned on visible patches.\n\nThe map completion method predicts long-term goals by generating semantic maps of unknown regions. Undoubtedly, the higher the reliability of the generated maps, the greater the improvement in the performance of the ObjectNav system. However, the quality of the unknown-area maps generated by the aforementioned methods remains suboptimal. On one hand, these methods focus solely on objects themselves, while the absolute spatial positions of objects may vary across different environments. On the other hand, existing methods fail to fully utilize the information from observed regions.\n\nWe observe that learning contextual object relations equates to learning the statistical distribution patterns of object placements within indoor rooms.\nGenerating the object distribution maps in unknown regions equates to sampling from the distributions that best match those in the observed areas.\n\nInspired by the remarkable success of diffusion models in representing complex distributions , we propose replacing conventional CNN-based and Transformer-based architectures in self-supervised map generation with a diffusion model framework. Unlike existing approaches that focus solely on object-level features, our method incorporates room category information as conditional guidance.\nWhile object locations may exhibit ambiguity across different room layouts, specific room categories (e.g., bedrooms) consistently contain characteristic object classes (e.g., beds, pillows). By leveraging prior knowledge of a region\u0027s room category, our model can more effectively constrain the generation process to avoid implausible object distributions, thereby significantly improving the quality of generated maps for unknown regions.\n\nIn this paper, we present a modular solution for the ObjectNav problem, introducing a Diffusion-based Approach for long-term goal Reasoning (DAR). Our framework conditions on the agent\u0027s environmental memory while leveraging room category distributions as guidance to generate reliable semantic content in unknown areas. The DAR module selects potential target object locations within these generated regions as long-term goals.\nFor training DAR, we utilize standard ObjectNav datasets (Gibson, Matterport3D, etc.) to obtain ground-truth semantic room maps. These maps undergo random rotations, mirroring, and translations to construct the training set for our diffusion model. Following the self-supervised learning paradigm of DDPM, our DAR model learns to generate plausible, novel room semantic maps - distinct from training samples - through iterative denoising from pure Gaussian noise.\n\nHowever, unlike image datasets, training scenes for ObjectNav suffer from notable limitations in both quantity and diversity. To enhance generalization capability, previous approaches have incorporated general knowledge from LLMs. Our DAR module adopts this paradigm through object-room relationship modeling.\nThe denoising model in DAR takes both the denoised result xt from the previous time step and a room guidance map as input. This approach enables the diffusion model to generate specific object clusters according to room types. Since diffusion models have demonstrated the capability to generate pixel values in unknown regions based on partially known areas without requiring retraining, we leverage this mechanism by using the semantic map of known regions from actual navigation as a condition. The pre-trained diffusion model is employed to generate semantic maps for unknown regions, with room type guidance also applied.\nWe first determine the room type of frontier regions using common-sense knowledge extracted from LLMs, based on the observed object distribution in the established semantic map. For instance, if objects near frontier point $p$ include oven, sink, and refrigerator ordered by proximity, the LLM would identify $p$ as located in a kitchen. This room type then guides the diffusion model to generate object distributions around $p$ that are consistent with a kitchen (e.g., cup, bottle) rather than incompatible objects (e.g., bed, tv).\n\nWe evaluate our DAR model in simulation environments, including photorealistic 3D platforms: AI2-Thor, Robo-Thor, Gibson, and MP3D. Experimental results demonstrate that our DAR model significantly outperforms previous map completion methods, achieving substantial improvements in map quality, object diversity, and room structure/contour accuracy.\nThe precise semantic understanding enables DAR to produce more accurate semantic maps, leading to better long-term goal prediction and more efficient object navigation.\n\nIn summary, our study makes the following key contributions:\n\\begin{itemize}\n \\item We propose a modular-based ObjectNav approach, DAR, that learns statistical distributions of rooms and objects, enabling accurate generation of unknown-region object layouts based on explored-area maps.\n \\item Leveraging room-object correlations extracted from LLMs, we infer potential room types at frontier regions of partial semantic maps.\n \\item We design a denoising model augmented with a room-type guidance submodule, ensuring generated object distributions adhere to inferred spatial semantics.\n \\item Extensive evaluations on Gibson and Matterport3D (MP3D) demonstrate that DAR achieves state-of-the-art Success Rate (SR). Notably, it improves SPL (efficiency metric) by 10.6% on average over prior works across both datasets.\n\\end{itemize}", "extraction_method": "main_file_section", "length": 9392, "success": true, "tex_filename": "DAR_main.tex"}, "llm_validation": {"Diffusion_reasoning": {"justification": "The paper uses a diffusion model for generating semantic maps in object navigation, involving iterative refinement for spatial predictions, but it does not adapt this process to multi-step logical reasoning or treat a Chain-of-Thought as a single entity for holistic correction. It focuses on environmental mapping and object distribution, not complex logical tasks.", "llm_relevant": "no", "similarity_score": 0.5859894825670287, "validated": true}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.275109823976525, "validated": false}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.3318150174536303, "validated": false}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.2807445126302675, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2410.21842v2", "published": "2024-10-29T08:10:06Z", "scores": {"Diffusion_reasoning": 0.5859894825670287, "Distributed_training": 0.275109823976525, "RLHF": 0.3318150174536303, "Weak_supervision": 0.2807445126302675}, "title": "Diffusion as Reasoning: Enhancing Object Navigation via Diffusion Model\n  Conditioned on LLM-based Object-Room Knowledge", "updated": "2025-06-06T02:18:14Z", "url": "http://arxiv.org/abs/2410.21842v2"}, {"abstract": "Latent Diffusion Models have shown remarkable results in text-guided image\nsynthesis in recent years. In the domain of natural (RGB) images, recent works\nhave shown that such models can be adapted to various vision-language\ndownstream tasks with little to no supervision involved. On the contrary,\ntext-to-image Latent Diffusion Models remain relatively underexplored in the\nfield of medical imaging, primarily due to limited data availability (e.g., due\nto privacy concerns). In this work, focusing on the chest X-ray modality, we\nfirst demonstrate that a standard text-conditioned Latent Diffusion Model has\nnot learned to align clinically relevant information in free-text radiology\nreports with the corresponding areas of the given scan. Then, to alleviate this\nissue, we propose a fine-tuning framework to improve multi-modal alignment in a\npre-trained model such that it can be efficiently repurposed for downstream\ntasks such as phrase grounding. Our method sets a new state-of-the-art on a\nstandard benchmark dataset (MS-CXR), while also exhibiting robust performance\non out-of-distribution data (VinDr-CXR). Our code will be made publicly\navailable.", "arxiv_id": "2506.10633v1", "arxiv_url": "http://arxiv.org/abs/2506.10633v1", "author_h_indices": {"authors_found_count": 0, "authors_with_h_index_count": 0, "average_h_index": null, "highest_h_index": null, "individual_h_indices": [], "notable_authors_count": 0, "timestamp": "2025-07-09T20:52:31.230823", "total_authors": 5}, "authors": ["Konstantinos Vilouras", "Ilias Stogiannidis", "Junyu Yan", "Alison Q. O\u0027Neil", "Sotirios A. Tsaftaris"], "categories": ["cs.CV (Computer Vision and Pattern Recognition)"], "embedding_model": "openai-large", "highest_score": 0.409932673532709, "highest_score_topic": "Diffusion_reasoning", "introduction": {"content": "Latent Diffusion Models (LDMs) \\citep{rombach2022high} form a class of powerful text-to-image generators that achieve state-of-the-art performance in conditional image synthesis. Recently, the research community has shown increasing interest in the applicability of LDMs to various downstream tasks that require fine-grained image-text alignment, such as image editing \\citep{mokady2023null}, semantic correspondence \\citep{luo2023diffusion}, and keypoint detection \\citep{hedlin2024unsupervised}, with minimal supervision. In the context of biomedical vision-language processing (VLP), and focusing on the chest X-ray (CXR) modality in particular, LDMs have been repurposed to stress test task-specific models \\citep{perez2024radedit}, improve their robustness to distribution shifts via synthetic data augmentation \\citep{ktena2024generative}, or directly as classifiers \\citep{favero2025conditional}.\n\n\\begin{figure}[t!]\n \\centering\n \\includegraphics[width=\\columnwidth]{fig1.png}\n \\caption{Cross-attention leakage in a pre-trained LDM. Using a sentence extracted from the radiology report as a prompt that describes the input image (\\textcolor{orange}{orange} line path), we observe that the resulting cross-attention activations (averaged across selected layers and timesteps) are diffused over the image area. Also, following a simpler \\texttt{``\\{location\\} \\{pathology\\}\u0027\u0027} prompt format (\\textcolor{purple}{purple} line path) clearly does not improve the activations. Instead, our proposed weakly supervised fine-tuning method (\\textcolor{green}{green} line path) yields more localized cross-attention activations with respect to the anatomical area mentioned in the prompt.}\n\n\\end{figure}\n\nHowever, as shown in Figure , a closer inspection of the pre-trained LDM\u0027s cross-attention layers reveals a high level of attention leakage, i.e., the effect of a model associating tokens with unrelated image regions. Note that this finding is consistent with prior works. More specifically, \\citealp{mcinerney2022s} observed similar behavior in a model trained with a multi-modal contrastive learning objective, and proposed a few-shot fine-tuning method (based on ground truth bounding box annotations for the underlying pathology) to alleviate this issue. Furthermore, one could argue that this issue can be attributed to the high complexity of the sentences extracted from unstructured radiology reports; yet, as shown in Figure , establishing a standardized, simpler prompt format (referred to as \\texttt{``\\{location\\} \\{pathology\\}\u0027\u0027}) does not yield any direct improvement off-the-shelf.\n\nIn this study, we draw inspiration from techniques that use guidance from another modality to improve image-text alignment. For example, there exist works \\citep{ma2024eye} that incorporate a small set of available eye tracking data (collected while a radiologist was examining a CXR scan) during the vision-language pre-training stage. Instead, to avoid the need to collect inputs from another modality, we derive coarse supervision signals directly from free-text reports with the help of a pre-trained clinical entity recognition model and a small set of anatomical location annotations. Moreover, we propose a fine-tuning method that refines the model\u0027s multi-modal alignment in a data- and parameter-efficient manner by simply updating the anatomy token embeddings.\n\nOverall, our contributions are the following:\n\n\\begin{itemize}\n \\item We propose a novel approach to improving image-text alignment in biomedical VLP scenarios by extracting a supervision signal for pathology localisation directly from unstructured radiology reports. To this end, we combine a clinical entity recognition model with a few annotations of various anatomical regions commonly depicted in CXRs.\n \\item We develop an efficient fine-tuning framework to steer the pre-trained LDM\u0027s cross-attention activations towards the anatomical area specified in text.\n \\item We evaluate our proposed approach on an established phrase grounding benchmark dataset (MS-CXR), as well as an OOD dataset (VinDr-CXR) with ground truth bounding box annotations and synthetic prompts. In both cases, we show superior performance compared to previous SoTA.\n\\end{itemize}", "extraction_method": "main_file_section", "length": 4228, "success": true, "tex_filename": "main.tex"}, "llm_validation": {"Diffusion_reasoning": {"justification": "The paper discusses fine-tuning latent diffusion models for image-text alignment in medical imaging, specifically for tasks like phrase grounding in chest X-rays. It does not involve adapting the diffusion process for multi-step logical reasoning or treating a Chain-of-Thought as a single entity for holistic correction.", "llm_relevant": "no", "similarity_score": 0.409932673532709, "validated": true}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.23344414270365052, "validated": false}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.31921321867371877, "validated": false}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.3501391071835666, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2506.10633v1", "published": "2025-06-12T12:19:18Z", "scores": {"Diffusion_reasoning": 0.409932673532709, "Distributed_training": 0.23344414270365052, "RLHF": 0.31921321867371877, "Weak_supervision": 0.3501391071835666}, "title": "Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent\n  Diffusion Models", "updated": "2025-06-12T12:19:18Z", "url": "http://arxiv.org/abs/2506.10633v1"}, {"abstract": "Large language models (LLMs) are powerful but static; they lack mechanisms to\nadapt their weights in response to new tasks, knowledge, or examples. We\nintroduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to\nself-adapt by generating their own finetuning data and update directives. Given\na new input, the model produces a self-edit-a generation that may restructure\nthe information in different ways, specify optimization hyperparameters, or\ninvoke tools for data augmentation and gradient-based updates. Through\nsupervised finetuning (SFT), these self-edits result in persistent weight\nupdates, enabling lasting adaptation. To train the model to produce effective\nself-edits, we use a reinforcement learning loop with the downstream\nperformance of the updated model as the reward signal. Unlike prior approaches\nthat rely on separate adaptation modules or auxiliary networks, SEAL directly\nuses the model\u0026#x27;s own generation to control its adaptation process. Experiments\non knowledge incorporation and few-shot generalization show that SEAL is a\npromising step toward language models capable of self-directed adaptation. Our\nwebsite and code is available at https://jyopari.github.io/posts/seal.", "arxiv_id": "2506.10943v1", "arxiv_url": "http://arxiv.org/abs/2506.10943v1", "author_h_indices": {"authors_found_count": 0, "authors_with_h_index_count": 0, "average_h_index": null, "highest_h_index": null, "individual_h_indices": [], "notable_authors_count": 0, "timestamp": "2025-07-09T20:52:31.230791", "total_authors": 6}, "authors": ["Adam Zweiger", "Jyothish Pari", "Han Guo", "Ekin Aky\u00fcrek", "Yoon Kim", "Pulkit Agrawal"], "categories": ["cs.LG (Machine Learning)"], "embedding_model": "openai-large", "highest_score": 0.4021040375755671, "highest_score_topic": "RLHF", "introduction": {"content": "Large language models (LLMs) pretrained on vast text corpora exhibit remarkable abilities in language understanding and generation \\citep{brown2020language,touvron2023llama,grattafiori2024llama3herdmodels,groeneveld2024olmo,qwen2025qwen25technicalreport}. However, adapting these powerful models for specific tasks~\\citep{gururangan2020finetune}, integrating new information \\citep{zhu2021modifying}, or mastering novel reasoning skills \\citep{chollet2024arc} remains challenging due to the limited availability of task-specific data. In this paper, we explore an intriguing hypothesis: can an LLM self-adapt by transforming or generating its own training data and learning procedure?\n\nAs an analogy, consider a human student preparing for the final exam of a machine learning class. Many students rely on their notes to prepare for the exam. These notes are often derived from the lecture content, textbooks, or information available on the internet. Instead of relying on the raw content, assimilating and rewriting the information in the form of notes often improves the ability of students to understand the content and answer exam questions. This phenomenon of reinterpreting and augmenting external knowledge in a way that is easier to understand is not limited to just taking exams, but seems to be universally true of human learning across tasks. Furthermore, different humans assimilate information in different ways---some might condense the information into a visual diagram, some into text, or some might rely more on concrete mathematical descriptions.\n\nSuch assimilation, restructuring, or rewriting of data as part of the learning process is in contrast with how large language models (LLMs) are typically trained and deployed. Given a new task, current LLMs consume and learn from the task data ``as-is\u0027\u0027 via finetuning or in-context learning \\citep{wei2022finetuned,rozi\u00e8re2024codellamaopenfoundation,chen2023meditron70bscalingmedicalpretraining,colombo2024saullm7bpioneeringlargelanguage}. However, such data may not be in an optimal format (or volume) for learning, and current approaches do not enable models to develop bespoke strategies for how to best transform and learn from their training data.\n\nAs a step towards scalable and efficient adaptation of language models, we propose equipping LLMs with the ability to generate their own training data and finetuning directives for utilizing such data. In particular, we introduce a reinforcement learning algorithm that trains LLMs to generate ``self-edits\u0027\u0027---natural-language instructions that specify the data and, optionally, the optimization hyperparameters for updating the model\u0027s weights (see Figure~). We refer to such models as Self-Adapting LLMs (\\methodacronym).\n\nWe evaluate \\methodacronym on two applications. We first consider the task of integrating new factual knowledge into an LLM. Rather than finetuning directly on the passage text, we finetune on synthetic data generated by the \\methodacronym model. Our results show that, following reinforcement learning (RL) training, finetuning on self-generated synthetic data improves question-answering performance on the no-passage-in-context variant of SQuAD~\\citep{rajpurkar2016squad} from 33.5% to 47.0%. Notably, self-generated data from \\methodacronym outperforms synthetic data generated by GPT-4.1.\n\nWe further evaluate \\methodacronym on few-shot learning on a simplified subset of the ARC-AGI benchmark~\\citep{chollet2019ARC}, where the model leverages a set of tools to autonomously select both synthetic data augmentations and optimization hyperparameters (e.g., learning rate, training epochs, selective loss computation over token types). Our experiments demonstrate that automatic selection and configuration of these tools using \\methodacronym enhances performance compared to both standard in-context learning (ICL) and self-editing without RL training to use the tools effectively. These results collectively show that \\methodacronym is a versatile framework for enabling language models to self-adapt.\n\n\\begin{figure}[t]\n \\centering\n \\includegraphics[width=1\\linewidth]{figures/main_fig.pdf}\n \\caption{Overview of \\methodacronym. In each RL outer loop iteration, the model generates candidate self-edits (SE)---directives on how to update the weights---applies updates, evaluates performance on a downstream task, and uses the resulting rewards to improve the self-edit generation policy.}\n \\vspace{-8pt}\n\n\\end{figure}", "extraction_method": "main_file_section", "length": 4471, "success": true, "tex_filename": "neurips_2025.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.3514622079674554, "validated": false}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.301765047875554, "validated": false}, "RLHF": {"justification": "The paper describes a reinforcement learning framework where the reward signal is based on downstream task performance, not human feedback. Since RLHF specifically requires human-ranked data and a separate reward model trained on human preferences, this does not qualify.", "llm_relevant": "no", "similarity_score": 0.4021040375755671, "validated": true}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.3804727485057127, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2506.10943v1", "published": "2025-06-12T17:48:13Z", "scores": {"Diffusion_reasoning": 0.3514622079674554, "Distributed_training": 0.301765047875554, "RLHF": 0.4021040375755671, "Weak_supervision": 0.3804727485057127}, "title": "Self-Adapting Language Models", "updated": "2025-06-12T17:48:13Z", "url": "http://arxiv.org/abs/2506.10943v1"}, {"abstract": "As has been shown in our previous work, the parallel-in-time direct inverse\n(ParaDIn) method introduced by Yamaleev and Paudel in (arXiv: 2406.00878v1,\n2024) imposes some constraint on the maximum number of time levels, $N_t$, that\ncan be integrated in parallel. To circumvent this problem and further increase\nthe speedup, we combine the ParaDIn method with the Parareal algorithm to\nefficiently parallelize the first-order time derivative term in nonlinear\npartial differential equations discretized by the method of lines. The main\nidea of the proposed approach is to use a block-Jacobi preconditioner, so that\neach block is solved by using the ParaDIn method. To accelerate the convergence\nof Jacobi iterations, we use the Parareal method which can be interpreted as a\ntwo-level multigrid method in time. In contrast to the conventional Parareal\nalgorithm whose coarse grid correction step is performed sequentially, both the\ncoarse- and fine-grid propagators in the proposed approach are implemented in\nparallel by using the ParaDIn method, thus significantly increasing the\nparallel performance of the combined algorithm. Numerical results show that the\nnew combined ParaDIn-Parareal method provides the speedup of up to 124 on 480\ncomputing cores as compared with the sequential first-order implicit backward\ndifference (BDF1) scheme for the 2-D nonlinear heat and Burgers equations with\nboth smooth and discontinuous solutions.", "arxiv_id": "2506.10820v1", "arxiv_url": "http://arxiv.org/abs/2506.10820v1", "author_h_indices": {"authors_found_count": 0, "authors_with_h_index_count": 0, "average_h_index": null, "highest_h_index": null, "individual_h_indices": [], "notable_authors_count": 0, "timestamp": "2025-07-09T20:51:53.359189", "total_authors": 2}, "authors": ["Subhash Paudel", "Nail K. Yamaleev"], "categories": ["math.NA (Numerical Analysis)", "cs.NA (Numerical Analysis)"], "embedding_model": "openai-large", "highest_score": 0.31198494714104585, "highest_score_topic": "Distributed_training", "introduction": {"content": "\\quad \\ Parallel-in-time numerical schemes have recently attracted a lot of attention due to their potential of\ndrastically reducing the computational time and achieving much higher scalability than spatial domain decomposition methods for solving large-scale unsteady problems on modern supercomputers with hundreds of thousands of computing cores. Despite that various parallel-in-time methods are available in the literature, they have not been used in production codes.\nAs follows from two comprehensive literature reviews , the existing parallel-in-time methods have\nnot yet reached such level of maturity that is required for practical applications governed by highly nonlinear partial differential equations.\n\nOne of the most widely-used parallel-in-time methods that can be directly applied to both linear and nonlinear problems is the Parareal algorithm introduced by Lions, Maday, and Turinici in . This method can be considered as an iterative shooting method or a two-level multigrid method with special restriction and prolongation operators in the time domain . The main idea of the Parareal method is to divide the entire time interval into subintervals, so that the discretized governing equations can be independently integrated in each subinterval. To couple local equations defined on the original fine grid in each subinterval, a computationally inexpensive {\\it sequential} solver is used to find the solution on the coarse temporal grid.\n\nSince the fine-grid solves are independent from each other, they can be performed in parallel. The difference between the coarse- and fine-grid solutions is propagated sequentially on the coarse grid, which is called a coarse grid correction step. The coarse- and fine-grid steps are repeated iteratively until some residual norm becomes smaller than a user-defined tolerance .\nFor a linear system of ordinary differential equations (ODEs) with a constant coefficient matrix $A \\in {R}^{m \\times m}$, it has been proven that the Parareal method converges if the governing equation is discretized by using an implicit $L$-stable scheme (e.g., the backward Euler scheme) and $A$ is either positive symmetric positive definite (SPD) or has complex eigenvalues .\nVarious parallel-in-time algorithms have been developed to enhance the efficiency and stability properties of the classical Parareal method including Parareal methods with spatial coarsening, Krylov-subspace enhanced Parareal methods, hybrid spectral deferred correction methods, multigrid reduction in time (MGRIT), parallel exponential integrators (ParaExp) and others .\n\nDespite their success, there are various factors that affect the performance of the Parareal-type methods including nonlinearity and a type of PDE solved (e.g., its parallel efficiency significantly deteriorates for hyperbolic PDEs ), a choice of the coarse- and fine-grid solvers (e.g., the convergence rate significantly decreases for discrete operators with imaginary eigenvalues ), stiffness caused by input parameters (e.g., small values of the viscosity coefficient ), and others.\nOne of the main bottlenecks of the Parareal-type methods is the coarse grid correction step that is carried out sequentially, thus dramatically reducing the speedup of the overall algorithm.\n\nRecently, several attempts have been made to overcome this problem. One of the approaches is to combine the Parareal algorithm with a diagonalization technique introduced in , which is used to parallelize the coarse grid correction step . Another approach uses a spatially coarsened grid at the coarse grid correction step with the goal of reducing the computational cost as compared with that of the fine-grid propagator . Unfortunately, the parallel efficiency of these time-parallelization methods deteriorates dramatically if the governing equations are essentially nonlinear and convection-dominated.\n\nWe have recently developed a novel approach to parallelization of the implicit first-order backward difference (BDF1) scheme for unsteady nonlinear partial differential equations of arbitrary type in .\nThe global system of nonlinear discrete equations in the space-time domain is solved by the Newton method for all time levels simultaneously. This all-at-once system at each Newton iteration is block bidiagonal, which can be solved analytically by using the Gaussian elimination in a blockwise manner, thus leading to a set of fully decoupled equations associated with each time level. Note that the product matrices on the left- and right-hand sides of the all-at-once system of equations for all time levels can be efficiently computed in parallel, such that the number of operations performed by each computing core is linear in the number of spatial degrees of freedom.\nFurthermore, the computational cost of solving each block matrix is nearly identical to that of the sequential BDF1 scheme at each Newton iteration on each time step if the same direct solver is used for both systems of linear equations.\nThis allows for an efficient parallel-in-time implementation of the implicit BDF1 discretization for nonlinear differential equations. In contrast to the existing parallel-in-time algorithms, the proposed ParaDIn method preserves the quadratic rate of convergence of the Newton method of the corresponding sequential scheme. As has been shown in , some upper bound should be imposed\non the total number of time steps $N_t$ that can be integrated in parallel by using the ParaDIn method.\nThis constraint is due to the fact that the condition number of a product matrix on the left-hand side of the ParaDIn method increases together with the number of time steps. To circumvent this problem, we propose a novel strategy based on combining the ParaDIn\nmethod with the Parareal algorithm, so that both the coarse- and fine-grid correction steps are performed in parallel. Combining the ParaDIn and Parareal methods allow us to further increase the parallel efficiency of the ParaDIn method by a factor of 2-4 for 2-D nonlinear heat and Burgers equations with both smooth and discontinuous solutions.\n\nThe paper is organized as follows. In section 1, we present governing equations and the baseline BDF1 scheme. The ParaDIn method is briefly presented in section 3. Then, we discuss the block-Jacobi method and its limitations in section 4. We present the new combined ParaDIn-Parareal method and study its properties in sections 5-6. Numerical results demonstrating the parallel efficiency and scalability of the proposed method are presented in section 7 and conclusions are drawn in section 8.", "extraction_method": "main_file_section", "length": 6585, "success": true, "tex_filename": "paradin_parareal_arxiv.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.2727627986904799, "validated": false}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.31198494714104585, "validated": false}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.12735458301820451, "validated": false}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.1278930371244934, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2506.10820v1", "published": "2025-06-12T15:38:56Z", "scores": {"Diffusion_reasoning": 0.2727627986904799, "Distributed_training": 0.31198494714104585, "RLHF": 0.12735458301820451, "Weak_supervision": 0.1278930371244934}, "title": "A Combined Parallel-in-time Direct Inverse (ParaDIn)-Parareal Method for\n  Nonlinear Differential Equations", "updated": "2025-06-12T15:38:56Z", "url": "http://arxiv.org/abs/2506.10820v1"}, {"abstract": "The convergence of Artificial Intelligence (AI) and the Internet of Things\nhas accelerated the development of distributed, network-sensitive applications,\nnecessitating ultra-low latency, high throughput, and real-time processing\ncapabilities. While 5G networks represent a significant technological\nmilestone, their ability to support AI-driven edge applications remains\nconstrained by performance gaps observed in real-world deployments. This paper\naddresses these limitations and highlights critical advancements needed to\nrealize a robust and scalable 6G ecosystem optimized for AI applications.\nFurthermore, we conduct an empirical evaluation of 5G network infrastructure in\ncentral Europe, with latency measurements ranging from 61 ms to 110 ms across\ndifferent close geographical areas. These values exceed the requirements of\nlatency-critical AI applications by approximately 270%, revealing significant\nshortcomings in current deployments. Building on these findings, we propose a\nset of recommendations to bridge the gap between existing 5G performance and\nthe requirements of next-generation AI applications.", "arxiv_id": "2506.10570v1", "arxiv_url": "http://arxiv.org/abs/2506.10570v1", "author_h_indices": {"authors_found_count": 0, "authors_with_h_index_count": 0, "average_h_index": null, "highest_h_index": null, "individual_h_indices": [], "notable_authors_count": 0, "timestamp": "2025-07-09T20:52:31.230812", "total_authors": 6}, "authors": ["Kurt Horvath", "Shpresa Tuda", "Blerta Idrizi", "Stojan Kitanov", "Fisnik Doko", "Dragi Kimovski"], "categories": ["cs.DC (Distributed, Parallel, and Cluster Computing)"], "embedding_model": "openai-large", "highest_score": 0.310979806970613, "highest_score_topic": "Distributed_training", "introduction": {"content": "The proliferation of distributed, network-sensitive applications has surged dramatically in recent years, fueled by the convergence of Artificial Intelligence (AI) and the Internet of Things (IoT) . These advancements have relied heavily on mobile network technologies, with 5G representing the most recent milestone. However, as we approach the era of 6G, it is crucial to evaluate the real-world performance of 5G networks and their ability to support AI-driven edge applications effectively.\n\nAI-enabled applications, such as autonomous vehicles, immersive virtual and augmented reality, robotics, and IoT systems, demand ultra-low latency, high throughput, and real-time data processing . These requirements are critical for ensuring seamless operation and responsiveness. While 5G networks promise advancements like enhanced mobile broadband and ultra-reliable low-latency communication, practical deployment scenarios reveal performance gaps that hinder these applications\u0027 potential.\n\nA primary issue limiting the deployment of AI-driven edge applications is the inability of current 5G networks to consistently deliver the low-latency performance required. Although 5G technologies claim to provide latencies as low as 1-4 milliseconds, real-world benchmarks indicate significant discrepancies. For example, tests conducted in central Europe, including a 5G network in Klagenfurt, measured latencies ranging from 7 to 12 milliseconds when connecting to the Exoscale Cloud . These values fall short of the stringent thresholds for latency-critical applications like autonomous vehicle coordination or real-time robotics control.\n\nAdditionally, current 5G networks lack the robust integration of cloud and edge computing resources necessary to support scalable AI applications . Without dedicated, high-performance connections between the edge, core, and cloud, applications suffer from degraded performance, reduced scalability, and limited responsiveness. This infrastructural gap poses a significant roadblock to enabling the intelligent edge computing envisioned in the 6G era.\n\nThe main goal of this work is to bridge the gap between real-world 5G performance and the requirements of emerging AI applications. It highlights the essential advancements to enable a robust, intelligent, and scalable 6G ecosystem. Therefore, this paper addresses the critical gaps in supporting AI-driven applications at the edge by:\n\\begin{itemize}\n \\item Defining a set of characteristics and conducting a requirements analysis of the performance of the current communication networks for AI applications. This includes examining next-generation AI systems\u0027 latency, throughput, and scalability needed to inform the design of 6G technologies.\n \\item Conducting a detailed real-world evaluation of 5G infrastructure in central Europe, including latency, bandwidth, and scalability benchmarks, to identify the specific shortcomings of 5G networks in meeting the demands of AI-based IoT applications.\n \\item Providing deployment solutions for more efficient utilization of communication networks for AI services.\n\\end{itemize}\n\nThe paper has six sections. Section characterizes the 6G network infrastructures, while Section presents a formal application requirements analysis. Section presents a real-life evaluation of the currently available 5G networks and identifies their shortcomings. Section defines a set of recommendations for the future 6G networks for seamless supporting of AI applications on the Edge. We conclude the paper by summarizing the contributions in Section .", "extraction_method": "main_file_section", "length": 3572, "success": true, "tex_filename": "conference_101719.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.21807982278364726, "validated": false}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.310979806970613, "validated": false}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.22807692391839213, "validated": false}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.2147524989910522, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2506.10570v1", "published": "2025-06-12T10:59:08Z", "scores": {"Diffusion_reasoning": 0.21807982278364726, "Distributed_training": 0.310979806970613, "RLHF": 0.22807692391839213, "Weak_supervision": 0.2147524989910522}, "title": "6G Infrastructures for Edge AI: An Analytical Perspective", "updated": "2025-06-12T10:59:08Z", "url": "http://arxiv.org/abs/2506.10570v1"}, {"abstract": "Accurate classification of software bugs is essential for improving software\nquality. This paper presents a rule-based automated framework for classifying\nissues in quantum software repositories by bug type, category, severity, and\nimpacted quality attributes, with additional focus on quantum-specific bug\ntypes. The framework applies keyword and heuristic-based techniques tailored to\nquantum computing. To assess its reliability, we manually classified a\nstratified sample of 4,984 issues from a dataset of 12,910 issues across 36\nQiskit repositories. Automated classifications were compared with ground truth\nusing accuracy, precision, recall, and F1-score. The framework achieved up to\n85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393\n(quality attribute). Statistical validation via paired t-tests and Cohen\u0026#x27;s\nKappa showed substantial to almost perfect agreement for bug type (k = 0.696),\ncategory (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug\ntype (k = 0.712). Severity classification showed slight agreement (k = 0.162),\nsuggesting room for improvement. Large-scale analysis revealed that classical\nbugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug\ncategories included compatibility, functional, and quantum-specific defects,\nwhile usability, maintainability, and interoperability were the most impacted\nquality attributes. Most issues (93.7%) were low severity; only 4.3% were\ncritical. A detailed review of 1,550 quantum-specific bugs showed that over\nhalf involved quantum circuit-level problems, followed by gate errors and\nhardware-related issues.", "arxiv_id": "2506.10397v1", "arxiv_url": "http://arxiv.org/abs/2506.10397v1", "author_h_indices": {"authors_found_count": 0, "authors_with_h_index_count": 0, "average_h_index": null, "highest_h_index": null, "individual_h_indices": [], "notable_authors_count": 0, "timestamp": "2025-07-09T20:51:28.359945", "total_authors": 2}, "authors": ["Mir Mohammad Yousuf", "Shabir Ahmad Sofi"], "categories": ["cs.SE (Software Engineering)", "cs.CY (Computers and Society)", "cs.DC (Distributed, Parallel, and Cluster Computing)"], "embedding_model": "openai-large", "highest_score": 0.22031377646956254, "highest_score_topic": "Distributed_training", "introduction": {"content": "Quantum software engineering presents unique challenges that stem from the hybrid nature of quantum programs, which combine classical orchestration with quantum circuit execution. These challenges include managing the complexity of quantum operations, debugging under hardware noise, and dealing with limited observability in quantum states . As the quantum software ecosystem matures, understanding and managing software defects becomes essential to ensure reliability, maintainability, and performance in quantum applications.\nAutomated bug classification plays a vital role in software quality assurance by enabling systematic defect analysis, supporting large-scale empirical studies, and informing tooling and development practices . In classical software domains, both rule-based and machine learning-based classifiers have been employed to categorize issues by type, severity, and quality impact with considerable success . However, there is a significant gap in tools and techniques specifically designed to classify bugs in quantum software, where the nature and semantics of defects often differ markedly from classical counterparts.\nIn this study, we introduce a rule-based automated classification framework tailored to the quantum software domain. The framework classifies issues across five dimensions:\n\\begin{enumerate}\n \\item Bug type (quantum or classical).\n \\item Bug category (e.g., logical, syntax, quantum-specific).\n \\item Severity (critical, high, medium, low).\n \\item Impacted quality attribute (e.g., maintainability, reliability, performance).\n \\item Type of quantum-specific issue (e.g., quantum gate errors, measurement errors, transpilation issues).\n\\end{enumerate}\n\nThis system uses curated keyword lists and heuristic rules grounded in domain knowledge of quantum computing and common failure modes in quantum programs. We collected 12,910 issues from the 36 repositories of Qiskit and performed the classification. To evaluate its effectiveness, we conducted a large-scale manual annotation of 4,984 GitHub issues from 36 repositories within the Qiskit ecosystem. Issues were selected using random stratified sampling to ensure representation across severity levels and bug types within this specific framework, but not across different categories of quantum software. Each issue was manually labeled for bug type, category, severity, and impacted quality attribute, forming a ground truth dataset for benchmarking.\nWe assessed the framework\u2019s performance against this ground truth using standard metrics including accuracy, precision, recall, and F1-score, as well as inter-rater agreement via Cohen\u2019s Kappa. We also employed paired t-tests to compare manual and automated classifications statistically.\nOur findings demonstrate that the framework achieves substantial to near-perfect agreement ($\\kappa \u003e 0.69$) for bug type, category, and quality attribute classification. However, severity classification showed only slight agreement ($\\kappa = 0.162$), suggesting it is more difficult to infer severity heuristically. These results highlight the framework\u2019s utility for large-scale issue analysis in quantum software while also identifying areas for future enhancement.\n\nThis work offers a reproducible and interpretable approach to quantum bug classification and supports broader efforts to build robust quantum software analysis tools.\nContributions\nThis work makes the following key contributions toward improving automated bug classification in quantum software:\n\\begin{enumerate}\n \\item A Rule-Based Framework for Multi-Dimensional Bug Classification:\nWe propose a rule-based classification framework that automatically labels quantum software issues across five dimensions: bug type, bug category, severity level, impacted quality attribute, and quantum-specific subtype. The framework is designed to be interpretable, adaptable, and grounded in domain-specific keyword analysis.\n\\item A Manually Annotated Dataset of Quantum Software Issues:\nWe construct a manually labeled dataset comprising 4984 issues from 36 Qiskit repositories, annotated along all five classification axes. This dataset serves as a benchmark for evaluating bug classification techniques in the quantum domain.\n\n\\item Empirical Evaluation of Rule-Based Classification:\nWe evaluate the classification performance of our rule-based system using standard metrics (precision, recall, F1-score) against the annotated dataset. Additionally, we apply a paired t-test to assess the statistical significance of performance differences and use Cohen\u0027s kappa to measure inter-rater agreement between the rule-based predictions and the human annotations. The results demonstrate that rule-based approaches can deliver high accuracy, strong agreement with human annotations, and interpretability in domains with limited labeled data and strong domain-specific characteristics.\n\n\\end{enumerate}\nOverall structure of the paper\nThe paper is structured to provide a comprehensive exploration of a rule-based framework for automated classification of quantum software issues. Section 1 introduces the motivation and scope of the study, followed by a summary of the key contributions of the work. Section reviews the related work, positioning the paper within the broader context of research on software bug classification and quantum software engineering. Section\npresents the methodology, beginning with the process of dataset construction and manual annotation. It then outlines the multiple classification dimensions considered in this study, including classical vs. quantum classification, mapping of issues to bug categories, quality attributes, severity levels, and quantum-specific bug categories, while emphasizing transparency and reproducibility of the process.\n\nSection reports the results of the evaluation. It includes a comparison of manual and automated classifications through statistical agreement, and presents classification performance metrics. The section also explores the distribution of bug types, bug categories, quality attributes, severity levels, and quantum-specific bug categories, along with a summary of the key findings. Section offers the conclusion, summarizing the primary insights and outcomes of the study. Section discusses directions for future work to extend and refine the framework. Finally, Section addresses threats to validity, covering internal, external, and construct validity to assess the robustness and generalizability of the study\u0027s findings.", "extraction_method": "main_file_section", "length": 6494, "success": true, "tex_filename": "template.tex"}, "llm_validation": {"Diffusion_reasoning": {"reason": "below_threshold", "similarity_score": 0.2187273386863689, "validated": false}, "Distributed_training": {"reason": "below_threshold", "similarity_score": 0.22031377646956254, "validated": false}, "RLHF": {"reason": "below_threshold", "similarity_score": 0.20153207213322474, "validated": false}, "Weak_supervision": {"reason": "below_threshold", "similarity_score": 0.2109422650723228, "validated": false}}, "pdf_url": "http://arxiv.org/pdf/2506.10397v1", "published": "2025-06-12T06:42:10Z", "scores": {"Diffusion_reasoning": 0.2187273386863689, "Distributed_training": 0.22031377646956254, "RLHF": 0.20153207213322474, "Weak_supervision": 0.2109422650723228}, "title": "Bug Classification in Quantum Software: A Rule-Based Framework and Its\n  Evaluation", "updated": "2025-06-12T06:42:10Z", "url": "http://arxiv.org/abs/2506.10397v1"}];
        let filteredPapers = [...papers];
        
        // DOM elements
        const sortSelect = document.getElementById('sortBy');
        const topicCheckboxes = document.querySelectorAll('.topic-checkbox');
        const llmFilterCheckboxes = document.querySelectorAll('.llm-filter-checkbox');
        const hIndexFilterCheckboxes = document.querySelectorAll('.h-index-filter-checkbox');
        const minScoreInput = document.getElementById('minScore');
        const maxScoreInput = document.getElementById('maxScore');
        const resetButton = document.getElementById('resetFilters');
        const papersContainer = document.getElementById('papers-container');
        const paperCountElement = document.getElementById('paper-count');
        const filterCountElement = document.getElementById('filter-count');
        
        // Get effective highest score for a paper based on selected topics
        function getEffectiveScore(paper, selectedTopics, minScore, maxScore) {
            if (!paper.scores) return 0;
            
            if (selectedTopics.length === 0) {
                // No topics selected, use overall highest score
                return paper.highest_score || 0;
            } else {
                // Topics selected, find highest score among selected topics that meet min/max threshold
                const selectedTopicScores = selectedTopics
                    .map(topic => paper.scores[topic] || 0)
                    .filter(score => score >= minScore && score <= maxScore);
                
                return selectedTopicScores.length > 0 ? Math.max(...selectedTopicScores) : 0;
            }
        }

        // Sort function
        function sortPapers(papers, sortBy, selectedTopics, minScore, maxScore) {
            return papers.sort((a, b) => {
                switch(sortBy) {
                    case 'overall_score_desc':
                        const overallA = a.scores_data?.overall_priority ?? 0;
                        const overallB = b.scores_data?.overall_priority ?? 0;
                        return overallB - overallA;
                    case 'overall_score_asc':
                        const overallA2 = a.scores_data?.overall_priority ?? 0;
                        const overallB2 = b.scores_data?.overall_priority ?? 0;
                        return overallA2 - overallB2;
                    case 'similarity_desc':
                        const scoreA = getEffectiveScore(a, selectedTopics, minScore, maxScore);
                        const scoreB = getEffectiveScore(b, selectedTopics, minScore, maxScore);
                        return scoreB - scoreA;
                    case 'similarity_asc':
                        const scoreA2 = getEffectiveScore(a, selectedTopics, minScore, maxScore);
                        const scoreB2 = getEffectiveScore(b, selectedTopics, minScore, maxScore);
                        return scoreA2 - scoreB2;
                    case 'title':
                        return a.title.localeCompare(b.title);
                    case 'arxiv_id':
                        return (a.arxiv_id || '').localeCompare(b.arxiv_id || '');
                    case 'max_h_index_desc':
                        const maxHA = a.author_h_indices?.highest_h_index ?? -1;
                        const maxHB = b.author_h_indices?.highest_h_index ?? -1;
                        return maxHB - maxHA;
                    case 'max_h_index_asc':
                        const maxHA2 = a.author_h_indices?.highest_h_index ?? 999999;
                        const maxHB2 = b.author_h_indices?.highest_h_index ?? 999999;
                        return maxHA2 - maxHB2;
                    case 'avg_h_index_desc':
                        const avgHA = a.author_h_indices?.average_h_index ?? -1;
                        const avgHB = b.author_h_indices?.average_h_index ?? -1;
                        return avgHB - avgHA;
                    case 'avg_h_index_asc':
                        const avgHA2 = a.author_h_indices?.average_h_index ?? 999999;
                        const avgHB2 = b.author_h_indices?.average_h_index ?? 999999;
                        return avgHA2 - avgHB2;
                    default:
                        return 0;
                }
            });
        }
        
        // Get selected topics
        function getSelectedTopics() {
            return Array.from(topicCheckboxes)
                .filter(checkbox => checkbox.checked)
                .map(checkbox => checkbox.value);
        }
        
        // Get selected LLM validation filters
        function getSelectedLLMFilters() {
            return Array.from(llmFilterCheckboxes)
                .filter(checkbox => checkbox.checked)
                .map(checkbox => checkbox.value);
        }
        
        // Get selected H-Index filters
        function getSelectedHIndexFilters() {
            return Array.from(hIndexFilterCheckboxes)
                .filter(checkbox => checkbox.checked)
                .map(checkbox => checkbox.value);
        }
        
        // Check if paper meets LLM validation criteria
        function passesLLMValidation(paper, selectedTopics, selectedLLMFilters) {
            if (selectedLLMFilters.length === 0) return true; // No LLM filter applied
            
            const llmVal = paper.llm_validation;
            if (!llmVal) return false;
            
            // Determine which topics to check
            const topicsToCheck = selectedTopics.length > 0 ? selectedTopics : 
                ['RLHF', 'Weak_supervision', 'Diffusion_reasoning', 'Distributed_training'];
            
            // Check if any of the topics meet the LLM filter criteria
            return topicsToCheck.some(topic => {
                const topicData = llmVal[topic];
                if (!topicData) return false;
                
                return selectedLLMFilters.some(filter => {
                    switch(filter) {
                        case 'yes':
                            return topicData.validated && topicData.llm_relevant === 'yes';
                        case 'no':
                            return topicData.validated && topicData.llm_relevant === 'no';
                        case 'not_validated':
                            return !topicData.validated;
                        default:
                            return false;
                    }
                });
            });
        }
        
        // Check if paper meets H-Index criteria
        function passesHIndexValidation(paper, selectedHIndexFilters) {
            if (selectedHIndexFilters.length === 0) return true; // No H-Index filter applied
            
            const hIndices = paper.author_h_indices;
            if (!hIndices) {
                return selectedHIndexFilters.includes('none');
            }
            
            const totalAuthors = hIndices.total_authors || 0;
            const authorsWithHIndex = hIndices.authors_with_h_index_count || 0;
            
            return selectedHIndexFilters.some(filter => {
                switch(filter) {
                    case 'full':
                        return totalAuthors > 0 && authorsWithHIndex === totalAuthors;
                    case 'partial':
                        return authorsWithHIndex > 0 && authorsWithHIndex < totalAuthors;
                    case 'none':
                        return authorsWithHIndex === 0 || !hIndices.success;
                    default:
                        return false;
                }
            });
        }
        
        // Filter function with LLM validation and H-Index support
        function filterPapers(papers, selectedTopics, selectedLLMFilters, selectedHIndexFilters, minScore, maxScore) {
            return papers.filter(paper => {
                if (!paper.scores) return false;
                
                // Check if paper has at least one topic score within min/max range
                const validTopicScores = Object.entries(paper.scores)
                    .filter(([topic, score]) => score >= minScore && score <= maxScore);
                
                // If no topic scores meet the range, hide the paper
                if (validTopicScores.length === 0) {
                    return false;
                }
                
                // If specific topics are selected, check if paper has valid scores for those topics
                if (selectedTopics.length > 0) {
                    const hasValidSelectedTopic = selectedTopics.some(topic => 
                        paper.scores[topic] >= minScore && paper.scores[topic] <= maxScore
                    );
                    if (!hasValidSelectedTopic) {
                        return false;
                    }
                }
                
                // Check LLM validation filter
                if (!passesLLMValidation(paper, selectedTopics, selectedLLMFilters)) {
                    return false;
                }
                
                // Check H-Index validation filter
                if (!passesHIndexValidation(paper, selectedHIndexFilters)) {
                    return false;
                }
                
                return true;
            });
        }
        
        // Render papers
        function renderPapers() {
            const sortBy = sortSelect.value;
            const selectedTopics = getSelectedTopics();
            const selectedLLMFilters = getSelectedLLMFilters();
            const selectedHIndexFilters = getSelectedHIndexFilters();
            const minScore = parseFloat(minScoreInput.value) || 0;
            const maxScore = parseFloat(maxScoreInput.value) || 1;
            
            // Filter and sort
            filteredPapers = filterPapers([...papers], selectedTopics, selectedLLMFilters, selectedHIndexFilters, minScore, maxScore);
            filteredPapers = sortPapers(filteredPapers, sortBy, selectedTopics, minScore, maxScore);
            
            // Update paper count
            paperCountElement.textContent = `${filteredPapers.length} papers`;
            
            // Update filter count
            if (filterCountElement) {
                filterCountElement.textContent = `Showing ${filteredPapers.length}/${papers.length} papers`;
            }
            
            // Hide all paper cards first
            const paperCards = document.querySelectorAll('.paper-card');
            paperCards.forEach(card => card.classList.add('hidden'));
            
            // Create a document fragment to reorder the cards
            const fragment = document.createDocumentFragment();
            
            filteredPapers.forEach((paper, index) => {
                const originalIndex = papers.findIndex(p => p.arxiv_id === paper.arxiv_id);
                const card = document.querySelector(`[data-paper-index="${originalIndex}"]`);
                if (card) {
                    card.classList.remove('hidden');
                    // Update paper number
                    const numberElement = card.querySelector('.paper-number');
                    if (numberElement) {
                        numberElement.textContent = `#${index + 1}`;
                    }
                    
                    // Update similarity scores visibility and toggle state
                    updateTopicVisibility(card, selectedTopics, minScore, maxScore, 'similarity');
                    
                    // Update LLM validation visibility and toggle state  
                    updateTopicVisibility(card, selectedTopics, minScore, maxScore, 'llm-validation');
                    
                    // Add the card to the fragment in the correct order
                    fragment.appendChild(card);
                }
            });
            
            // Append the reordered cards back to the container
            papersContainer.appendChild(fragment);
        }
        
        // Function to update topic visibility based on filters
        function updateTopicVisibility(card, selectedTopics, minScore, maxScore, type) {
            if (type === 'similarity') {
                // Check if we're in normalized mode
                const summaryContainer = card.querySelector('.similarity-summary');
                const isNormalizedMode = summaryContainer && summaryContainer.classList.contains('normalized');
                
                // Skip filtering in normalized mode
                if (isNormalizedMode) {
                    return;
                }
                
                // Handle similarity scores with new grid structure
                const labels = card.querySelectorAll('.similarity-label');
                const rightColumns = card.querySelectorAll('.similarity-right-column');
                const content = card.querySelector('.similarity-scores-content');
                
                // Clear any existing separators first
                content.querySelectorAll('.topic-separator').forEach(sep => sep.remove());
                
                let totalValidTopics = 0;
                let currentlyVisibleTopics = 0;
                
                // First pass: determine what topics are valid (meet score criteria)
                const validTopics = [];
                labels.forEach((label, index) => {
                    const topic = label.getAttribute('data-topic');
                    const rightColumn = rightColumns[index];
                    const scoreElement = rightColumn.querySelector('.similarity-value');
                    const originalScore = parseFloat(scoreElement.getAttribute('data-original-score') || scoreElement.textContent);
                    
                    if (originalScore >= minScore && originalScore <= maxScore) {
                        validTopics.push({ topic, label, rightColumn, index });
                        totalValidTopics++;
                    } else {
                        // Hide invalid topics
                        label.style.display = 'none';
                        rightColumn.style.display = 'none';
                    }
                });
                
                // Second pass: show/hide valid topics based on selection
                validTopics.forEach(({ topic, label, rightColumn }) => {
                    const isSelected = selectedTopics.length === 0 || selectedTopics.includes(topic);
                    
                    if (isSelected) {
                        label.style.display = 'block';
                        rightColumn.style.display = 'flex';
                        currentlyVisibleTopics++;
                    } else {
                        label.style.display = 'none';
                        rightColumn.style.display = 'none';
                    }
                });
                
                // Update toggle button visibility
                const toggleButton = card.querySelector('.similarity-toggle');
                const normalizeButton = card.querySelector('.similarity-normalize-toggle');
                const toggleSection = card.querySelector('.similarity-toggle-section');
                if (toggleButton && toggleSection) {
                    // Show section if there are buttons to display
                    if (totalValidTopics > currentlyVisibleTopics || normalizeButton) {
                        toggleSection.style.display = 'flex';
                        if (totalValidTopics > currentlyVisibleTopics) {
                            toggleButton.style.display = 'inline-block';
                            toggleButton.textContent = 'Show other topics ‚ñº';
                        } else {
                            toggleButton.style.display = 'none';
                        }
                    } else {
                        toggleSection.style.display = 'none';
                    }
                }
            } else {
                // Handle LLM validation
                const items = card.querySelectorAll('.llm-validation-item');
                const content = card.querySelector('.llm-validation-summary');
                
                // Clear any existing separators first
                content.querySelectorAll('.topic-separator').forEach(sep => sep.remove());
                
                let totalTopics = items.length;
                let currentlyVisibleTopics = 0;
                
                items.forEach(item => {
                    const topic = item.getAttribute('data-topic');
                    const isSelected = selectedTopics.length === 0 || selectedTopics.includes(topic);
                    
                    if (isSelected) {
                        item.style.display = 'flex';
                        currentlyVisibleTopics++;
                    } else {
                        item.style.display = 'none';
                    }
                });
                
                // Update toggle button visibility
                const toggleButton = card.querySelector('.llm-validation-toggle');
                const buttonsRow = card.querySelector('.llm-buttons-row');
                if (toggleButton && buttonsRow) {
                    const hasJustifications = card.querySelector('.llm-toggle');
                    
                    // Show buttons row if there are hidden topics OR justifications
                    if (totalTopics > currentlyVisibleTopics || hasJustifications) {
                        buttonsRow.style.display = 'flex';
                        toggleButton.style.display = (totalTopics > currentlyVisibleTopics) ? 'inline-block' : 'none';
                        if (totalTopics > currentlyVisibleTopics) {
                            toggleButton.textContent = 'Show other topics ‚ñº';
                        }
                    } else {
                        buttonsRow.style.display = 'none';
                    }
                }
            }
        }
        
        // Event listeners
        sortSelect.addEventListener('change', renderPapers);
        topicCheckboxes.forEach(checkbox => {
            checkbox.addEventListener('change', renderPapers);
        });
        llmFilterCheckboxes.forEach(checkbox => {
            checkbox.addEventListener('change', renderPapers);
        });
        hIndexFilterCheckboxes.forEach(checkbox => {
            checkbox.addEventListener('change', renderPapers);
        });
        minScoreInput.addEventListener('input', renderPapers);
        maxScoreInput.addEventListener('input', renderPapers);
        
        resetButton.addEventListener('click', () => {
            sortSelect.value = 'overall_score_desc';
            topicCheckboxes.forEach(checkbox => {
                checkbox.checked = false;
            });
            llmFilterCheckboxes.forEach(checkbox => {
                checkbox.checked = false;
            });
            hIndexFilterCheckboxes.forEach(checkbox => {
                checkbox.checked = false;
            });
            minScoreInput.value = '0';
            maxScoreInput.value = '1';
            renderPapers();
        });
        
        // Initial render
        renderPapers();
        
        // Normalized scores toggle functionality
        function toggleNormalizedScores(button) {
            const card = button.closest('.paper-card');
            const content = card.querySelector('.similarity-scores-content');
            const summaryContainer = card.querySelector('.similarity-summary');
            const currentMode = button.getAttribute('data-mode');
            const showHideButton = card.querySelector('.similarity-toggle');
            
            if (currentMode === 'raw') {
                // Switch to normalized mode
                switchToNormalizedMode(card, content, summaryContainer, button, showHideButton);
            } else {
                // Switch back to raw mode
                switchToRawMode(card, content, summaryContainer, button, showHideButton);
            }
        }
        
        function switchToNormalizedMode(card, content, summaryContainer, normalizeButton, showHideButton) {
            // 1. Show ALL topics and collect scores
            const allLabels = content.querySelectorAll('.similarity-label');
            const allRightColumns = content.querySelectorAll('.similarity-right-column');
            const allScores = [];
            
            // Collect all raw scores
            allLabels.forEach((label, index) => {
                const rightColumn = allRightColumns[index];
                const scoreElement = rightColumn.querySelector('.similarity-value');
                const originalScore = parseFloat(scoreElement.getAttribute('data-original-score'));
                allScores.push(originalScore);
                
                // Show all elements
                label.style.display = 'block';
                rightColumn.style.display = 'flex';
            });
            
            // 2. Calculate normalized scores with threshold
            const normalizedScores = calculateNormalizedScores(allScores);
            
            // 3. Update display
            allLabels.forEach((label, index) => {
                const rightColumn = allRightColumns[index];
                const scoreElement = rightColumn.querySelector('.similarity-value');
                const normalizedScore = normalizedScores[index];
                
                if (normalizedScore === 0) {
                    scoreElement.textContent = '0%';
                } else {
                    scoreElement.textContent = normalizedScore.toFixed(1) + '%';
                }
            });
            
            // 4. Update UI state
            summaryContainer.classList.add('normalized');
            normalizeButton.classList.add('active');
            normalizeButton.setAttribute('data-mode', 'normalized');
            normalizeButton.textContent = 'Show raw scores';
            
            // 5. Hide show/hide button
            if (showHideButton) {
                showHideButton.style.display = 'none';
            }
            
            // 6. Create topic separator like in expanded mode
            content.querySelectorAll('.topic-separator').forEach(sep => sep.remove());
            
            // Get current topic filters to determine selected vs non-selected
            const selectedTopics = getSelectedTopics();
            const selectedLabels = [];
            const nonSelectedLabels = [];
            
            allLabels.forEach((label, index) => {
                const topic = label.getAttribute('data-topic');
                const isSelected = selectedTopics.length === 0 || selectedTopics.includes(topic);
                if (isSelected) {
                    selectedLabels.push({ label, rightColumn: allRightColumns[index] });
                } else {
                    nonSelectedLabels.push({ label, rightColumn: allRightColumns[index] });
                }
            });
            
            // Add separator if there are both selected and non-selected topics
            if (selectedLabels.length > 0 && nonSelectedLabels.length > 0) {
                // Find the position to insert separator (after last selected topic)
                const lastSelectedIndex = Array.from(content.children).indexOf(selectedLabels[selectedLabels.length - 1].rightColumn);
                
                const separator = document.createElement('div');
                separator.className = 'topic-separator';
                separator.style.gridColumn = '1 / -1'; // Span both columns
                
                // Insert separator after the last selected topic
                if (lastSelectedIndex >= 0 && lastSelectedIndex + 1 < content.children.length) {
                    content.insertBefore(separator, content.children[lastSelectedIndex + 1]);
                }
            }
        }
        
        function switchToRawMode(card, content, summaryContainer, normalizeButton, showHideButton) {
            const allLabels = content.querySelectorAll('.similarity-label');
            const allRightColumns = content.querySelectorAll('.similarity-right-column');
            
            // 1. Restore original scores
            allLabels.forEach((label, index) => {
                const rightColumn = allRightColumns[index];
                const scoreElement = rightColumn.querySelector('.similarity-value');
                const originalScore = scoreElement.getAttribute('data-original-score');
                scoreElement.textContent = originalScore;
            });
            
            // 2. Update UI state
            summaryContainer.classList.remove('normalized');
            normalizeButton.classList.remove('active');
            normalizeButton.setAttribute('data-mode', 'raw');
            normalizeButton.textContent = 'Show normalized scores';
            
            // 3. Show show/hide button
            if (showHideButton) {
                showHideButton.style.display = 'inline-block';
            }
            
            // 4. Reapply current filters
            const selectedTopics = getSelectedTopics();
            const minScore = parseFloat(minScoreInput.value) || 0;
            const maxScore = parseFloat(maxScoreInput.value) || 1;
            updateTopicVisibility(card, selectedTopics, minScore, maxScore, 'similarity');
        }
        
        function calculateNormalizedScores(rawScores) {
            const MIN_SCORE_THRESHOLD = 0.25; // Based on data analysis
            const EPSILON = 1e-10; // Prevent division by zero
            
            // Step 1: Handle negative scores - clamp to 0
            const clampedScores = rawScores.map(score => Math.max(0, score));
            
            // Step 2: Apply minimum threshold - scores below 0.25 become 0
            const thresholdedScores = clampedScores.map(score => 
                score < MIN_SCORE_THRESHOLD ? 0 : score
            );
            
            // Step 3: Calculate sum with epsilon to prevent division by zero
            const sum = thresholdedScores.reduce((a, b) => a + b, 0) + EPSILON;
            
            // Step 4: Normalize only non-zero scores
            const normalizedScores = thresholdedScores.map(score => {
                if (score === 0) return 0;
                return (score / sum) * 100;
            });
            
            return normalizedScores;
        }
        
        // Similarity scores toggle functionality
        function toggleSimilarityDetails(button) {
            const card = button.closest('.paper-card');
            const content = card.querySelector('.similarity-scores-content');
            const summaryContainer = card.querySelector('.similarity-summary');
            
            // Don't allow toggling in normalized mode
            if (summaryContainer && summaryContainer.classList.contains('normalized')) {
                return;
            }
            
            const selectedTopics = getSelectedTopics();
            const minScore = parseFloat(minScoreInput.value) || 0;
            const maxScore = parseFloat(maxScoreInput.value) || 1;
            
            const allLabels = content.querySelectorAll('.similarity-label');
            const allRightColumns = content.querySelectorAll('.similarity-right-column');
            const isExpanded = button.textContent.includes('Hide');
            
            // Remove any existing separators
            content.querySelectorAll('.topic-separator').forEach(sep => sep.remove());
            
            if (isExpanded) {
                // Collapse - show only selected topics (or all if none selected)
                allLabels.forEach((label, index) => {
                    const topic = label.getAttribute('data-topic');
                    const rightColumn = allRightColumns[index];
                    const scoreElement = rightColumn.querySelector('.similarity-value');
                    const scoreValue = parseFloat(scoreElement.getAttribute('data-original-score') || scoreElement.textContent);
                    
                    // Skip topics that don't meet score criteria
                    if (scoreValue < minScore || scoreValue > maxScore) {
                        label.style.display = 'none';
                        rightColumn.style.display = 'none';
                        return;
                    }
                    
                    // Show only selected topics (or all if none selected)
                    const isSelected = selectedTopics.length === 0 || selectedTopics.includes(topic);
                    if (isSelected) {
                        label.style.display = 'block';
                        rightColumn.style.display = 'flex';
                    } else {
                        label.style.display = 'none';
                        rightColumn.style.display = 'none';
                    }
                });
                button.textContent = 'Show other topics ‚ñº';
            } else {
                // Expand - show selected topics first, then separator, then non-selected topics
                
                // Collect all valid topics with their data
                const validTopics = [];
                allLabels.forEach((label, index) => {
                    const topic = label.getAttribute('data-topic');
                    const rightColumn = allRightColumns[index];
                    const scoreElement = rightColumn.querySelector('.similarity-value');
                    const scoreValue = parseFloat(scoreElement.getAttribute('data-original-score') || scoreElement.textContent);
                    
                    if (scoreValue >= minScore && scoreValue <= maxScore) {
                        const isSelected = selectedTopics.length === 0 || selectedTopics.includes(topic);
                        validTopics.push({
                            topic,
                            label,
                            rightColumn,
                            index,
                            isSelected,
                            scoreValue
                        });
                    }
                });
                
                // Separate selected and non-selected topics (preserve original order within each group)
                const selectedValidTopics = validTopics.filter(t => t.isSelected);
                const nonSelectedValidTopics = validTopics.filter(t => !t.isSelected);
                
                // Clear the content and rebuild it in the correct order
                content.innerHTML = '';
                
                // Add selected topics first
                selectedValidTopics.forEach(({ topic, scoreValue }) => {
                    const labelSpan = document.createElement('span');
                    labelSpan.className = 'similarity-label';
                    labelSpan.setAttribute('data-topic', topic);
                    labelSpan.textContent = topic.replace('_', ' ') + ':';
                    
                    const rightDiv = document.createElement('div');
                    rightDiv.className = 'similarity-right-column';
                    rightDiv.setAttribute('data-topic', topic);
                    
                    const barDiv = document.createElement('div');
                    barDiv.className = 'similarity-bar';
                    
                    const fillDiv = document.createElement('div');
                    fillDiv.className = 'similarity-bar-fill';
                    fillDiv.style.width = `${(scoreValue * 100).toFixed(1)}%`;
                    
                    const valueSpan = document.createElement('span');
                    valueSpan.className = 'similarity-value';
                    valueSpan.setAttribute('data-original-score', scoreValue.toFixed(3));
                    valueSpan.textContent = scoreValue.toFixed(3);
                    
                    barDiv.appendChild(fillDiv);
                    rightDiv.appendChild(barDiv);
                    rightDiv.appendChild(valueSpan);
                    
                    content.appendChild(labelSpan);
                    content.appendChild(rightDiv);
                });
                
                // Add separator if there are both selected and non-selected topics
                if (selectedValidTopics.length > 0 && nonSelectedValidTopics.length > 0) {
                    const separator = document.createElement('div');
                    separator.className = 'topic-separator';
                    separator.style.gridColumn = '1 / -1'; // Span both columns
                    content.appendChild(separator);
                }
                
                // Add non-selected topics after separator
                nonSelectedValidTopics.forEach(({ topic, scoreValue }) => {
                    const labelSpan = document.createElement('span');
                    labelSpan.className = 'similarity-label';
                    labelSpan.setAttribute('data-topic', topic);
                    labelSpan.textContent = topic.replace('_', ' ') + ':';
                    
                    const rightDiv = document.createElement('div');
                    rightDiv.className = 'similarity-right-column';
                    rightDiv.setAttribute('data-topic', topic);
                    
                    const barDiv = document.createElement('div');
                    barDiv.className = 'similarity-bar';
                    
                    const fillDiv = document.createElement('div');
                    fillDiv.className = 'similarity-bar-fill';
                    fillDiv.style.width = `${(scoreValue * 100).toFixed(1)}%`;
                    
                    const valueSpan = document.createElement('span');
                    valueSpan.className = 'similarity-value';
                    valueSpan.setAttribute('data-original-score', scoreValue.toFixed(3));
                    valueSpan.textContent = scoreValue.toFixed(3);
                    
                    barDiv.appendChild(fillDiv);
                    rightDiv.appendChild(barDiv);
                    rightDiv.appendChild(valueSpan);
                    
                    content.appendChild(labelSpan);
                    content.appendChild(rightDiv);
                });
                
                button.textContent = 'Hide other topics ‚ñ≤';
            }
        }
        
        // LLM validation toggle functionality  
        function toggleLLMValidationDetails(button) {
            const card = button.closest('.paper-card');
            const content = card.querySelector('.llm-validation-summary');
            const selectedTopics = getSelectedTopics();
            
            const allItems = content.querySelectorAll('.llm-validation-item');
            const isExpanded = button.textContent.includes('Hide');
            
            // Remove any existing separators
            content.querySelectorAll('.topic-separator').forEach(sep => sep.remove());
            
            if (isExpanded) {
                // Collapse - show only selected topics (or all if none selected)
                allItems.forEach(item => {
                    const topic = item.getAttribute('data-topic');
                    const isSelected = selectedTopics.length === 0 || selectedTopics.includes(topic);
                    
                    if (isSelected) {
                        item.style.display = 'flex';
                    } else {
                        item.style.display = 'none';
                    }
                });
                button.textContent = 'Show other topics ‚ñº';
            } else {
                // Expand - show selected topics first, then separator, then non-selected topics
                
                // Collect all topics with their data
                const allTopics = [];
                allItems.forEach((item, index) => {
                    const topic = item.getAttribute('data-topic');
                    const isSelected = selectedTopics.length === 0 || selectedTopics.includes(topic);
                    const topicLabel = item.querySelector('.llm-topic-label').textContent;
                    const statusElement = item.querySelector('.llm-status');
                    const statusText = statusElement.textContent;
                    const statusClass = statusElement.className;
                    
                    allTopics.push({
                        topic,
                        item,
                        index,
                        isSelected,
                        topicLabel,
                        statusText,
                        statusClass
                    });
                });
                
                // Separate selected and non-selected topics (preserve original order within each group)
                const selectedTopics_items = allTopics.filter(t => t.isSelected);
                const nonSelectedTopics_items = allTopics.filter(t => !t.isSelected);
                
                // Remove only the topic items, preserve buttons and details
                const buttonsRow = content.querySelector('.llm-buttons-row');
                const llmDetails = content.querySelector('.llm-details');
                
                // Remove all topic items
                const topicItems = content.querySelectorAll('.llm-validation-item');
                topicItems.forEach(item => item.remove());
                
                // Remove any existing separators
                const separators = content.querySelectorAll('.topic-separator');
                separators.forEach(sep => sep.remove());
                
                // Add selected topics first
                selectedTopics_items.forEach(({ topic, topicLabel, statusText, statusClass }) => {
                    const itemDiv = document.createElement('div');
                    itemDiv.className = 'llm-validation-item';
                    itemDiv.setAttribute('data-topic', topic);
                    
                    const labelSpan = document.createElement('span');
                    labelSpan.className = 'llm-topic-label';
                    labelSpan.textContent = topicLabel;
                    
                    const statusSpan = document.createElement('span');
                    statusSpan.className = statusClass;
                    statusSpan.textContent = statusText;
                    
                    itemDiv.appendChild(labelSpan);
                    itemDiv.appendChild(statusSpan);
                    
                    // Insert before buttons row if it exists, otherwise just append
                    if (buttonsRow) {
                        content.insertBefore(itemDiv, buttonsRow);
                    } else {
                        content.appendChild(itemDiv);
                    }
                });
                
                // Add separator if there are both selected and non-selected topics
                if (selectedTopics_items.length > 0 && nonSelectedTopics_items.length > 0) {
                    const separator = document.createElement('div');
                    separator.className = 'topic-separator';
                    
                    // Insert before buttons row if it exists, otherwise just append
                    if (buttonsRow) {
                        content.insertBefore(separator, buttonsRow);
                    } else {
                        content.appendChild(separator);
                    }
                }
                
                // Add non-selected topics after separator
                nonSelectedTopics_items.forEach(({ topic, topicLabel, statusText, statusClass }) => {
                    const itemDiv = document.createElement('div');
                    itemDiv.className = 'llm-validation-item';
                    itemDiv.setAttribute('data-topic', topic);
                    
                    const labelSpan = document.createElement('span');
                    labelSpan.className = 'llm-topic-label';
                    labelSpan.textContent = topicLabel;
                    
                    const statusSpan = document.createElement('span');
                    statusSpan.className = statusClass;
                    statusSpan.textContent = statusText;
                    
                    itemDiv.appendChild(labelSpan);
                    itemDiv.appendChild(statusSpan);
                    
                    // Insert before buttons row if it exists, otherwise just append
                    if (buttonsRow) {
                        content.insertBefore(itemDiv, buttonsRow);
                    } else {
                        content.appendChild(itemDiv);
                    }
                });
                
                button.textContent = 'Hide other topics ‚ñ≤';
            }
        }
        
        // H-index toggle functionality
        function toggleHIndexDetails(button) {
            const details = button.parentNode.querySelector('.h-index-details');
            if (details.style.display === 'none' || details.style.display === '') {
                details.style.display = 'block';
                button.textContent = 'Hide individual H-indices ‚ñ≤';
            } else {
                details.style.display = 'none';
                button.textContent = 'Show individual H-indices ‚ñº';
            }
        }
        
        // LLM justification toggle functionality
        function toggleLLMDetails(button) {
            const card = button.closest('.paper-card');
            const details = card.querySelector('.llm-details');
            if (details.style.display === 'none' || details.style.display === '') {
                details.style.display = 'block';
                button.textContent = 'Hide justifications ‚ñ≤';
            } else {
                details.style.display = 'none';
                button.textContent = 'Show justifications ‚ñº';
            }
        }
    </script>
    
    <!-- KaTeX auto-render initialization -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false}
                ],
                throwOnError: false,
                errorColor: 'inherit',
                strict: false,
                trust: false
            });
        });
    </script>
</body>
</html> 