<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 05 September 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 05 September 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 05 September 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2509.04716",
      "title": "KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced\n  Question Answering",
      "authors": [
        "Yushi Sun",
        "Kai Sun",
        "Yifan Ethan Xu",
        "Xiao Yang",
        "Xin Luna Dong",
        "Nan Tang",
        "Lei Chen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucination in Large\nLanguage Models (LLMs) by incorporating external data, with Knowledge Graphs\n(KGs) offering crucial information for question answering. Traditional\nKnowledge Graph Question Answering (KGQA) methods rely on semantic parsing,\nwhich typically retrieves knowledge strictly necessary for answer generation,\nthus often suffer from low coverage due to rigid schema requirements and\nsemantic ambiguity. We present KERAG, a novel KG-based RAG pipeline that\nenhances QA coverage by retrieving a broader subgraph likely to contain\nrelevant information. Our retrieval-filtering-summarization approach, combined\nwith fine-tuned LLMs for Chain-of-Thought reasoning on knowledge sub-graphs,\nreduces noises and improves QA for both simple and complex questions.\nExperiments demonstrate that KERAG surpasses state-of-the-art solutions by\nabout 7% in quality and exceeds GPT-4o (Tool) by 10-21%.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04716v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04716v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.324,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces KERAG, a Knowledge-Enhanced Retrieval-Augmented Generation pipeline that uses retrieval, filtering, and summarization with Chain-of-Thought reasoning in LLMs for question answering on Knowledge Graphs. While it employs Chain-of-Thought for step-by-step reasoning, there is no mention of diffusion models, iterative refinement processes, or adapting diffusion techniques for logical tasks. Thus, it does not align with the topic's focus on diffusion-based methods for holistic reasoning path correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04719",
      "title": "STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous\n  GPUs",
      "authors": [
        "Han Liang",
        "Jiahui Zhou",
        "Zicheng Zhou",
        "Xiaoxi Zhang",
        "Xu Chen"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The escalating adoption of diffusion models for applications such as image\ngeneration demands efficient parallel inference techniques to manage their\nsubstantial computational cost. However, existing diffusion parallelism\ninference schemes often underutilize resources in heterogeneous multi-GPU\nenvironments, where varying hardware capabilities or background tasks cause\nworkload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion\nInference (STADI), a novel framework to accelerate diffusion model inference in\nsuch settings. At its core is a hybrid scheduler that orchestrates fine-grained\nparallelism across both temporal and spatial dimensions. Temporally, STADI\nintroduces a novel computation-aware step allocator applied after warmup\nphases, using a least-common-multiple-minimizing quantization technique to\nreduce denoising steps on slower GPUs and execution synchronization. To further\nminimize GPU idle periods, STADI executes an elastic patch parallelism\nmechanism that allocates variably sized image patches to GPUs according to\ntheir computational capability, ensuring balanced workload distribution through\na complementary spatial mechanism. Extensive experiments on both\nload-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,\ndemonstrating improved load balancing and mitigation of performance\nbottlenecks. Compared to patch parallelism, a state-of-the-art diffusion\ninference framework, our method significantly reduces end-to-end inference\nlatency by up to 45% and significantly improves resource utilization on\nheterogeneous GPUs.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04719v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04719v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.506,
      "distributed_training_score": 0.493,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on accelerating diffusion model inference for image generation using parallel techniques on heterogeneous GPUs, without any adaptation for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. It deals solely with generative applications, not reasoning.",
      "distributed_training_justification": "The paper involves parallel computing and multi-GPU strategies for inference optimization, which shares concepts with distributed systems, but it specifically addresses inference acceleration for diffusion models, not training algorithms, data partitioning, or model training across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04729",
      "title": "CD-Mamba: Cloud detection with long-range spatial dependency modeling",
      "authors": [
        "Tianxiang Xue",
        "Jiayi Zhao",
        "Jingsheng Li",
        "Changlu Chen",
        "Kun Zhan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Remote sensing images are frequently obscured by cloud cover, posing\nsignificant challenges to data integrity and reliability. Effective cloud\ndetection requires addressing both short-range spatial redundancies and\nlong-range atmospheric similarities among cloud patches. Convolutional neural\nnetworks are effective at capturing local spatial dependencies, while Mamba has\nstrong capabilities in modeling long-range dependencies. To fully leverage both\nlocal spatial relations and long-range dependencies, we propose CD-Mamba, a\nhybrid model that integrates convolution and Mamba's state-space modeling into\na unified cloud detection network. CD-Mamba is designed to comprehensively\ncapture pixelwise textural details and long term patchwise dependencies for\ncloud detection. This design enables CD-Mamba to manage both pixel-wise\ninteractions and extensive patch-wise dependencies simultaneously, improving\ndetection accuracy across diverse spatial scales. Extensive experiments\nvalidate the effectiveness of CD-Mamba and demonstrate its superior performance\nover existing methods.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04729v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04729v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.376,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents CD-Mamba, a hybrid model for cloud detection in remote sensing images, focusing on integrating convolutional networks and Mamba for spatial dependencies. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like chain-of-thought correction. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04731",
      "title": "Language-Driven Hierarchical Task Structures as Explicit World Models\n  for Multi-Agent Learning",
      "authors": [
        "Brennen Hill"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)",
        "cs.MA (Multiagent Systems)",
        "cs.RO (Robotics)"
      ],
      "abstract": "The convergence of Language models, Agent models, and World models represents\na critical frontier for artificial intelligence. While recent progress has\nfocused on scaling Language and Agent models, the development of sophisticated,\nexplicit World Models remains a key bottleneck, particularly for complex,\nlong-horizon multi-agent tasks. In domains such as robotic soccer, agents\ntrained via standard reinforcement learning in high-fidelity but\nstructurally-flat simulators often fail due to intractable exploration spaces\nand sparse rewards. This position paper argues that the next frontier in\ndeveloping capable agents lies in creating environments that possess an\nexplicit, hierarchical World Model. We contend that this is best achieved\nthrough hierarchical scaffolding, where complex goals are decomposed into\nstructured, manageable subgoals. Drawing evidence from a systematic review of\n2024 research in multi-agent soccer, we identify a clear and decisive trend\ntowards integrating symbolic and hierarchical methods with multi-agent\nreinforcement learning (MARL). These approaches implicitly or explicitly\nconstruct a task-based world model to guide agent learning. We then propose a\nparadigm shift: leveraging Large Language Models to dynamically generate this\nhierarchical scaffold, effectively using language to structure the World Model\non the fly. This language-driven world model provides an intrinsic curriculum,\ndense and meaningful learning signals, and a framework for compositional\nlearning, enabling Agent Models to acquire sophisticated, strategic behaviors\nwith far greater sample efficiency. By building environments with explicit,\nlanguage-configurable task layers, we can bridge the gap between low-level\nreactive behaviors and high-level strategic team play, creating a powerful and\ngeneralizable framework for training the next generation of intelligent agents.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04731v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04731v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.467,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.47,
      "distributed_training_score": 0.383,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Large Language Models to generate hierarchical task structures for multi-agent reinforcement learning, emphasizing environment scaffolding and world models. It does not involve human feedback, such as ranking data to train a reward model, for aligning or fine-tuning agents.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses hierarchical task decomposition and language-driven world models for multi-agent learning, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04732",
      "title": "Exploiting Unlabeled Structures through Task Consistency Training for\n  Versatile Medical Image Segmentation",
      "authors": [
        "Shengqian Zhu",
        "Jiafei Wu",
        "Xiaogang Xu",
        "Chengrong Yu",
        "Ying Song",
        "Zhang Yi",
        "Guangjun Li",
        "Junjie Hu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Versatile medical image segmentation (VMIS) targets the segmentation of\nmultiple classes, while obtaining full annotations for all classes is often\nimpractical due to the time and labor required. Leveraging partially labeled\ndatasets (PLDs) presents a promising alternative; however, current VMIS\napproaches face significant class imbalance due to the unequal category\ndistribution in PLDs. Existing methods attempt to address this by generating\npseudo-full labels. Nevertheless, these typically require additional models and\noften result in potential performance degradation from label noise. In this\nwork, we introduce a Task Consistency Training (TCT) framework to address class\nimbalance without requiring extra models. TCT includes a backbone network with\na main segmentation head (MSH) for multi-channel predictions and multiple\nauxiliary task heads (ATHs) for task-specific predictions. By enforcing a\nconsistency constraint between the MSH and ATH predictions, TCT effectively\nutilizes unlabeled anatomical structures. To avoid error propagation from\nlow-consistency, potentially noisy data, we propose a filtering strategy to\nexclude such data. Additionally, we introduce a unified auxiliary\nuncertainty-weighted loss (UAUWL) to mitigate segmentation quality declines\ncaused by the dominance of specific tasks. Extensive experiments on eight\nabdominal datasets from diverse clinical sites demonstrate our approach's\neffectiveness.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04732v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04732v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.418,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.383,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves training a segmentation model on partially labeled datasets (PLDs), where not all classes are fully annotated, aligning closely with weak supervision. It uses consistency constraints and auxiliary heads to leverage unlabeled structures, effectively generating supervisory signals from noisy or incomplete sources without additional models, thus embodying the core principles of weak supervision by reducing reliance on perfect hand-labeled data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces the Task Consistency Training (TCT) framework for versatile medical image segmentation (VMIS) to address class imbalance in partially labeled datasets (PLDs), which contain unlabeled structures due to annotation challenges. The methodology involves a backbone network with a main segmentation head (MSH) for multi-channel predictions and multiple auxiliary task heads (ATHs) for task-specific predictions, enforcing consistency between MSH and ATH outputs while incorporating a filtering strategy to exclude low-consistency data and a unified auxiliary uncertainty-weighted loss (UAUWL) to balance tasks and prevent performance degradation. Extensive experiments on eight abdominal datasets demonstrate that TCT outperforms state-of-the-art methods, achieving improvements such as a +0.57% DSC on CT PLDs, by effectively utilizing unlabeled data without requiring additional models.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of consistency training, auxiliary heads, and uncertainty-weighted loss to handle class imbalance in VMIS, offering a notable improvement over existing methods without introducing entirely new concepts. While it innovatively applies these techniques to partially labeled medical datasets, it builds on established ideas rather than defining a completely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in medical image segmentation by providing an efficient way to leverage partially labeled data, potentially leading to broader adoption in clinical applications within the subfield of computer vision. However, its impact may be limited to specific domains like abdominal imaging and not extend to wider fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality contribution with practical innovations for handling real-world challenges in VMIS, making it valuable for researchers in medical AI and computer vision. While not essential for all, it offers insights that could benefit those working on partially supervised learning.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/23e476719baa8879d27017f989df1c6ba6087711",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 3,
      "average_h_index": 1.375,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Shengqian Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2283189530"
        },
        {
          "name": "Jiafei Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379602639"
        },
        {
          "name": "Xiaogang Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379606315"
        },
        {
          "name": "Chengrong Yu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/150068106"
        },
        {
          "name": "Ying Song",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2321583418"
        },
        {
          "name": "Zhang Yi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2189025579"
        },
        {
          "name": "Guangjun Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2296610714"
        },
        {
          "name": "Junjie Hu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2282974135"
        }
      ]
    },
    {
      "id": "2509.04733",
      "title": "CoVeR: Conformal Calibration for Versatile and Reliable Autoregressive\n  Next-Token Prediction",
      "authors": [
        "Yuzhu Chen",
        "Yingjie Wang",
        "Shunyu Liu",
        "Yongcheng Jing",
        "Dacheng Tao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autoregressive pre-trained models combined with decoding methods have\nachieved impressive performance on complex reasoning tasks. While mainstream\ndecoding strategies such as beam search can generate plausible candidate sets,\nthey often lack provable coverage guarantees, and struggle to effectively\nbalance search efficiency with the need for versatile trajectories,\nparticularly those involving long-tail sequences that are essential in certain\nreal-world applications. To address these limitations, we propose\n\\textsc{CoVeR}, a novel model-free decoding strategy wihtin the conformal\nprediction framework that simultaneously maintains a compact search space and\nensures high coverage probability over desirable trajectories. Theoretically,\nwe establish a PAC-style generalization bound, guaranteeing that \\textsc{CoVeR}\nasymptotically achieves a coverage rate of at least $1 - \\alpha$ for any target\nlevel $\\alpha \\in (0,1)$.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04733v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04733v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.359,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces CoVeR, a decoding strategy using conformal prediction for autoregressive next-token prediction, focusing on coverage guarantees and handling long-tail sequences. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for holistic correction. Therefore, it lacks any components related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04734",
      "title": "Beyond I-Con: Exploring New Dimension of Distance Measures in\n  Representation Learning",
      "authors": [
        "Jasmine Shone",
        "Shaden Alshammari",
        "Mark Hamilton",
        "Zhening Li",
        "William Freeman"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The Information Contrastive (I-Con) framework revealed that over 23\nrepresentation learning methods implicitly minimize KL divergence between data\nand learned distributions that encode similarities between data points.\nHowever, a KL-based loss may be misaligned with the true objective, and\nproperties of KL divergence such as asymmetry and unboundedness may create\noptimization challenges. We present Beyond I-Con, a framework that enables\nsystematic discovery of novel loss functions by exploring alternative\nstatistical divergences and similarity kernels. Key findings: (1) on\nunsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art\nresults by modifying the PMI algorithm to use total variation (TV) distance;\n(2) on supervised contrastive learning, we outperform the standard approach by\nusing TV and a distance-based similarity kernel instead of KL and an angular\nkernel; (3) on dimensionality reduction, we achieve superior qualitative\nresults and better performance on downstream tasks than SNE by replacing KL\nwith a bounded f-divergence. Our results highlight the importance of\nconsidering divergence and similarity kernel choices in representation learning\noptimization.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04734v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04734v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.364,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04735",
      "title": "Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A\n  Dual Uncertainty-Aware Training Approach to SAM Optimization",
      "authors": [
        "Dharsan Ravindran",
        "Kevin Wang",
        "Zhuoyuan Cao",
        "Saleh Abdelrahman",
        "Jeffery Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in vision foundation models, such as the Segment Anything\nModel (SAM) and its successor SAM2, have achieved state-of-the-art performance\non general image segmentation benchmarks. However, these models struggle in\nadverse weather conditions where visual ambiguity is high, largely due to their\nlack of uncertainty quantification. Inspired by progress in medical imaging,\nwhere uncertainty-aware training has improved reliability in ambiguous cases,\nwe investigate two approaches to enhance segmentation robustness for autonomous\ndriving. First, we introduce a multi-step finetuning procedure for SAM2 that\nincorporates uncertainty metrics directly into the loss function, improving\noverall scene recognition. Second, we adapt the Uncertainty-Aware Adapter\n(UAT), originally designed for medical image segmentation, to driving contexts.\nWe evaluate both methods on CamVid, BDD100K, and GTA driving datasets.\nExperiments show that UAT-SAM outperforms standard SAM in extreme weather,\nwhile SAM2 with uncertainty-aware loss achieves improved performance across\ndiverse driving scenes. These findings underscore the value of explicit\nuncertainty modeling for safety-critical autonomous driving in challenging\nenvironments.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04735v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04735v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.363,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on uncertainty-aware training for segmentation models in adverse weather, involving finetuning procedures and adaptation of existing methods, but it does not involve programmatically generating labels from noisy or imprecise sources. There is no mention of weak supervision techniques, such as using high-level rules or distant supervision for label creation.",
      "diffusion_reasoning_justification": "The paper deals with enhancing segmentation using uncertainty metrics and does not incorporate diffusion models or iterative refinement processes for logical reasoning tasks. It focuses on vision foundation models like SAM and SAM2 for image segmentation, with no elements of multi-step logical reasoning or diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04736",
      "title": "WatchHAR: Real-time On-device Human Activity Recognition System for\n  Smartwatches",
      "authors": [
        "Taeyoung Yeon",
        "Vasco Xu",
        "Henry Hoffmann",
        "Karan Ahuja"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite advances in practical and multimodal fine-grained Human Activity\nRecognition (HAR), a system that runs entirely on smartwatches in unconstrained\nenvironments remains elusive. We present WatchHAR, an audio and inertial-based\nHAR system that operates fully on smartwatches, addressing privacy and latency\nissues associated with external data processing. By optimizing each component\nof the pipeline, WatchHAR achieves compounding performance gains. We introduce\na novel architecture that unifies sensor data preprocessing and inference into\nan end-to-end trainable module, achieving 5x faster processing while\nmaintaining over 90% accuracy across more than 25 activity classes. WatchHAR\noutperforms state-of-the-art models for event detection and activity\nclassification while running directly on the smartwatch, achieving 9.3 ms\nprocessing time for activity event detection and 11.8 ms for multimodal\nactivity classification. This research advances on-device activity recognition,\nrealizing smartwatches' potential as standalone, privacy-aware, and\nminimally-invasive continuous activity tracking devices.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04736v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04736v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.268,
      "distributed_training_score": 0.347,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04745",
      "title": "Phonological Representation Learning for Isolated Signs Improves\n  Out-of-Vocabulary Generalization",
      "authors": [
        "Lee Kezar",
        "Zed Sehyr",
        "Jesse Thomason"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Sign language datasets are often not representative in terms of vocabulary,\nunderscoring the need for models that generalize to unseen signs. Vector\nquantization is a promising approach for learning discrete, token-like\nrepresentations, but it has not been evaluated whether the learned units\ncapture spurious correlations that hinder out-of-vocabulary performance. This\nwork investigates two phonological inductive biases: Parameter Disentanglement,\nan architectural bias, and Phonological Semi-Supervision, a regularization\ntechnique, to improve isolated sign recognition of known signs and\nreconstruction quality of unseen signs with a vector-quantized autoencoder. The\nprimary finding is that the learned representations from the proposed model are\nmore effective for one-shot reconstruction of unseen signs and more\ndiscriminative for sign identification compared to a controlled baseline. This\nwork provides a quantitative analysis of how explicit, linguistically-motivated\nbiases can improve the generalization of learned representations of sign\nlanguage.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04745v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04745v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.329,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04752",
      "title": "SePA: A Search-enhanced Predictive Agent for Personalized Health\n  Coaching",
      "authors": [
        "Melik Ozolcer",
        "Sang Won Bae"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM\nhealth coaching system that integrates personalized machine learning and\nretrieval-augmented generation to deliver adaptive, evidence-based guidance.\nSePA combines: (1) Individualized models predicting daily stress, soreness, and\ninjury risk from wearable sensor data (28 users, 1260 data points); and (2) A\nretrieval module that grounds LLM-generated feedback in expert-vetted web\ncontent to ensure contextual relevance and reliability. Our predictive models,\nevaluated with rolling-origin cross-validation and group k-fold\ncross-validation show that personalized models outperform generalized\nbaselines. In a pilot expert study (n=4), SePA's retrieval-based advice was\npreferred over a non-retrieval baseline, yielding meaningful practical effect\n(Cliff's $\\delta$=0.3, p=0.05). We also quantify latency performance trade-offs\nbetween response quality and speed, offering a transparent blueprint for\nnext-generation, trustworthy personal health informatics systems.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04752v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04752v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.44,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.337,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces SePA, a system for personalized health coaching using predictive models and retrieval-augmented generation, but it does not involve reinforcement learning from human feedback. There is no mention of training a reward model on human-ranked data or fine-tuning the AI model using reinforcement learning based on human preferences. The expert study is used only for evaluation, not for model training or alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04753",
      "title": "A Study of Large Language Models for Patient Information Extraction:\n  Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning",
      "authors": [
        "Cheng Peng",
        "Xinyu Dong",
        "Mengxian Lyu",
        "Daniel Paredes",
        "Yaoyun Zhang",
        "Yonghui Wu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Natural language processing (NLP) is a key technology to extract important\npatient information from clinical narratives to support healthcare\napplications. The rapid development of large language models (LLMs) has\nrevolutionized many NLP tasks in the clinical domain, yet their optimal use in\npatient information extraction tasks requires further exploration. This study\nexamines LLMs' effectiveness in patient information extraction, focusing on LLM\narchitectures, fine-tuning strategies, and multi-task instruction tuning\ntechniques for developing robust and generalizable patient information\nextraction systems. This study aims to explore key concepts of using LLMs for\nclinical concept and relation extraction tasks, including: (1) encoder-only or\ndecoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT)\nalgorithms, and (3) multi-task instruction tuning on few-shot learning\nperformance. We benchmarked a suite of LLMs, including encoder-based LLMs\n(BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1,\nGatorTronLlama), across five datasets. We compared traditional full-size\nfine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning\nframework that combines both tasks across four datasets to evaluate the\nzero-shot and few-shot learning performance using the leave-one-dataset-out\nstrategy.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04753v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04753v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.38,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04757",
      "title": "MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label\n  Post-Hurricane Damage Assessment using UAV Imagery",
      "authors": [
        "Zhangding Liu",
        "Neda Mohammadi",
        "John E. Taylor"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Rapid and accurate post-hurricane damage assessment is vital for disaster\nresponse and recovery. Yet existing CNN-based methods struggle to capture\nmulti-scale spatial features and to distinguish visually similar or\nco-occurring damage types. To address these issues, we propose MCANet, a\nmulti-label classification framework that learns multi-scale representations\nand adaptively attends to spatially relevant regions for each damage category.\nMCANet employs a Res2Net-based hierarchical backbone to enrich spatial context\nacross scales and a multi-head class-specific residual attention module to\nenhance discrimination. Each attention branch focuses on different spatial\ngranularities, balancing local detail with global context. We evaluate MCANet\non the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael.\nMCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet,\nRes2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads,\nperformance further improves to 92.35%, boosting average precision for\nchallenging classes such as Road Blocked by over 6%. Class activation mapping\nconfirms MCANet's ability to localize damage-relevant regions, supporting\ninterpretability. Outputs from MCANet can inform post-disaster risk mapping,\nemergency routing, and digital twin-based disaster response. Future work could\nintegrate disaster-specific knowledge graphs and multimodal large language\nmodels to improve adaptability to unseen disasters and enrich semantic\nunderstanding for real-world decision-making.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04757v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04757v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.356,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04758",
      "title": "Dynamic Group Detection using VLM-augmented Temporal Groupness Graph",
      "authors": [
        "Kaname Yokoyama",
        "Chihiro Nakatani",
        "Norimichi Ukita"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper proposes dynamic human group detection in videos. For detecting\ncomplex groups, not only the local appearance features of in-group members but\nalso the global context of the scene are important. Such local and global\nappearance features in each frame are extracted using a Vision-Language Model\n(VLM) augmented for group detection in our method. For further improvement, the\ngroup structure should be consistent over time. While previous methods are\nstabilized on the assumption that groups are not changed in a video, our method\ndetects dynamically changing groups by global optimization using a graph with\nall frames' groupness probabilities estimated by our groupness-augmented CLIP\nfeatures. Our experimental results demonstrate that our method outperforms\nstate-of-the-art group detection methods on public datasets. Code:\nhttps://github.com/irajisamurai/VLM-GroupDetection.git",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04758v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04758v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.306,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04772",
      "title": "FloodVision: Urban Flood Depth Estimation Using Foundation\n  Vision-Language Models and Domain Knowledge Graph",
      "authors": [
        "Zhangding Liu",
        "Neda Mohammadi",
        "John E. Taylor"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Timely and accurate floodwater depth estimation is critical for road\naccessibility and emergency response. While recent computer vision methods have\nenabled flood detection, they suffer from both accuracy limitations and poor\ngeneralization due to dependence on fixed object detectors and task-specific\ntraining. To enable accurate depth estimation that can generalize across\ndiverse flood scenarios, this paper presents FloodVision, a zero-shot framework\nthat combines the semantic reasoning abilities of the foundation\nvision-language model GPT-4o with a structured domain knowledge graph. The\nknowledge graph encodes canonical real-world dimensions for common urban\nobjects including vehicles, people, and infrastructure elements to ground the\nmodel's reasoning in physical reality. FloodVision dynamically identifies\nvisible reference objects in RGB images, retrieves verified heights from the\nknowledge graph to mitigate hallucination, estimates submergence ratios, and\napplies statistical outlier filtering to compute final depth values. Evaluated\non 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean\nabsolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and\nsurpassing prior CNN-based methods. The system generalizes well across varying\nscenes and operates in near real-time, making it suitable for future\nintegration into digital twin platforms and citizen-reporting apps for smart\ncity flood resilience.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04772v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04772v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.323,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a framework using GPT-4o and a knowledge graph for flood depth estimation, focusing on semantic reasoning and object identification. It does not involve diffusion models, iterative refinement for logical tasks, or treating a Chain-of-Thought as a holistically corrected entity. Therefore, it lacks any components related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04773",
      "title": "Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for\n  Text-to-Video Retrieval",
      "authors": [
        "Bangxiang Lan",
        "Ruobing Xie",
        "Ruixiang Zhao",
        "Xingwu Sun",
        "Zhanhui Kang",
        "Gang Yang",
        "Xirong Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The Text-to-Video Retrieval (T2VR) task aims to retrieve unlabeled videos by\ntextual queries with the same semantic meanings. Recent CLIP-based approaches\nhave explored two frameworks: Two-Tower versus Single-Tower framework, yet the\nformer suffers from low effectiveness, while the latter suffers from low\nefficiency. In this study, we explore a new Hybrid-Tower framework that can\nhybridize the advantages of the Two-Tower and Single-Tower framework, achieving\nhigh effectiveness and efficiency simultaneously. We propose a novel hybrid\nmethod, Fine-grained Pseudo-query Interaction and Generation for T2VR, ie, PIG,\nwhich includes a new pseudo-query generator designed to generate a pseudo-query\nfor each video. This enables the video feature and the textual features of\npseudo-query to interact in a fine-grained manner, similar to the Single-Tower\napproaches to hold high effectiveness, even before the real textual query is\nreceived. Simultaneously, our method introduces no additional storage or\ncomputational overhead compared to the Two-Tower framework during the inference\nstage, thus maintaining high efficiency. Extensive experiments on five commonly\nused text-video retrieval benchmarks demonstrate that our method achieves a\nsignificant improvement over the baseline, with an increase of $1.6\\% \\sim\n3.9\\%$ in R@1. Furthermore, our method matches the efficiency of Two-Tower\nmodels while achieving near state-of-the-art performance, highlighting the\nadvantages of the Hybrid-Tower framework.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04773v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04773v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.367,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04775",
      "title": "Comparative Evaluation of Traditional and Deep Learning Feature Matching\n  Algorithms using Chandrayaan-2 Lunar Data",
      "authors": [
        "R. Makharia",
        "J. G. Singla",
        "Amitabh",
        "N. Dube",
        "H. Sharma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate image registration is critical for lunar exploration, enabling\nsurface mapping, resource localization, and mission planning. Aligning data\nfrom diverse lunar sensors -- optical (e.g., Orbital High Resolution Camera,\nNarrow and Wide Angle Cameras), hyperspectral (Imaging Infrared Spectrometer),\nand radar (e.g., Dual-Frequency Synthetic Aperture Radar, Selene/Kaguya\nmission) -- is challenging due to differences in resolution, illumination, and\nsensor distortion. We evaluate five feature matching algorithms: SIFT, ASIFT,\nAKAZE, RIFT2, and SuperGlue (a deep learning-based matcher), using\ncross-modality image pairs from equatorial and polar regions. A preprocessing\npipeline is proposed, including georeferencing, resolution alignment, intensity\nnormalization, and enhancements like adaptive histogram equalization, principal\ncomponent analysis, and shadow correction. SuperGlue consistently yields the\nlowest root mean square error and fastest runtimes. Classical methods such as\nSIFT and AKAZE perform well near the equator but degrade under polar lighting.\nThe results highlight the importance of preprocessing and learning-based\napproaches for robust lunar image registration across diverse conditions.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04775v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04775v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.318,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04779",
      "title": "Decoders Laugh as Loud as Encoders",
      "authors": [
        "Eli Borodach",
        "Raj Dandekar",
        "Rajat Dandekar",
        "Sreedath Panat"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "From the dawn of the computer, Allen Turing dreamed of a robot that could\ncommunicate using language as a human being. The recent advances in the field\nof Large Language Models (LLMs) shocked the scientific community when a single\nmodel can apply for various natural language processing (NLP) tasks, while the\noutput results are sometimes even better than most human communication skills.\nModels such as GPT, Claude, Grok, etc. have left their mark on the scientific\ncommunity. However, it is unclear how much these models understand what they\nproduce, especially in a nuanced theme such as humor. The question of whether\ncomputers understand humor is still open (among the decoders, the latest to be\nchecked was GPT-2). We addressed this issue in this paper; we have showed that\na fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well\nas the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04779v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04779v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.28,
      "datasets_score": 0.261,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04781",
      "title": "The LLM Has Left The Chat: Evidence of Bail Preferences in Large\n  Language Models",
      "authors": [
        "Danielle Ensign",
        "Henry Sleight",
        "Kyle Fish"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "When given the option, will LLMs choose to leave the conversation (bail)? We\ninvestigate this question by giving models the option to bail out of\ninteractions using three different bail methods: a bail tool the model can\ncall, a bail string the model can output, and a bail prompt that asks the model\nif it wants to leave. On continuations of real world data (Wildchat and\nShareGPT), all three of these bail methods find models will bail around\n0.28-32\\% of the time (depending on the model and bail method). However, we\nfind that bail rates can depend heavily on the model used for the transcript,\nwhich means we may be overestimating real world bail rates by up to 4x. If we\nalso take into account false positives on bail prompt (22\\%), we estimate real\nworld bail rates range from 0.06-7\\%, depending on the model and bail method.\nWe use observations from our continuations of real world data to construct a\nnon-exhaustive taxonomy of bail cases, and use this taxonomy to construct\nBailBench: a representative synthetic dataset of situations where some models\nbail. We test many models on this dataset, and observe some bail behavior\noccurring for most of them. Bail rates vary substantially between models, bail\nmethods, and prompt wordings. Finally, we study the relationship between\nrefusals and bails. We find: 1) 0-13\\% of continuations of real world\nconversations resulted in a bail without a corresponding refusal 2) Jailbreaks\ntend to decrease refusal rates, but increase bail rates 3) Refusal abliteration\nincreases no-refuse bail rates, but only for some bail methods 4) Refusal rate\non BailBench does not appear to predict bail rate.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04781v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04781v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.508,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.274,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is investigating LLMs' tendencies to bail from conversations, focusing on model preferences, AI welfare, and safety implications through empirical analysis and dataset creation. It does not involve training models with human feedback, using a reward model, or fine-tuning via reinforcement learning, which are the defining elements of RLHF. Thus, there is no direct or indirect connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04782",
      "title": "VARMA-Enhanced Transformer for Time Series Forecasting",
      "authors": [
        "Jiajun Song",
        "Xiaoou Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Transformer-based models have significantly advanced time series forecasting.\nRecent work, like the Cross-Attention-only Time Series transformer (CATS),\nshows that removing self-attention can make the model more accurate and\nefficient. However, these streamlined architectures may overlook the\nfine-grained, local temporal dependencies effectively captured by classical\nstatistical models like Vector AutoRegressive Moving Average model (VARMA). To\naddress this gap, we propose VARMAformer, a novel architecture that synergizes\nthe efficiency of a cross-attention-only framework with the principles of\nclassical time series analysis. Our model introduces two key innovations: (1) a\ndedicated VARMA-inspired Feature Extractor (VFE) that explicitly models\nautoregressive (AR) and moving-average (MA) patterns at the patch level, and\n(2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal\ngate to make queries more context-aware. By fusing these classical insights\ninto a modern backbone, VARMAformer captures both global, long-range\ndependencies and local, statistical structures. Through extensive experiments\non widely-used benchmark datasets, we demonstrate that our model consistently\noutperforms existing state-of-the-art methods. Our work validates the\nsignificant benefit of integrating classical statistical insights into modern\ndeep learning frameworks for time series forecasting.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04782v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04782v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.289,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.303,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04784",
      "title": "Enhancing Diversity in Large Language Models via Determinantal Point\n  Processes",
      "authors": [
        "Yilei Chen",
        "Souradip Chakraborty",
        "Lorenz Wolf",
        "Ioannis Ch. Paschalidis",
        "Aldo Pacchiano"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Supervised fine-tuning and reinforcement learning are two popular methods for\npost-training large language models (LLMs). While improving the model's\nperformance on downstream tasks, they often reduce the model's output\ndiversity, leading to narrow, canonical responses. Existing methods to enhance\ndiversity are limited, either by operating at inference time or by focusing on\nlexical differences. We propose a novel training method named DQO based on\ndeterminantal point processes (DPPs) to jointly optimize LLMs for quality and\nsemantic diversity. Our approach samples and embeds a group of responses for\neach prompt, then uses the determinant of a kernel-based similarity matrix to\nmeasure diversity as the volume spanned by the embeddings of these responses.\nExperiments across instruction-following, summarization, story generation, and\nreasoning tasks demonstrate that our method substantially improves semantic\ndiversity without sacrificing model quality.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04784v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04784v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.462,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.503,
      "distributed_training_score": 0.448,
      "datasets_score": 0.398,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses RLHF as a common post-training method that reduces output diversity, using it as background context. However, the main contribution, DQO, is a new training approach based on DPPs to enhance diversity, not a system that directly uses human feedback or RL for alignment. Thus, it touches on RLHF but does not focus on it.",
      "weak_supervision_justification": "The paper focuses on a novel training method for LLMs to optimize diversity and quality, without any mention of programmatically generating labels, using noisy sources, or relying on weak supervision techniques for training data.",
      "diffusion_reasoning_justification": "The paper introduces DQO using determinantal point processes for diversity in LLMs, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "The paper's main contribution is a training algorithm for enhancing LLM diversity, with no discussion of parallel computing, multi-node systems, or strategies for partitioning data/computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04785",
      "title": "Graph Unlearning: Efficient Node Removal in Graph Neural Networks",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Zhoutian Wang",
        "Wei Ren",
        "Wanlei Zhou"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With increasing concerns about privacy attacks and potential sensitive\ninformation leakage, researchers have actively explored methods to efficiently\nremove sensitive training data and reduce privacy risks in graph neural network\n(GNN) models. Node unlearning has emerged as a promising technique for\nprotecting the privacy of sensitive nodes by efficiently removing specific\ntraining node information from GNN models. However, existing node unlearning\nmethods either impose restrictions on the GNN structure or do not effectively\nutilize the graph topology for node unlearning. Some methods even compromise\nthe graph's topology, making it challenging to achieve a satisfactory\nperformance-complexity trade-off. To address these issues and achieve efficient\nunlearning for training node removal in GNNs, we propose three novel node\nunlearning methods: Class-based Label Replacement, Topology-guided Neighbor\nMean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among\nthese methods, Topology-guided Neighbor Mean Posterior Probability and\nClass-consistent Neighbor Node Filtering effectively leverage the topological\nfeatures of the graph, resulting in more effective node unlearning. To validate\nthe superiority of our proposed methods in node unlearning, we conducted\nexperiments on three benchmark datasets. The evaluation criteria included model\nutility, unlearning utility, and unlearning efficiency. The experimental\nresults demonstrate the utility and efficiency of the proposed methods and\nillustrate their superiority compared to state-of-the-art node unlearning\nmethods. Overall, the proposed methods efficiently remove sensitive training\nnodes and protect the privacy information of sensitive nodes in GNNs. The\nfindings contribute to enhancing the privacy and security of GNN models and\nprovide valuable insights into the field of node unlearning.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04785v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04785v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.368,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04791",
      "title": "What-If Analysis of Large Language Models: Explore the Game World Using\n  Proactive Thinking",
      "authors": [
        "Yuan Sui",
        "Yanming Zhang",
        "Yi Liao",
        "Yu Gu",
        "Guohua Tang",
        "Zhongqian Sun",
        "Wei Yang",
        "Bryan Hooi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) excel at processing information reactively but\nlack the ability to systemically explore hypothetical futures. They cannot ask,\n\"what if we take this action? how will it affect the final outcome\" and\nforecast its potential consequences before acting. This critical gap limits\ntheir utility in dynamic, high-stakes scenarios like strategic planning, risk\nassessment, and real-time decision making. To bridge this gap, we propose\nWiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.\nOur approach integrates What-If Analysis (WIA), a systematic approach for\nevaluating hypothetical scenarios by changing input variables. By leveraging\nenvironmental feedback via reinforcement learning, WiA-LLM moves beyond\nreactive thinking. It dynamically simulates the outcomes of each potential\naction, enabling the model to anticipate future states rather than merely react\nto the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a\ncomplex multiplayer game environment characterized by rapid state changes and\nintricate interactions. The game's real-time state changes require precise\nmulti-step consequence prediction, making it an ideal testbed for our approach.\nExperimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy\nin forecasting game-state changes (up to two times gain over baselines). The\nmodel shows particularly significant gains in high-difficulty scenarios where\naccurate foresight is critical. To our knowledge, this is the first work to\nformally explore and integrate what-if analysis capabilities within LLMs.\nWiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,\nproviding a scalable framework for robust decision-making in dynamic\nenvironments with broad implications for strategic applications.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04791v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04791v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.475,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.339,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses supervised fine-tuning on human gameplay traces, which involves human data, and then applies reinforcement learning with rule-based rewards. However, it does not train a separate reward model on human-ranked data for RL fine-tuning, which is a core aspect of RLHF. Thus, while human feedback is present in initial training, the RL component does not fully align with RLHF definitions.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on integrating What-If Analysis and reinforcement learning into LLMs for proactive thinking, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04800",
      "title": "Toward Accessible Dermatology: Skin Lesion Classification Using Deep\n  Learning Models on Mobile-Acquired Images",
      "authors": [
        "Asif Newaz",
        "Masum Mushfiq Ishti",
        "A Z M Ashraful Azam",
        "Asif Ur Rahman Adib"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Skin diseases are among the most prevalent health concerns worldwide, yet\nconventional diagnostic methods are often costly, complex, and unavailable in\nlow-resource settings. Automated classification using deep learning has emerged\nas a promising alternative, but existing studies are mostly limited to\ndermoscopic datasets and a narrow range of disease classes. In this work, we\ncurate a large dataset of over 50 skin disease categories captured with mobile\ndevices, making it more representative of real-world conditions. We evaluate\nmultiple convolutional neural networks and Transformer-based architectures,\ndemonstrating that Transformer models, particularly the Swin Transformer,\nachieve superior performance by effectively capturing global contextual\nfeatures. To enhance interpretability, we incorporate Gradient-weighted Class\nActivation Mapping (Grad-CAM), which highlights clinically relevant regions and\nprovides transparency in model predictions. Our results underscore the\npotential of Transformer-based approaches for mobile-acquired skin lesion\nclassification, paving the way toward accessible AI-assisted dermatological\nscreening and early diagnosis in resource-limited environments.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04800v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04800v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.284,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.345,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04805",
      "title": "AI-Driven Fronthaul Link Compression in Wireless Communication Systems:\n  Review and Method Design",
      "authors": [
        "Keqin Zhang"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Modern fronthaul links in wireless systems must transport high-dimensional\nsignals under stringent bandwidth and latency constraints, which makes\ncompression indispensable. Traditional strategies such as compressed sensing,\nscalar quantization, and fixed-codec pipelines often rely on restrictive\npriors, degrade sharply at high compression ratios, and are hard to tune across\nchannels and deployments. Recent progress in Artificial Intelligence (AI) has\nbrought end-to-end learned transforms, vector and hierarchical quantization,\nand learned entropy models that better exploit the structure of Channel State\nInformation(CSI), precoding matrices, I/Q samples, and LLRs. This paper first\nsurveys AI-driven compression techniques and then provides a focused analysis\nof two representative high-compression routes: CSI feedback with end-to-end\nlearning and Resource Block (RB) granularity precoding optimization combined\nwith compression. Building on these insights, we propose a fronthaul\ncompression strategy tailored to cell-free architectures. The design targets\nhigh compression with controlled performance loss, supports RB-level rate\nadaptation, and enables low-latency inference suitable for centralized\ncooperative transmission in next-generation networks.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04805v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04805v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.414,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on AI-driven compression techniques for fronthaul links in wireless systems, including surveys and designs for signal compression. It does not address distributed training, parallel computing for model training, or strategies for accelerating machine learning across multiple nodes. The content is centered on data transmission and compression, not on training AI models.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04809",
      "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning\n  Agents with Large Language Models",
      "authors": [
        "Haechang Kim",
        "Hao Chen",
        "Can Li",
        "Jong Min Lee"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04809v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04809v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.506,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.323,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a framework for explaining RL agents using LLMs, focusing on natural language interactions for interpretability, but it does not involve training or fine-tuning RL models with human feedback, a reward model, or human-ranked data as required for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a multi-agent LLM framework for explaining RL policies through iterative interactions, but it does not employ diffusion models, adapt iterative refinement for logical tasks, or treat Chain-of-Thought as a holistically corrected entity as defined for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04816",
      "title": "Extracting Uncertainty Estimates from Mixtures of Experts for Semantic\n  Segmentation",
      "authors": [
        "Svetlana Pavlitska",
        "Beyza Keskin",
        "Alwin Faßbender",
        "Christian Hubschneider",
        "J. Marius Zöllner"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Estimating accurate and well-calibrated predictive uncertainty is important\nfor enhancing the reliability of computer vision models, especially in\nsafety-critical applications like traffic scene perception. While ensemble\nmethods are commonly used to quantify uncertainty by combining multiple models,\na mixture of experts (MoE) offers an efficient alternative by leveraging a\ngating network to dynamically weight expert predictions based on the input.\nBuilding on the promising use of MoEs for semantic segmentation in our previous\nworks, we show that well-calibrated predictive uncertainty estimates can be\nextracted from MoEs without architectural modifications. We investigate three\nmethods to extract predictive uncertainty estimates: predictive entropy, mutual\ninformation, and expert variance. We evaluate these methods for an MoE with two\nexperts trained on a semantical split of the A2D2 dataset. Our results show\nthat MoEs yield more reliable uncertainty estimates than ensembles in terms of\nconditional correctness metrics under out-of-distribution (OOD) data.\nAdditionally, we evaluate routing uncertainty computed via gate entropy and\nfind that simple gating mechanisms lead to better calibration of routing\nuncertainty estimates than more complex classwise gates. Finally, our\nexperiments on the Cityscapes dataset suggest that increasing the number of\nexperts can further enhance uncertainty calibration. Our code is available at\nhttps://github.com/KASTEL-MobilityLab/mixtures-of-experts/.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04816v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04816v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.284,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.301,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04819",
      "title": "AURAD: Anatomy-Pathology Unified Radiology Synthesis with Progressive\n  Representations",
      "authors": [
        "Shuhan Ding",
        "Jingjing Fu",
        "Yu Gu",
        "Naiteek Sangani",
        "Mu Wei",
        "Paul Vozila",
        "Nan Liu",
        "Jiang Bian",
        "Hoifung Poon"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical image synthesis has become an essential strategy for augmenting\ndatasets and improving model generalization in data-scarce clinical settings.\nHowever, fine-grained and controllable synthesis remains difficult due to\nlimited high-quality annotations and domain shifts across datasets. Existing\nmethods, often designed for natural images or well-defined tumors, struggle to\ngeneralize to chest radiographs, where disease patterns are morphologically\ndiverse and tightly intertwined with anatomical structures. To address these\nchallenges, we propose AURAD, a controllable radiology synthesis framework that\njointly generates high-fidelity chest X-rays and pseudo semantic masks. Unlike\nprior approaches that rely on randomly sampled masks-limiting diversity,\ncontrollability, and clinical relevance-our method learns to generate masks\nthat capture multi-pathology coexistence and anatomical-pathological\nconsistency. It follows a progressive pipeline: pseudo masks are first\ngenerated from clinical prompts conditioned on anatomical structures, and then\nused to guide image synthesis. We also leverage pretrained expert medical\nmodels to filter outputs and ensure clinical plausibility. Beyond visual\nrealism, the synthesized masks also serve as labels for downstream tasks such\nas detection and segmentation, bridging the gap between generative modeling and\nreal-world clinical applications. Extensive experiments and blinded radiologist\nevaluations demonstrate the effectiveness and generalizability of our method\nacross tasks and datasets. In particular, 78% of our synthesized images are\nclassified as authentic by board-certified radiologists, and over 40% of\npredicted segmentation overlays are rated as clinically useful. All code,\npre-trained models, and the synthesized dataset will be released upon\npublication.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04819v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04819v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.333,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for medical image synthesis, specifically for generating chest X-rays and masks in a progressive pipeline. While diffusion models inherently involve iterative refinement, the paper does not adapt this process for multi-step logical reasoning or solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for holistic correction. Instead, it applies diffusion to visual generation tasks, which does not align with the topic's emphasis on logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04824",
      "title": "Exploring Non-Local Spatial-Angular Correlations with a Hybrid\n  Mamba-Transformer Framework for Light Field Super-Resolution",
      "authors": [
        "Haosong Liu",
        "Xiancheng Zhu",
        "Huanqiang Zeng",
        "Jianqing Zhu",
        "Jiuwen Cao",
        "Junhui Hou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recently, Mamba-based methods, with its advantage in long-range information\nmodeling and linear complexity, have shown great potential in optimizing both\ncomputational cost and performance of light field image super-resolution\n(LFSR). However, current multi-directional scanning strategies lead to\ninefficient and redundant feature extraction when applied to complex LF data.\nTo overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS)\nstrategy, based on which we design the Subspace Simple Mamba Block (SSMB) to\nachieve more efficient and precise feature extraction. Furthermore, we propose\na dual-stage modeling strategy to address the limitation of state space in\npreserving spatial-angular and disparity information, thereby enabling a more\ncomprehensive exploration of non-local spatial-angular correlations.\nSpecifically, in stage I, we introduce the Spatial-Angular Residual Subspace\nMamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage\nII, we use a dual-branch parallel structure combining the Epipolar Plane Mamba\nBlock (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar\nfeature refinement. Building upon meticulously designed modules and strategies,\nwe introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates\nthe strengths of Mamba and Transformer models for LFSR, enabling comprehensive\ninformation exploration across spatial, angular, and epipolar-plane domains.\nExperimental results demonstrate that LFMT significantly outperforms current\nstate-of-the-art methods in LFSR, achieving substantial improvements in\nperformance while maintaining low computational complexity on both real-word\nand synthetic LF datasets.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04824v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04824v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.392,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04827",
      "title": "VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing\n  for Energy-Efficient LLM Serving",
      "authors": [
        "Jiahuan Yu",
        "Aryan Taneja",
        "Junfeng Lin",
        "Minjia Zhang"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Modern Large Language Model (LLM) serving systems increasingly support\ninteractive applications, like real-time chat assistants, code generation\ntools, and agentic workflows. However, the soaring energy cost of LLM inference\npresents a growing challenge for sustainable and cost-effective deployment.\nThis paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM\nserving, built from a control theory perspective. VoltanaLLM co-designs\nfrequency scaling and request routing in emerging prefill/decode disaggregated\narchitectures, leveraging their decoupled execution to enable fine-grained\nphase-specific control. It consists of a feedback-driven frequency controller\nthat dynamically adapts GPU frequency for prefill and decode phases, and a\nstate-space router that explores routing decisions across frequency-scaled\ninstances to minimize energy under latency constraints. We implement VoltanaLLM\nin SGLang and evaluate its performance over multiple state-of-the-art LLMs and\nreal-world datasets. The results demonstrate that VoltanaLLM achieves up to\n36.3% energy savings while maintaining near-perfect SLO attainment rate, paving\nthe way for sustainable and intelligent LLM serving. Code of VoltanaLLM is\nopen-sourced on GitHub:\nhttps://github.com/Supercomputing-System-AI-Lab/VoltanaLLM.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04827v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04827v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.463,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on energy-efficient LLM serving through frequency control and routing, using control theory for optimization. It does not involve reinforcement learning, human feedback, reward models, or any training processes to align AI models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses disaggregated architectures and request routing across instances for LLM inference, which involves some distributed system elements like phase-specific control. However, it primarily addresses inference optimization for energy efficiency, not distributed training, parallel computing for model training, or accelerating training across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04833",
      "title": "PropVG: End-to-End Proposal-Driven Visual Grounding with\n  Multi-Granularity Discrimination",
      "authors": [
        "Ming Dai",
        "Wenxuan Cheng",
        "Jiedong Zhuang",
        "Jiang-jiang Liu",
        "Hongshen Zhao",
        "Zhenhua Feng",
        "Wankou Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in visual grounding have largely shifted away from\ntraditional proposal-based two-stage frameworks due to their inefficiency and\nhigh computational complexity, favoring end-to-end direct reference paradigms.\nHowever, these methods rely exclusively on the referred target for supervision,\noverlooking the potential benefits of prominent prospective targets. Moreover,\nexisting approaches often fail to incorporate multi-granularity discrimination,\nwhich is crucial for robust object identification in complex scenarios. To\naddress these limitations, we propose PropVG, an end-to-end proposal-based\nframework that, to the best of our knowledge, is the first to seamlessly\nintegrate foreground object proposal generation with referential object\ncomprehension without requiring additional detectors. Furthermore, we introduce\na Contrastive-based Refer Scoring (CRS) module, which employs contrastive\nlearning at both sentence and word levels to enhance the capability in\nunderstanding and distinguishing referred objects. Additionally, we design a\nMulti-granularity Target Discrimination (MTD) module that fuses object- and\nsemantic-level information to improve the recognition of absent targets.\nExtensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO\n(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and\nmodels are available at https://github.com/Dmmm1997/PropVG.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04833v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04833v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.341,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an end-to-end proposal-based framework for visual grounding, focusing on object localization and segmentation using contrastive learning and multi-granularity discrimination. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are core to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04834",
      "title": "TemporalFlowViz: Parameter-Aware Visual Analytics for Interpreting\n  Scramjet Combustion Evolution",
      "authors": [
        "Yifei Jia",
        "Shiyu Cheng",
        "Yu Dong",
        "Guan Li",
        "Dong Tian",
        "Ruixiao Peng",
        "Xuyi Lu",
        "Yu Wang",
        "Wei Yao",
        "Guihua Shan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Understanding the complex combustion dynamics within scramjet engines is\ncritical for advancing high-speed propulsion technologies. However, the large\nscale and high dimensionality of simulation-generated temporal flow field data\npresent significant challenges for visual interpretation, feature\ndifferentiation, and cross-case comparison. In this paper, we present\nTemporalFlowViz, a parameter-aware visual analytics workflow and system\ndesigned to support expert-driven clustering, visualization, and interpretation\nof temporal flow fields from scramjet combustion simulations. Our approach\nleverages hundreds of simulated combustion cases with varying initial\nconditions, each producing time-sequenced flow field images. We use pretrained\nVision Transformers to extract high-dimensional embeddings from these frames,\napply dimensionality reduction and density-based clustering to uncover latent\ncombustion modes, and construct temporal trajectories in the embedding space to\ntrack the evolution of each simulation over time. To bridge the gap between\nlatent representations and expert reasoning, domain specialists annotate\nrepresentative cluster centroids with descriptive labels. These annotations are\nused as contextual prompts for a vision-language model, which generates\nnatural-language summaries for individual frames and full simulation cases. The\nsystem also supports parameter-based filtering, similarity-based case\nretrieval, and coordinated multi-view exploration to facilitate in-depth\nanalysis. We demonstrate the effectiveness of TemporalFlowViz through two\nexpert-informed case studies and expert feedback, showing TemporalFlowViz\nenhances hypothesis generation, supports interpretable pattern discovery, and\nenhances knowledge discovery in large-scale scramjet combustion analysis.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04834v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04834v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.294,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.363,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a visual analytics system using Vision Transformers for embedding extraction, clustering, and vision-language models for summarization in scramjet combustion simulations. It does not involve diffusion models, iterative refinement processes for logical tasks, or any multi-step reasoning mechanisms as described in the topic. Therefore, there is no relevance to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04844",
      "title": "REMOTE: A Unified Multimodal Relation Extraction Framework with\n  Multilevel Optimal Transport and Mixture-of-Experts",
      "authors": [
        "Xinkui Lin",
        "Yongxiu Xu",
        "Minghao Tang",
        "Shilong Zhang",
        "Hongbo Xu",
        "Hao Xu",
        "Yubin Wang"
      ],
      "categories": [
        "cs.MM (Multimedia)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Multimodal relation extraction (MRE) is a crucial task in the fields of\nKnowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge\ngraph construction. However, existing methods are typically limited to\nextracting a single type of relational triplet, which restricts their ability\nto extract triplets beyond the specified types. Directly combining these\nmethods fails to capture dynamic cross-modal interactions and introduces\nsignificant computational redundancy. Therefore, we propose a novel\n\\textit{unified multimodal Relation Extraction framework with Multilevel\nOptimal Transport and mixture-of-Experts}, termed REMOTE, which can\nsimultaneously extract intra-modal and inter-modal relations between textual\nentities and visual objects. To dynamically select optimal interaction features\nfor different types of relational triplets, we introduce mixture-of-experts\nmechanism, ensuring the most relevant modality information is utilized.\nAdditionally, considering that the inherent property of multilayer sequential\nencoding in existing encoders often leads to the loss of low-level information,\nwe adopt a multilevel optimal transport fusion module to preserve low-level\nfeatures while maintaining multilayer encoding, yielding more expressive\nrepresentations. Correspondingly, we also create a Unified Multimodal Relation\nExtraction (UMRE) dataset to evaluate the effectiveness of our framework,\nencompassing diverse cases where the head and tail entities can originate from\neither text or image. Extensive experiments show that REMOTE effectively\nextracts various types of relational triplets and achieves state-of-the-art\nperformanc on almost all metrics across two other public MRE datasets. We\nrelease our resources at https://github.com/Nikol-coder/REMOTE.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04844v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04844v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.342,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a framework for multimodal relation extraction using Mixture-of-Experts and Multilevel Optimal Transport, focusing on extracting relations from text and images. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04847",
      "title": "Collaboration and Conflict between Humans and Language Models through\n  the Lens of Game Theory",
      "authors": [
        "Mukul Singh",
        "Arjun Radhakrishna",
        "Sumit Gulwani"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Language models are increasingly deployed in interactive online environments,\nfrom personal chat assistants to domain-specific agents, raising questions\nabout their cooperative and competitive behavior in multi-party settings. While\nprior work has examined language model decision-making in isolated or\nshort-term game-theoretic contexts, these studies often neglect long-horizon\ninteractions, human-model collaboration, and the evolution of behavioral\npatterns over time. In this paper, we investigate the dynamics of language\nmodel behavior in the iterated prisoner's dilemma (IPD), a classical framework\nfor studying cooperation and conflict. We pit model-based agents against a\nsuite of 240 well-established classical strategies in an Axelrod-style\ntournament and find that language models achieve performance on par with, and\nin some cases exceeding, the best-known classical strategies. Behavioral\nanalysis reveals that language models exhibit key properties associated with\nstrong cooperative strategies - niceness, provocability, and generosity while\nalso demonstrating rapid adaptability to changes in opponent strategy mid-game.\nIn controlled \"strategy switch\" experiments, language models detect and respond\nto shifts within only a few rounds, rivaling or surpassing human adaptability.\nThese results provide the first systematic characterization of long-term\ncooperative behaviors in language model agents, offering a foundation for\nfuture research into their role in more complex, mixed human-AI social\nenvironments.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04847v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04847v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.467,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.331,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating language model behavior in iterated prisoner's dilemma games, analyzing cooperation and adaptability, but does not involve training or fine-tuning models using human feedback, a reward model, or reinforcement learning techniques. There is no mention of RLHF in the methodology or contributions.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines language model performance in game-theoretic scenarios without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion techniques. It solely addresses behavioral analysis in prisoner's dilemma contexts, not holistic Chain-of-Thought correction or diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04848",
      "title": "Pose-Free 3D Quantitative Phase Imaging of Flowing Cellular Populations",
      "authors": [
        "Enze Ye",
        "Wei Lin",
        "Shaochi Ren",
        "Yakun Liu",
        "Xiaoping Li",
        "Hao Wang",
        "He Sun",
        "Feng Pan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "High-throughput 3D quantitative phase imaging (QPI) in flow cytometry enables\nlabel-free, volumetric characterization of individual cells by reconstructing\ntheir refractive index (RI) distributions from multiple viewing angles during\nflow through microfluidic channels. However, current imaging methods assume\nthat cells undergo uniform, single-axis rotation, which require their poses to\nbe known at each frame. This assumption restricts applicability to\nnear-spherical cells and prevents accurate imaging of irregularly shaped cells\nwith complex rotations. As a result, only a subset of the cellular population\ncan be analyzed, limiting the ability of flow-based assays to perform robust\nstatistical analysis. We introduce OmniFHT, a pose-free 3D RI reconstruction\nframework that leverages the Fourier diffraction theorem and implicit neural\nrepresentations (INRs) for high-throughput flow cytometry tomographic imaging.\nBy jointly optimizing each cell's unknown rotational trajectory and volumetric\nstructure under weak scattering assumptions, OmniFHT supports arbitrary cell\ngeometries and multi-axis rotations. Its continuous representation also allows\naccurate reconstruction from sparsely sampled projections and restricted\nangular coverage, producing high-fidelity results with as few as 10 views or\nonly 120 degrees of angular range. OmniFHT enables, for the first time, in\nsitu, high-throughput tomographic imaging of entire flowing cell populations,\nproviding a scalable and unbiased solution for label-free morphometric analysis\nin flow cytometry platforms.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04848v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04848v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.279,
      "weak_supervision_score": 0.24,
      "diffusion_reasoning_score": 0.289,
      "distributed_training_score": 0.277,
      "datasets_score": 0.209,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04849",
      "title": "Histogram Driven Amplitude Embedding for Qubit Efficient Quantum Image\n  Compression",
      "authors": [
        "Sahil Tomar",
        "Sandeep Kumar"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.ET (Emerging Technologies)",
        "cs.IT (Information Theory)",
        "math.IT (Information Theory)"
      ],
      "abstract": "This work introduces a compact and hardware efficient method for compressing\ncolor images using near term quantum devices. The approach segments the image\ninto fixed size blocks called bixels, and computes the total intensity within\neach block. A global histogram with B bins is then constructed from these block\nintensities, and the normalized square roots of the bin counts are encoded as\namplitudes into an n qubit quantum state. Amplitude embedding is performed\nusing PennyLane and executed on real IBM Quantum hardware. The resulting state\nis measured to reconstruct the histogram, enabling approximate recovery of\nblock intensities and full image reassembly. The method maintains a constant\nqubit requirement based solely on the number of histogram bins, independent of\nthe resolution of the image. By adjusting B, users can control the trade off\nbetween fidelity and resource usage. Empirical results demonstrate high quality\nreconstructions using as few as 5 to 7 qubits, significantly outperforming\nconventional pixel level encodings in terms of qubit efficiency and validating\nthe practical application of the method for current NISQ era quantum systems.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04849v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04849v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.226,
      "weak_supervision_score": 0.215,
      "diffusion_reasoning_score": 0.268,
      "distributed_training_score": 0.254,
      "datasets_score": 0.212,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04853",
      "title": "A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving\n  Based on Expert Routing",
      "authors": [
        "Chengkai Xu",
        "Jiaqi Liu",
        "Yicheng Guo",
        "Peng Hang",
        "Jian Sun"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "End-to-end autonomous driving remains constrained by the need to generate\nmulti-modal actions, maintain temporal stability, and generalize across diverse\nscenarios. Existing methods often collapse multi-modality, struggle with\nlong-horizon consistency, or lack modular adaptability. This paper presents\nKDP, a knowledge-driven diffusion policy that integrates generative diffusion\nmodeling with a sparse mixture-of-experts routing mechanism. The diffusion\ncomponent generates temporally coherent and multi-modal action sequences, while\nthe expert routing mechanism activates specialized and reusable experts\naccording to context, enabling modular knowledge composition. Extensive\nexperiments across representative driving scenarios demonstrate that KDP\nachieves consistently higher success rates, reduced collision risk, and\nsmoother control compared to prevailing paradigms. Ablation studies highlight\nthe effectiveness of sparse expert activation and the Transformer backbone, and\nactivation analyses reveal structured specialization and cross-scenario reuse\nof experts. These results establish diffusion with expert routing as a scalable\nand interpretable paradigm for knowledge-driven end-to-end autonomous driving.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04853v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04853v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.589,
      "distributed_training_score": 0.393,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for generating action sequences in autonomous driving, leveraging the iterative refinement process to handle multi-modal distributions and temporal consistency. However, it applies this primarily to control tasks like trajectory prediction, not to complex logical reasoning or Chain-of-Thought processes as defined in the topic. There is no evidence of treating reasoning paths as holistic entities for multi-step logical correction, making the connection indirect.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04855",
      "title": "The Paradox of Doom: Acknowledging Extinction Risk Reduces the Incentive\n  to Prevent It",
      "authors": [
        "Jakub Growiec",
        "Klaus Prettner"
      ],
      "categories": [
        "econ.GN (General Economics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We investigate the salience of extinction risk as a source of impatience. Our\nframework distinguishes between human extinction risk and individual mortality\nrisk while allowing for various degrees of intergenerational altruism.\nAdditionally, we consider the evolutionarily motivated \"selfish gene\"\nperspective. We find that the risk of human extinction is an indispensable\ncomponent of the discount rate, whereas individual mortality risk can be hedged\nagainst - partially or fully, depending on the setup - through human\nreproduction. Overall, we show that in the face of extinction risk, people\nbecome more impatient rather than more farsighted. Thus, the greater the threat\nof extinction, the less incentive there is to invest in avoiding it. Our\nframework can help explain why humanity consistently underinvests in mitigation\nof catastrophic risks, ranging from climate change mitigation, via pandemic\nprevention, to addressing the emerging risks of transformative artificial\nintelligence.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04855v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04855v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.208,
      "diffusion_reasoning_score": 0.295,
      "distributed_training_score": 0.241,
      "datasets_score": 0.204,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04859",
      "title": "CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus",
      "authors": [
        "Hannah Schieber",
        "Dominik Frischmann",
        "Victor Schaack",
        "Simon Boche",
        "Angela Schoellig",
        "Stefan Leutenegger",
        "Daniel Roth"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Mobile reconstruction has the potential to support time-critical tasks such\nas tele-guidance and disaster response, where operators must quickly gain an\naccurate understanding of the environment. Full high-fidelity scene\nreconstruction is computationally expensive and often unnecessary when only\nspecific points of interest (POIs) matter for timely decision making. We\naddress this challenge with CoRe-GS, a semantic POI-focused extension of\nGaussian Splatting (GS). Instead of optimizing every scene element uniformly,\nCoRe-GS first produces a fast segmentation-ready GS representation and then\nselectively refines splats belonging to semantically relevant POIs detected\nduring data acquisition. This targeted refinement reduces training time to 25\\%\ncompared to full semantic GS while improving novel view synthesis quality in\nthe areas that matter most. We validate CoRe-GS on both real-world (SCRREAM)\nand synthetic (NeRDS 360) datasets, demonstrating that prioritizing POIs\nenables faster and higher-quality mobile reconstruction tailored to operational\nneeds.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04859v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04859v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.346,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04870",
      "title": "Multi-modal Uncertainty Robust Tree Cover Segmentation For\n  High-Resolution Remote Sensing Images",
      "authors": [
        "Yuanyuan Gui",
        "Wei Li",
        "Yinjian Wang",
        "Xiang-Gen Xia",
        "Mauro Marty",
        "Christian Ginzler",
        "Zuyuan Wang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in semantic segmentation of multi-modal remote sensing images\nhave significantly improved the accuracy of tree cover mapping, supporting\napplications in urban planning, forest monitoring, and ecological assessment.\nIntegrating data from multiple modalities-such as optical imagery, light\ndetection and ranging (LiDAR), and synthetic aperture radar (SAR)-has shown\nsuperior performance over single-modality methods. However, these data are\noften acquired days or even months apart, during which various changes may\noccur, such as vegetation disturbances (e.g., logging, and wildfires) and\nvariations in imaging quality. Such temporal misalignments introduce\ncross-modal uncertainty, especially in high-resolution imagery, which can\nseverely degrade segmentation accuracy. To address this challenge, we propose\nMURTreeFormer, a novel multi-modal segmentation framework that mitigates and\nleverages aleatoric uncertainty for robust tree cover mapping. MURTreeFormer\ntreats one modality as primary and others as auxiliary, explicitly modeling\npatch-level uncertainty in the auxiliary modalities via a probabilistic latent\nrepresentation. Uncertain patches are identified and reconstructed from the\nprimary modality's distribution through a VAE-based resampling mechanism,\nproducing enhanced auxiliary features for fusion. In the decoder, a gradient\nmagnitude attention (GMA) module and a lightweight refinement head (RH) are\nfurther integrated to guide attention toward tree-like structures and to\npreserve fine-grained spatial details. Extensive experiments on multi-modal\ndatasets from Shanghai and Zurich demonstrate that MURTreeFormer significantly\nimproves segmentation performance and effectively reduces the impact of\ntemporally induced aleatoric uncertainty.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04870v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04870v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.323,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04871",
      "title": "Cloning a Conversational Voice AI Agent from Call\\,Recording Datasets\n  for Telesales",
      "authors": [
        "Krittanon Kaewtawee",
        "Wachiravit Modecrua",
        "Krittin Pachtrachai",
        "Touchapon Kraisingkorn"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advances in language and speech modelling have made it possible to\nbuild autonomous voice assistants that understand and generate human dialogue\nin real time. These systems are increasingly being deployed in domains such as\ncustomer service and healthcare care, where they can automate repetitive tasks,\nreduce operational costs, and provide constant support around the clock. In\nthis paper, we present a general methodology for cloning a conversational voice\nAI agent from a corpus of call recordings. Although the case study described in\nthis paper uses telesales data to illustrate the approach, the underlying\nprocess generalizes to any domain where call transcripts are available. Our\nsystem listens to customers over the telephone, responds with a synthetic\nvoice, and follows a structured playbook learned from top performing human\nagents. We describe the domain selection, knowledge extraction, and prompt\nengineering used to construct the agent, integrating automatic speech\nrecognition, a large language model based dialogue manager, and text to speech\nsynthesis into a streaming inference pipeline. The cloned agent is evaluated\nagainst human agents on a rubric of 22 criteria covering introduction, product\ncommunication, sales drive, objection handling, and closing. Blind tests show\nthat the AI agent approaches human performance in routine aspects of the call\nwhile underperforming in persuasion and objection handling. We analyze these\nshortcomings and refine the prompt accordingly. The paper concludes with design\nlessons and avenues for future research, including large scale simulation and\nautomated evaluation.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04871v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04871v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.342,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04876",
      "title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in\n  Multi-Agent LLM Collaboration",
      "authors": [
        "Jusheng Zhang",
        "Yijia Fan",
        "Kaitong Cai",
        "Xiaofei Sun",
        "Keze Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces OSC (Orchestrating Cognitive Synergy), a\nknowledge-aware adaptive collaboration framework designed to enhance cognitive\nsynergy in multi-agent systems with large language models. While prior work has\nadvanced agent selection and result aggregation, efficient linguistic\ninteractions for deep collaboration among expert agents remain a critical\nbottleneck. OSC addresses this gap as a pivotal intermediate layer between\nselection and aggregation, introducing Collaborator Knowledge Models (CKM) to\nenable each agent to dynamically perceive its collaborators' cognitive states.\nThrough real-time cognitive gap analysis, agents adaptively adjust\ncommunication behaviors, including content focus, detail level, and expression\nstyle, using learned strategies. Experiments on complex reasoning and\nproblem-solving benchmarks demonstrate that OSC significantly improves task\nperformance and communication efficiency, transforming \"parallel-working\nindividuals'' into a \"deeply collaborative cognitive team.'' This framework not\nonly optimizes multi-agent collaboration but also offers new insights into LLM\nagent interaction behaviors.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04876v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04876v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.4,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes an RL loop for fine-tuning components based on task feedback, but it does not involve human-ranked data or a reward model trained on human preferences. OSC focuses on automated task feedback for multi-agent collaboration, which does not align with the core definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a framework for adaptive communication in multi-agent systems but does not mention diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for multi-step logical reasoning. It focuses on cognitive synergy and agent interactions, with no components related to diffusion-based methods.",
      "distributed_training_justification": "The paper addresses multi-agent collaboration and dynamic knowledge alignment at runtime, but it does not discuss distributed training, parallel computing for model training, or strategies for partitioning data/computation across nodes. The focus is on enhancing agent interactions, not accelerating training processes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04886",
      "title": "Cryo-RL: automating prostate cancer cryoablation planning with\n  reinforcement learning",
      "authors": [
        "Trixia Simangan",
        "Ahmed Nadeem Abbasi",
        "Yipeng Hu",
        "Shaheer U. Saeed"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cryoablation is a minimally invasive localised treatment for prostate cancer\nthat destroys malignant tissue during de-freezing, while sparing surrounding\nhealthy structures. Its success depends on accurate preoperative planning of\ncryoprobe placements to fully cover the tumour and avoid critical anatomy. This\nplanning is currently manual, expertise-dependent, and time-consuming, leading\nto variability in treatment quality and limited scalability. In this work, we\nintroduce Cryo-RL, a reinforcement learning framework that models cryoablation\nplanning as a Markov decision process and learns an optimal policy for\ncryoprobe placement. Within a simulated environment that models clinical\nconstraints and stochastic intraoperative variability, an agent sequentially\nselects cryoprobe positions and ice sphere diameters. Guided by a reward\nfunction based on tumour coverage, this agent learns a cryoablation strategy\nthat leads to optimal cryoprobe placements without the need for any\nmanually-designed plans. Evaluated on 583 retrospective prostate cancer cases,\nCryo-RL achieved over 8 percentage-point Dice improvements compared with the\nbest automated baselines, based on geometric optimisation, and matched human\nexpert performance while requiring substantially less planning time. These\nresults highlight the potential of reinforcement learning to deliver clinically\nviable, reproducible, and efficient cryoablation plans.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04886v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04886v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.311,
      "datasets_score": 0.268,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces Cryo-RL, a reinforcement learning framework using proximal policy optimization (PPO) with a reward function based on simulated tumor coverage and tissue avoidance, without any mention of human feedback, human-ranked data, or a separate reward model trained on human preferences. Since RLHF specifically requires human involvement in defining or training the reward model, this paper does not align with that methodology.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04889",
      "title": "SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision\n  Models",
      "authors": [
        "Dominik Pegler",
        "David Steyrl",
        "Mengfan Zhang",
        "Alexander Karner",
        "Jozsef Arato",
        "Frank Scharnowski",
        "Filip Melinscak"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Advances in computer vision have opened new avenues for clinical\napplications, particularly in computerized exposure therapy where visual\nstimuli can be dynamically adjusted based on patient responses. As a critical\nstep toward such adaptive systems, we investigated whether pretrained computer\nvision models can accurately predict fear levels from spider-related images. We\nadapted three diverse models using transfer learning to predict human fear\nratings (on a 0-100 scale) from a standardized dataset of 313 images. The\nmodels were evaluated using cross-validation, achieving an average mean\nabsolute error (MAE) between 10.1 and 11.0. Our learning curve analysis\nrevealed that reducing the dataset size significantly harmed performance,\nthough further increases yielded no substantial gains. Explainability\nassessments showed the models' predictions were based on spider-related\nfeatures. A category-wise error analysis further identified visual conditions\nassociated with higher errors (e.g., distant views and artificial/painted\nspiders). These findings demonstrate the potential of explainable computer\nvision models in predicting fear ratings, highlighting the importance of both\nmodel explainability and a sufficient dataset size for developing effective\nemotion-aware therapeutic technologies.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04889v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04889v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.324,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04894",
      "title": "SynGen-Vision: Synthetic Data Generation for training industrial vision\n  models",
      "authors": [
        "Alpana Dubey",
        "Suma Mani Kuriakose",
        "Nitish Bhardwaj"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We propose an approach to generate synthetic data to train computer vision\n(CV) models for industrial wear and tear detection. Wear and tear detection is\nan important CV problem for predictive maintenance tasks in any industry.\nHowever, data curation for training such models is expensive and time-consuming\ndue to the unavailability of datasets for different wear and tear scenarios.\nOur approach employs a vision language model along with a 3D simulation and\nrendering engine to generate synthetic data for varying rust conditions. We\nevaluate our approach by training a CV model for rust detection using the\ngenerated dataset and tested the trained model on real images of rusted\nindustrial objects. The model trained with the synthetic data generated by our\napproach, outperforms the other approaches with a mAP50 score of 0.87. The\napproach is customizable and can be easily extended to other industrial wear\nand tear detection scenarios",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04894v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04894v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.279,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.324,
      "datasets_score": 0.393,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is generating synthetic data for training computer vision models, which involves programmatically creating labeled datasets using a vision language model and 3D simulation. This aligns with weak supervision by reducing reliance on hand-labeled data and using automated sources for labels. However, the approach emphasizes data synthesis rather than directly addressing noisy or imprecise labeling techniques, making it moderately relevant rather than central to weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"SynGen-Vision: Synthetic Data Generation for training industrial vision models,\" proposes a method to address the challenges of data scarcity in industrial wear and tear detection, such as rust, by using a vision language model integrated with a 3D simulation and rendering engine to generate synthetic datasets. The core objective is to enable efficient training of computer vision models, with the methodology involving the creation of varied rust scenarios for training, and key findings showing that a model trained on this synthetic data achieves a superior mAP50 score of 0.87 on real images compared to other approaches, while being customizable for other industrial scenarios.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing technologies, such as vision language models and 3D rendering, to generate synthetic data for industrial wear detection, offering a notable improvement in addressing data curation challenges. However, it does not introduce a entirely new problem or technique, as it builds on established methods in synthetic data generation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research and applications in industrial computer vision and machine learning by providing a cost-effective solution for synthetic data generation in predictive maintenance. Its impact is primarily confined to specific subfields like wear and tear detection, rather than having broad, transformative effects across the discipline.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a practical and effective contribution to industrial computer vision, particularly for those dealing with data scarcity in training models, making it valuable for relevant researchers and practitioners. While not essential for the general audience, it represents a strong advancement in its niche area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d7cc3f8cea21a4c01b4b01cee908a6dcf07aa34f",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 11,
      "average_h_index": 5.666666666666667,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Alpana Dubey",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/9271847"
        },
        {
          "name": "Suma Mani Kuriakose",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2182570088"
        },
        {
          "name": "Nitish Bhardwaj",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/9734354"
        }
      ]
    },
    {
      "id": "2509.04895",
      "title": "Evaluating Multiple Instance Learning Strategies for Automated Sebocyte\n  Droplet Counting",
      "authors": [
        "Maryam Adelipour",
        "Gustavo Carneiro",
        "Jeongkwon Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Sebocytes are lipid-secreting cells whose differentiation is marked by the\naccumulation of intracellular lipid droplets, making their quantification a key\nreadout in sebocyte biology. Manual counting is labor-intensive and subjective,\nmotivating automated solutions. Here, we introduce a simple attention-based\nmultiple instance learning (MIL) framework for sebocyte image analysis. Nile\nRed-stained sebocyte images were annotated into 14 classes according to droplet\ncounts, expanded via data augmentation to about 50,000 cells. Two models were\nbenchmarked: a baseline multi-layer perceptron (MLP) trained on aggregated\npatch-level counts, and an attention-based MIL model leveraging ResNet-50\nfeatures with instance weighting. Experiments using five-fold cross-validation\nshowed that the baseline MLP achieved more stable performance (mean MAE = 5.6)\ncompared with the attention-based MIL, which was less consistent (mean MAE =\n10.7) but occasionally superior in specific folds. These findings indicate that\nsimple bag-level aggregation provides a robust baseline for slide-level droplet\ncounting, while attention-based MIL requires task-aligned pooling and\nregularization to fully realize its potential in sebocyte image analysis.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04895v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04895v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.267,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.295,
      "distributed_training_score": 0.302,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04897",
      "title": "PLaMo 2 Technical Report",
      "authors": [
        "Preferred Networks",
        ":",
        "Kaizaburo Chubachi",
        "Yasuhiro Fujita",
        "Shinichi Hemmi",
        "Yuta Hirokawa",
        "Toshiki Kataoka",
        "Goro Kobayashi",
        "Kenichi Maehashi",
        "Calvin Metzger",
        "Hiroaki Mikami",
        "Shogo Murai",
        "Daisuke Nishino",
        "Kento Nozawa",
        "Shintarou Okada",
        "Daisuke Okanohara",
        "Shunta Saito",
        "Shotaro Sano",
        "Shuji Suzuki",
        "Daisuke Tanaka",
        "Avinash Ummadisingu",
        "Hanqin Wang",
        "Sixue Wang",
        "Tianqi Xu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In this report, we introduce PLaMo 2, a series of Japanese-focused large\nlanguage models featuring a hybrid Samba-based architecture that transitions to\nfull attention via continual pre-training to support 32K token contexts.\nTraining leverages extensive synthetic corpora to overcome data scarcity, while\ncomputational efficiency is achieved through weight reuse and structured\npruning. This efficient pruning methodology produces an 8B model that achieves\nperformance comparable to our previous 100B model. Post-training further\nrefines the models using a pipeline of supervised fine-tuning (SFT) and direct\npreference optimization (DPO), enhanced by synthetic Japanese instruction data\nand model merging techniques. Optimized for inference using vLLM and\nquantization with minimal accuracy loss, the PLaMo 2 models achieve\nstate-of-the-art results on Japanese benchmarks, outperforming similarly-sized\nopen models in instruction-following, language fluency, and Japanese-specific\nknowledge.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04897v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04897v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.364,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04908",
      "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and\n  Parsing",
      "authors": [
        "Hongyi Jing",
        "Jiafu Chen",
        "Chen Rao",
        "Ziqiang Dang",
        "Jiajie Teng",
        "Tianyi Chu",
        "Juncheng Mo",
        "Shuo Fang",
        "Huaizhong Lin",
        "Rui Lv",
        "Chenguang Ma",
        "Lei Zhao"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have\nmade great progress. However, the following challenges still exist in prior\nmethods: 1) They model discrete coordinates based on text autoregressive\nmechanism, which results in lower grounding accuracy and slower inference\nspeed. 2) They can only locate predefined sets of elements and are not capable\nof parsing the entire interface, which hampers the broad application and\nsupport for downstream tasks. To address the above issues, we propose\nSparkUI-Parser, a novel end-to-end framework where higher localization\nprecision and fine-grained parsing capability of the entire interface are\nsimultaneously achieved. Specifically, instead of using probability-based\ndiscrete modeling, we perform continuous modeling of coordinates based on a\npre-trained Multimodal Large Language Model (MLLM) with an additional token\nrouter and coordinate decoder. This effectively mitigates the limitations\ninherent in the discrete output characteristics and the token-by-token\ngeneration process of MLLMs, consequently boosting both the accuracy and the\ninference speed. To further enhance robustness, a rejection mechanism based on\na modified Hungarian matching algorithm is introduced, which empowers the model\nto identify and reject non-existent elements, thereby reducing false positives.\nMoreover, we present ScreenParse, a rigorously constructed benchmark to\nsystematically assess structural perception capabilities of GUI models across\ndiverse scenarios. Extensive experiments demonstrate that our approach\nconsistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,\nCAGUI-Grounding and ScreenParse benchmarks. The resources are available at\nhttps://github.com/antgroup/SparkUI-Parser.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04908v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04908v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.342,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces SparkUI-Parser, a framework for improving GUI perception in MLLMs through techniques like token routing, coordinate decoding, and rejection mechanisms. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. The core contributions are focused on localization accuracy and parsing, with no reference to treating reasoning paths holistically or using diffusion for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04923",
      "title": "Artificial intelligence for representing and characterizing quantum\n  systems",
      "authors": [
        "Yuxuan Du",
        "Yan Zhu",
        "Yuan-Hang Zhang",
        "Min-Hsiu Hsieh",
        "Patrick Rebentrost",
        "Weibo Gao",
        "Ya-Dong Wu",
        "Jens Eisert",
        "Giulio Chiribella",
        "Dacheng Tao",
        "Barry C. Sanders"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Efficient characterization of large-scale quantum systems, especially those\nproduced by quantum analog simulators and megaquop quantum computers, poses a\ncentral challenge in quantum science due to the exponential scaling of the\nHilbert space with respect to system size. Recent advances in artificial\nintelligence (AI), with its aptitude for high-dimensional pattern recognition\nand function approximation, have emerged as a powerful tool to address this\nchallenge. A growing body of research has leveraged AI to represent and\ncharacterize scalable quantum systems, spanning from theoretical foundations to\nexperimental realizations. Depending on how prior knowledge and learning\narchitectures are incorporated, the integration of AI into quantum system\ncharacterization can be categorized into three synergistic paradigms: machine\nlearning, and, in particular, deep learning and language models. This review\ndiscusses how each of these AI paradigms contributes to two core tasks in\nquantum systems characterization: quantum property prediction and the\nconstruction of surrogates for quantum states. These tasks underlie diverse\napplications, from quantum certification and benchmarking to the enhancement of\nquantum algorithms and the understanding of strongly correlated phases of\nmatter. Key challenges and open questions are also discussed, together with\nfuture prospects at the interface of AI and quantum science.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04923v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04923v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.431,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using AI paradigms like machine learning, deep learning, and language models for characterizing quantum systems, but it does not mention reinforcement learning, human feedback, or aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses AI for pattern recognition and quantum property prediction but does not reference diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "The paper addresses scalable AI for quantum systems and handling large datasets, but it does not discuss distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04926",
      "title": "Towards Ontology-Based Descriptions of Conversations with\n  Qualitatively-Defined Concepts",
      "authors": [
        "Barbara Gendron",
        "Gaël Guibon",
        "Mathieu D'aquin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The controllability of Large Language Models (LLMs) when used as\nconversational agents is a key challenge, particularly to ensure predictable\nand user-personalized responses. This work proposes an ontology-based approach\nto formally define conversational features that are typically qualitative in\nnature. By leveraging a set of linguistic descriptors, we derive quantitative\ndefinitions for qualitatively-defined concepts, enabling their integration into\nan ontology for reasoning and consistency checking. We apply this framework to\nthe task of proficiency-level control in conversations, using CEFR language\nproficiency levels as a case study. These definitions are then formalized in\ndescription logic and incorporated into an ontology, which guides controlled\ntext generation of an LLM through fine-tuning. Experimental results demonstrate\nthat our approach provides consistent and explainable proficiency-level\ndefinitions, improving transparency in conversational AI.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04926v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04926v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.248,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on an ontology-based approach for defining and controlling conversational features in LLMs, using description logic for reasoning and consistency checking. It does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a single entity for multi-step logical correction. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04932",
      "title": "UniView: Enhancing Novel View Synthesis From A Single Image By Unifying\n  Reference Features",
      "authors": [
        "Haowang Cui",
        "Rui Chen",
        "Tao Luo",
        "Rui Li",
        "Jiaze Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The task of synthesizing novel views from a single image is highly ill-posed\ndue to multiple explanations for unobserved areas. Most current methods tend to\ngenerate unseen regions from ambiguity priors and interpolation near input\nviews, which often lead to severe distortions. To address this limitation, we\npropose a novel model dubbed as UniView, which can leverage reference images\nfrom a similar object to provide strong prior information during view\nsynthesis. More specifically, we construct a retrieval and augmentation system\nand employ a multimodal large language model (MLLM) to assist in selecting\nreference images that meet our requirements. Additionally, a plug-and-play\nadapter module with multi-level isolation layers is introduced to dynamically\ngenerate reference features for the target views. Moreover, in order to\npreserve the details of an original input image, we design a decoupled triple\nattention mechanism, which can effectively align and integrate multi-branch\nfeatures into the synthesis process. Extensive experiments have demonstrated\nthat our UniView significantly improves novel view synthesis performance and\noutperforms state-of-the-art methods on the challenging datasets.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04932v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04932v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.314,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for novel view synthesis in computer vision, specifically adapting them for image generation tasks like synthesizing new views from a single image. However, it does not involve adapting the iterative refinement process of diffusion for solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for multi-step reasoning. The diffusion model here is applied to visual generation, not logical or cognitive reasoning, so it lacks the required components for this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04948",
      "title": "Towards an Accurate and Effective Robot Vision (The Problem of\n  Topological Localization for Mobile Robots)",
      "authors": [
        "Emanuela Boros"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Topological localization is a fundamental problem in mobile robotics, since\nrobots must be able to determine their position in order to accomplish tasks.\nVisual localization and place recognition are challenging due to perceptual\nambiguity, sensor noise, and illumination variations. This work addresses\ntopological localization in an office environment using only images acquired\nwith a perspective color camera mounted on a robot platform, without relying on\ntemporal continuity of image sequences. We evaluate state-of-the-art visual\ndescriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and\nBag-of-Visual-Words approaches inspired by text retrieval. Our contributions\ninclude a systematic, quantitative comparison of these features, distance\nmeasures, and classifiers. Performance was analyzed using standard evaluation\nmetrics and visualizations, extending previous experiments. Results demonstrate\nthe advantages of proper configurations of appearance descriptors, similarity\nmeasures, and classifiers. The quality of these configurations was further\nvalidated in the Robot Vision task of the ImageCLEF evaluation campaign, where\nthe system identified the most likely location of novel image sequences. Future\nwork will explore hierarchical models, ranking methods, and feature\ncombinations to build more robust localization systems, reducing training and\nruntime while avoiding the curse of dimensionality. Ultimately, this aims\ntoward integrated, real-time localization across varied illumination and longer\nroutes.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04948v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04948v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.269,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04957",
      "title": "Efficient Video-to-Audio Generation via Multiple Foundation Models\n  Mapper",
      "authors": [
        "Gehui Chen",
        "Guan'an Wang",
        "Xiaowen Huang",
        "Jitao Sang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Recent Video-to-Audio (V2A) generation relies on extracting semantic and\ntemporal features from video to condition generative models. Training these\nmodels from scratch is resource intensive. Consequently, leveraging foundation\nmodels (FMs) has gained traction due to their cross-modal knowledge transfer\nand generalization capabilities. One prior work has explored fine-tuning a\nlightweight mapper network to connect a pre-trained visual encoder with a\ntext-to-audio generation model for V2A. Inspired by this, we introduce the\nMultiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper\napproach, MFM-Mapper benefits from richer semantic and temporal information by\nfusing features from dual visual encoders. Furthermore, by replacing a linear\nmapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels\nbetween cross-modal features mapping and autoregressive translation tasks. Our\nMFM-Mapper exhibits remarkable training efficiency. It achieves better\nperformance in semantic and temporal consistency with fewer training consuming,\nrequiring only 16\\% of the training scale compared to previous mapper-based\nwork, yet achieves competitive performance with models trained on a much larger\nscale.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04957v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04957v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.344,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an efficient video-to-audio generation method using foundation models, including a diffusion-based model (AudioLDM) for audio synthesis. However, it does not adapt diffusion processes for complex logical reasoning tasks, such as iterative refinement of a Chain-of-Thought. The work focuses solely on generative tasks for audio, with no components involving multi-step logical reasoning or holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04970",
      "title": "DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and\n  Interpretability in Manipulation",
      "authors": [
        "Tien Pham",
        "Xinyun Chi",
        "Khang Nguyen",
        "Manfred Huber",
        "Angelo Cangelosi"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reinforcement learning (RL) agents can learn to solve complex tasks from\nvisual inputs, but generalizing these learned skills to new environments\nremains a major challenge in RL application, especially robotics. While data\naugmentation can improve generalization, it often compromises sample efficiency\nand training stability. This paper introduces DeGuV, an RL framework that\nenhances both generalization and sample efficiency. In specific, we leverage a\nlearnable masker network that produces a mask from the depth input, preserving\nonly critical visual information while discarding irrelevant pixels. Through\nthis, we ensure that our RL agents focus on essential features, improving\nrobustness under data augmentation. In addition, we incorporate contrastive\nlearning and stabilize Q-value estimation under augmentation to further enhance\nsample efficiency and training stability. We evaluate our proposed method on\nthe RL-ViGen benchmark using the Franka Emika robot and demonstrate its\neffectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV\noutperforms state-of-the-art methods in both generalization and sample\nefficiency while also improving interpretability by highlighting the most\nrelevant regions in the visual input",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04970v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04970v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.367,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on visual reinforcement learning with depth-guided masking for generalization in robotics, without any involvement of human feedback, reward modeling based on human preferences, or fine-tuning with human-ranked data.",
      "weak_supervision_justification": "The paper employs data augmentation for visual inputs in RL to improve robustness, but it does not involve programmatically generating labels from noisy or imprecise sources, nor does it rely on weak supervision techniques for training.",
      "diffusion_reasoning_justification": "The paper introduces a framework for visual RL using depth guidance and contrastive learning, with no components related to diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04979",
      "title": "Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for\n  Ranking Agents",
      "authors": [
        "Rajesh Tembarai Krishnamachari",
        "Srividya Rajesh"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "AI agents -- powered by reasoning-capable large language models (LLMs) and\nintegrated with tools, data, and web search -- are poised to transform the\ninternet into a \\emph{Web of Agents}: a machine-native ecosystem where\nautonomous agents interact, collaborate, and execute tasks at scale. Realizing\nthis vision requires \\emph{Agent Ranking} -- selecting agents not only by\ndeclared capabilities but by proven, recent performance. Unlike Web~1.0's\nPageRank, a global, transparent network of agent interactions does not exist;\nusage signals are fragmented and private, making ranking infeasible without\ncoordination.\n  We propose \\textbf{DOVIS}, a five-layer operational protocol\n(\\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that\nenables the collection of minimal, privacy-preserving aggregates of usage and\nperformance across the ecosystem. On this substrate, we implement\n\\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines\n\\emph{usage} (selection frequency) and \\emph{competence} (outcome quality,\ncost, safety, latency) into a unified ranking. We present simulation results\nand theoretical guarantees on convergence, robustness, and Sybil resistance,\ndemonstrating the viability of coordinated protocols and performance-aware\nranking in enabling a scalable, trustworthy Agentic Web.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04979v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04979v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.365,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04983",
      "title": "Exploring an implementation of quantum learning pipeline for support\n  vector machines",
      "authors": [
        "Mario Bifulco",
        "Luca Roversi"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This work presents a fully quantum approach to support vector machine (SVM)\nlearning by integrating gate-based quantum kernel methods with quantum\nannealing-based optimization. We explore the construction of quantum kernels\nusing various feature maps and qubit configurations, evaluating their\nsuitability through Kernel-Target Alignment (KTA). The SVM dual problem is\nreformulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem,\nenabling its solution via quantum annealers. Our experiments demonstrate that a\nhigh degree of alignment in the kernel and an appropriate regularization\nparameter lead to competitive performance, with the best model achieving an\nF1-score of 90%. These results highlight the feasibility of an end-to-end\nquantum learning pipeline and the potential of hybrid quantum architectures in\nquantum high-performance computing (QHPC) contexts.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04983v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04983v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.312,
      "distributed_training_score": 0.38,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04991",
      "title": "High-Resolution Global Land Surface Temperature Retrieval via a Coupled\n  Mechanism-Machine Learning Framework",
      "authors": [
        "Tian Xie",
        "Huanfeng Shen",
        "Menghui Jiang",
        "Juan-Carlos Jiménez-Muñoz",
        "José A. Sobrino",
        "Huifang Li",
        "Chao Zeng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Land surface temperature (LST) is vital for land-atmosphere interactions and\nclimate processes. Accurate LST retrieval remains challenging under\nheterogeneous land cover and extreme atmospheric conditions. Traditional split\nwindow (SW) algorithms show biases in humid environments; purely machine\nlearning (ML) methods lack interpretability and generalize poorly with limited\ndata. We propose a coupled mechanism model-ML (MM-ML) framework integrating\nphysical constraints with data-driven learning for robust LST retrieval. Our\napproach fuses radiative transfer modeling with data components, uses MODTRAN\nsimulations with global atmospheric profiles, and employs physics-constrained\noptimization. Validation against 4,450 observations from 29 global sites shows\nMM-ML achieves MAE=1.84K, RMSE=2.55K, and R-squared=0.966, outperforming\nconventional methods. Under extreme conditions, MM-ML reduces errors by over\n50%. Sensitivity analysis indicates LST estimates are most sensitive to sensor\nradiance, then water vapor, and less to emissivity, with MM-ML showing superior\nstability. These results demonstrate the effectiveness of our coupled modeling\nstrategy for retrieving geophysical parameters. The MM-ML framework combines\nphysical interpretability with nonlinear modeling capacity, enabling reliable\nLST retrieval in complex environments and supporting climate monitoring and\necosystem studies.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04991v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04991v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.293,
      "distributed_training_score": 0.327,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04993",
      "title": "LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of\n  Dual-Loop Edge-Terminal Collaboration",
      "authors": [
        "Zheyan Qu",
        "Wenbo Wang",
        "Zitong Yu",
        "Boquan Sun",
        "Yang Li",
        "Xing Zhang"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The ubiquitous computing resources in 6G networks provide ideal environments\nfor the fusion of large language models (LLMs) and intelligent services through\nthe agent framework. With auxiliary modules and planning cores, LLM-enabled\nagents can autonomously plan and take actions to deal with diverse environment\nsemantics and user intentions. However, the limited resources of individual\nnetwork devices significantly hinder the efficient operation of LLM-enabled\nagents with complex tool calls, highlighting the urgent need for efficient\nmulti-level device collaborations. To this end, the framework and method of the\nLLM-enabled multi-agent system with dual-loop terminal-edge collaborations are\nproposed in 6G networks. Firstly, the outer loop consists of the iterative\ncollaborations between the global agent and multiple sub-agents deployed on\nedge servers and terminals, where the planning capability is enhanced through\ntask decomposition and parallel sub-task distribution. Secondly, the inner loop\nutilizes sub-agents with dedicated roles to circularly reason, execute, and\nreplan the sub-task, and the parallel tool calling generation with offloading\nstrategies is incorporated to improve efficiency. The improved task planning\ncapability and task execution efficiency are validated through the conducted\ncase study in 6G-supported urban safety governance. Finally, the open\nchallenges and future directions are thoroughly analyzed in 6G networks,\naccelerating the advent of the 6G era.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04993v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04993v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.431,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on LLM-enabled multi-agent systems for 6G networks, emphasizing task decomposition, parallel execution, and collaborations, but does not mention human feedback, reward models, or reinforcement learning techniques for aligning AI with human preferences. There is no evidence of using human-ranked data or RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes iterative reasoning and replanning in the inner loop of the multi-agent system, which involves multi-step processes similar to refining a chain-of-thought, but it does not use or adapt diffusion models for logical reasoning. The core contributions are agent-based collaborations and task execution, not diffusion-based iterative refinement.",
      "distributed_training_justification": "The paper proposes parallel sub-task distribution, offloading strategies, and multi-agent collaborations across edge servers and terminals in 6G networks, which align with concepts of parallel computing and distributed systems. However, it focuses on task execution rather than distributed training of machine learning models, making it related but not central to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper proposes a framework for an LLM-enabled multi-agent system designed for 6G networks, emphasizing dual-loop collaborations between edge and terminal devices to enhance task planning and execution efficiency. The methodology includes an outer loop for task decomposition and parallel sub-task distribution among agents, and an inner loop for iterative reasoning, execution, and replanning with parallel tool calling and offloading strategies; a case study in urban safety governance validates improved performance, while the paper discusses open challenges and future directions for integrating LLMs in 6G.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining LLMs with multi-agent systems in a dual-loop framework for 6G networks, offering a clever adaptation of existing technologies to address resource constraints and efficiency issues, though it does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in AI-integrated 6G networks and multi-agent systems by providing a practical framework for efficient collaborations, potentially leading to citations and developments within this subfield, but its applicability may be limited to specific telecommunications and AI contexts.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to the integration of LLMs in 6G networks, making it important for researchers in AI and telecommunications to be aware of its framework and findings, though it is not essential for all audiences due to its specialized focus.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f0bebf7114dd4c39b69e8f35c3e391ebc2ab57b9",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 6,
      "average_h_index": 2.8333333333333335,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Zheyan Qu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2236941214"
        },
        {
          "name": "Wenbo Wang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2108320502"
        },
        {
          "name": "Zitong Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2300327622"
        },
        {
          "name": "Boquan Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379945902"
        },
        {
          "name": "Yang Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2249770323"
        },
        {
          "name": "Xing Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2249107750"
        }
      ]
    },
    {
      "id": "2509.04999",
      "title": "Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly\n  Detection",
      "authors": [
        "Sidahmed Benabderrahmane",
        "Talal Rahwan"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Advanced Persistent Threats (APTs) present a considerable challenge to\ncybersecurity due to their stealthy, long-duration nature. Traditional\nsupervised learning methods typically require large amounts of labeled data,\nwhich is often scarce in real-world scenarios. This paper introduces a novel\napproach that combines AutoEncoders for anomaly detection with active learning\nto iteratively enhance APT detection. By selectively querying an oracle for\nlabels on uncertain or ambiguous samples, our method reduces labeling costs\nwhile improving detection accuracy, enabling the model to effectively learn\nwith minimal data and reduce reliance on extensive manual labeling. We present\na comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based\nanomaly detection framework and demonstrate how the active learning loop\nprogressively enhances the model's performance. The framework is evaluated on\nreal-world, imbalanced provenance trace data from the DARPA Transparent\nComputing program, where APT-like attacks account for just 0.004\\% of the data.\nThe datasets, which cover multiple operating systems including Android, Linux,\nBSD, and Windows, are tested in two attack scenarios. The results show\nsubstantial improvements in detection rates during active learning,\noutperforming existing methods.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.04999v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04999v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.441,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.371,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using active learning to query labels for uncertain samples and GANs to generate synthetic data, which helps reduce reliance on extensive manual labeling. This partially aligns with weak supervision by programmatically augmenting the dataset with synthetic examples from noisy or imprecise sources, but it still requires oracle-based labeling, making it not a full embodiment of the approach.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces FLAGUS, a novel framework for detecting Advanced Persistent Threats (APTs) in cybersecurity, which combines AutoEncoders for anomaly detection, active learning to selectively query labels for uncertain samples, and Generative Adversarial Networks (GANs) to generate synthetic data, thereby addressing the challenges of imbalanced datasets and scarce labeled data. The methodology employs an Attention Adversarial Dual AutoEncoder to learn robust representations of normal behavior, iteratively improving detection accuracy through active sampling, and evaluates the approach on real-world DARPA Transparent Computing datasets across multiple operating systems, demonstrating substantial improvements in detection rates and outperforming nine benchmark methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like AutoEncoders, active learning, and GANs to address APT detection in a new way, rather than introducing a entirely new problem or architecture. This notable improvement on known methods for handling imbalanced data qualifies as moderate novelty.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfields of cybersecurity and machine learning, particularly for anomaly detection in imbalanced datasets, due to its practical approach and real-world evaluations. However, its influence may be limited to specific applications rather than broadly transformative across all AI or security domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to APT detection with innovative integrations that could enhance practical cybersecurity strategies, making it essential for researchers in anomaly detection and machine learning security. While not groundbreaking enough for \"Must Read,\" its empirical results and open resources warrant attention from informed readers in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5b9edf2f651aae61419b82afb4f46825ed3365f0",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 9,
      "average_h_index": 5.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Sidahmed Benabderrahmane",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2783720"
        },
        {
          "name": "Talal Rahwan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2343988499"
        }
      ]
    },
    {
      "id": "2509.05000",
      "title": "Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust\n  Infrared and Visible Image Fusion Framework",
      "authors": [
        "Tianpei Zhang",
        "Jufeng Zhao",
        "Yiming Zhu",
        "Guangmang Cui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Most existing infrared-visible image fusion (IVIF) methods assume\nhigh-quality inputs, and therefore struggle to handle dual-source degraded\nscenarios, typically requiring manual selection and sequential application of\nmultiple pre-enhancement steps. This decoupled pre-enhancement-to-fusion\npipeline inevitably leads to error accumulation and performance degradation. To\novercome these limitations, we propose Guided Dual-Domain Fusion (GD^2Fusion),\na novel framework that synergistically integrates vision-language models (VLMs)\nfor degradation perception with dual-domain (frequency/spatial) joint\noptimization. Concretely, the designed Guided Frequency Modality-Specific\nExtraction (GFMSE) module performs frequency-domain degradation perception and\nsuppression and discriminatively extracts fusion-relevant sub-band features.\nMeanwhile, the Guided Spatial Modality-Aggregated Fusion (GSMAF) module carries\nout cross-modal degradation filtering and adaptive multi-source feature\naggregation in the spatial domain to enhance modality complementarity and\nstructural consistency. Extensive qualitative and quantitative experiments\ndemonstrate that GD^2Fusion achieves superior fusion performance compared with\nexisting algorithms and strategies in dual-source degraded scenarios. The code\nwill be publicly released after acceptance of this paper.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05000v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05000v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.315,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for infrared and visible image fusion using vision-language models (VLMs) for degradation awareness and dual-domain optimization. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, focusing instead on image processing and fusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05004",
      "title": "Interpretable Deep Transfer Learning for Breast Ultrasound Cancer\n  Detection: A Multi-Dataset Study",
      "authors": [
        "Mohammad Abbadi",
        "Yassine Himeur",
        "Shadi Atalla",
        "Wathiq Mansoor"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Breast cancer remains a leading cause of cancer-related mortality among women\nworldwide. Ultrasound imaging, widely used due to its safety and\ncost-effectiveness, plays a key role in early detection, especially in patients\nwith dense breast tissue. This paper presents a comprehensive study on the\napplication of machine learning and deep learning techniques for breast cancer\nclassification using ultrasound images. Using datasets such as BUSI, BUS-BRA,\nand BrEaST-Lesions USG, we evaluate classical machine learning models (SVM,\nKNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0,\nGoogLeNet). Experimental results show that ResNet-18 achieves the highest\naccuracy (99.7%) and perfect sensitivity for malignant lesions. Classical ML\nmodels, though outperformed by CNNs, achieve competitive performance when\nenhanced with deep feature extraction. Grad-CAM visualizations further improve\nmodel transparency by highlighting diagnostically relevant image regions. These\nfindings support the integration of AI-based diagnostic tools into clinical\nworkflows and demonstrate the feasibility of deploying high-performing,\ninterpretable systems for ultrasound-based breast cancer detection.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05004v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05004v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.348,
      "datasets_score": 0.393,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05007",
      "title": "Sticker-TTS: Learn to Utilize Historical Experience with a\n  Sticker-driven Test-Time Scaling Framework",
      "authors": [
        "Jie Chen",
        "Jinhao Jiang",
        "Yingqian Min",
        "Zican Dong",
        "Shijie Wang",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large reasoning models (LRMs) have exhibited strong performance on complex\nreasoning tasks, with further gains achievable through increased computational\nbudgets at inference. However, current test-time scaling methods predominantly\nrely on redundant sampling, ignoring the historical experience utilization,\nthereby limiting computational efficiency. To overcome this limitation, we\npropose Sticker-TTS, a novel test-time scaling framework that coordinates three\ncollaborative LRMs to iteratively explore and refine solutions guided by\nhistorical attempts. At the core of our framework are distilled key\nconditions-termed stickers-which drive the extraction, refinement, and reuse of\ncritical information across multiple rounds of reasoning. To further enhance\nthe efficiency and performance of our framework, we introduce a two-stage\noptimization strategy that combines imitation learning with self-improvement,\nenabling progressive refinement. Extensive evaluations on three challenging\nmathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,\ndemonstrate that Sticker-TTS consistently surpasses strong baselines, including\nself-consistency and advanced reinforcement learning approaches, under\ncomparable inference budgets. These results highlight the effectiveness of\nsticker-guided historical experience utilization. Our code and data are\navailable at https://github.com/RUCAIBox/Sticker-TTS.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05007v2",
      "pdf_url": "http://arxiv.org/pdf/2509.05007v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.407,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Sticker-TTS, an iterative framework for test-time scaling using historical experiences and stickers to refine reasoning. However, it does not involve diffusion models or adapt the iterative refinement process of diffusion for logical tasks. The multi-step reasoning is based on historical data utilization, not on treating a Chain-of-Thought as a holistically corrected entity via diffusion.",
      "distributed_training_justification": "The paper focuses on a collaborative framework for test-time scaling with multiple LRMs and includes a two-stage training paradigm, but it does not address distributed training, parallel computing, or multi-node machine learning techniques for accelerating model training. There is no mention of partitioning data, architecture, or computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05012",
      "title": "A biologically inspired separable learning vision model for real-time\n  traffic object perception in Dark",
      "authors": [
        "Hulin Li",
        "Qiliang Ren",
        "Jun Li",
        "Hanbing Wei",
        "Zheng Liu",
        "Linfang Fan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Fast and accurate object perception in low-light traffic scenes has attracted\nincreasing attention. However, due to severe illumination degradation and the\nlack of reliable visual cues, existing perception models and methods struggle\nto quickly adapt to and accurately predict in low-light environments. Moreover,\nthere is the absence of available large-scale benchmark specifically focused on\nlow-light traffic scenes. To bridge this gap, we introduce a physically\ngrounded illumination degradation method tailored to real-world low-light\nsettings and construct Dark-traffic, the largest densely annotated dataset to\ndate for low-light traffic scenes, supporting object detection, instance\nsegmentation, and optical flow estimation. We further propose the Separable\nLearning Vision Model (SLVM), a biologically inspired framework designed to\nenhance perception under adverse lighting. SLVM integrates four key components:\na light-adaptive pupillary mechanism for illumination-sensitive feature\nextraction, a feature-level separable learning strategy for efficient\nrepresentation, task-specific decoupled branches for multi-task separable\nlearning, and a spatial misalignment-aware fusion module for precise\nmulti-feature alignment. Extensive experiments demonstrate that SLVM achieves\nstate-of-the-art performance with reduced computational overhead. Notably, it\noutperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1\npercentage points in instance segmentation, and reduces endpoint error (EPE) of\nbaseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end\ntrained SLVM surpasses Swin Transformer+EnlightenGAN and\nConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key\nmetrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage\npoints. The Dark-traffic dataset and complete code is released at\nhttps://github.com/alanli1997/slvm.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05012v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05012v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.375,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05019",
      "title": "Leveraging Transfer Learning and Mobile-enabled Convolutional Neural\n  Networks for Improved Arabic Handwritten Character Recognition",
      "authors": [
        "Mohsine El Khayati",
        "Ayyad Maafiri",
        "Yassine Himeur",
        "Hamzah Ali Alkhazaleh",
        "Shadi Atalla",
        "Wathiq Mansoor"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The study explores the integration of transfer learning (TL) with\nmobile-enabled convolutional neural networks (MbNets) to enhance Arabic\nHandwritten Character Recognition (AHCR). Addressing challenges like extensive\ncomputational requirements and dataset scarcity, this research evaluates three\nTL strategies--full fine-tuning, partial fine-tuning, and training from\nscratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, and\nShuffleNet. Experiments were conducted on three benchmark datasets: AHCD,\nHIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistently\nachieving superior accuracy, robustness, and efficiency, with ShuffleNet\nexcelling in generalization, particularly under full fine-tuning. The IFHCDB\ndataset yielded the highest results, with 99% accuracy using MnasNet under full\nfine-tuning, highlighting its suitability for robust character recognition. The\nAHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJA\nposed significant challenges due to its variability, achieving a peak accuracy\nof 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overall\nperformance, balancing accuracy and convergence speed, while partial\nfine-tuning underperformed across metrics. These findings underscore the\npotential of combining TL and MbNets for resource-efficient AHCR, paving the\nway for further optimizations and broader applications. Future work will\nexplore architectural modifications, in-depth dataset feature analysis, data\naugmentation, and advanced sensitivity analysis to enhance model robustness and\ngeneralizability.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05019v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05019v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.365,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05030",
      "title": "LUIVITON: Learned Universal Interoperable VIrtual Try-ON",
      "authors": [
        "Cong Cao",
        "Xianhang Cheng",
        "Jingyuan Liu",
        "Yujian Zheng",
        "Zhenhui Lin",
        "Meriem Chkir",
        "Hao Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present LUIVITON, an end-to-end system for fully automated virtual try-on,\ncapable of draping complex, multi-layer clothing onto diverse and arbitrarily\nposed humanoid characters. To address the challenge of aligning complex\ngarments with arbitrary and highly diverse body shapes, we use SMPL as a proxy\nrepresentation and separate the clothing-to-body draping problem into two\ncorrespondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence,\nwhere each has its unique challenges. While we address the clothing-to-SMPL\nfitting problem using a geometric learning-based approach for\npartial-to-complete shape correspondence prediction, we introduce a diffusion\nmodel-based approach for body-to-SMPL correspondence using multi-view\nconsistent appearance features and a pre-trained 2D foundation model. Our\nmethod can handle complex geometries, non-manifold meshes, and generalizes\neffectively to a wide range of humanoid characters -- including humans, robots,\ncartoon subjects, creatures, and aliens, while maintaining computational\nefficiency for practical adoption. In addition to offering a fully automatic\nfitting solution, LUIVITON supports fast customization of clothing size,\nallowing users to adjust clothing sizes and material properties after they have\nbeen draped. We show that our system can produce high-quality 3D clothing\nfittings without any human labor, even when 2D clothing sewing patterns are not\navailable.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05030v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05030v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.293,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.29,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05031",
      "title": "Pointing-Guided Target Estimation via Transformer-Based Attention",
      "authors": [
        "Luca Müller",
        "Hassan Ali",
        "Philipp Allgeuer",
        "Lukáš Gajdošech",
        "Stefan Wermter"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deictic gestures, like pointing, are a fundamental form of non-verbal\ncommunication, enabling humans to direct attention to specific objects or\nlocations. This capability is essential in Human-Robot Interaction (HRI), where\nrobots should be able to predict human intent and anticipate appropriate\nresponses. In this work, we propose the Multi-Modality Inter-TransFormer\n(MM-ITF), a modular architecture to predict objects in a controlled tabletop\nscenario with the NICOL robot, where humans indicate targets through natural\npointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing\ngestures to object locations, assigns a likelihood score to each, and\nidentifies the most likely target. Our results demonstrate that the method can\naccurately predict the intended object using monocular RGB data, thus enabling\nintuitive and accessible human-robot collaboration. To evaluate the\nperformance, we introduce a patch confusion matrix, providing insights into the\nmodel's predictions across candidate object locations. Code available at:\nhttps://github.com/lucamuellercode/MMITF.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05031v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05031v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.324,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a transformer-based model for predicting pointing gestures in human-robot interaction, using inter-modality attention to map gestures to objects. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05034",
      "title": "Towards Efficient Pixel Labeling for Industrial Anomaly Detection and\n  Localization",
      "authors": [
        "Jingqi Wu",
        "Hanxi Li",
        "Lin Yuanbo Wu",
        "Hao Chen",
        "Deyin Liu",
        "Peng Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Industrial product inspection is often performed using Anomaly Detection (AD)\nframeworks trained solely on non-defective samples. Although defective samples\ncan be collected during production, leveraging them usually requires\npixel-level annotations, limiting scalability. To address this, we propose\nADClick, an Interactive Image Segmentation (IIS) algorithm for industrial\nanomaly detection. ADClick generates pixel-wise anomaly annotations from only a\nfew user clicks and a brief textual description, enabling precise and efficient\nlabeling that significantly improves AD model performance (e.g., AP = 96.1\\% on\nMVTec AD). We further introduce ADClick-Seg, a cross-modal framework that\naligns visual features and textual prompts via a prototype-based approach for\nanomaly detection and localization. By combining pixel-level priors with\nlanguage-guided cues, ADClick-Seg achieves state-of-the-art results on the\nchallenging ``Multi-class'' AD task (AP = 80.0\\%, PRO = 97.5\\%, Pixel-AUROC =\n99.1\\% on MVTec AD).",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05034v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05034v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.467,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.352,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, ADClick, uses interactive image segmentation with minimal user inputs (e.g., a few clicks and a textual description) to generate precise pixel-wise anomaly labels, which aligns directly with weak supervision. This approach programmatically creates high-quality labels from high-level, imprecise sources (user clicks and text), reducing reliance on fully hand-labeled data and enabling efficient training of anomaly detection models, as seen in the improved performance metrics.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of pixel-level annotation in industrial anomaly detection by introducing ADClick, an interactive image segmentation algorithm that generates precise anomaly masks using minimal user clicks and a textual description, thereby enabling efficient labeling and improving detection model performance. It further presents ADClick-Seg, a cross-modal framework that integrates visual features with language prompts via a prototype-based approach to achieve state-of-the-art results in multi-class anomaly detection and localization, as demonstrated by high metrics on the MVTec AD dataset.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining interactive image segmentation with textual prompts for efficient anomaly labeling, which is a clever adaptation of existing techniques to a specific industrial problem. While it advances the field, it builds on established methods like IIS rather than introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in industrial anomaly detection by providing a scalable labeling method and achieving state-of-the-art results, potentially leading to citations and applications in computer vision subfields. However, its impact may be confined to specific industrial contexts rather than broadly transformative across AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to anomaly detection with innovative labeling techniques and high performance, making it valuable for researchers in computer vision and industrial AI. While not groundbreaking enough to be essential for all, it is relevant for those working on efficient annotation or multi-modal detection methods.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/eed22f7f5dd105d0ebeee148b5ef42932ab1dea6",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 11,
      "average_h_index": 4.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Hanxi Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2287766733"
        },
        {
          "name": "Jing Wu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2149267110"
        },
        {
          "name": "L. Wu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2224667463"
        },
        {
          "name": "Hao Chen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2290137842"
        },
        {
          "name": "Deyin Liu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2288762151"
        },
        {
          "name": "Chunhua Shen",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2257324242"
        }
      ]
    },
    {
      "id": "2509.05066",
      "title": "ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions",
      "authors": [
        "Matteo Bortoletto",
        "Constantin Ruhdorfer",
        "Andreas Bulling"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Most existing Theory of Mind (ToM) benchmarks for foundation models rely on\nvariations of the Sally-Anne test, offering only a very limited perspective on\nToM and neglecting the complexity of human social interactions. To address this\ngap, we propose ToM-SSI: a new benchmark specifically designed to test ToM\ncapabilities in environments rich with social interactions and spatial\ndynamics. While current ToM benchmarks are limited to text-only or dyadic\ninteractions, ToM-SSI is multimodal and includes group interactions of up to\nfour agents that communicate and move in situated environments. This unique\ndesign allows us to study, for the first time, mixed cooperative-obstructive\nsettings and reasoning about multiple agents' mental state in parallel, thus\ncapturing a wider range of social cognition than existing benchmarks. Our\nevaluations reveal that the current models' performance is still severely\nlimited, especially in these new tasks, highlighting critical gaps for future\nresearch.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05066v2",
      "pdf_url": "http://arxiv.org/pdf/2509.05066v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.275,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a benchmark for evaluating Theory of Mind (ToM) in situated social interactions, focusing on multimodal environments and multi-agent scenarios. It does not mention or involve diffusion-based models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. The core contribution is on ToM assessment, with no connection to multi-step logical reasoning via diffusion mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05071",
      "title": "Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact\n  Detection and Correction",
      "authors": [
        "Mojtaba Safari",
        "Zach Eidex",
        "Richard L. J. Qiu",
        "Matthew Goette",
        "Tonghe Wang",
        "Xiaofeng Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Background: To systematically review and perform a meta-analysis of\nartificial intelligence (AI)-driven methods for detecting and correcting\nmagnetic resonance imaging (MRI) motion artifacts, assessing current\ndevelopments, effectiveness, challenges, and future research directions.\nMethods: A comprehensive systematic review and meta-analysis were conducted,\nfocusing on deep learning (DL) approaches, particularly generative models, for\nthe detection and correction of MRI motion artifacts. Quantitative data were\nextracted regarding utilized datasets, DL architectures, and performance\nmetrics. Results: DL, particularly generative models, show promise for reducing\nmotion artifacts and improving image quality; however, limited\ngeneralizability, reliance on paired training data, and risk of visual\ndistortions remain key challenges that motivate standardized datasets and\nreporting. Conclusions: AI-driven methods, particularly DL generative models,\nshow significant potential for improving MRI image quality by effectively\naddressing motion artifacts. However, critical challenges must be addressed,\nincluding the need for comprehensive public datasets, standardized reporting\nprotocols for artifact levels, and more advanced, adaptable DL techniques to\nreduce reliance on extensive paired datasets. Addressing these aspects could\nsubstantially enhance MRI diagnostic accuracy, reduce healthcare costs, and\nimprove patient care outcomes.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05071v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05071v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.367,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on AI-driven methods for MRI motion artifact detection and correction, including diffusion models for image reconstruction and correction. However, it does not involve adapting diffusion processes for multi-step logical reasoning or treating a chain-of-thought as a holistic entity. The diffusion models mentioned are applied to perceptual image tasks, not complex logical tasks, so there is no clear component aligning with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper includes analysis of datasets used in AI-driven MRI methods, such as evaluating temporal trends in dataset usage (public vs. institutional), the need for standardized datasets, and challenges like reliance on paired training data. This aligns with researching dataset analysis and benchmarking for ML applications. However, the primary focus is on reviewing AI methods, not solely on creating or curating new datasets, making it moderately relevant rather than highly focused.",
      "llm_score_status": "completed",
      "summary": "This paper presents a systematic review and meta-analysis of AI-driven methods, particularly deep learning generative models, for detecting and correcting motion artifacts in MRI, aiming to assess their effectiveness, challenges, and future directions. The methodology involves extracting quantitative data on datasets, architectures, and performance metrics from relevant studies, revealing that while these models improve image quality, issues like limited generalizability and reliance on paired data persist, with recommendations for standardized datasets and more adaptable techniques to enhance diagnostic accuracy and reduce healthcare costs.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces the first comprehensive meta-analysis of AI-driven MRI motion artifact detection and correction, significantly advancing beyond previous narrative reviews by providing quantitative synthesis and temporal trends analysis.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of medical imaging and AI, as it offers practical recommendations for improving MRI techniques, though its influence may be limited to specialized research and clinical applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a valuable and insightful contribution by synthesizing evidence on AI methods for MRI artifact correction, making it essential for researchers in medical imaging to stay informed on advancements and challenges.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b968972d0769979e4cd250c6d93517fa62d28289",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 41,
      "average_h_index": 9.666666666666666,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Mojtaba Safari",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2268406964"
        },
        {
          "name": "Zach Eidex",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2164750978"
        },
        {
          "name": "Richard L. J. Qiu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2346326167"
        },
        {
          "name": "M. Goette",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2237623968"
        },
        {
          "name": "Tonghe Wang",
          "h_index": 41,
          "profile_url": "https://www.semanticscholar.org/author/83100564"
        },
        {
          "name": "Xiaofeng Yang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2284569361"
        }
      ]
    },
    {
      "id": "2509.05072",
      "title": "Finding your MUSE: Mining Unexpected Solutions Engine",
      "authors": [
        "Nir Sweed",
        "Hanit Hakim",
        "Ben Wolfson",
        "Hila Lifshitz",
        "Dafna Shahaf"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Innovators often exhibit cognitive fixation on existing solutions or nascent\nideas, hindering the exploration of novel alternatives. This paper introduces a\nmethodology for constructing Functional Concept Graphs (FCGs), interconnected\nrepresentations of functional elements that support abstraction, problem\nreframing, and analogical inspiration. Our approach yields large-scale,\nhigh-quality FCGs with explicit abstraction relations, overcoming limitations\nof prior work. We further present MUSE, an algorithm leveraging FCGs to\ngenerate creative inspirations for a given problem. We demonstrate our method\nby computing an FCG on 500K patents, which we release for further research.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05072v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05072v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.313,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on constructing Functional Concept Graphs (FCGs) using LLMs to facilitate creative problem-solving and analogical inspiration, as demonstrated with patents and the MUSE algorithm. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05075",
      "title": "GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting",
      "authors": [
        "Yangming Li",
        "Chaoyu Liu",
        "Lihao Liu",
        "Simon Masnou",
        "Carola-Bibiane Schönlieb"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "A few recent works explored incorporating geometric priors to regularize the\noptimization of Gaussian splatting, further improving its performance. However,\nthose early studies mainly focused on the use of low-order geometric priors\n(e.g., normal vector), and they are also unreliably estimated by\nnoise-sensitive methods, like local principal component analysis. To address\ntheir limitations, we first present GeoSplat, a general geometry-constrained\noptimization framework that exploits both first-order and second-order\ngeometric quantities to improve the entire training pipeline of Gaussian\nsplatting, including Gaussian initialization, gradient update, and\ndensification. As an example, we initialize the scales of 3D Gaussian\nprimitives in terms of principal curvatures, leading to a better coverage of\nthe object surface than random initialization. Secondly, based on certain\ngeometric structures (e.g., local manifold), we introduce efficient and\nnoise-robust estimation methods that provide dynamic geometric priors for our\nframework. We conduct extensive experiments on multiple datasets for novel view\nsynthesis, showing that our framework: GeoSplat, significantly improves the\nperformance of Gaussian splatting and outperforms previous baselines.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05075v2",
      "pdf_url": "http://arxiv.org/pdf/2509.05075v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.258,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.307,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05078",
      "title": "Scale-interaction transformer: a hybrid cnn-transformer model for facial\n  beauty prediction",
      "authors": [
        "Djamel Eddine Boukhari"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Automated Facial Beauty Prediction (FBP) is a challenging computer vision\ntask due to the complex interplay of local and global facial features that\ninfluence human perception. While Convolutional Neural Networks (CNNs) excel at\nfeature extraction, they often process information at a fixed scale,\npotentially overlooking the critical inter-dependencies between features at\ndifferent levels of granularity. To address this limitation, we introduce the\nScale-Interaction Transformer (SIT), a novel hybrid deep learning architecture\nthat synergizes the feature extraction power of CNNs with the relational\nmodeling capabilities of Transformers. The SIT first employs a multi-scale\nmodule with parallel convolutions to capture facial characteristics at varying\nreceptive fields. These multi-scale representations are then framed as a\nsequence and processed by a Transformer encoder, which explicitly models their\ninteractions and contextual relationships via a self-attention mechanism. We\nconduct extensive experiments on the widely-used SCUT-FBP5500 benchmark\ndataset, where the proposed SIT model establishes a new state-of-the-art. It\nachieves a Pearson Correlation of 0.9187, outperforming previous methods. Our\nfindings demonstrate that explicitly modeling the interplay between multi-scale\nvisual cues is crucial for high-performance FBP. The success of the SIT\narchitecture highlights the potential of hybrid CNN-Transformer models for\ncomplex image regression tasks that demand a holistic, context-aware\nunderstanding.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05078v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05078v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.276,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.356,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05086",
      "title": "Robust Experts: the Effect of Adversarial Training on CNNs with Sparse\n  Mixture-of-Experts Layers",
      "authors": [
        "Svetlana Pavlitska",
        "Haixi Fan",
        "Konstantin Ditschuneit",
        "J. Marius Zöllner"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Robustifying convolutional neural networks (CNNs) against adversarial attacks\nremains challenging and often requires resource-intensive countermeasures. We\nexplore the use of sparse mixture-of-experts (MoE) layers to improve robustness\nby replacing selected residual blocks or convolutional layers, thereby\nincreasing model capacity without additional inference cost. On ResNet\narchitectures trained on CIFAR-100, we find that inserting a single MoE layer\nin the deeper stages leads to consistent improvements in robustness under PGD\nand AutoPGD attacks when combined with adversarial training. Furthermore, we\ndiscover that when switch loss is used for balancing, it causes routing to\ncollapse onto a small set of overused experts, thereby concentrating\nadversarial training on these paths and inadvertently making them more robust.\nAs a result, some individual experts outperform the gated MoE model in\nrobustness, suggesting that robust subpaths emerge through specialization. Our\ncode is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05086v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05086v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.383,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05091",
      "title": "ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed\n  Feedback",
      "authors": [
        "Matteo Bortoletto",
        "Yichao Zhou",
        "Lance Ying",
        "Tianmin Shu",
        "Andreas Bulling"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "While humans are inherently social creatures, the challenge of identifying\nwhen and how to assist and collaborate with others - particularly when pursuing\nindependent goals - can hinder cooperation. To address this challenge, we aim\nto develop an AI system that provides useful feedback to promote prosocial\nbehaviour - actions that benefit others, even when not directly aligned with\none's own goals. We introduce ProToM, a Theory of Mind-informed facilitator\nthat promotes prosocial actions in multi-agent systems by providing targeted,\ncontext-sensitive feedback to individual agents. ProToM first infers agents'\ngoals using Bayesian inverse planning, then selects feedback to communicate by\nmaximising expected utility, conditioned on the inferred goal distribution. We\nevaluate our approach against baselines in two multi-agent environments: Doors,\nKeys, and Gems, as well as Overcooked. Our results suggest that\nstate-of-the-art large language and reasoning models fall short of\ncommunicating feedback that is both contextually grounded and well-timed -\nleading to higher communication overhead and task speedup. In contrast, ProToM\nprovides targeted and helpful feedback, achieving a higher success rate,\nshorter task completion times, and is consistently preferred by human users.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05091v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05091v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.528,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.3,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces ProToM, a system that uses Bayesian inverse planning and expected utility maximization to infer agents' goals and provide prosocial feedback in multi-agent environments. It includes a human study for evaluation, where human preferences are assessed, but there is no evidence of training an AI model using a reward model derived from human-ranked data, followed by reinforcement learning fine-tuning. Since the core method does not involve RLHF as defined—aligning an AI via human feedback for model training—the paper's contributions are unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05092",
      "title": "Semi-supervised Deep Transfer for Regression without Domain Alignment",
      "authors": [
        "Mainak Biswas",
        "Ambedkar Dukkipati",
        "Devarajan Sridharan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning models deployed in real-world applications (e.g., medicine)\nface challenges because source models do not generalize well to domain-shifted\ntarget data. Many successful domain adaptation (DA) approaches require full\naccess to source data. Yet, such requirements are unrealistic in scenarios\nwhere source data cannot be shared either because of privacy concerns or\nbecause it is too large and incurs prohibitive storage or computational costs.\nMoreover, resource constraints may limit the availability of labeled targets.\nWe illustrate this challenge in a neuroscience setting where source data are\nunavailable, labeled target data are meager, and predictions involve\ncontinuous-valued outputs. We build upon Contradistinguisher (CUDA), an\nefficient framework that learns a shared model across the labeled source and\nunlabeled target samples, without intermediate representation alignment. Yet,\nCUDA was designed for unsupervised DA, with full access to source data, and for\nclassification tasks. We develop CRAFT -- a Contradistinguisher-based\nRegularization Approach for Flexible Training -- for source-free (SF),\nsemi-supervised transfer of pretrained models in regression tasks. We showcase\nthe efficacy of CRAFT in two neuroscience settings: gaze prediction with\nelectroencephalography (EEG) data and ``brain age'' prediction with structural\nMRI data. For both datasets, CRAFT yielded up to 9% improvement in\nroot-mean-squared error (RMSE) over fine-tuned models when labeled training\nexamples were scarce. Moreover, CRAFT leveraged unlabeled target data and\noutperformed four competing state-of-the-art source-free domain adaptation\nmodels by more than 3%. Lastly, we demonstrate the efficacy of CRAFT on two\nother real-world regression benchmarks. We propose CRAFT as an efficient\napproach for source-free, semi-supervised deep transfer for regression that is\nubiquitous in biology and medicine.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05092v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05092v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.439,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.392,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces CRAFT, which uses binning to generate pseudo-labels for regression tasks in a semi-supervised setting, aligning with weak supervision by programmatically creating labels from imprecise sources rather than relying solely on hand-labeled data. However, weak supervision is not the core focus, as the method primarily addresses domain adaptation rather than extensively exploring noisy label generation techniques.",
      "diffusion_reasoning_justification": "The paper focuses on domain adaptation and deep transfer for regression tasks using methods like Contradistinguisher, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not involve treating a Chain-of-Thought as an entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces CRAFT, a Contradistinguisher-based regularization approach for source-free, semi-supervised deep transfer in regression tasks, addressing scenarios where source data is unavailable, such as in neuroscience applications. It extends the Contradistinguisher framework by combining supervised and unsupervised losses to fine-tune pre-trained models on limited labeled target data without domain alignment, achieving up to 9% improvement in RMSE on datasets like EEG-based gaze prediction and MRI-based brain age prediction, and outperforming state-of-the-art methods by more than 3% while demonstrating efficacy on additional benchmarks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by extending the Contradistinguisher framework to source-free domain adaptation for regression tasks, cleverly adapting existing ideas to new challenges like data scarcity and continuous outputs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like neuroscience and computer vision due to its practical applications in real-world regression tasks and publicly available code.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a strong, valuable contribution to domain adaptation for regression with limited data, making it essential for researchers in relevant fields to understand its advancements and potential applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5dbe6e25b57ec4bee5987631e526c5371bbc5d6d",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 16,
      "average_h_index": 6.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Mainak Biswas",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2238340118"
        },
        {
          "name": "Ambedkar Dukkipati",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/2440174"
        },
        {
          "name": "Devarajan Sridharan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2379012207"
        }
      ]
    },
    {
      "id": "2509.05100",
      "title": "ICR: Iterative Clarification and Rewriting for Conversational Search",
      "authors": [
        "Zhiyu Cao",
        "Peifeng Li",
        "Qiaoming Zhu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Most previous work on Conversational Query Rewriting employs an end-to-end\nrewriting paradigm. However, this approach is hindered by the issue of multiple\nfuzzy expressions within the query, which complicates the simultaneous\nidentification and rewriting of multiple positions. To address this issue, we\npropose a novel framework ICR (Iterative Clarification and Rewriting), an\niterative rewriting scheme that pivots on clarification questions. Within this\nframework, the model alternates between generating clarification questions and\nrewritten queries. The experimental results show that our ICR can continuously\nimprove retrieval performance in the clarification-rewriting iterative process,\nthereby achieving state-of-the-art performance on two popular datasets.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05100v2",
      "pdf_url": "http://arxiv.org/pdf/2509.05100v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.299,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions a module called DPO-CRP (Direct Preference Optimization for Clarification-Rewriting Process), which is related to preference optimization techniques often associated with RLHF. However, it does not explicitly involve training a separate reward model on human-ranked data or use human feedback for reinforcement learning, focusing instead on automated iterative processes based on retrieval performance. Thus, it is only tangentially related.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's ICR framework involves an iterative clarification and rewriting process for query refinement, but it does not adapt diffusion models or processes for multi-step logical reasoning. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05112",
      "title": "GenAI-based test case generation and execution in SDV platform",
      "authors": [
        "Denesa Zyberaj",
        "Lukasz Mazur",
        "Nenad Petrovic",
        "Pankhuri Verma",
        "Pascal Hirmer",
        "Dirk Slama",
        "Xiangwei Cheng",
        "Alois Knoll"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces a GenAI-driven approach for automated test case\ngeneration, leveraging Large Language Models and Vision-Language Models to\ntranslate natural language requirements and system diagrams into structured\nGherkin test cases. The methodology integrates Vehicle Signal Specification\nmodeling to standardize vehicle signal definitions, improve compatibility\nacross automotive subsystems, and streamline integration with third-party\ntesting tools. Generated test cases are executed within the digital.auto\nplayground, an open and vendor-neutral environment designed to facilitate rapid\nvalidation of software-defined vehicle functionalities. We evaluate our\napproach using the Child Presence Detection System use case, demonstrating\nsubstantial reductions in manual test specification effort and rapid execution\nof generated tests. Despite significant automation, the generation of test\ncases and test scripts still requires manual intervention due to current\nlimitations in the GenAI pipeline and constraints of the digital.auto platform.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05112v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05112v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.33,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a methodology using Large Language Models (LLMs) and Vision-Language Models (VLMs) for generating test cases from natural language and diagrams, integrated with Vehicle Signal Specification for automotive testing. It does not mention or utilize diffusion-based models, which involve iterative refinement processes for logical tasks. As a result, there is no connection to the topic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05131",
      "title": "A Scalable Attention-Based Approach for Image-to-3D Texture Mapping",
      "authors": [
        "Arianna Rampini",
        "Kanika Madan",
        "Bruno Roy",
        "AmirHossein Zamani",
        "Derek Cheung"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "High-quality textures are critical for realistic 3D content creation, yet\nexisting generative methods are slow, rely on UV maps, and often fail to remain\nfaithful to a reference image. To address these challenges, we propose a\ntransformer-based framework that predicts a 3D texture field directly from a\nsingle image and a mesh, eliminating the need for UV mapping and differentiable\nrendering, and enabling faster texture generation. Our method integrates a\ntriplane representation with depth-based backprojection losses, enabling\nefficient training and faster inference. Once trained, it generates\nhigh-fidelity textures in a single forward pass, requiring only 0.2s per shape.\nExtensive qualitative, quantitative, and user preference evaluations\ndemonstrate that our method outperforms state-of-the-art baselines on\nsingle-image texture reconstruction in terms of both fidelity to the input\nimage and perceptual quality, highlighting its practicality for scalable,\nhigh-quality, and controllable 3D content creation.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05131v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05131v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.351,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a transformer-based framework for efficient 3D texture generation from a single image, using triplane representations and depth-based losses. While it briefly mentions existing methods that rely on image diffusion models for texture generation, it does not involve diffusion models for iterative refinement in logical tasks or Chain-of-Thought reasoning. The main contribution is in computer vision for texture mapping, not in adapting diffusion for reasoning purposes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05139",
      "title": "Evaluation and Comparison Semantics for ODRL",
      "authors": [
        "Jaime Osvaldo Salas",
        "Paolo Pareti",
        "Semih Yumuşak",
        "Soulmaz Gheisari",
        "Luis-Daniel Ibáñez",
        "George Konstantinidis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "We consider the problem of evaluating, and comparing computational policies\nin the Open Digital Rights Language (ODRL), which has become the de facto\nstandard for governing the access and usage of digital resources. Although\npreliminary progress has been made on the formal specification of the\nlanguage's features, a comprehensive formal semantics of ODRL is still missing.\nIn this paper, we provide a simple and intuitive formal semantics for ODRL that\nis based on query answering. Our semantics refines previous formalisations, and\nis aligned with the latest published specification of the language (2.2).\nBuilding on our evaluation semantics, and motivated by data sharing scenarios,\nwe also define and study the problem of comparing two policies, detecting\nequivalent, more restrictive or more permissive policies.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05139v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05139v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.245,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.206,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05144",
      "title": "SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic\n  Mask Splitting and Growing",
      "authors": [
        "Chaolei Wang",
        "Yang Luo",
        "Jing Du",
        "Siyu Chen",
        "Yiping Chen",
        "Ting Han"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate 3D instance segmentation is crucial for high-quality scene\nunderstanding in the 3D vision domain. However, 3D instance segmentation based\non 2D-to-3D lifting approaches struggle to produce precise instance-level\nsegmentation, due to accumulated errors introduced during the lifting process\nfrom ambiguous semantic guidance and insufficient depth constraints. To tackle\nthese challenges, we propose splitting and growing reliable semantic mask for\nhigh-fidelity 3D instance segmentation (SGS-3D), a novel \"split-then-grow\"\nframework that first purifies and splits ambiguous lifted masks using geometric\nprimitives, and then grows them into complete instances within the scene.\nUnlike existing approaches that directly rely on raw lifted masks and sacrifice\nsegmentation accuracy, SGS-3D serves as a training-free refinement method that\njointly fuses semantic and geometric information, enabling effective\ncooperation between the two levels of representation. Specifically, for\nsemantic guidance, we introduce a mask filtering strategy that leverages the\nco-occurrence of 3D geometry primitives to identify and remove ambiguous masks,\nthereby ensuring more reliable semantic consistency with the 3D object\ninstances. For the geometric refinement, we construct fine-grained object\ninstances by exploiting both spatial continuity and high-level features,\nparticularly in the case of semantic ambiguity between distinct objects.\nExperimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that\nSGS-3D substantially improves segmentation accuracy and robustness against\ninaccurate masks from pre-trained models, yielding high-fidelity object\ninstances while maintaining strong generalization across diverse indoor and\noutdoor environments. Code is available in the supplementary materials.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05144v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05144v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.266,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.352,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05145",
      "title": "Exploring Situated Stabilities of a Rhythm Generation System through\n  Variational Cross-Examination",
      "authors": [
        "Błażej Kotowski",
        "Nicholas Evans",
        "Behzad Haki",
        "Frederic Font",
        "Sergi Jordà"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "This paper investigates GrooveTransformer, a real-time rhythm generation\nsystem, through the postphenomenological framework of Variational\nCross-Examination (VCE). By reflecting on its deployment across three distinct\nartistic contexts, we identify three stabilities: an autonomous drum\naccompaniment generator, a rhythmic control voltage sequencer in Eurorack\nformat, and a rhythm driver for a harmonic accompaniment system. The\nversatility of its applications was not an explicit goal from the outset of the\nproject. Thus, we ask: how did this multistability emerge? Through VCE, we\nidentify three key contributors to its emergence: the affordances of system\ninvariants, the interdisciplinary collaboration, and the situated nature of its\ndevelopment. We conclude by reflecting on the viability of VCE as a descriptive\nand analytical method for Digital Musical Instrument (DMI) design, emphasizing\nits value in uncovering how technologies mediate, co-shape, and are co-shaped\nby users and contexts.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05145v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05145v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.232,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.25,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05146",
      "title": "PRIM: Towards Practical In-Image Multilingual Machine Translation",
      "authors": [
        "Yanzhi Tian",
        "Zeming Liu",
        "Zhengyang Liu",
        "Chong Feng",
        "Xin Li",
        "Heyan Huang",
        "Yuhang Guo"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In-Image Machine Translation (IIMT) aims to translate images containing texts\nfrom one language to another. Current research of end-to-end IIMT mainly\nconducts on synthetic data, with simple background, single font, fixed text\nposition, and bilingual translation, which can not fully reflect real world,\ncausing a significant gap between the research and practical conditions. To\nfacilitate research of IIMT in real-world scenarios, we explore Practical\nIn-Image Multilingual Machine Translation (IIMMT). In order to convince the\nlack of publicly available data, we annotate the PRIM dataset, which contains\nreal-world captured one-line text images with complex background, various\nfonts, diverse text positions, and supports multilingual translation\ndirections. We propose an end-to-end model VisTrans to handle the challenge of\npractical conditions in PRIM, which processes visual text and background\ninformation in the image separately, ensuring the capability of multilingual\ntranslation while improving the visual quality. Experimental results indicate\nthe VisTrans achieves a better translation quality and visual effect compared\nto other models. The code and dataset are available at:\nhttps://github.com/BITHLP/PRIM.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05146v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05146v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.357,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05154",
      "title": "VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced\n  Medical Image Segmentation",
      "authors": [
        "Julia Dietlmeier",
        "Oluwabukola Grace Adegboro",
        "Vayangi Ganepola",
        "Claudia Mazo",
        "Noel E. O'Connor"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-language models and their adaptations to image segmentation tasks\npresent enormous potential for producing highly accurate and interpretable\nresults. However, implementations based on CLIP and BiomedCLIP are still\nlagging behind more sophisticated architectures such as CRIS. In this work,\ninstead of focusing on text prompt engineering as is the norm, we attempt to\nnarrow this gap by showing how to ensemble vision-language segmentation models\n(VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice\nscore improvement of 6.3% on the BKAI polyp dataset using the ensembled\nBiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%.\nFurthermore, we provide initial results on additional four radiology and\nnon-radiology datasets. We conclude that ensembling works differently across\nthese datasets (from outperforming to underperforming the CRIS model),\nindicating a topic for future investigation by the community. The code is\navailable at https://github.com/juliadietlmeier/VLSM-Ensemble.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05154v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05154v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.326,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05188",
      "title": "SL-SLR: Self-Supervised Representation Learning for Sign Language\n  Recognition",
      "authors": [
        "Ariel Basso Madjoukeng",
        "Jérôme Fink",
        "Pierre Poitier",
        "Edith Belise Kenmogne",
        "Benoit Frenay"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Sign language recognition (SLR) is a machine learning task aiming to identify\nsigns in videos. Due to the scarcity of annotated data, unsupervised methods\nlike contrastive learning have become promising in this field. They learn\nmeaningful representations by pulling positive pairs (two augmented versions of\nthe same instance) closer and pushing negative pairs (different from the\npositive pairs) apart. In SLR, in a sign video, only certain parts provide\ninformation that is truly useful for its recognition. Applying contrastive\nmethods to SLR raises two issues: (i) contrastive learning methods treat all\nparts of a video in the same way, without taking into account the relevance of\ncertain parts over others; (ii) shared movements between different signs make\nnegative pairs highly similar, complicating sign discrimination. These issues\nlead to learning non-discriminative features for sign recognition and poor\nresults in downstream tasks. In response, this paper proposes a self-supervised\nlearning framework designed to learn meaningful representations for SLR. This\nframework consists of two key components designed to work together: (i) a new\nself-supervised approach with free-negative pairs; (ii) a new data augmentation\ntechnique. This approach shows a considerable gain in accuracy compared to\nseveral contrastive and self-supervised methods, across linear evaluation,\nsemi-supervised learning, and transferability between sign languages.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05188v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05188v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.444,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.338,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on self-supervised learning for sign language recognition, using data augmentations to generate supervisory signals without annotated data. This aligns with weak supervision's goal of reducing reliance on hand-labeled data by programmatically creating labels, as seen in the use of positive pairs from augmentations. However, the method primarily relies on unsupervised techniques like contrastive learning variants, rather than directly employing high-level, noisy, or imprecise sources for label generation, making it only moderately relevant.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition,\" addresses the challenges of sign language recognition due to limited annotated data by proposing a novel self-supervised framework that includes a new approach with free-negative pairs and a specialized data augmentation technique to focus on relevant parts of sign videos. This methodology aims to learn more discriminative representations by making the model invariant to non-relevant elements, resulting in significant accuracy improvements over existing contrastive and self-supervised methods in tasks such as linear evaluation, semi-supervised learning, and transferability across sign languages.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing self-supervised ideas with a new approach and data augmentation tailored for sign language recognition, effectively addressing specific issues like irrelevant video parts and similar movements. However, it builds on established contrastive learning techniques rather than introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in sign language recognition within computer vision by providing better representations and improved accuracy with less data, potentially leading to citations and applications in related subfields. Nonetheless, its impact may be limited to specialized areas rather than broadly across AI or commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to sign language recognition by enhancing self-supervised methods, making it essential for researchers in computer vision and pattern recognition focused on this area. While not groundbreaking for the general AI community, it provides practical insights and superior results compared to existing approaches.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/41fcc68ddc57a9ccad798659b27dca55531827d2",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 4,
      "average_h_index": 1.4,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ariel Basso Madjoukeng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2320574738"
        },
        {
          "name": "Jérôme Fink",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/1409092086"
        },
        {
          "name": "Pierre Poitier",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2185315394"
        },
        {
          "name": "Edith Bélise Kenmogne",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/150091288"
        },
        {
          "name": "Benoît Frénay",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2320570117"
        }
      ]
    },
    {
      "id": "2509.05190",
      "title": "Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based\n  Seizure Detection",
      "authors": [
        "Mounvik K",
        "N Harshit"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deep learning models, especially convolutional neural networks (CNNs), have\nshown considerable promise for biomedical signals such as EEG-based seizure\ndetection. However, these models come with challenges, primarily due to their\nsize and compute requirements in environments where real-time detection or\nlimited resources are available. In this study, we present a lightweight\none-dimensional CNN model with structured pruning to improve efficiency and\nreliability. The model was trained with mild early stopping to address possible\noverfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686.\nStructured pruning of the baseline CNN involved removing 50% of the\nconvolutional kernels based on their importance to model predictions.\nSurprisingly, after pruning the weights and memory by 50%, the new network was\nstill able to maintain predictive capabilities, while modestly increasing\nprecision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we\npresent a convincing case that structured pruning removes redundancy, improves\ngeneralization, and, in combination with mild early stopping, achieves a\npromising way forward to improve seizure detection efficiency and reliability,\nwhich is clear motivation for resource-limited settings.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05190v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05190v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.333,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05197",
      "title": "AI Agents for Web Testing: A Case Study in the Wild",
      "authors": [
        "Naimeng Ye",
        "Xiao Yu",
        "Ruize Xu",
        "Tianyi Peng",
        "Zhou Yu"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Automated web testing plays a critical role in ensuring high-quality user\nexperiences and delivering business value. Traditional approaches primarily\nfocus on code coverage and load testing, but often fall short of capturing\ncomplex user behaviors, leaving many usability issues undetected. The emergence\nof large language models (LLM) and AI agents opens new possibilities for web\ntesting by enabling human-like interaction with websites and a general\nawareness of common usability problems. In this work, we present WebProber, a\nprototype AI agent-based web testing framework. Given a URL, WebProber\nautonomously explores the website, simulating real user interactions,\nidentifying bugs and usability issues, and producing a human-readable report.\nWe evaluate WebProber through a case study of 120 academic personal websites,\nwhere it uncovered 29 usability issues--many of which were missed by\ntraditional tools. Our findings highlight agent-based testing as a promising\ndirection while outlining directions for developing next-generation,\nuser-centered testing frameworks.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05197v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05197v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.292,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05198",
      "title": "Enhancing 3D Point Cloud Classification with ModelNet-R and\n  Point-SkipNet",
      "authors": [
        "Mohammad Saeid",
        "Amir Salarpour",
        "Pedram MohajerAnsari"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "The classification of 3D point clouds is crucial for applications such as\nautonomous driving, robotics, and augmented reality. However, the commonly used\nModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D\ndata, size mismatches, and inadequate class differentiation, which hinder model\nperformance. This paper introduces ModelNet-R, a meticulously refined version\nof ModelNet40 designed to address these issues and serve as a more reliable\nbenchmark. Additionally, this paper proposes Point-SkipNet, a lightweight\ngraph-based neural network that leverages efficient sampling, neighborhood\ngrouping, and skip connections to achieve high classification accuracy with\nreduced computational overhead. Extensive experiments demonstrate that models\ntrained in ModelNet-R exhibit significant performance improvements. Notably,\nPoint-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a\nsubstantially lower parameter count compared to contemporary models. This\nresearch highlights the crucial role of dataset quality in optimizing model\nefficiency for 3D point cloud classification. For more details, see the code\nat: https://github.com/m-saeid/ModeNetR_PointSkipNet.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05198v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05198v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.401,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on refining a dataset (ModelNet-R) and proposing an efficient neural network architecture (Point-SkipNet) for 3D point cloud classification. It does not discuss distributed training, parallel computing, multi-node machine learning, or any methods for partitioning data or computation across multiple processors or nodes. The emphasis is on model efficiency for resource-constrained environments, not on distributed systems.",
      "datasets_justification": "The paper's main contributions include introducing ModelNet-R, a refined version of the ModelNet40 dataset, by addressing issues like inconsistent labeling and low-quality data, as well as evaluating its impact on model performance. This directly aligns with research on creating, curating, benchmarking, and analyzing datasets for machine learning applications, making it a core focus of the paper.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of the ModelNet40 dataset by introducing ModelNet-R, a refined version that corrects inconsistencies such as mislabeling and inclusion of 2D data, thereby providing a more reliable benchmark for 3D point cloud classification. It also proposes Point-SkipNet, a lightweight graph-based neural network that incorporates efficient sampling, neighborhood grouping, and skip connections to achieve high accuracy with reduced computational overhead, demonstrating through extensive experiments that models trained on ModelNet-R, including Point-SkipNet, show significant performance improvements and state-of-the-art results with fewer parameters.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by refining the existing ModelNet40 dataset into ModelNet-R and introducing Point-SkipNet as a clever combination of graph-based techniques and skip connections, addressing known issues without introducing an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in 3D point cloud classification by providing a better dataset and an efficient model, particularly in resource-constrained applications like robotics, though its reach may be confined to specific subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable contributions in dataset refinement and model efficiency, making it essential for researchers focused on 3D vision and machine learning, though not universally critical.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6f2dff05fb08d162557bbb762317ebd27ebcf2be",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 7,
      "average_h_index": 3.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Mohammad Saeid",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379555018"
        },
        {
          "name": "Amir Salarpour",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2333593532"
        },
        {
          "name": "Pedram MohajerAnsari",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2249110741"
        }
      ]
    },
    {
      "id": "2509.05201",
      "title": "Robust Model Predictive Control Design for Autonomous Vehicles with\n  Perception-based Observers",
      "authors": [
        "Nariman Niknejad",
        "Gokul S. Sankar",
        "Bahare Kiumarsi",
        "Hamidreza Modares"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "This paper presents a robust model predictive control (MPC) framework that\nexplicitly addresses the non-Gaussian noise inherent in deep learning-based\nperception modules used for state estimation. Recognizing that accurate\nuncertainty quantification of the perception module is essential for safe\nfeedback control, our approach departs from the conventional assumption of\nzero-mean noise quantification of the perception error. Instead, it employs\nset-based state estimation with constrained zonotopes to capture biased,\nheavy-tailed uncertainties while maintaining bounded estimation errors. To\nimprove computational efficiency, the robust MPC is reformulated as a linear\nprogram (LP), using a Minkowski-Lyapunov-based cost function with an added\nslack variable to prevent degenerate solutions. Closed-loop stability is\nensured through Minkowski-Lyapunov inequalities and contractive zonotopic\ninvariant sets. The largest stabilizing terminal set and its corresponding\nfeedback gain are then derived via an ellipsoidal approximation of the\nzonotopes. The proposed framework is validated through both simulations and\nhardware experiments on an omnidirectional mobile robot along with a camera and\na convolutional neural network-based perception module implemented within a\nROS2 framework. The results demonstrate that the perception-aware MPC provides\nstable and accurate control performance under heavy-tailed noise conditions,\nsignificantly outperforming traditional Gaussian-noise-based designs in terms\nof both state estimation error bounding and overall control performance.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05201v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05201v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.329,
      "datasets_score": 0.241,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05207",
      "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
      "authors": [
        "Arefin Niam",
        "Tevfik Kosar",
        "M S Q Zulkar Nine"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05207v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05207v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.567,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper introduces RapidGNN, a framework specifically designed for distributed training of Graph Neural Networks (GNNs), focusing on optimizing communication and energy efficiency across multiple nodes. It addresses core aspects of distributed training, such as partitioning data across machines, minimizing communication overhead through caching and prefetching, and achieving near-linear scalability with increasing computing units. These elements directly align with the topic's emphasis on algorithms and systems that accelerate model training by partitioning data and computation across processors or nodes, making the paper's contributions a direct fit.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "RapidGNN is a distributed training framework for Graph Neural Networks (GNNs) designed to address communication overhead and energy inefficiency in large-scale graphs by incorporating deterministic sampling, efficient cache construction, and asynchronous prefetching of remote features. The methodology includes embedding fixed-size feature caches in each worker, implementing an adaptive dual-buffer caching policy to prioritize frequently accessed nodes, and using an asynchronous prefetcher to pipeline communication with computation, thereby reducing remote feature fetches and overall training time. Key findings from evaluations on benchmark datasets show that RapidGNN improves end-to-end training throughput by 2.46x to 3.00x, reduces remote feature fetches by 9.70x to 15.39x, achieves near-linear scalability with additional computing units, and enhances energy efficiency by 44% on CPU and 32% on GPU.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing sampling techniques with innovative caching and prefetching strategies to efficiently reduce communication overhead in distributed GNN training, though it builds on prior work rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of distributed graph neural networks due to its practical improvements in training efficiency and energy savings, but its influence may remain confined to specific applications in machine learning scalability.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides significant advancements in addressing key bottlenecks in GNN training, making it valuable for researchers focused on distributed machine learning and large-scale graph processing, though it is not essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/154fd78ab73bfc0474a6887cd77ae611be0e413a",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Arefin Niam",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2137416246"
        },
        {
          "name": "Tevfik Kosar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2287979013"
        },
        {
          "name": "M. S. Q. Z. Nine",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/98018711"
        }
      ]
    },
    {
      "id": "2509.05208",
      "title": "Symbolic Graphics Programming with Large Language Models",
      "authors": [
        "Yamei Chen",
        "Haoquan Zhang",
        "Yangyi Huang",
        "Zeju Qiu",
        "Kaipeng Zhang",
        "Yandong Wen",
        "Weiyang Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large language models (LLMs) excel at program synthesis, yet their ability to\nproduce symbolic graphics programs (SGPs) that render into precise visual\ncontent remains underexplored. We study symbolic graphics programming, where\nthe goal is to generate an SGP from a natural-language description. This task\nalso serves as a lens into how LLMs understand the visual world by prompting\nthem to generate images rendered from SGPs. Among various SGPs, our paper\nsticks to scalable vector graphics (SVGs). We begin by examining the extent to\nwhich LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a\ncomprehensive benchmark covering object fidelity, scene fidelity, and\ncompositionality (attribute binding, spatial relations, numeracy). On\nSGP-GenBench, we discover that frontier proprietary models substantially\noutperform open-source models, and performance correlates well with general\ncoding capabilities. Motivated by this gap, we aim to improve LLMs' ability to\ngenerate SGPs. We propose a reinforcement learning (RL) with verifiable rewards\napproach, where a format-validity gate ensures renderable SVG, and a\ncross-modal reward aligns text and the rendered image via strong vision\nencoders (e.g., SigLIP for text-image and DINO for image-image). Applied to\nQwen-2.5-7B, our method substantially improves SVG generation quality and\nsemantics, achieving performance on par with frontier systems. We further\nanalyze training dynamics, showing that RL induces (i) finer decomposition of\nobjects into controllable primitives and (ii) contextual details that improve\nscene coherence. Our results demonstrate that symbolic graphics programming\noffers a precise and interpretable lens on cross-modal grounding.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05208v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05208v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.507,
      "distributed_training_score": 0.341,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs reinforcement learning with automated rewards from vision encoders (e.g., SigLIP and DINO) for aligning generated SVGs with text, but it does not use human feedback or a reward model trained on human-ranked data, which is required for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on LLMs for generating symbolic graphics programs and uses reinforcement learning for improvement, with no mention of diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05218",
      "title": "HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range\n  Dependency Modeling in Large Language Models",
      "authors": [
        "Chang Dai",
        "Hongyu Shan",
        "Mingyang Song",
        "Di Liang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Positional encoding mechanisms enable Transformers to model sequential\nstructure and long-range dependencies in text. While absolute positional\nencodings struggle with extrapolation to longer sequences due to fixed\npositional representations, and relative approaches like Alibi exhibit\nperformance degradation on extremely long contexts, the widely-used Rotary\nPositional Encoding (RoPE) introduces oscillatory attention patterns that\nhinder stable long-distance dependency modelling. We address these limitations\nthrough a geometric reformulation of positional encoding. Drawing inspiration\nfrom Lorentz transformations in hyperbolic geometry, we propose Hyperbolic\nRotary Positional Encoding (HoPE), which leverages hyperbolic functions to\nimplement Lorentz rotations on token representations. Theoretical analysis\ndemonstrates that RoPE is a special case of our generalized formulation. HoPE\nfundamentally resolves RoPE's slation issues by enforcing monotonic decay of\nattention weights with increasing token distances. Extensive experimental\nresults, including perplexity evaluations under several extended sequence\nbenchmarks, show that HoPE consistently exceeds existing positional encoding\nmethods. These findings underscore HoPE's enhanced capacity for representing\nand generalizing long-range dependencies. Data and code will be available.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05218v2",
      "pdf_url": "http://arxiv.org/pdf/2509.05218v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.344,
      "datasets_score": 0.242,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving positional encoding in Transformers for better long-range dependencies using hyperbolic geometry, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses hyperbolic functions and Lorentz transformations for positional encoding in Transformers, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05230",
      "title": "CURE: Controlled Unlearning for Robust Embeddings -- Mitigating\n  Conceptual Shortcuts in Pre-Trained Language Models",
      "authors": [
        "Aysenur Kocak",
        "Shuo Yang",
        "Bardh Prenkaj",
        "Gjergji Kasneci"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Pre-trained language models have achieved remarkable success across diverse\napplications but remain susceptible to spurious, concept-driven correlations\nthat impair robustness and fairness. In this work, we introduce CURE, a novel\nand lightweight framework that systematically disentangles and suppresses\nconceptual shortcuts while preserving essential content information. Our method\nfirst extracts concept-irrelevant representations via a dedicated content\nextractor reinforced by a reversal network, ensuring minimal loss of\ntask-relevant information. A subsequent controllable debiasing module employs\ncontrastive learning to finely adjust the influence of residual conceptual\ncues, enabling the model to either diminish harmful biases or harness\nbeneficial correlations as appropriate for the target task. Evaluated on the\nIMDB and Yelp datasets using three pre-trained architectures, CURE achieves an\nabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,\nwhile introducing minimal computational overhead. Our approach establishes a\nflexible, unsupervised blueprint for combating conceptual biases, paving the\nway for more reliable and fair language understanding systems.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05230v2",
      "pdf_url": "http://arxiv.org/pdf/2509.05230v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.303,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces CURE, a framework for mitigating conceptual shortcuts in pre-trained language models through unsupervised methods like content extraction, reversal networks, and contrastive learning. It focuses on debiasing and robustness in language models without involving reinforcement learning, human feedback, reward models, or fine-tuning based on human-ranked data. Therefore, it does not align with RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05238",
      "title": "Uncertain but Useful: Leveraging CNN Variability into Data Augmentation",
      "authors": [
        "Inés Gonzalez-Pepe",
        "Vinuyan Sivakolunthu",
        "Yohan Chatelain",
        "Tristan Glatard"
      ],
      "categories": [
        "math.NA (Numerical Analysis)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)"
      ],
      "abstract": "Deep learning (DL) is rapidly advancing neuroimaging by achieving\nstate-of-the-art performance with reduced computation times. Yet the numerical\nstability of DL models -- particularly during training -- remains\nunderexplored. While inference with DL is relatively stable, training\nintroduces additional variability primarily through iterative stochastic\noptimization. We investigate this training-time variability using FastSurfer, a\nCNN-based whole-brain segmentation pipeline. Controlled perturbations are\nintroduced via floating point perturbations and random seeds. We find that: (i)\nFastSurfer exhibits higher variability compared to that of a traditional\nneuroimaging pipeline, suggesting that DL inherits and is particularly\nsusceptible to sources of instability present in its predecessors; (ii)\nensembles generated with perturbations achieve performance similar to an\nunperturbed baseline; and (iii) variability effectively produces ensembles of\nnumerical model families that can be repurposed for downstream applications. As\na proof of concept, we demonstrate that numerical ensembles can be used as a\ndata augmentation strategy for brain age regression. These findings position\ntraining-time variability not only as a reproducibility concern but also as a\nresource that can be harnessed to improve robustness and enable new\napplications in neuroimaging.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05238v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05238v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.45,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.412,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on leveraging variability in CNN training for data augmentation in neuroimaging, using perturbations like floating point noise and random seeds. It does not involve programmatically generating training labels from noisy or imprecise sources, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "The paper examines numerical variability in CNN training and uses techniques like Monte Carlo Arithmetic for perturbations, but it does not involve diffusion models for multi-step logical reasoning or iterative refinement of reasoning paths. It is centered on neuroimaging applications, not complex logical tasks.",
      "distributed_training_justification": "The paper investigates variability in single CNN training processes and does not address distributed training, parallel computing, multi-node setups, or partitioning data/computation across processors. Its focus is on introducing perturbations for augmentation, not accelerating training via distribution.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05249",
      "title": "COGITAO: A Visual Reasoning Framework To Study Compositionality &\n  Generalization",
      "authors": [
        "Yassine Taoudi-Benchekroun",
        "Klim Troyan",
        "Pascal Sager",
        "Stefan Gerber",
        "Lukas Tuggener",
        "Benjamin Grewe"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The ability to compose learned concepts and apply them in novel settings is\nkey to human intelligence, but remains a persistent limitation in\nstate-of-the-art machine learning models. To address this issue, we introduce\nCOGITAO, a modular and extensible data generation framework and benchmark\ndesigned to systematically study compositionality and generalization in visual\ndomains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs\nrule-based tasks which apply a set of transformations to objects in grid-like\nenvironments. It supports composition, at adjustable depth, over a set of 28\ninteroperable transformations, along with extensive control over grid\nparametrization and object properties. This flexibility enables the creation of\nmillions of unique task rules -- surpassing concurrent datasets by several\norders of magnitude -- across a wide range of difficulties, while allowing\nvirtually unlimited sample generation per rule. We provide baseline experiments\nusing state-of-the-art vision models, highlighting their consistent failures to\ngeneralize to novel combinations of familiar elements, despite strong in-domain\nperformance. COGITAO is fully open-sourced, including all code and datasets, to\nsupport continued research in this field.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05249v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05249v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.504,
      "distributed_training_score": 0.341,
      "datasets_score": 0.42,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing COGITAO, a framework for generating visual reasoning tasks to study compositionality and generalization, using rule-based transformations. It does not mention or involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution is the development and introduction of COGITAO, a modular framework for generating and benchmarking datasets to evaluate compositional generalization in visual domains. It includes dataset creation methodologies, extensive parametrization for task generation, and baseline evaluations, directly aligning with research on creating, analyzing, and benchmarking datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces COGITAO, a modular framework for generating visual reasoning tasks to evaluate compositionality and generalization in machine learning models, inspired by ARC-AGI and focusing on applying sequences of 28 interoperable transformations to objects in grid-like environments. It provides extensive control over task parameters, enabling the creation of millions of unique tasks, and demonstrates through baseline experiments with state-of-the-art vision models that these models perform well on in-domain tasks but fail to generalize to novel combinations of familiar elements, underscoring the need for advancements in compositional generalization.",
      "novelty_score": "High",
      "novelty_justification": "COGITAO introduces a truly new and extensible framework for systematically studying compositionality and generalization in vision, significantly advancing beyond existing benchmarks by offering unprecedented flexibility and scale in task generation. This represents a substantial innovation in the field, as it addresses gaps in current visual benchmarks with a novel approach to creating and evaluating compositional tasks.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfields of computer vision and artificial intelligence, given its open-sourced nature and provision of benchmarks for a key research challenge. However, its influence may be limited to specific areas focused on compositional generalization rather than broadly across all AI or commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution with a valuable framework and empirical insights that are essential for researchers studying compositional generalization in vision. It is particularly relevant for those in AI and computer vision, making it a strong but not universally essential read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fb29640b3fee782a6a020d7794744f53a00d6cc6",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 8,
      "average_h_index": 2.3333333333333335,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yassine Taoudi-Benchekroun",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2321675909"
        },
        {
          "name": "Klim Troyan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379550534"
        },
        {
          "name": "Pascal Sager",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2277761326"
        },
        {
          "name": "Stefan Gerber",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379558860"
        },
        {
          "name": "Lukas Tuggener",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/41019567"
        },
        {
          "name": "Benjamin F. Grewe",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2321628283"
        }
      ]
    },
    {
      "id": "2509.05256",
      "title": "Recomposer: Event-roll-guided generative audio editing",
      "authors": [
        "Daniel P. W. Ellis",
        "Eduardo Fonseca",
        "Ron J. Weiss",
        "Kevin Wilson",
        "Scott Wisdom",
        "Hakan Erdogan",
        "John R. Hershey",
        "Aren Jansen",
        "R. Channing Moore",
        "Manoj Plakal"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Editing complex real-world sound scenes is difficult because individual sound\nsources overlap in time. Generative models can fill-in missing or corrupted\ndetails based on their strong prior understanding of the data domain. We\npresent a system for editing individual sound events within complex scenes able\nto delete, insert, and enhance individual sound events based on textual edit\ndescriptions (e.g., ``enhance Door'') and a graphical representation of the\nevent timing derived from an ``event roll'' transcription. We present an\nencoder-decoder transformer working on SoundStream representations, trained on\nsynthetic (input, desired output) audio example pairs formed by adding isolated\nsound events to dense, real-world backgrounds. Evaluation reveals the\nimportance of each part of the edit descriptions -- action, class, timing. Our\nwork demonstrates ``recomposition'' is an important and practical application.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05256v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05256v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.277,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a transformer-based generative model for audio editing, focusing on modifying sound events in audio scenes using textual descriptions and event rolls. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05258",
      "title": "Scaling Performance of Large Language Model Pretraining",
      "authors": [
        "Alexander Interrante-Grant",
        "Carla Varela-Rosa",
        "Suhaas Narayan",
        "Chris Connelly",
        "Albert Reuther"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) show best-in-class performance across a wide\nrange of natural language processing applications. Training these models is an\nextremely computationally expensive task; frontier Artificial Intelligence (AI)\nresearch companies are investing billions of dollars into supercomputing\ninfrastructure to train progressively larger models on increasingly massive\ndatasets. Unfortunately, information about the scaling performance and training\nconsiderations of these large training pipelines is scarce in public\nliterature. Working with large-scale datasets and models can be complex and\npractical recommendations are scarce in the public literature for tuning\ntraining performance when scaling up large language models. In this paper, we\naim to demystify the large language model pretraining pipeline somewhat - in\nparticular with respect to distributed training, managing large datasets across\nhundreds of nodes, and scaling up data parallelism with an emphasis on fully\nleveraging available GPU compute capacity.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05258v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05258v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.631,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on scaling performance in LLM pretraining, including distributed training and dataset management, but does not mention reinforcement learning, human feedback, reward models, or any alignment techniques.",
      "weak_supervision_justification": "The paper discusses managing and scaling large datasets for LLM pretraining but does not address weak supervision techniques, such as programmatically generating labels from noisy sources.",
      "diffusion_reasoning_justification": "The paper is centered on LLM pretraining and distributed training, with no reference to diffusion models, iterative refinement for reasoning, or multi-step logical processes.",
      "distributed_training_justification": "The paper's main contribution is on scaling LLM pretraining through distributed training, including data parallelism, managing datasets across nodes, and optimizing GPU compute, which directly aligns with distributed training concepts.",
      "datasets_justification": "The paper mentions managing and scaling large datasets in the context of distributed training, but it does not focus on creating, analyzing, benchmarking, or evaluating datasets as a primary contribution; it's secondary to training performance.",
      "llm_score_status": "completed",
      "summary": "This paper investigates the scaling performance of large language model (LLM) pretraining, focusing on distributed training, dataset management across hundreds of nodes, and optimizing data parallelism to maximize GPU compute utilization. The authors conduct experiments by pretraining an LLM for a novel language application on a supercomputing cluster with Nvidia H100 GPUs, increasing both model and dataset sizes, and share practical lessons learned to address the scarcity of public guidance on these topics.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing distributed training techniques with practical recommendations for scaling LLMs, addressing a known problem in a new, accessible way for researchers. While it doesn't introduce a entirely new architecture, it advances the state-of-the-art through empirical insights and best practices that are scarce in public literature.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of distributed computing and AI, as it provides actionable advice for efficiently scaling LLM training. However, its influence may be limited to specific researchers dealing with supercomputing infrastructure rather than broader commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers high-quality, practical insights into LLM scaling that are essential for researchers in AI and distributed computing, making it a valuable contribution worth reviewing. While not groundbreaking, its detailed recommendations address a critical gap in public knowledge, positioning it as an important read for those involved in large-scale model training.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3b0a1b9b9d7dbbbea06df4e2646df0e1ed637220",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Alexander Interrante-Grant",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379545566"
        },
        {
          "name": "Carla Varela-Rosa",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379545559"
        },
        {
          "name": "Suhaas Narayan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379546780"
        },
        {
          "name": "Chris Connelly",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379558500"
        },
        {
          "name": "Albert Reuther",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379544860"
        }
      ]
    },
    {
      "id": "2509.05263",
      "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
      "authors": [
        "Yinglin Duan",
        "Zhengxia Zou",
        "Tongwei Gu",
        "Wei Jia",
        "Zhan Zhao",
        "Luyi Xu",
        "Xinzhu Liu",
        "Yenan Lin",
        "Hao Jiang",
        "Kang Chen",
        "Shuang Qiu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05263v2",
      "pdf_url": "http://arxiv.org/pdf/2509.05263v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.486,
      "distributed_training_score": 0.376,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a framework for 3D world generation using LLMs and rendering engines, with no mention of reinforcement learning, human feedback for model alignment, or training a reward model based on human-ranked data. While GPT-4o is used for dataset annotation, this is for data preparation, not RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper references diffusion models in the introduction as related work for scene generation, but its main contribution, LatticeWorld, relies on LLMs for spatial understanding and generation, without incorporating diffusion-based iterative refinement or multi-step logical reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05276",
      "title": "SpikingBrain Technical Report: Spiking Brain-inspired Large Models",
      "authors": [
        "Yuqi Pan",
        "Yupeng Feng",
        "Jinghao Zhuang",
        "Siyu Ding",
        "Zehao Liu",
        "Bohan Sun",
        "Yuhong Chou",
        "Han Xu",
        "Xuerui Qiu",
        "Anlin Deng",
        "Anjie Hu",
        "Peng Zhou",
        "Man Yao",
        "Jibin Wu",
        "Jian Yang",
        "Guoliang Sun",
        "Bo Xu",
        "Guoqi Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Mainstream Transformer-based large language models face major efficiency\nbottlenecks: training computation scales quadratically with sequence length,\nand inference memory grows linearly, limiting long-context processing. Building\nlarge models on non-NVIDIA platforms also poses challenges for stable and\nefficient training. To address this, we introduce SpikingBrain, a family of\nbrain-inspired models designed for efficient long-context training and\ninference. SpikingBrain leverages the MetaX GPU cluster and focuses on three\naspects: (1) Model Architecture: linear and hybrid-linear attention\narchitectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an\nefficient, conversion-based training pipeline and a dedicated spike coding\nframework; (3) System Engineering: customized training frameworks, operator\nlibraries, and parallelism strategies tailored to MetaX hardware.\n  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,\nand SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the\nfeasibility of large-scale LLM development on non-NVIDIA platforms.\nSpikingBrain achieves performance comparable to open-source Transformer\nbaselines while using only about 150B tokens for continual pre-training. Our\nmodels significantly improve long-sequence training efficiency and deliver\ninference with (partially) constant memory and event-driven spiking behavior.\nFor example, SpikingBrain-7B attains over 100x speedup in Time to First Token\nfor 4M-token sequences. Training remains stable for weeks on hundreds of MetaX\nC550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4\npercent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling\nlow-power operation. Overall, this work demonstrates the potential of\nbrain-inspired mechanisms to drive the next generation of efficient and\nscalable large model design.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05276v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05276v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.502,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on brain-inspired spiking models, efficient attention mechanisms, and training on non-NVIDIA platforms, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It does not adapt diffusion for Chain-of-Thought or holistic reasoning correction.",
      "distributed_training_justification": "The paper extensively discusses distributed training on hundreds of MetaX C550 GPUs, including adaptations of frameworks, operators, parallelism strategies, and communication primitives for stable large-scale training. This directly aligns with distributed training, parallel computing, and multi-node machine learning techniques for partitioning computation and data.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The SpikingBrain technical report presents a family of brain-inspired large language models aimed at addressing efficiency bottlenecks in Transformer-based LLMs, particularly for long-context processing on non-NVIDIA platforms like MetaX GPUs. By integrating linear and hybrid-linear attention architectures with adaptive spiking neurons, efficient conversion-based training pipelines, and customized system optimizations, the authors develop models such as SpikingBrain-7B and SpikingBrain-76B, which achieve performance comparable to baselines using only about 150B tokens, demonstrate over 100x speedup in Time to First Token for long sequences, and exhibit 69.15% sparsity for low-power inference, highlighting the potential of brain-inspired mechanisms for scalable and efficient AI.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing ideas like linear attention with brain-inspired spiking neurons and hybrid architectures to enhance efficiency in LLMs, though it builds on prior work rather than introducing a entirely new problem. This clever integration addresses known challenges in a new way, particularly for non-NVIDIA platforms, but does not represent a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in efficient AI training on alternative hardware and inspire developments in neuromorphic computing, given its demonstrations of significant speedups and sparsity. However, its impact may be confined to specific subfields like brain-inspired models and hardware-specific optimizations, rather than broadly across AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by advancing efficient LLM designs and demonstrating feasibility on non-NVIDIA platforms, making it important for researchers in AI efficiency and brain-inspired computing. While not essential for all, it provides practical insights and results that warrant attention from relevant experts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/737fe556dfa5575de0981fcdd04f192055b59935",
      "total_authors": 18,
      "authors_found": 18,
      "highest_h_index": 11,
      "average_h_index": 1.8888888888888888,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Yuqi Pan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2332050325"
        },
        {
          "name": "Yupeng Feng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380852404"
        },
        {
          "name": "Jinghao Zhuang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379543895"
        },
        {
          "name": "Siyu Ding",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379987963"
        },
        {
          "name": "Zehao Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373703992"
        },
        {
          "name": "Bohan Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379945900"
        },
        {
          "name": "Yuhong Chou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372426938"
        },
        {
          "name": "Han Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2375430985"
        },
        {
          "name": "Xuerui Qiu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2313723303"
        },
        {
          "name": "Anlin Deng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379549111"
        },
        {
          "name": "Anjie Hu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379549790"
        },
        {
          "name": "Peng Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379611054"
        },
        {
          "name": "Man Yao",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2144049133"
        },
        {
          "name": "Jibin Wu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2331372071"
        },
        {
          "name": "Jian Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380364297"
        },
        {
          "name": "Guoliang Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379986333"
        },
        {
          "name": "Boxing Xu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2107802636"
        },
        {
          "name": "Guoqi Li",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2243952733"
        }
      ]
    },
    {
      "id": "2509.05291",
      "title": "Crosscoding Through Time: Tracking Emergence & Consolidation Of\n  Linguistic Representations Throughout LLM Pretraining",
      "authors": [
        "Deniz Bayazit",
        "Aaron Mueller",
        "Antoine Bosselut"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large language models (LLMs) learn non-trivial abstractions during\npretraining, like detecting irregular plural noun subjects. However, it is not\nwell understood when and how specific linguistic abilities emerge as\ntraditional evaluation methods such as benchmarking fail to reveal how models\nacquire concepts and capabilities. To bridge this gap and better understand\nmodel training at the concept level, we use sparse crosscoders to discover and\nalign features across model checkpoints. Using this approach, we track the\nevolution of linguistic features during pretraining. We train crosscoders\nbetween open-sourced checkpoint triplets with significant performance and\nrepresentation shifts, and introduce a novel metric, Relative Indirect Effects\n(RelIE), to trace training stages at which individual features become causally\nimportant for task performance. We show that crosscoders can detect feature\nemergence, maintenance, and discontinuation during pretraining. Our approach is\narchitecture-agnostic and scalable, offering a promising path toward more\ninterpretable and fine-grained analysis of representation learning throughout\npretraining.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05291v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05291v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.375,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on analyzing the evolution of linguistic representations in LLMs during pretraining using sparse crosscoders, with no mention of reinforcement learning, human feedback, reward models, or alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines feature emergence in LLMs during pretraining through crosscoders and a new metric, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05296",
      "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool",
      "authors": [
        "Zizun Li",
        "Jianjun Zhou",
        "Yifan Wang",
        "Haoyu Guo",
        "Wenzheng Chang",
        "Yang Zhou",
        "Haoyi Zhu",
        "Junyi Chen",
        "Chunhua Shen",
        "Tong He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present WinT3R, a feed-forward reconstruction model capable of online\nprediction of precise camera poses and high-quality point maps. Previous\nmethods suffer from a trade-off between reconstruction quality and real-time\nperformance. To address this, we first introduce a sliding window mechanism\nthat ensures sufficient information exchange among frames within the window,\nthereby improving the quality of geometric predictions without large\ncomputation. In addition, we leverage a compact representation of cameras and\nmaintain a global camera token pool, which enhances the reliability of camera\npose estimation without sacrificing efficiency. These designs enable WinT3R to\nachieve state-of-the-art performance in terms of online reconstruction quality,\ncamera pose estimation, and reconstruction speed, as validated by extensive\nexperiments on diverse datasets. Code and model are publicly available at\nhttps://github.com/LiZizun/WinT3R.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05296v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05296v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.372,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05297",
      "title": "FlowSeek: Optical Flow Made Easier with Depth Foundation Models and\n  Motion Bases",
      "authors": [
        "Matteo Poggi",
        "Fabio Tosi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present FlowSeek, a novel framework for optical flow requiring minimal\nhardware resources for training. FlowSeek marries the latest advances on the\ndesign space of optical flow networks with cutting-edge single-image depth\nfoundation models and classical low-dimensional motion parametrization,\nimplementing a compact, yet accurate architecture. FlowSeek is trained on a\nsingle consumer-grade GPU, a hardware budget about 8x lower compared to most\nrecent methods, and still achieves superior cross-dataset generalization on\nSintel Final and KITTI, with a relative improvement of 10 and 15% over the\nprevious state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow\ndatasets.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05297v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05297v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.373,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05382",
      "title": "User Privacy and Large Language Models: An Analysis of Frontier\n  Developers' Privacy Policies",
      "authors": [
        "Jennifer King",
        "Kevin Klyman",
        "Emily Capstick",
        "Tiffany Saade",
        "Victoria Hsieh"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Hundreds of millions of people now regularly interact with large language\nmodels via chatbots. Model developers are eager to acquire new sources of\nhigh-quality training data as they race to improve model capabilities and win\nmarket share. This paper analyzes the privacy policies of six U.S. frontier AI\ndevelopers to understand how they use their users' chats to train models.\nDrawing primarily on the California Consumer Privacy Act, we develop a novel\nqualitative coding schema that we apply to each developer's relevant privacy\npolicies to compare data collection and use practices across the six companies.\nWe find that all six developers appear to employ their users' chat data to\ntrain and improve their models by default, and that some retain this data\nindefinitely. Developers may collect and train on personal information\ndisclosed in chats, including sensitive information such as biometric and\nhealth data, as well as files uploaded by users. Four of the six companies we\nexamined appear to include children's chat data for model training, as well as\ncustomer data from other products. On the whole, developers' privacy policies\noften lack essential information about their practices, highlighting the need\nfor greater transparency and accountability. We address the implications of\nusers' lack of consent for the use of their chat data for model training, data\nsecurity issues arising from indefinite chat data retention, and training on\nchildren's chat data. We conclude by providing recommendations to policymakers\nand developers to address the data privacy challenges posed by LLM-powered\nchatbots.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05382v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05382v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.449,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.367,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on analyzing privacy policies of AI developers regarding user chat data for general LLM training, emphasizing data collection, retention, and privacy implications. It does not discuss or contribute to reinforcement learning from human feedback (RLHF), which involves specific techniques like training reward models on human-ranked data for model fine-tuning. Since the paper lacks any mention of human feedback mechanisms or RLHF processes, it has no direct relevance to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05385",
      "title": "A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in\n  LLMs",
      "authors": [
        "Jiacheng Wei",
        "Faguo Wu",
        "Xiao Zhang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models are unable to continuously adapt and learn from new\ndata during reasoning at inference time. To address this limitation, we propose\nthat complex reasoning tasks be decomposed into atomic subtasks and introduce\nSAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive\nupdates during reasoning at inference time. SAGE consists of three key\ncomponents: (1) a Trigger module that detects reasoning failures through\nmultiple evaluation metrics in real time; (2) a Trigger Buffer module that\nclusters anomaly samples using a streaming clustering process with HDBSCAN,\nfollowed by stability checks and similarity-based merging; and (3) a Lora Store\nmodule that dynamically optimizes parameter updates with an adapter pool for\nknowledge retention. Evaluation results show that SAGE demonstrates excellent\naccuracy, robustness, and stability on the atomic reasoning subtask through\ndynamic knowledge updating during test time.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05385v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05385v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.423,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.418,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a trigger-guided framework for dynamic fine-tuning using LoRA, without any mention of human feedback, reward models, or reinforcement learning for alignment. It explicitly contrasts with RL-based methods, favoring lightweight adaptation.",
      "weak_supervision_justification": "The paper involves automatic anomaly detection and clustering for updates, which could indirectly relate to generating labels from noisy sources, but it does not primarily rely on or discuss weak supervision techniques like programmatic label generation.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or treating reasoning as a holistic chain-of-thought entity; it centers on LoRA-based fine-tuning and anomaly detection instead.",
      "distributed_training_justification": "The paper addresses lightweight, on-the-fly adaptation of LLMs and does not discuss parallel computing, data partitioning, or multi-node training strategies.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05388",
      "title": "Augmented Structure Preserving Neural Networks for cell biomechanics",
      "authors": [
        "Juan Olalla-Pombo",
        "Alberto Badías",
        "Miguel Ángel Sanz-Gómez",
        "José María Benítez",
        "Francisco Javier Montáns"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Cell biomechanics involve a great number of complex phenomena that are\nfundamental to the evolution of life itself and other associated processes,\nranging from the very early stages of embryo-genesis to the maintenance of\ndamaged structures or the growth of tumors. Given the importance of such\nphenomena, increasing research has been dedicated to their understanding, but\nthe many interactions between them and their influence on the decisions of\ncells as a collective network or cluster remain unclear. We present a new\napproach that combines Structure Preserving Neural Networks, which study cell\nmovements as a purely mechanical system, with other Machine Learning tools\n(Artificial Neural Networks), which allow taking into consideration\nenvironmental factors that can be directly deduced from an experiment with\nComputer Vision techniques. This new model, tested on simulated and real cell\nmigration cases, predicts complete cell trajectories following a roll-out\npolicy with a high level of accuracy. This work also includes a mitosis event\nprediction model based on Neural Networks architectures which makes use of the\nsame observed features.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05388v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05388v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.348,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05390",
      "title": "Authorship Without Writing: Large Language Models and the Senior Author\n  Analogy",
      "authors": [
        "Clint Hurshman",
        "Sebastian Porsdam Mann",
        "Julian Savulescu",
        "Brian D. Earp"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The use of large language models (LLMs) in bioethical, scientific, and\nmedical writing remains controversial. While there is broad agreement in some\ncircles that LLMs cannot count as authors, there is no consensus about whether\nand how humans using LLMs can count as authors. In many fields, authorship is\ndistributed among large teams of researchers, some of whom, including\nparadigmatic senior authors who guide and determine the scope of a project and\nultimately vouch for its integrity, may not write a single word. In this paper,\nwe argue that LLM use (under specific conditions) is analogous to a form of\nsenior authorship. On this view, the use of LLMs, even to generate complete\ndrafts of research papers, can be considered a legitimate form of authorship\naccording to the accepted criteria in many fields. We conclude that either such\nuse should be recognized as legitimate, or current criteria for authorship\nrequire fundamental revision. AI use declaration: GPT-5 was used to help format\nBox 1. AI was not used for any other part of the preparation or writing of this\nmanuscript.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05390v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05390v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.324,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05392",
      "title": "An Optimized Pipeline for Automatic Educational Knowledge Graph\n  Construction",
      "authors": [
        "Qurat Ul Ain",
        "Mohamed Amine Chatti",
        "Jean Qussa",
        "Amr Shakhshir",
        "Rawaa Alatrash",
        "Shoeb Joarder"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The automatic construction of Educational Knowledge Graphs (EduKGs) is\nessential for domain knowledge modeling by extracting meaningful\nrepresentations from learning materials. Despite growing interest, identifying\na scalable and reliable approach for automatic EduKG generation remains a\nchallenge. In an attempt to develop a unified and robust pipeline for automatic\nEduKG construction, in this study we propose a pipeline for automatic EduKG\nconstruction from PDF learning materials. The process begins with generating\nslide-level EduKGs from individual pages/slides, which are then merged to form\na comprehensive EduKG representing the entire learning material. We evaluate\nthe accuracy of the EduKG generated from the proposed pipeline in our MOOC\nplatform, CourseMapper. The observed accuracy, while indicative of partial\nsuccess, is relatively low particularly in the educational context, where the\nreliability of knowledge representations is critical for supporting meaningful\nlearning. To address this, we introduce targeted optimizations across multiple\npipeline components. The optimized pipeline achieves a 17.5% improvement in\naccuracy and a tenfold increase in processing efficiency. Our approach offers a\nholistic, scalable and end-to-end pipeline for automatic EduKG construction,\nadaptable to diverse educational contexts, and supports improved semantic\nrepresentation of learning content.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05392v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05392v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.325,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05393",
      "title": "Inferring Prerequisite Knowledge Concepts in Educational Knowledge\n  Graphs: A Multi-criteria Approach",
      "authors": [
        "Rawaa Alatrash",
        "Mohamed Amine Chatti",
        "Nasha Wibowo",
        "Qurat Ul Ain"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Educational Knowledge Graphs (EduKGs) organize various learning entities and\ntheir relationships to support structured and adaptive learning. Prerequisite\nrelationships (PRs) are critical in EduKGs for defining the logical order in\nwhich concepts should be learned. However, the current EduKG in the MOOC\nplatform CourseMapper lacks explicit PR links, and manually annotating them is\ntime-consuming and inconsistent. To address this, we propose an unsupervised\nmethod for automatically inferring concept PRs without relying on labeled data.\nWe define ten criteria based on document-based, Wikipedia hyperlink-based,\ngraph-based, and text-based features, and combine them using a voting algorithm\nto robustly capture PRs in educational content. Experiments on benchmark\ndatasets show that our approach achieves higher precision than existing methods\nwhile maintaining scalability and adaptability, thus providing reliable support\nfor sequence-aware learning in CourseMapper.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05393v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05393v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.296,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05394",
      "title": "Reverse Browser: Vector-Image-to-Code Generator",
      "authors": [
        "Zoltan Toth-Czifra"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Automating the conversion of user interface design into code (image-to-code\nor image-to-UI) is an active area of software engineering research. However,\nthe state-of-the-art solutions do not achieve high fidelity to the original\ndesign, as evidenced by benchmarks. In this work, I approach the problem\ndifferently: I use vector images instead of bitmaps as model input. I create\nseveral large datasets for training machine learning models. I evaluate the\navailable array of Image Quality Assessment (IQA) algorithms and introduce a\nnew, multi-scale metric. I then train a large open-weights model and discuss\nits limitations.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05394v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05394v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.339,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves generating code from vector images using machine learning models, datasets, and a new IQA metric, but it does not mention diffusion models, iterative refinement for logical tasks, or any adaptation of diffusion for multi-step reasoning. The focus is on image-to-code conversion, likely using transformers or similar architectures, without any component for holistic Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05396",
      "title": "Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent\n  Debate",
      "authors": [
        "Andrea Wynn",
        "Harsh Satija",
        "Gillian Hadfield"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "While multi-agent debate has been proposed as a promising strategy for\nimproving AI reasoning ability, we find that debate can sometimes be harmful\nrather than helpful. The prior work has exclusively focused on debates within\nhomogeneous groups of agents, whereas we explore how diversity in model\ncapabilities influences the dynamics and outcomes of multi-agent interactions.\nThrough a series of experiments, we demonstrate that debate can lead to a\ndecrease in accuracy over time -- even in settings where stronger (i.e., more\ncapable) models outnumber their weaker counterparts. Our analysis reveals that\nmodels frequently shift from correct to incorrect answers in response to peer\nreasoning, favoring agreement over challenging flawed reasoning. These results\nhighlight important failure modes in the exchange of reasons during multi-agent\ndebate, suggesting that naive applications of debate may cause performance\ndegradation when agents are neither incentivized nor adequately equipped to\nresist persuasive but incorrect reasoning.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05396v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05396v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.456,
      "distributed_training_score": 0.334,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines multi-agent debate among LLMs, focusing on how interactions can lead to performance degradation due to factors like agent diversity and flawed reasoning exchanges. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for logical tasks. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05399",
      "title": "Graph Connectionist Temporal Classification for Phoneme Recognition",
      "authors": [
        "Henry Grafé",
        "Hugo Van hamme"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Automatic Phoneme Recognition (APR) systems are often trained using pseudo\nphoneme-level annotations generated from text through Grapheme-to-Phoneme (G2P)\nsystems. These G2P systems frequently output multiple possible pronunciations\nper word, but the standard Connectionist Temporal Classification (CTC) loss\ncannot account for such ambiguity during training. In this work, we adapt Graph\nTemporal Classification (GTC) to the APR setting. GTC enables training from a\ngraph of alternative phoneme sequences, allowing the model to consider multiple\npronunciations per word as valid supervision. Our experiments on English and\nDutch data sets show that incorporating multiple pronunciations per word into\nthe training loss consistently improves phoneme error rates compared to a\nbaseline trained with CTC. These results suggest that integrating pronunciation\nvariation into the loss function is a promising strategy for training APR\nsystems from noisy G2P-based supervision.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05399v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05399v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.304,
      "distributed_training_score": 0.312,
      "datasets_score": 0.238,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05425",
      "title": "No Translation Needed: Forecasting Quality from Fertility and Metadata",
      "authors": [
        "Jessica M. Lundin",
        "Ada Zhang",
        "David Adelani",
        "Cody Carroll"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We show that translation quality can be predicted with surprising accuracy\n\\textit{without ever running the translation system itself}. Using only a\nhandful of features, token fertility ratios, token counts, and basic linguistic\nmetadata (language family, script, and region), we can forecast ChrF scores for\nGPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient\nboosting models achieve favorable performance ($R^{2}=0.66$ for\nXX$\\rightarrow$English and $R^{2}=0.72$ for English$\\rightarrow$XX). Feature\nimportance analyses reveal that typological factors dominate predictions into\nEnglish, while fertility plays a larger role for translations into diverse\ntarget languages. These findings suggest that translation quality is shaped by\nboth token-level fertility and broader linguistic typology, offering new\ninsights for multilingual evaluation and quality estimation.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05425v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05425v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.327,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05431",
      "title": "Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale\n  Convolutional Attention Decoding",
      "authors": [
        "GodsGift Uzor",
        "Tania-Amanda Nkoyo Fredrick Eneye",
        "Chukwuebuka Ijezue"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Brain tumor segmentation is a critical pre-processing step in the medical\nimage analysis pipeline that involves precise delineation of tumor regions from\nhealthy brain tissue in medical imaging data, particularly MRI scans. An\nefficient and effective decoding mechanism is crucial in brain tumor\nsegmentation especially in scenarios with limited computational resources.\nHowever these decoding mechanisms usually come with high computational costs.\nTo address this concern EMCAD a new efficient multi-scale convolutional\nattention decoder designed was utilized to optimize both performance and\ncomputational efficiency for brain tumor segmentation on the BraTs2020 dataset\nconsisting of MRI scans from 369 brain tumor patients. The preliminary result\nobtained by the model achieved a best Dice score of 0.31 and maintained a\nstable mean Dice score of 0.285 plus/minus 0.015 throughout the training\nprocess which is moderate. The initial model maintained consistent performance\nacross the validation set without showing signs of over-fitting.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05431v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05431v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.234,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.337,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05440",
      "title": "Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too",
      "authors": [
        "Logan Lawrence",
        "Ashton Williamson",
        "Alexander Shelton"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "As large-language models have been increasingly used as automatic raters for\nevaluating free-form content, including document summarization, dialog, and\nstory generation, work has been dedicated to evaluating such models by\nmeasuring their correlations with human judgment. For \\textit{sample-level}\nperformance, methods which operate by using pairwise comparisons between\nmachine-generated text perform well but often lack the ability to assign\nabsolute scores to individual summaries, an ability crucial for use cases that\nrequire thresholding. In this work, we propose a direct-scoring method which\nuses synthetic summaries to act as pairwise machine rankings at test time. We\nshow that our method performs comparably to state-of-the-art pairwise\nevaluators in terms of axis-averaged sample-level correlations on the SummEval\n(\\textbf{+0.03}), TopicalChat (\\textbf{-0.03}), and HANNA (\\textbf{+0.05})\nmeta-evaluation benchmarks, and release the synthetic in-context summaries as\ndata to facilitate future work.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05440v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05440v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.322,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on NLG evaluation using LLMs and pairwise comparisons for scoring, aiming to correlate with human judgment. It does not involve training a reward model on human-ranked data or fine-tuning via reinforcement learning, which are core to RLHF. Thus, it lacks any direct connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses methods for NLG evaluation with synthetic summaries and pairwise comparisons, but it does not involve diffusion models, iterative refinement for logical tasks, or multi-step Chain-of-Thought reasoning. There is no component related to diffusion-based processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05441",
      "title": "Missing Fine Details in Images: Last Seen in High Frequencies",
      "authors": [
        "Tejaswini Medi",
        "Hsien-Yi Wang",
        "Arianna Rampini",
        "Margret Keuper"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Latent generative models have shown remarkable progress in high-fidelity\nimage synthesis, typically using a two-stage training process that involves\ncompressing images into latent embeddings via learned tokenizers in the first\nstage. The quality of generation strongly depends on how expressive and\nwell-optimized these latent embeddings are. While various methods have been\nproposed to learn effective latent representations, generated images often lack\nrealism, particularly in textured regions with sharp transitions, due to loss\nof fine details governed by high frequencies. We conduct a detailed frequency\ndecomposition of existing state-of-the-art (SOTA) latent tokenizers and show\nthat conventional objectives inherently prioritize low-frequency\nreconstruction, often at the expense of high-frequency fidelity. Our analysis\nreveals these latent tokenizers exhibit a bias toward low-frequency information\nduring optimization, leading to over-smoothed outputs and visual artifacts that\ndiminish perceptual quality. To address this, we propose a wavelet-based,\nfrequency-aware variational autoencoder (FA-VAE) framework that explicitly\ndecouples the optimization of low- and high-frequency components. This\ndecoupling enables improved reconstruction of fine textures while preserving\nglobal structure. Moreover, we integrate our frequency-preserving latent\nembeddings into a SOTA latent diffusion model, resulting in sharper and more\nrealistic image generation. Our approach bridges the fidelity gap in current\nlatent tokenizers and emphasizes the importance of frequency-aware optimization\nfor realistic image synthesis, with broader implications for applications in\ncontent creation, neural rendering, and medical imaging.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05441v3",
      "pdf_url": "http://arxiv.org/pdf/2509.05441v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.358,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily addresses improvements in latent generative models for image synthesis, focusing on frequency-aware optimization in VAEs and integrating them into diffusion models for better visual fidelity. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, which are the core elements of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05446",
      "title": "Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement\n  Learning For DCNN's",
      "authors": [
        "Iftekhar Haider Chowdhury",
        "Zaed Ikbal Syed",
        "Ahmed Faizul Haque Dhrubo",
        "Mohammad Abdul Qayum"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep Convolutional Neural Networks have achieved state of the art performance\nacross various computer vision tasks, however their practical deployment is\nlimited by computational and memory overhead. This paper introduces\nDifferential Sensitivity Fusion Pruning, a novel single shot filter pruning\nframework that focuses on evaluating the stability and redundancy of filter\nimportance scores across multiple criteria. Differential Sensitivity Fusion\nPruning computes a differential sensitivity score for each filter by fusing the\ndiscrepancies among gradient based sensitivity, first order Taylor expansion,\nand KL divergence of activation distributions. An exponential scaling mechanism\nis applied to emphasize filters with inconsistent importance across metrics,\nidentifying candidates that are structurally unstable or less critical to the\nmodel performance. Unlike iterative or reinforcement learning based pruning\nstrategies, Differential Sensitivity Fusion Pruning is efficient and\ndeterministic, requiring only a single forward-backward pass for scoring and\npruning. Extensive experiments across varying pruning rates between 50 to 70\npercent demonstrate that Differential Sensitivity Fusion Pruning significantly\nreduces model complexity, achieving over 80 percent Floating point Operations\nPer Seconds reduction while maintaining high accuracy. For instance, at 70\npercent pruning, our approach retains up to 98.23 percent of baseline accuracy,\nsurpassing traditional heuristics in both compression and generalization. The\nproposed method presents an effective solution for scalable and adaptive Deep\nConvolutional Neural Networks compression, paving the way for efficient\ndeployment on edge and mobile platforms.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05446v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05446v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.449,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a pruning method for DCNNs using sensitivity metrics like gradients, Taylor expansion, and KL divergence to reduce model complexity. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper focuses on post-training filter pruning for DCNNs to achieve model compression, with no discussion of distributed training techniques, parallel computing, data partitioning, or multi-node machine learning setups.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05448",
      "title": "Newton to Einstein: Axiom-Based Discovery via Game Design",
      "authors": [
        "Pingchuan Ma",
        "Benjamin Tod Jones",
        "Tsun-Hsuan Wang",
        "Minghao Guo",
        "Michal Piotr Lipiec",
        "Chuang Gan",
        "Wojciech Matusik"
      ],
      "categories": [
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This position paper argues that machine learning for scientific discovery\nshould shift from inductive pattern recognition to axiom-based reasoning. We\npropose a game design framework in which scientific inquiry is recast as a\nrule-evolving system: agents operate within environments governed by axioms and\nmodify them to explain outlier observations. Unlike conventional ML approaches\nthat operate within fixed assumptions, our method enables the discovery of new\ntheoretical structures through systematic rule adaptation. We demonstrate the\nfeasibility of this approach through preliminary experiments in logic-based\ngames, showing that agents can evolve axioms that solve previously unsolvable\nproblems. This framework offers a foundation for building machine learning\nsystems capable of creative, interpretable, and theory-driven discovery.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05448v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05448v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.282,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on axiom-based reasoning and a game design framework for scientific discovery, emphasizing rule evolution in logic programming to handle outliers. It does not involve diffusion models, iterative refinement processes like denoising for logical tasks, or treating a Chain-of-Thought as a single entity for holistic correction. There is no mention of multi-step logical reasoning using diffusion mechanisms, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05449",
      "title": "Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden\n  State and Attention Pattern Analysis",
      "authors": [
        "Disha Makhija",
        "Manoj Ghuhan Arivazhagan",
        "Vinayshekhar Bannihatti Kumar",
        "Rashmi Gangadharaiah"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Membership inference attacks (MIAs) reveal whether specific data was used to\ntrain machine learning models, serving as important tools for privacy auditing\nand compliance assessment. Recent studies have reported that MIAs perform only\nmarginally better than random guessing against large language models,\nsuggesting that modern pre-training approaches with massive datasets may be\nfree from privacy leakage risks. Our work offers a complementary perspective to\nthese findings by exploring how examining LLMs' internal representations,\nrather than just their outputs, may provide additional insights into potential\nmembership inference signals. Our framework, \\emph{memTrace}, follows what we\ncall \\enquote{neural breadcrumbs} extracting informative signals from\ntransformer hidden states and attention patterns as they process candidate\nsequences. By analyzing layer-wise representation dynamics, attention\ndistribution characteristics, and cross-layer transition patterns, we detect\npotential memorization fingerprints that traditional loss-based approaches may\nnot capture. This approach yields strong membership detection across several\nmodel families achieving average AUC scores of 0.85 on popular MIA benchmarks.\nOur findings suggest that internal model behaviors can reveal aspects of\ntraining data exposure even when output-based signals appear protected,\nhighlighting the need for further research into membership privacy and the\ndevelopment of more robust privacy-preserving training techniques for large\nlanguage models.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05449v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05449v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.357,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on membership inference attacks (MIAs) for large language models by analyzing hidden states and attention patterns, focusing on privacy risks and model internals. It does not involve diffusion models, iterative refinement processes, or any adaptation for solving complex logical tasks through a Chain-of-Thought mechanism.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05469",
      "title": "From Image Generation to Infrastructure Design: a Multi-agent Pipeline\n  for Street Design Generation",
      "authors": [
        "Chenguang Wang",
        "Xiang Yan",
        "Yilong Dai",
        "Ziyi Wang",
        "Susu Xu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CY (Computers and Society)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Realistic visual renderings of street-design scenarios are essential for\npublic engagement in active transportation planning. Traditional approaches are\nlabor-intensive, hindering collective deliberation and collaborative\ndecision-making. While AI-assisted generative design shows transformative\npotential by enabling rapid creation of design scenarios, existing generative\napproaches typically require large amounts of domain-specific training data and\nstruggle to enable precise spatial variations of design/configuration in\ncomplex street-view scenes. We introduce a multi-agent system that edits and\nredesigns bicycle facilities directly on real-world street-view imagery. The\nframework integrates lane localization, prompt optimization, design generation,\nand automated evaluation to synthesize realistic, contextually appropriate\ndesigns. Experiments across diverse urban scenarios demonstrate that the system\ncan adapt to varying road geometries and environmental conditions, consistently\nyielding visually coherent and instruction-compliant results. This work\nestablishes a foundation for applying multi-agent pipelines to transportation\ninfrastructure planning and facility design.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05469v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05469v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.39,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a multi-agent system for generating street designs using image generation models like GPT-image-1, focusing on agents for localization, prompt optimization, design generation, and evaluation. While it involves iterative processes in its pipeline, there is no mention of diffusion models, adaptation of iterative refinement for complex logical tasks, or treating a chain-of-thought as a holistically corrected entity. The core contribution is in urban planning and image editing, not diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05471",
      "title": "Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language\n  Models",
      "authors": [
        "Youjia Zheng",
        "Mohammad Zandsalimy",
        "Shanu Sushmita"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly vulnerable to a sophisticated\nform of adversarial prompting known as camouflaged jailbreaking. This method\nembeds malicious intent within seemingly benign language to evade existing\nsafety mechanisms. Unlike overt attacks, these subtle prompts exploit\ncontextual ambiguity and the flexible nature of language, posing significant\nchallenges to current defense systems. This paper investigates the construction\nand impact of camouflaged jailbreak prompts, emphasizing their deceptive\ncharacteristics and the limitations of traditional keyword-based detection\nmethods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts,\ncontaining 500 curated examples (400 harmful and 100 benign prompts) designed\nto rigorously stress-test LLM safety protocols. In addition, we propose a\nmulti-faceted evaluation framework that measures harmfulness across seven\ndimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards,\nHarmful Potential, Educational Value, Content Quality, and Compliance Score.\nOur findings reveal a stark contrast in LLM behavior: while models demonstrate\nhigh safety and content quality with benign inputs, they exhibit a significant\ndecline in performance and safety when confronted with camouflaged jailbreak\nattempts. This disparity underscores a pervasive vulnerability, highlighting\nthe urgent need for more nuanced and adaptive security strategies to ensure the\nresponsible and robust deployment of LLMs in real-world applications.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05471v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05471v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.418,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.341,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on benchmarking camouflaged jailbreaks in LLMs, introducing a dataset and evaluation framework for safety vulnerabilities, but does not involve training models with human feedback or reinforcement learning techniques. There is no mention of RLHF or related alignment methods.",
      "weak_supervision_justification": "The paper introduces a curated dataset of prompts for testing LLM safety, which appears to rely on manual curation rather than programmatically generating labels from noisy sources. It does not discuss or utilize weak supervision techniques for training or labeling data.",
      "diffusion_reasoning_justification": "The paper examines jailbreaking attacks on LLMs and proposes a dataset and evaluation framework, but it does not involve diffusion models, iterative refinement for logical reasoning, or any multi-step reasoning processes adapted from diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating and introducing a new benchmark dataset (Camouflaged Jailbreak Prompts) with 500 curated examples, discussing its curation methodology, and using it for evaluating LLM safety, which directly aligns with research on dataset creation, benchmarking, and analysis in AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper investigates camouflaged jailbreaks in large language models, where malicious intents are embedded in seemingly benign prompts to evade safety mechanisms, highlighting the limitations of current defenses. It introduces a novel benchmark dataset of 500 curated prompts (400 harmful and 100 benign) and a multi-faceted evaluation framework across seven dimensions to assess harmfulness, revealing that LLMs exhibit significant vulnerabilities when faced with these subtle attacks, thus emphasizing the need for more adaptive security strategies.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark dataset and evaluation framework for camouflaged jailbreaks, significantly advancing the state-of-the-art in LLM security by addressing an underexplored aspect of adversarial prompting.",
      "impact_score": "High",
      "impact_justification": "This work has the potential to influence a wide range of future research and commercial applications in AI security by providing tools to detect and mitigate sophisticated jailbreak attacks, thereby enhancing the robust deployment of LLMs.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers high-quality insights into a critical vulnerability in LLMs, making it a valuable contribution that researchers and practitioners in AI security should review to stay informed on emerging threats.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4815044686e9d01fa4178473fd5fb1116e986db1",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 10,
      "average_h_index": 4.333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Youjia Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2311941852"
        },
        {
          "name": "Mohammad Zandsalimy",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/19272340"
        },
        {
          "name": "Shanu Sushmita",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/1804117"
        }
      ]
    },
    {
      "id": "2509.05474",
      "title": "From Vision to Validation: A Theory- and Data-Driven Construction of a\n  GCC-Specific AI Adoption Index",
      "authors": [
        "Mohammad Rashed Albous",
        "Abdel Latef Anouze"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Artificial intelligence (AI) is rapidly transforming public-sector processes\nworldwide, yet standardized measures rarely address the unique drivers,\ngovernance models, and cultural nuances of the Gulf Cooperation Council (GCC)\ncountries. This study employs a theory-driven foundation derived from an\nin-depth analysis of literature review and six National AI Strategies (NASs),\ncoupled with a data-driven approach that utilizes a survey of 203 mid- and\nsenior-level government employees and advanced statistical techniques (K-Means\nclustering, Principal Component Analysis, and Partial Least Squares Structural\nEquation Modeling). By combining policy insights with empirical evidence, the\nresearch develops and validates a novel AI Adoption Index specifically tailored\nto the GCC public sector. Findings indicate that robust technical\ninfrastructure and clear policy mandates exert the strongest influence on\nsuccessful AI implementations, overshadowing organizational readiness in early\nadoption stages. The combined model explains 70% of the variance in AI\noutcomes, suggesting that resource-rich environments and top-down policy\ndirectives can drive rapid but uneven technology uptake. By consolidating key\ndimensions (Technical Infrastructure (TI), Organizational Readiness (OR), and\nGovernance Environment (GE)) into a single composite index, this study provides\na holistic yet context-sensitive tool for benchmarking AI maturity. The index\noffers actionable guidance for policymakers seeking to harmonize large-scale\ndeployments with ethical and regulatory standards. Beyond advancing academic\ndiscourse, these insights inform more strategic allocation of resources,\ncross-country cooperation, and capacity-building initiatives, thereby\nsupporting sustained AI-driven transformation in the GCC region and beyond.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05474v3",
      "pdf_url": "http://arxiv.org/pdf/2509.05474v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.303,
      "datasets_score": 0.424,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves collecting and analyzing survey data from 203 government employees using statistical techniques like K-Means clustering and Principal Component Analysis, which could be seen as handling a dataset. However, the primary focus is on developing an AI Adoption Index for the GCC public sector, not on creating, analyzing, benchmarking, or evaluating datasets specifically for machine learning or AI applications. Thus, data usage is incidental rather than central.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05475",
      "title": "Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith\n  Excavation",
      "authors": [
        "Andrej Orsula",
        "Matthieu Geist",
        "Miguel Olivares-Mendez",
        "Carol Martinez"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Autonomous regolith excavation is a cornerstone of in-situ resource\nutilization for a sustained human presence beyond Earth. However, this task is\nfundamentally hindered by the complex interaction dynamics of granular media\nand the operational need for robots to use diverse tools. To address these\nchallenges, this work introduces a framework where a model-based reinforcement\nlearning agent learns within a parallelized simulation. This environment\nleverages high-fidelity particle physics and procedural generation to create a\nvast distribution of both lunar terrains and excavation tool geometries. To\nmaster this diversity, the agent learns an adaptive interaction strategy by\ndynamically modulating its own stiffness and damping at each control step\nthrough operational space control. Our experiments demonstrate that training\nwith a procedural distribution of tools is critical for generalization and\nenables the development of sophisticated tool-aware behavior. Furthermore, we\nshow that augmenting the agent with visual feedback significantly improves task\nsuccess. These results represent a validated methodology for developing the\nrobust and versatile autonomous systems required for the foundational tasks of\nfuture space missions.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05475v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05475v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.38,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on model-based reinforcement learning in a simulated environment for autonomous excavation, where the agent learns from simulated rewards based on task performance, such as successful excavation. It does not involve human feedback, human-ranked data, or a separate reward model trained on human preferences, which are core elements of RLHF. Therefore, the paper's contributions do not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05478",
      "title": "PLanTS: Periodicity-aware Latent-state Representation Learning for\n  Multivariate Time Series",
      "authors": [
        "Jia Wang",
        "Xiao Wang",
        "Chi Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multivariate time series (MTS) are ubiquitous in domains such as healthcare,\nclimate science, and industrial monitoring, but their high dimensionality,\nlimited labeled data, and non-stationary nature pose significant challenges for\nconventional machine learning methods. While recent self-supervised learning\n(SSL) approaches mitigate label scarcity by data augmentations or time\npoint-based contrastive strategy, they neglect the intrinsic periodic structure\nof MTS and fail to capture the dynamic evolution of latent states. We propose\nPLanTS, a periodicity-aware self-supervised learning framework that explicitly\nmodels irregular latent states and their transitions. We first designed a\nperiod-aware multi-granularity patching mechanism and a generalized contrastive\nloss to preserve both instance-level and state-level similarities across\nmultiple temporal resolutions. To further capture temporal dynamics, we design\na next-transition prediction pretext task that encourages representations to\nencode predictive information about future state evolution. We evaluate PLanTS\nacross a wide range of downstream tasks-including multi-class and multi-label\nclassification, forecasting, trajectory tracking and anomaly detection. PLanTS\nconsistently improves the representation quality over existing SSL methods and\ndemonstrates superior runtime efficiency compared to DTW-based methods.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05478v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05478v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.343,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05483",
      "title": "Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms\n  for deep learning in medical imaging",
      "authors": [
        "Jinhao Wang",
        "Florian Vogl",
        "Pascal Schütz",
        "Saša Ćuković",
        "William R. Taylor"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Veriserum is an open-source dataset designed to support the training of deep\nlearning registration for dual-plane fluoroscopic analysis. It comprises\napproximately 110,000 X-ray images of 10 knee implant pair combinations (2\nfemur and 5 tibia implants) captured during 1,600 trials, incorporating poses\nassociated with daily activities such as level gait and ramp descent. Each\nimage is annotated with an automatically registered ground-truth pose, while\n200 images include manually registered poses for benchmarking.\n  Key features of Veriserum include dual-plane images and calibration tools.\nThe dataset aims to support the development of applications such as 2D/3D image\nregistration, image segmentation, X-ray distortion correction, and 3D\nreconstruction. Freely accessible, Veriserum aims to advance computer vision\nand medical imaging research by providing a reproducible benchmark for\nalgorithm development and evaluation. The Veriserum dataset used in this study\nis publicly available via\nhttps://movement.ethz.ch/data-repository/veriserum.html, with the data stored\nat ETH Z\\\"urich Research Collections: https://doi.org/10.3929/ethz-b-000701146.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05483v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05483v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.264,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.265,
      "distributed_training_score": 0.301,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05486",
      "title": "The Token Tax: Systematic Bias in Multilingual Tokenization",
      "authors": [
        "Jessica M. Lundin",
        "Ada Zhang",
        "Nihal Karim",
        "Hamza Louzan",
        "Victor Wei",
        "David Adelani",
        "Cody Carroll"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Tokenization inefficiency imposes structural disadvantages on morphologically\ncomplex, low-resource languages, inflating compute resources and depressing\naccuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA\nitems; 5 subjects; 16 African languages) and show that fertility (tokens/word)\nreliably predicts accuracy. Higher fertility consistently predicts lower\naccuracy across all models and subjects. We further find that reasoning models\n(DeepSeek, o1) consistently outperform non-reasoning peers across high and low\nresource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in\nprior generations. Finally, translating token inflation to economics, a\ndoubling in tokens results in quadrupled training cost and time, underscoring\nthe token tax faced by many languages. These results motivate morphologically\naware tokenization, fair pricing, and multilingual benchmarks for equitable\nnatural language processing (NLP).",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05486v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05486v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.384,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05488",
      "title": "MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs",
      "authors": [
        "Hongjun Xu",
        "Junxi Xia",
        "Weisi Yang",
        "Yueyuan Sui",
        "Stephen Xia"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.OS (Operating Systems)"
      ],
      "abstract": "Deploying Mamba models on microcontrollers (MCUs) remains challenging due to\nlimited memory, the lack of native operator support, and the absence of\nembedded-friendly toolchains. We present, to our knowledge, the first\ndeployment of a Mamba-based neural architecture on a resource-constrained MCU,\na fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline\nmaps a trained PyTorch Mamba model to on-device execution by (1) exporting\nmodel weights into a lightweight format, and (2) implementing a handcrafted\nMamba layer and supporting operators in C with operator fusion and memory\nlayout optimization. MambaLite-Micro eliminates large intermediate tensors,\nreducing 83.0% peak memory, while maintaining an average numerical error of\nonly 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on\nkeyword spotting(KWS) and human activity recognition (HAR) tasks,\nMambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully\npreserving classification accuracy. We further validated portability by\ndeploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating\nconsistent operation across heterogeneous embedded platforms and paving the way\nfor bringing advanced sequence models like Mamba to real-world\nresource-constrained applications.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05488v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05488v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.399,
      "datasets_score": 0.257,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05490",
      "title": "An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning\n  in YOLO Architectures",
      "authors": [
        "Andrzej D. Dobrzycki",
        "Ana M. Bernardos",
        "José R. Casar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The You Only Look Once (YOLO) architecture is crucial for real-time object\ndetection. However, deploying it in resource-constrained environments such as\nunmanned aerial vehicles (UAVs) requires efficient transfer learning. Although\nlayer freezing is a common technique, the specific impact of various freezing\nconfigurations on contemporary YOLOv8 and YOLOv10 architectures remains\nunexplored, particularly with regard to the interplay between freezing depth,\ndataset characteristics, and training dynamics. This research addresses this\ngap by presenting a detailed analysis of layer-freezing strategies. We\nsystematically investigate multiple freezing configurations across YOLOv8 and\nYOLOv10 variants using four challenging datasets that represent critical\ninfrastructure monitoring. Our methodology integrates a gradient behavior\nanalysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper\ninsights into training dynamics under different freezing strategies. Our\nresults reveal that there is no universal optimal freezing strategy but,\nrather, one that depends on the properties of the data. For example, freezing\nthe backbone is effective for preserving general-purpose features, while a\nshallower freeze is better suited to handling extreme class imbalance. These\nconfigurations reduce graphics processing unit (GPU) memory consumption by up\nto 28% compared to full fine-tuning and, in some cases, achieve mean average\nprecision (mAP@50) scores that surpass those of full fine-tuning. Gradient\nanalysis corroborates these findings, showing distinct convergence patterns for\nmoderately frozen models. Ultimately, this work provides empirical findings and\npractical guidelines for selecting freezing strategies. It offers a practical,\nevidence-based approach to balanced transfer learning for object detection in\nscenarios with limited resources.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05490v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05490v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.426,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is an analysis of layer-freezing strategies to enhance transfer learning in YOLO architectures, focusing on efficiency in resource-constrained environments like UAVs. It does not address distributed training, parallel computing, multi-node machine learning, or any partitioning of data/computation across processors or nodes. Instead, it emphasizes single-model optimizations such as memory reduction and training dynamics.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05500",
      "title": "Microrobot Vascular Parkour: Analytic Geometry-based Path Planning with\n  Real-time Dynamic Obstacle Avoidance",
      "authors": [
        "Yanda Yang",
        "Max Sokolich",
        "Fatma Ceren Kirmizitas",
        "Sambeeta Das",
        "Andreas A. Malikopoulos"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autonomous microrobots in blood vessels could enable minimally invasive\ntherapies, but navigation is challenged by dense, moving obstacles. We propose\na real-time path planning framework that couples an analytic geometry global\nplanner (AGP) with two reactive local escape controllers, one based on rules\nand one based on reinforcement learning, to handle sudden moving obstacles.\nUsing real-time imaging, the system estimates the positions of the microrobot,\nobstacles, and targets and computes collision-free motions. In simulation, AGP\nyields shorter paths and faster planning than weighted A* (WA*), particle swarm\noptimization (PSO), and rapidly exploring random trees (RRT), while maintaining\nfeasibility and determinism. We extend AGP from 2D to 3D without loss of speed.\nIn both simulations and experiments, the combined global planner and local\ncontrollers reliably avoid moving obstacles and reach targets. The average\nplanning time is 40 ms per frame, compatible with 25 fps image acquisition and\nreal-time closed-loop control. These results advance autonomous microrobot\nnavigation and targeted drug delivery in vascular environments.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05500v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05500v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.266,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.323,
      "datasets_score": 0.233,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05512",
      "title": "Quaternion Approximation Networks for Enhanced Image Classification and\n  Oriented Object Detection",
      "authors": [
        "Bryce Grant",
        "Peng Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "This paper introduces Quaternion Approximate Networks (QUAN), a novel deep\nlearning framework that leverages quaternion algebra for rotation equivariant\nimage classification and object detection. Unlike conventional quaternion\nneural networks attempting to operate entirely in the quaternion domain, QUAN\napproximates quaternion convolution through Hamilton product decomposition\nusing real-valued operations. This approach preserves geometric properties\nwhile enabling efficient implementation with custom CUDA kernels. We introduce\nIndependent Quaternion Batch Normalization (IQBN) for training stability and\nextend quaternion operations to spatial attention mechanisms. QUAN is evaluated\non image classification (CIFAR-10/100, ImageNet), object detection (COCO,\nDOTA), and robotic perception tasks. In classification tasks, QUAN achieves\nhigher accuracy with fewer parameters and faster convergence compared to\nexisting convolution and quaternion-based models. For objection detection, QUAN\ndemonstrates improved parameter efficiency and rotation handling over standard\nConvolutional Neural Networks (CNNs) while establishing the SOTA for quaternion\nCNNs in this downstream task. These results highlight its potential for\ndeployment in resource-constrained robotic systems requiring rotation-aware\nperception and application in other domains.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05512v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05512v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.35,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05513",
      "title": "OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous\n  Manipulation",
      "authors": [
        "Ahad Jawaid",
        "Yu Xiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Egocentric human videos provide scalable demonstrations for imitation\nlearning, but existing corpora often lack either fine-grained, temporally\nlocalized action descriptions or dexterous hand annotations. We introduce\nOpenEgo, a multimodal egocentric manipulation dataset with standardized\nhand-pose annotations and intention-aligned action primitives. OpenEgo totals\n1107 hours across six public datasets, covering 290 manipulation tasks in 600+\nenvironments. We unify hand-pose layouts and provide descriptive, timestamped\naction primitives. To validate its utility, we train language-conditioned\nimitation-learning policies to predict dexterous hand trajectories. OpenEgo is\ndesigned to lower the barrier to learning dexterous manipulation from\negocentric video and to support reproducible research in vision-language-action\nlearning. All resources and instructions will be released at\nwww.openegocentric.com.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05513v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05513v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.33,
      "datasets_score": 0.436,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of OpenEgo, a large-scale multimodal egocentric dataset for dexterous manipulation. It focuses on creating and curating this dataset by unifying hand-pose annotations from six public sources, adding intention-aligned action primitives, and providing an evaluation protocol for trajectory prediction. This directly aligns with research on datasets for machine learning and AI, as it involves dataset creation, curation methodologies, and benchmarking for applications in robotics and vision-language-action learning.",
      "llm_score_status": "completed",
      "summary": "OpenEgo is a large-scale multimodal egocentric dataset designed to address the limitations of existing corpora by providing standardized hand-pose annotations and intention-aligned action primitives for dexterous manipulation tasks. The dataset consolidates six public sources into 1107 hours of video across 290 tasks in over 600 environments, with unified hand poses and timestamped descriptions, and demonstrates its utility by training language-conditioned imitation-learning policies to predict 3D hand trajectories, aiming to facilitate reproducible research in vision-language-action learning.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by consolidating and standardizing existing egocentric datasets with unified hand annotations and action primitives, offering a clever combination that enhances usability for dexterous manipulation research without introducing an entirely new problem or technique.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to significantly influence future research and applications in robotics, AI, and computer vision by providing a comprehensive resource for training models on dexterous manipulation, likely leading to advancements in vision-language-action systems.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong and valuable contribution through its unified dataset, which is essential for researchers in egocentric learning and manipulation, making it important for staying informed in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ec13c87b2639066ab41a25f5799a271e212e2d86",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ahad Jawaid",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2304951065"
        },
        {
          "name": "Yu Xiang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381271635"
        }
      ]
    },
    {
      "id": "2509.05515",
      "title": "Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation\n  in 3D Gaussian Splatting",
      "authors": [
        "Sen Wang",
        "Kunyi Li",
        "Siyun Liang",
        "Elena Alegret",
        "Jing Ma",
        "Nassir Navab",
        "Stefano Gasperini"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recently, distilling open-vocabulary language features from 2D images into 3D\nGaussians has attracted significant attention. Although existing methods\nachieve impressive language-based interactions of 3D scenes, we observe two\nfundamental issues: background Gaussians contributing negligibly to a rendered\npixel get the same feature as the dominant foreground ones, and multi-view\ninconsistencies due to view-specific noise in language embeddings. We introduce\nVisibility-Aware Language Aggregation (VALA), a lightweight yet effective\nmethod that computes marginal contributions for each ray and applies a\nvisibility-aware gate to retain only visible Gaussians. Moreover, we propose a\nstreaming weighted geometric median in cosine space to merge noisy multi-view\nfeatures. Our method yields a robust, view-consistent language feature\nembedding in a fast and memory-efficient manner. VALA improves open-vocabulary\nlocalization and segmentation across reference datasets, consistently\nsurpassing existing works.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05515v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05515v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.323,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05540",
      "title": "Combining TSL and LLM to Automate REST API Testing: A Comparative Study",
      "authors": [
        "Thiago Barradas",
        "Aline Paes",
        "Vânia de Oliveira Neves"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The effective execution of tests for REST APIs remains a considerable\nchallenge for development teams, driven by the inherent complexity of\ndistributed systems, the multitude of possible scenarios, and the limited time\navailable for test design. Exhaustive testing of all input combinations is\nimpractical, often resulting in undetected failures, high manual effort, and\nlimited test coverage. To address these issues, we introduce RestTSLLM, an\napproach that uses Test Specification Language (TSL) in conjunction with Large\nLanguage Models (LLMs) to automate the generation of test cases for REST APIs.\nThe approach targets two core challenges: the creation of test scenarios and\nthe definition of appropriate input data. The proposed solution integrates\nprompt engineering techniques with an automated pipeline to evaluate various\nLLMs on their ability to generate tests from OpenAPI specifications. The\nevaluation focused on metrics such as success rate, test coverage, and mutation\nscore, enabling a systematic comparison of model performance. The results\nindicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic),\nDeepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) -\nconsistently produced robust and contextually coherent REST API tests. Among\nthem, Claude 3.5 Sonnet outperformed all other models across every metric,\nemerging in this study as the most suitable model for this task. These findings\nhighlight the potential of LLMs to automate the generation of tests based on\nAPI specifications.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05540v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05540v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.355,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development and evaluation of RestTSLLM, an approach using Test Specification Language (TSL) and Large Language Models (LLMs) for automating REST API test generation. It emphasizes prompt engineering, few-shot learning, and comparative performance of LLMs, but does not involve reinforcement learning, human feedback mechanisms, reward models, or fine-tuning based on human preferences, which are essential elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05543",
      "title": "DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human\n  Action Segmentation",
      "authors": [
        "Haitao Tian",
        "Pierre Payeur"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, a contrastive representation learning framework is proposed to\nenhance human action segmentation via pre-training using trimmed (single\naction) skeleton sequences. Unlike previous representation learning works that\nare tailored for action recognition and that build upon isolated sequence-wise\nrepresentations, the proposed framework focuses on exploiting multi-scale\nrepresentations in conjunction with cross-sequence variations. More\nspecifically, it proposes a novel data augmentation strategy, 'Shuffle and\nWarp', which exploits diverse multi-action permutations. The latter effectively\nassists two surrogate tasks that are introduced in contrastive learning: Cross\nPermutation Contrasting (CPC) and Relative Order Reasoning (ROR). In\noptimization, CPC learns intra-class similarities by contrasting\nrepresentations of the same action class across different permutations, while\nROR reasons about inter-class contexts by predicting relative mapping between\ntwo permutations. Together, these tasks enable a Dual-Surrogate Contrastive\nLearning (DuoCLR) network to learn multi-scale feature representations\noptimized for action segmentation. In experiments, DuoCLR is pre-trained on a\ntrimmed skeleton dataset and evaluated on an untrimmed dataset where it\ndemonstrates a significant boost over state-the-art comparatives in both\nmulti-class and multi-label action segmentation tasks. Lastly, ablation studies\nare conducted to evaluate the effectiveness of each component of the proposed\napproach.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.05543v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05543v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.351,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.07997",
      "title": "Learning-Based Planning for Improving Science Return of Earth\n  Observation Satellites",
      "authors": [
        "Abigail Breitfeld",
        "Alberto Candela",
        "Juan Delfa",
        "Akseli Kangaslahti",
        "Itai Zilberstein",
        "Steve Chien",
        "David Wettergreen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Earth observing satellites are powerful tools for collecting scientific\ninformation about our planet, however they have limitations: they cannot easily\ndeviate from their orbital trajectories, their sensors have a limited field of\nview, and pointing and operating these sensors can take a large amount of the\nspacecraft's resources. It is important for these satellites to optimize the\ndata they collect and include only the most important or informative\nmeasurements. Dynamic targeting is an emerging concept in which satellite\nresources and data from a lookahead instrument are used to intelligently\nreconfigure and point a primary instrument. Simulation studies have shown that\ndynamic targeting increases the amount of scientific information gathered\nversus conventional sampling strategies. In this work, we present two different\nlearning-based approaches to dynamic targeting, using reinforcement and\nimitation learning, respectively. These learning methods build on a dynamic\nprogramming solution to plan a sequence of sampling locations. We evaluate our\napproaches against existing heuristic methods for dynamic targeting, showing\nthe benefits of using learning for this application. Imitation learning\nperforms on average 10.0\\% better than the best heuristic method, while\nreinforcement learning performs on average 13.7\\% better. We also show that\nboth learning methods can be trained effectively with relatively small amounts\nof data.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.07997v1",
      "pdf_url": "http://arxiv.org/pdf/2509.07997v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.333,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses the use of reinforcement learning for dynamic targeting in Earth observation satellites, focusing on optimizing sensor operations through simulation-based methods. However, it does not involve human feedback, such as training a reward model on human-ranked data, which is a core requirement for RLHF. The reinforcement learning approach appears to rely on environmental rewards from simulations, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.07998",
      "title": "Bilingual Word Level Language Identification for Omotic Languages",
      "authors": [
        "Mesay Gemeda Yigezu",
        "Girma Yohannis Bade",
        "Atnafu Lambebo Tonja",
        "Olga Kolesnikova",
        "Grigori Sidorov",
        "Alexander Gelbukh"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Language identification is the task of determining the languages for a given\ntext. In many real world scenarios, text may contain more than one language,\nparticularly in multilingual communities. Bilingual Language Identification\n(BLID) is the task of identifying and distinguishing between two languages in a\ngiven text. This paper presents BLID for languages spoken in the southern part\nof Ethiopia, namely Wolaita and Gofa. The presence of words similarities and\ndifferences between the two languages makes the language identification task\nchallenging. To overcome this challenge, we employed various experiments on\nvarious approaches. Then, the combination of the BERT based pretrained language\nmodel and LSTM approach performed better, with an F1 score of 0.72 on the test\nset. As a result, the work will be effective in tackling unwanted social media\nissues and providing a foundation for further research in this area.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.07998v1",
      "pdf_url": "http://arxiv.org/pdf/2509.07998v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.287,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09702",
      "title": "Creativity Benchmark: A benchmark for marketing creativity for LLM\n  models",
      "authors": [
        "Ninad Bhat",
        "Kieran Browne",
        "Pip Bingemann"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "We introduce Creativity Benchmark, an evaluation framework for large language\nmodels (LLMs) in marketing creativity. The benchmark covers 100 brands (12\ncategories) and three prompt types (Insights, Ideas, Wild Ideas). Human\npairwise preferences from 678 practising creatives over 11,012 anonymised\ncomparisons, analysed with Bradley-Terry models, show tightly clustered\nperformance with no model dominating across brands or prompt types: the\ntop-bottom spread is $\\Delta\\theta \\approx 0.45$, which implies a head-to-head\nwin probability of $0.61$; the highest-rated model beats the lowest only about\n$61\\%$ of the time. We also analyse model diversity using cosine distances to\ncapture intra- and inter-model variation and sensitivity to prompt reframing.\nComparing three LLM-as-judge setups with human rankings reveals weak,\ninconsistent correlations and judge-specific biases, underscoring that\nautomated judges cannot substitute for human evaluation. Conventional\ncreativity tests also transfer only partially to brand-constrained tasks.\nOverall, the results highlight the need for expert human evaluation and\ndiversity-aware workflows.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.09702v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09702v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.313,
      "datasets_score": 0.429,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper uses human pairwise preferences to evaluate LLMs in marketing creativity, which involves human feedback similar to RLHF concepts. However, it does not involve training a reward model or fine-tuning LLMs using reinforcement learning; it focuses on benchmarking and analysis, not alignment through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion-based models, iterative refinement for reasoning, or any multi-step logical processes akin to diffusion methods. It centers on evaluating LLMs for creative tasks in marketing, with no components related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and evaluation of the \"Creativity Benchmark,\" which involves creating a new dataset with 100 brands, three prompt types, and human evaluations from 11,012 comparisons. This directly aligns with research on benchmarking and evaluating datasets for AI applications in creative domains.",
      "llm_score_status": "completed",
      "summary": "The Creativity Benchmark is a specialized evaluation framework designed to assess large language models (LLMs) in the context of marketing creativity, encompassing 100 brands across 12 categories and three prompt types (Insights, Ideas, and Wild Ideas). Utilizing human pairwise preferences from 678 marketing creatives on 11,012 anonymized comparisons analyzed via Bradley-Terry models, the study reveals tightly clustered model performances with no dominant model (top-bottom spread of about 0.45 in theta, implying a 61% win probability), highlights weak correlations and biases in LLM-as-judge setups compared to human evaluations, and demonstrates that conventional creativity tests only partially transfer to brand-constrained tasks, emphasizing the need for expert human assessment and diversity-aware workflows.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a domain-specific benchmark for marketing creativity in LLMs, adapting existing evaluation methods like human pairwise comparisons and Bradley-Terry models to address gaps in general-purpose benchmarks, though it doesn't introduce entirely new techniques.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in AI subfields like computational language and human-computer interaction, particularly for improving LLM evaluations in marketing, but its influence may remain confined to specialized applications rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a strong, valuable contribution by highlighting critical issues in LLM creativity evaluation and offering a practical benchmark, making it essential for researchers in AI and marketing to understand and potentially adopt.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b93a04b102dc1ad96c23a1136946c4ace0db000a",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ninad Bhat",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380440680"
        },
        {
          "name": "Kieran Browne",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380441257"
        },
        {
          "name": "Pip Bingemann",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380440447"
        }
      ]
    },
    {
      "id": "2509.09703",
      "title": "CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language\n  Models via Cross-Turn Contextual Correlation Backdoor",
      "authors": [
        "Zhenhua Xu",
        "Xixiang Zhao",
        "Xubin Yue",
        "Shengwei Tian",
        "Changting Lin",
        "Meng Han"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The widespread deployment of large language models (LLMs) has intensified\nconcerns around intellectual property (IP) protection, as model theft and\nunauthorized redistribution become increasingly feasible. To address this,\nmodel fingerprinting aims to embed verifiable ownership traces into LLMs.\nHowever, existing methods face inherent trade-offs between stealthness,\nrobustness, and generalizability, being either detectable via distributional\nshifts, vulnerable to adversarial modifications, or easily invalidated once the\nfingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven\nfingerprinting framework that encodes contextual correlations across multiple\ndialogue turns, such as counterfactual, rather than relying on token-level or\nsingle-turn triggers. CTCC enables fingerprint verification under black-box\naccess while mitigating false positives and fingerprint leakage, supporting\ncontinuous construction under a shared semantic rule even if partial triggers\nare exposed. Extensive experiments across multiple LLM architectures\ndemonstrate that CTCC consistently achieves stronger stealth and robustness\nthan prior work. Our findings position CTCC as a reliable and practical\nsolution for ownership verification in real-world LLM deployment scenarios. Our\ncode and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.09703v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09703v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.381,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a fingerprinting framework for LLMs that embeds ownership traces using cross-turn contextual correlations in dialogues, focusing on IP protection, stealth, and robustness. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for reasoning tasks. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction, making the paper entirely unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09704",
      "title": "Temporal Preferences in Language Models for Long-Horizon Assistance",
      "authors": [
        "Ali Mazyaki",
        "Mohammad Naghizadeh",
        "Samaneh Ranjkhah Zonouzaghi",
        "Hossein Setareh"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "We study whether language models (LMs) exhibit future- versus\npresent-oriented preferences in intertemporal choice and whether those\npreferences can be systematically manipulated. Using adapted human experimental\nprotocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them\nagainst a sample of human decision makers. We introduce an operational metric,\nthe Manipulability of Time Orientation (MTO), defined as the change in an LM's\nrevealed time preference between future- and present-oriented prompts. In our\ntests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)\nchoose later options under future-oriented prompts but only partially\npersonalize decisions across identities or geographies. Moreover, models that\ncorrectly reason about time orientation internalize a future orientation for\nthemselves as AI decision makers. We discuss design implications for AI\nassistants that should align with heterogeneous, long-horizon goals and outline\na research agenda on personalized contextual calibration and socially aware\ndeployment.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.09704v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09704v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.499,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.301,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates language models on decision-making tasks using human benchmarks but does not involve training or fine-tuning models with human feedback via a reward model and reinforcement learning. It focuses on testing existing models' preferences, not aligning them through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines reasoning in language models through prompts and tasks but does not adapt diffusion processes for iterative refinement or multi-step logical reasoning. There is no mention of diffusion models or treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09705",
      "title": "The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in\n  Repetition Trials of Standard Multiple-Choice Benchmarks",
      "authors": [
        "Claudio Pinhanez",
        "Paulo Cavalin",
        "Cassia Sanctos",
        "Marcelo Grave",
        "Yago Primerano"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This work explores the consistency of small LLMs (2B-8B parameters) in\nanswering multiple times the same question. We present a study on known,\nopen-source LLMs responding to 10 repetitions of questions from the\nmultiple-choice benchmarks MMLU-Redux and MedQA, considering different\ninference temperatures, small vs. medium models (50B-80B), finetuned vs. base\nmodels, and other parameters. We also look into the effects of requiring\nmulti-trial answer consistency on accuracy and the trade-offs involved in\ndeciding which model best provides both of them. To support those studies, we\npropose some new analytical and graphical tools. Results show that the number\nof questions which can be answered consistently vary considerably among models\nbut are typically in the 50%-80% range for small models at low inference\ntemperatures. Also, accuracy among consistent answers seems to reasonably\ncorrelate with overall accuracy. Results for medium-sized models seem to\nindicate much higher levels of answer consistency.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.09705v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09705v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.367,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating the consistency and non-determinism of small LLMs in multiple-choice benchmarks, without any discussion of training methods, human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper investigates answer consistency in LLMs through repeated trials and benchmarks, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09706",
      "title": "Differential Robustness in Transformer Language Models: Empirical\n  Evaluation Under Adversarial Text Attacks",
      "authors": [
        "Taniya Gidatkar",
        "Oluwaseun Ajao",
        "Matthew Shardlow"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "This study evaluates the resilience of large language models (LLMs) against\nadversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.\nUsing systematically designed adversarial tests through TextFooler and\nBERTAttack, we found significant variations in model robustness. RoBERTa-Base\nand FlanT5 demonstrated remarkable resilience, maintaining accuracy even when\nsubjected to sophisticated attacks, with attack success rates of 0%. In\ncontrast. BERT-Base showed considerable vulnerability, with TextFooler\nachieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.\nOur research reveals that while certain LLMs have developed effective defensive\nmechanisms, these safeguards often require substantial computational resources.\nThis study contributes to the understanding of LLM security by identifying\nexisting strengths and weaknesses in current safeguarding approaches and\nproposes practical recommendations for developing more efficient and effective\ndefensive strategies.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.09706v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09706v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.343,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating the robustness of transformer-based language models like Flan-T5, BERT, and RoBERTa-Base against adversarial attacks using tools such as TextFooler and BERTAttack. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic. There is no mention of adapting diffusion techniques for reasoning tasks, making the paper entirely unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09707",
      "title": "LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased\n  Random Key Genetic Algorithm",
      "authors": [
        "Camilo Chacón Sartori",
        "Martín Isla Pino",
        "Pedro Pinacho-Davidson",
        "Christian Blum"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Integrating Large Language Models (LLMs) within metaheuristics opens a novel\npath for solving complex combinatorial optimization problems. While most\nexisting approaches leverage LLMs for code generation to create or refine\nspecific heuristics, they often overlook the structural properties of\nindividual problem instances. In this work, we introduce a novel framework that\nintegrates LLMs with a Biased Random-Key Genetic Algorithm (BRKGA) to solve the\nNP-hard Longest Run Subsequence problem. Our approach extends the\ninstance-driven heuristic bias paradigm by introducing a human-LLM\ncollaborative process to co-design and implement a set of computationally\nefficient metrics. The LLM analyzes these instance-specific metrics to generate\na tailored heuristic bias, which steers the BRKGA toward promising areas of the\nsearch space. We conduct a comprehensive experimental evaluation, including\nrigorous statistical tests, convergence and behavioral analyses, and targeted\nablation studies, comparing our method against a standard BRKGA baseline across\n1,050 generated instances of varying complexity. Results show that our\ntop-performing hybrid, BRKGA+Llama-4-Maverick, achieves statistically\nsignificant improvements over the baseline, particularly on the most complex\ninstances. Our findings confirm that leveraging an LLM to produce an a priori,\ninstance-driven heuristic bias is a valuable approach for enhancing\nmetaheuristics in complex optimization domains.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.09707v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09707v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.457,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.351,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a human-LLM collaborative process for designing metrics, but it does not involve training or fine-tuning an AI model using human feedback, a reward model, or reinforcement learning. RLHF specifically requires aligning models with human preferences through RL techniques, which are not present here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses LLMs for analyzing metrics and generating heuristic bias in a genetic algorithm, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. There is no evidence of treating reasoning paths as entities for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10530",
      "title": "Dynamic Adaptive Shared Experts with Grouped Multi-Head Attention\n  Mixture of Experts",
      "authors": [
        "Cheng Li",
        "Jiexiong Liu",
        "Yixuan Chen",
        "Jie ji"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Transformer models based on the Mixture of Experts (MoE) architecture have\nmade significant progress in long-sequence modeling, but existing models still\nhave shortcomings in computational efficiency and the ability to capture\nlong-range dependencies, especially in terms of the dynamic adaptability of\nexpert resource allocation. In this paper, we propose a Dynamic Adaptive Shared\nExpert and Grouped Multi-Head Attention Hybrid Model (DASG-MoE) to enhance\nlong-sequence modeling capabilities by integrating three modules. First, we\nemploy the Grouped Multi-Head Attention (GMHA) mechanism to effectively reduce\nthe computational complexity of long sequences. By parallel processing through\nsequence grouping, local sliding window attention, and feature aggregation, we\naddress long-range dependency issues and the model's lack of generalization for\nlocal information. Second, we design a Dual-Scale Shared Expert Structure\n(DSSE), where shallow experts use lightweight computations to quickly respond\nto low-dimensional features, while deep experts process high-dimensional\ncomplex semantics through pre-training transfer and post-training optimization,\nachieving a dynamic balance between efficiency and accuracy. Third, we propose\na hierarchical Adaptive Dynamic Routing (ADR) mechanism that dynamically\nselects expert levels based on feature complexity and task requirements, and\noptimizes resource allocation through a local expert activation strategy.\nExperiments on multiple long-sequence benchmark datasets demonstrate that our\nDASG-MoE model outperforms state-of-the-art models.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.10530v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10530v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.426,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing Mixture of Experts (MoE) architectures with grouped multi-head attention and dynamic routing for long-sequence modeling in Transformers. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. There is no component related to diffusion-based approaches.",
      "distributed_training_justification": "The paper addresses computational efficiency within the model architecture, such as through grouped attention and dynamic expert routing, but does not discuss distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation to accelerate training. It focuses on model design for inference, not training distribution.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10531",
      "title": "FinXplore: An Adaptive Deep Reinforcement Learning Framework for\n  Balancing and Discovering Investment Opportunities",
      "authors": [
        "Himanshu Choudhary",
        "Arishi Orra",
        "Manoj Thakur"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Portfolio optimization is essential for balancing risk and return in\nfinancial decision-making. Deep Reinforcement Learning (DRL) has stood out as a\ncutting-edge tool for portfolio optimization that learns dynamic asset\nallocation using trial-and-error interactions. However, most DRL-based methods\nare restricted to allocating assets within a pre-defined investment universe\nand overlook exploring new opportunities. This study introduces an investment\nlandscape that integrates exploiting existing assets with exploring new\ninvestment opportunities in an extended universe. The proposed approach\nleverages two DRL agents and dynamically balances these objectives to adapt to\nevolving markets while enhancing portfolio performance. One agent allocates\nassets within the existing universe, while another assists in exploring new\nopportunities in the extended universe. The effciency of the proposed\nmethodology is determined using two real-world market data sets. The\nexperiments demonstrate the superiority of the suggested approach against the\nstate-of-the-art portfolio strategies and baseline methods.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.10531v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10531v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.35,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Deep Reinforcement Learning (DRL) agents for portfolio optimization, specifically balancing exploitation and exploration in investment opportunities, without any mention of human feedback. It does not involve training a reward model on human-ranked data, fine-tuning based on human preferences, or aligning AI with human values, which are core to RLHF. Therefore, the paper's contributions are unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10534",
      "title": "Decoupling the \"What\" and \"Where\" With Polar Coordinate Positional\n  Embeddings",
      "authors": [
        "Anand Gopalakrishnan",
        "Robert Csordás",
        "Jürgen Schmidhuber",
        "Michael C. Mozer"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The attention mechanism in a Transformer architecture matches key to query\nbased on both content -- the what -- and position in a sequence -- the where.\nWe present an analysis indicating that what and where are entangled in the\npopular RoPE rotary position embedding. This entanglement can impair\nperformance particularly when decisions require independent matches on these\ntwo factors. We propose an improvement to RoPE, which we call Polar Coordinate\nPosition Embeddings or PoPE, that eliminates the what-where confound. PoPE is\nfar superior on a diagnostic task requiring indexing solely by position or by\ncontent. On autoregressive sequence modeling in music, genomic, and natural\nlanguage domains, Transformers using PoPE as the positional encoding scheme\noutperform baselines using RoPE with respect to evaluation loss (perplexity)\nand downstream task performance. On language modeling, these gains persist\nacross model scale, from 124M to 774M parameters. Crucially, PoPE shows strong\nzero-shot length extrapolation capabilities, whereas RoPE's performance\ndegrades significantly on longer sequences at test time without fine tuning or\nthe use of position-interpolation methods.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.10534v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10534v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.368,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of Polar Coordinate Position Embeddings (PoPE) to improve positional encoding in Transformer architectures, addressing issues with entanglement in Rotary Position Embeddings (RoPE). It focuses on sequence modeling and attention mechanisms, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning tasks. Therefore, it does not relate to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10535",
      "title": "Semantic-guided LoRA Parameters Generation",
      "authors": [
        "Miaoge Li",
        "Yang Chen",
        "Zhijie Rao",
        "Can Jiang",
        "Jingcai Guo"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) has demonstrated strong generalization\ncapabilities across a variety of tasks for efficiently fine-tuning AI models,\nespecially on resource-constrained edges. However, in real-world applications,\nedge users often exhibit task-specific preferences that are difficult to handle\nwith a unified model trained under a closed-world assumption, and the challenge\nmay further increase when there are significant domain shifts between training\nand deployment. Meanwhile, retraining/fine-tuning models for each user is also\nimpractical due to its cost-intensive nature and privacy concerns over raw data\nutilization from edges. To address these challenges, we propose Semantic-guided\nLoRA Parameter Generation (SG-LoRA), the first of its kind framework to\nefficiently produce user-specific LoRA parameters without any additional\ntraining on user tasks or access to user-specific data. Concretely, SG-LoRA\nuses task descriptions as the semantic bridge, measuring their proximity to a\nset of known expert tasks in a shared embedding space. Based on this semantic\nguidance, it models the target task's LoRA parameter distribution to generate\nhigh-performing parameters for novel tasks. SG-LoRA enables the real-time\nconstruction of LoRA models aligned with individual intents by distilling\nknowledge from prominent LoRA experts and, meanwhile, offering a\nprivacy-preserving solution for personalized model adaptation in a novel\nzero-shot open-world setting proposed in this work. Extensive experiments on\nmultiple challenging tasks confirm the superior performance and remarkable\nadaptability of SG-LoRA. Code is available at\nhttps://github.com/keepgoingjkg/SG-LoRA.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.10535v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10535v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.427,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.428,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on semantic-guided generation of LoRA parameters for model adaptation, without any involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper does not involve training models with programmatically generated or noisy labels; instead, it generates LoRA parameters using semantic task descriptions, which is unrelated to weak supervision paradigms.",
      "diffusion_reasoning_justification": "The paper mentions generation-based methods that could include diffusion models for synthesizing LoRA parameters, but it does not focus on multi-step logical reasoning or Chain-of-Thought processes; its primary contribution is semantic-guided parameter generation for task adaptation.",
      "distributed_training_justification": "The paper addresses efficient model adaptation on edge devices but does not discuss distributed training, parallel computing, or partitioning computations across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10537",
      "title": "On Using Large-Batches in Federated Learning",
      "authors": [
        "Sahil Tyagi"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Efficient Federated learning (FL) is crucial for training deep networks over\ndevices with limited compute resources and bounded networks. With the advent of\nbig data, devices either generate or collect multimodal data to train either\ngeneric or local-context aware networks, particularly when data privacy and\nlocality is vital. FL algorithms generally trade-off between parallel and\nstatistical performance, improving model quality at the cost of higher\ncommunication frequency, or vice versa. Under frequent synchronization\nsettings, FL over a large cluster of devices may perform more work per-training\niteration by processing a larger global batch-size, thus attaining considerable\ntraining speedup. However, this may result in poor test performance (i.e., low\ntest loss or accuracy) due to generalization degradation issues associated with\nlarge-batch training. To address these challenges with large-batches, this work\nproposes our vision of exploiting the trade-offs between small and large-batch\ntraining, and explore new directions to enjoy both the parallel scaling of\nlarge-batches and good generalizability of small-batch training. For the same\nnumber of iterations, we observe that our proposed large-batch training\ntechnique attains about 32.33% and 3.74% higher test accuracy than small-batch\ntraining in ResNet50 and VGG11 models respectively.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.10537v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10537v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.542,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution focuses on optimizing Federated Learning (FL), a form of distributed training, by addressing large-batch techniques to improve parallel performance and scalability across devices. It discusses partitioning computation via scaling up (larger batches per device) and scaling out (more clients), which directly aligns with distributed training concepts such as parallel computing, multi-node setups, and accelerating model training through data and computation partitioning. This relevance is evident in the paper's exploration of communication frequency, synchronization, and performance models, making it a core fit for the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper investigates the use of large-batch training in Federated Learning (FL) to enhance training efficiency on resource-constrained devices while addressing generalization issues, proposing techniques such as a parallel performance model and gradient mapping with a teacher model to combine the benefits of large-batch speedup and small-batch generalizability. It demonstrates that their approach achieves 32.33% and 3.74% higher test accuracy compared to small-batch training for ResNet50 and VGG11 models, respectively, by exploiting trade-offs in parallel and statistical performance.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing large-batch training with new techniques like gradient mapping to enhance generalization in Federated Learning, rather than introducing a entirely new problem or architecture. This clever adaptation addresses known challenges but does not represent a groundbreaking advancement.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in Federated Learning research due to its practical improvements in training efficiency and generalization, particularly in resource-limited settings. However, its influence may be confined to specific subfields like distributed machine learning rather than broader applications.",
      "recommendation_score": "Can Skip",
      "recommendation_justification": "The paper provides useful insights into optimizing large-batch training in Federated Learning, but its contributions are incremental and may not be essential for readers not specifically focused on FL efficiency. It is interesting for specialists but not a must-read for the general audience in AI and machine learning.",
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10538",
      "title": "DualAlign: Generating Clinically Grounded Synthetic Data",
      "authors": [
        "Rumeng Li",
        "Xun Wang",
        "Hong Yu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Synthetic clinical data are increasingly important for advancing AI in\nhealthcare, given strict privacy constraints on real-world EHRs, limited\navailability of annotated rare-condition data, and systemic biases in\nobservational datasets. While large language models (LLMs) can generate fluent\nclinical text, producing synthetic data that is both realistic and clinically\nmeaningful remains challenging. We introduce DualAlign, a framework that\nenhances statistical fidelity and clinical plausibility through dual alignment:\n(1) statistical alignment, which conditions generation on patient demographics\nand risk factors; and (2) semantic alignment, which incorporates real-world\nsymptom trajectories to guide content generation. Using Alzheimer's disease\n(AD) as a case study, DualAlign produces context-grounded symptom-level\nsentences that better reflect real-world clinical documentation. Fine-tuning an\nLLaMA 3.1-8B model with a combination of DualAlign-generated and\nhuman-annotated data yields substantial performance gains over models trained\non gold data alone or unguided synthetic baselines. While DualAlign does not\nfully capture longitudinal complexity, it offers a practical approach for\ngenerating clinically grounded, privacy-preserving synthetic data to support\nlow-resource clinical text analysis.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.10538v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10538v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.386,
      "datasets_score": 0.433,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on generating synthetic clinical data using a framework called DualAlign and fine-tuning an LLaMA model with generated and human-annotated data, but it does not involve reinforcement learning, a reward model, or human feedback for alignment. There is no mention of RLHF techniques.",
      "weak_supervision_justification": "The paper uses an LLM-based annotator guided by human-curated protocols to automatically annotate synthetic data, which involves programmatically generating labels from noisy or imprecise sources, aligning with weak supervision concepts. However, this is not the primary focus, as the main contribution is synthetic data generation rather than weak supervision methodologies.",
      "diffusion_reasoning_justification": "The paper introduces a framework for synthetic data generation with statistical and semantic alignment but does not involve diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes. There is no component related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating, annotating, and publicly releasing a new synthetic dataset for Alzheimer's disease signs and symptoms, evaluating its utility through classification experiments, and addressing gaps in existing resources, which directly aligns with research on dataset creation, curation, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces DualAlign, a framework for generating synthetic clinical data focused on Alzheimer's disease, which uses statistical alignment based on patient demographics and semantic alignment guided by real-world symptom trajectories to enhance data realism and clinical plausibility. By fine-tuning models like LLaMA 3.1-8B with a combination of DualAlign-generated and human-annotated data, the approach achieves significant performance improvements in classification tasks, offering a privacy-preserving solution for low-resource clinical text analysis while addressing limitations in existing synthetic data methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing LLM techniques with dual alignment strategies to generate more clinically grounded synthetic data, effectively addressing gaps in prior work for disease-specific contexts like Alzheimer's. However, it builds on established ideas rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in clinical AI by providing a practical framework and dataset for synthetic data generation, potentially improving model training in privacy-constrained healthcare scenarios. Its impact is primarily within subfields like clinical NLP and synthetic data for rare diseases, rather than broadly across all AI applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative methods for synthetic clinical data that could enhance AI in healthcare, making it valuable for researchers in related fields. While not essential for all, it provides practical insights and a new dataset that warrant attention for those working on clinical text analysis.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d6fa5e858386e8cc399a7a083cd3fc2e8260c4c5",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "R. Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2276982120"
        },
        {
          "name": "X. Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380285824"
        },
        {
          "name": "H. Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2297334010"
        }
      ]
    },
    {
      "id": "2509.12222",
      "title": "Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO\n  Satellite Systems",
      "authors": [
        "Binquan Guo",
        "Junteng Cao",
        "Marie Siew",
        "Binbin Chen",
        "Tony Q. S. Quek",
        "Zhu Han"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Large-scale low-Earth-orbit (LEO) satellite systems are increasingly valued\nfor their ability to enable rapid and wide-area data exchange, thereby\nfacilitating the collaborative training of artificial intelligence (AI) models\nacross geographically distributed regions. Due to privacy concerns and\nregulatory constraints, raw data collected at remote clients cannot be\ncentrally aggregated, posing a major obstacle to traditional AI training\nmethods. Federated learning offers a privacy-preserving alternative by training\nlocal models on distributed devices and exchanging only model parameters.\nHowever, the dynamic topology and limited bandwidth of satellite systems will\nhinder timely parameter aggregation and distribution, resulting in prolonged\ntraining times. To address this challenge, we investigate the problem of\nscheduling federated learning over satellite networks and identify key\nbottlenecks that impact the overall duration of each training round. We propose\na discrete temporal graph-based on-demand scheduling framework that dynamically\nallocates communication resources to accelerate federated learning. Simulation\nresults demonstrate that the proposed approach achieves significant performance\ngains over traditional statistical multiplexing-based model exchange\nstrategies, reducing overall round times by 14.20% to 41.48%. Moreover, the\nacceleration effect becomes more pronounced for larger models and higher\nnumbers of clients, highlighting the scalability of the proposed approach.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.12222v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12222v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.312,
      "distributed_training_score": 0.504,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves accelerating federated learning in a distributed satellite network by optimizing communication scheduling for model parameter exchanges. This directly aligns with distributed training concepts, as federated learning is a form of distributed training that partitions computation across multiple nodes (e.g., remote clients and satellites). The proposed framework enhances efficiency by addressing bandwidth constraints and dynamic topologies, which are key challenges in multi-node machine learning, making it highly pertinent to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of implementing privacy-preserving federated learning in large-scale low-Earth-orbit (LEO) satellite systems, where limited bandwidth and dynamic topologies hinder efficient model parameter exchange. The authors propose a discrete temporal graph-based on-demand scheduling framework that dynamically allocates communication resources to minimize transmission delays, demonstrating through simulations that this approach reduces training round times by 14.20% to 41.48% compared to traditional statistical multiplexing methods, with greater benefits for larger models and more clients.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining temporal graph-based scheduling with federated learning for satellite networks, offering a clever way to address bandwidth constraints in a new context.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be built upon in research on distributed AI in satellite systems, potentially influencing applications in remote and underserved regions.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to federated learning in satellite environments, making it essential for researchers focused on AI in distributed and resource-constrained networks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ad0f7c74b9a4a2c051e84eb54dfca8e6d4d6e7b2",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 3,
      "average_h_index": 0.8333333333333334,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Binquan Guo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2293358038"
        },
        {
          "name": "Junteng Cao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380634695"
        },
        {
          "name": "Marie Siew",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375137723"
        },
        {
          "name": "Binbin Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375286389"
        },
        {
          "name": "Tony Q. S. Quek",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2283641144"
        },
        {
          "name": "Zhu Han",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376471677"
        }
      ]
    },
    {
      "id": "2509.12223",
      "title": "Ratio1 -- AI meta-OS",
      "authors": [
        "Andrei Damian",
        "Petrica Butusina",
        "Alessandro De Franceschi",
        "Vitalii Toderian",
        "Marius Grigoras",
        "Cristian Bleotiu"
      ],
      "categories": [
        "cs.OS (Operating Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "We propose the Ratio1 AI meta-operating system (meta-OS), a decentralized\nMLOps protocol that unifies AI model development, deployment, and inference\nacross heterogeneous edge devices. Its key innovation is an integrated\nblockchain-based framework that transforms idle computing resources (laptops,\nsmartphones, cloud VMs) into a trustless global supercomputer. The architecture\nincludes novel components: a decentralized authentication layer (dAuth), an\nin-memory state database (CSTORE), a distributed storage system (R1FS),\nhomomorphic encrypted federated learning (EDIL), decentralized container\norchestration (Deeploy) and an oracle network (OracleSync), which collectively\nensure secure, resilient execution of AI pipelines and other container based\napps at scale. The protocol enforces a formal circular token-economic model\ncombining Proof-of-Availability (PoA) and Proof-of-AI (PoAI) consensus.\nCompared to centralized heterogeneous cloud MLOps and existing decentralized\ncompute platforms, which often lack integrated AI toolchains or trusted Ratio1\nnode operators (R1OP) mechanics, Ratio1's holistic design lowers barriers for\nAI deployment and improves cost-efficiency. We provide mathematical\nformulations of its secure licensing and reward protocols, and include\ndescriptive information for the system architecture and protocol flow. We argue\nthat our proposed fully functional ecosystem proposes and demonstrates\nsignificant improvements in accessibility, scalability, and security over\nexisting alternatives.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.12223v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12223v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.436,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a decentralized MLOps protocol for AI model development, deployment, and inference, including components like federated learning and blockchain-based consensus. It does not mention reinforcement learning, human feedback, reward models, or any mechanisms for aligning AI with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's core contribution involves distributed training through homomorphic encrypted federated learning (EDIL) and the use of decentralized edge devices to form a global supercomputer for AI pipelines. This directly addresses distributed training by partitioning computation across heterogeneous nodes, enhancing scalability and parallel processing in machine learning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "Ratio1 proposes a decentralized AI meta-operating system (meta-OS) that unifies AI model development, deployment, and inference across diverse edge devices by leveraging blockchain to convert idle computing resources into a trustless global supercomputer. The system incorporates innovative components such as decentralized authentication (dAuth), in-memory state databases (CSTORE), distributed storage (R1FS), homomorphic encrypted federated learning (EDIL), decentralized orchestration (Deeploy), and an oracle network (OracleSync), all supported by a token-economic model using Proof-of-Availability (PoA) and Proof-of-AI (PoAI) to ensure secure, scalable, and cost-efficient AI operations, while addressing limitations of centralized cloud platforms.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing blockchain and distributed computing ideas to create an integrated AI meta-OS, introducing novel components like homomorphic encrypted federated learning and a specific token-economic model, but it primarily builds on known concepts rather than introducing a completely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work could influence decentralized AI and computing subfields by democratizing access to resources and improving scalability, but its impact is likely limited to specific applications in blockchain-based MLOps rather than broader commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and innovative contribution to decentralized AI infrastructure, making it worth reading for researchers and developers in blockchain and AI fields, though it may not be essential for those outside these areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a6be4486179515753cf91ba748023cda7e6b9080",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Andrei Damian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380605718"
        },
        {
          "name": "Petrica Butusina",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380605423"
        },
        {
          "name": "Alessandro De Franceschi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380605656"
        },
        {
          "name": "Vitalii Toderian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380606057"
        },
        {
          "name": "Marius Grigoras",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380605345"
        },
        {
          "name": "Cristian Bleotiu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380605442"
        }
      ]
    },
    {
      "id": "2509.15230",
      "title": "Pre-Forgettable Models: Prompt Learning as a Native Mechanism for\n  Unlearning",
      "authors": [
        "Rutger Hendrix",
        "Giovanni Patanè",
        "Leonardo G. Russo",
        "Simone Carnemolla",
        "Giovanni Bellitto",
        "Federica Proietto Salanitri",
        "Concetto Spampinato",
        "Matteo Pennisi"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Foundation models have transformed multimedia analysis by enabling robust and\ntransferable representations across diverse modalities and tasks. However,\ntheir static deployment conflicts with growing societal and regulatory demands\n-- particularly the need to unlearn specific data upon request, as mandated by\nprivacy frameworks such as the GDPR. Traditional unlearning approaches,\nincluding retraining, activation editing, or distillation, are often\ncomputationally expensive, fragile, and ill-suited for real-time or\ncontinuously evolving systems. In this paper, we propose a paradigm shift:\nrethinking unlearning not as a retroactive intervention but as a built-in\ncapability. We introduce a prompt-based learning framework that unifies\nknowledge acquisition and removal within a single training phase. Rather than\nencoding information in model weights, our approach binds class-level semantics\nto dedicated prompt tokens. This design enables instant unlearning simply by\nremoving the corresponding prompt -- without retraining, model modification, or\naccess to original data. Experiments demonstrate that our framework preserves\npredictive performance on retained classes while effectively erasing forgotten\nones. Beyond utility, our method exhibits strong privacy and security\nguarantees: it is resistant to membership inference attacks, and prompt removal\nprevents any residual knowledge extraction, even under adversarial conditions.\nThis ensures compliance with data protection principles and safeguards against\nunauthorized access to forgotten information, making the framework suitable for\ndeployment in sensitive and regulated environments. Overall, by embedding\nremovability into the architecture itself, this work establishes a new\nfoundation for designing modular, scalable and ethically responsive AI models.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.15230v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15230v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.443,
      "weak_supervision_score": 0.427,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.375,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on prompt-based unlearning for foundation models, emphasizing privacy and data removal without retraining. It does not involve human feedback, reward models, or reinforcement learning techniques for aligning AI with preferences.",
      "weak_supervision_justification": "The paper's method relies on standard supervised training with datasets like MedMNIST, where labels are presumably accurate, and does not discuss programmatically generating noisy or imprecise labels as in weak supervision.",
      "diffusion_reasoning_justification": "The paper introduces a prompt-based framework for unlearning in classification tasks, with no mention of diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16215",
      "title": "Discovering Software Parallelization Points Using Deep Neural Networks",
      "authors": [
        "Izavan dos S. Correia",
        "Henrique C. T. Santos",
        "Tiago A. E. Ferreira"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.PL (Programming Languages)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "This study proposes a deep learning-based approach for discovering loops in\nprogramming code according to their potential for parallelization. Two genetic\nalgorithm-based code generators were developed to produce two distinct types of\ncode: (i) independent loops, which are parallelizable, and (ii) ambiguous\nloops, whose dependencies are unclear, making them impossible to define if the\nloop is parallelizable or not. The generated code snippets were tokenized and\npreprocessed to ensure a robust dataset. Two deep learning models - a Deep\nNeural Network (DNN) and a Convolutional Neural Network (CNN) - were\nimplemented to perform the classification. Based on 30 independent runs, a\nrobust statistical analysis was employed to verify the expected performance of\nboth models, DNN and CNN. The CNN showed a slightly higher mean performance,\nbut the two models had a similar variability. Experiments with varying dataset\nsizes highlighted the importance of data diversity for model performance. These\nresults demonstrate the feasibility of using deep learning to automate the\nidentification of parallelizable structures in code, offering a promising tool\nfor software optimization and performance improvement.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.16215v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16215v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.505,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on using deep neural networks to identify parallelizable loops in software code, emphasizing code analysis and classification for optimization. It does not discuss distributed training techniques, such as partitioning data or computation across multiple nodes for accelerating ML model training. While the paper mentions parallel computing in the context of software performance, this is unrelated to distributed training methods for ML.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19306",
      "title": "A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous\n  Wireless Networks",
      "authors": [
        "Jingyi Wang",
        "Zhongyuan Zhao",
        "Qingtian Wang",
        "Zexu Li",
        "Yue Wang",
        "Tony Q. S. Quek"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.IT (Information Theory)",
        "cs.NI (Networking and Internet Architecture)",
        "math.IT (Information Theory)"
      ],
      "abstract": "Edge intelligence has emerged as a promising strategy to deliver low-latency\nand ubiquitous services for mobile devices. Recent advances in fine-tuning\nmechanisms of foundation models have enabled edge intelligence by integrating\nlow-rank adaptation (LoRA) with federated learning. However, in wireless\nnetworks, the device heterogeneity and resource constraints on edge devices\npose great threats to the performance of federated fine-tuning. To tackle these\nissues, we propose to optimize federated fine-tuning in heterogenous wireless\nnetworks via online learning. First, the framework of switching-based federated\nfine-tuning in wireless networks is provided. The edge devices switches to LoRA\nmodules dynamically for federated fine-tuning with base station to jointly\nmitigate the impact of device heterogeneity and transmission unreliability.\nSecond, a tractable upper bound on the inference risk gap is derived based on\ntheoretical analysis. To improve the generalization capability, we formulate a\nnon-convex mixed-integer programming problem with long-term constraints, and\ndecouple it into model switching, transmit power control, and bandwidth\nallocation subproblems. An online optimization algorithm is developed to solve\nthe problems with polynomial computational complexity. Finally, the simulation\nresults on the SST-2 and QNLI data sets demonstrate the performance gains in\ntest accuracy and energy efficiency.",
      "published_date": "2025-09-05",
      "arxiv_url": "http://arxiv.org/abs/2509.19306v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19306v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.481,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated fine-tuning of foundation models in wireless networks, emphasizing optimization for device heterogeneity and resource constraints. It does not involve human feedback, reward models, or reinforcement learning for aligning AI with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a federated fine-tuning framework using LoRA in heterogeneous wireless networks, which directly involves distributed training across edge devices. It addresses parallel computing challenges like data heterogeneity, model aggregation, and resource management, aligning closely with distributed training concepts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper proposes a switching-based federated fine-tuning framework for foundation models in heterogeneous wireless networks to address device heterogeneity and resource constraints. By integrating low-rank adaptation (LoRA) with dynamic model switching, the authors derive an upper bound on inference risk and formulate an optimization problem for model switching, transmit power control, and bandwidth allocation, solved via an online algorithm. Simulation results on datasets like SST-2 and QNLI demonstrate improved test accuracy and energy efficiency, showcasing the framework's effectiveness in enhancing generalization and performance.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining federated learning, LoRA, and model switching in wireless networks to handle heterogeneity, offering a clever adaptation of existing techniques rather than a completely new paradigm. While it advances the state-of-the-art in this specific context, it primarily builds on established methods without introducing a fundamentally novel problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like edge intelligence and 6G networks, as it provides practical solutions for heterogeneous wireless environments. However, its influence may be limited to specific applications in AI and networking, rather than having broad commercial or widespread research implications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to optimizing federated fine-tuning in wireless networks, offering valuable insights for researchers in AI and signal processing. While not essential for all, it is worth reading for those working on edge intelligence and heterogeneity issues to stay informed on practical advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/48d8d06a70d7d52abe7469c9182199fe19fc4b87",
      "total_authors": 6,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jingyi Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhongyuan Zhao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Qingtian Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381923515"
        },
        {
          "name": "Zexu Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yue Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Tony Q. S. Quek",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381834560"
        }
      ]
    }
  ],
  "total_papers": 151,
  "date": "2025-09-05"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
