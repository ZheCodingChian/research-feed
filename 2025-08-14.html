<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 14 August 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 14 August 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 14 August 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2508.10252",
      "title": "Facilitating Longitudinal Interaction Studies of AI Systems",
      "authors": [
        "Tao Long",
        "Sitong Wang",
        "Émilie Fabre",
        "Tony Wang",
        "Anup Sathya",
        "Jason Wu",
        "Savvas Petridis",
        "Dingzeyu Li",
        "Tuhin Chakrabarty",
        "Yue Jiang",
        "Jingyi Li",
        "Tiffany Tseng",
        "Ken Nakagaki",
        "Qian Yang",
        "Nikolas Martelaro",
        "Jeffrey V. Nickerson",
        "Lydia B. Chilton"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "UIST researchers develop tools to address user challenges. However, user\ninteractions with AI evolve over time through learning, adaptation, and\nrepurposing, making one time evaluations insufficient. Capturing these dynamics\nrequires longer-term studies, but challenges in deployment, evaluation design,\nand data collection have made such longitudinal research difficult to\nimplement. Our workshop aims to tackle these challenges and prepare researchers\nwith practical strategies for longitudinal studies. The workshop includes a\nkeynote, panel discussions, and interactive breakout groups for discussion and\nhands-on protocol design and tool prototyping sessions. We seek to foster a\ncommunity around longitudinal system research and promote it as a more embraced\nmethod for designing, building, and evaluating UIST tools.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10252v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10252v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.315,
      "distributed_training_score": 0.315,
      "datasets_score": 0.424,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on facilitating longitudinal studies for user interactions with AI systems, including workshop strategies for research design and data collection. It does not involve training AI models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper addresses challenges in data collection for longitudinal AI studies, which could indirectly relate to dataset creation and evaluation in AI research. However, it does not primarily focus on creating, analyzing, benchmarking, or evaluating datasets for machine learning applications; instead, it emphasizes research methodologies.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10256",
      "title": "Deep Learning for Crack Detection: A Review of Learning Paradigms,\n  Generalizability, and Datasets",
      "authors": [
        "Xinan Zhang",
        "Haolin Wang",
        "Yung-An Hsieh",
        "Zhongyu Yang",
        "Anthony Yezzi",
        "Yi-Chang Tsai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Crack detection plays a crucial role in civil infrastructures, including\ninspection of pavements, buildings, etc., and deep learning has significantly\nadvanced this field in recent years. While numerous technical and review papers\nexist in this domain, emerging trends are reshaping the landscape. These shifts\ninclude transitions in learning paradigms (from fully supervised learning to\nsemi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation\nand fine-tuning foundation models), improvements in generalizability (from\nsingle-dataset performance to cross-dataset evaluation), and diversification in\ndataset reacquisition (from RGB images to specialized sensor-based data). In\nthis review, we systematically analyze these trends and highlight\nrepresentative works. Additionally, we introduce a new dataset collected with\n3D laser scans, 3DCrack, to support future research and conduct extensive\nbenchmarking experiments to establish baselines for commonly used deep learning\nmethodologies, including recent foundation models. Our findings provide\ninsights into the evolving methodologies and future directions in deep\nlearning-based crack detection. Project page:\nhttps://github.com/nantonzhang/Awesome-Crack-Detection",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10256v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10256v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.387,
      "datasets_score": 0.421,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include introducing a new dataset (3DCrack) collected via 3D laser scans, systematically reviewing and analyzing existing datasets for crack detection, and conducting benchmarking experiments on these datasets using deep learning models. This directly aligns with the topic, as it involves creating a new dataset, analyzing dataset characteristics, and evaluating benchmarks for machine learning applications in AI.",
      "llm_score_status": "completed",
      "summary": "This review paper systematically analyzes recent trends in deep learning-based crack detection for civil infrastructures, focusing on shifts in learning paradigms from fully supervised to advanced methods like semi-supervised and domain adaptation, improvements in model generalizability across datasets, and diversification in data sources from RGB images to specialized sensors. The authors review existing works, introduce a new dataset called 3DCrack collected via 3D laser scans, and perform benchmarking experiments on various deep learning methodologies to establish baselines and highlight insights for future research directions.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by synthesizing emerging trends in learning paradigms and generalizability while introducing a new dataset, though it primarily builds on existing ideas rather than introducing a truly novel problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision for infrastructure inspection due to its comprehensive review, new dataset, and benchmarks, but its influence may be limited to specific applications in crack detection.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution through its structured analysis and new resources, making it essential for researchers in deep learning-based crack detection to stay informed on current trends and future opportunities.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8212dc2a86ab7f6bce5d1041f4cd33f20baa0022",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 6,
      "average_h_index": 2.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Xinan Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2297330295"
        },
        {
          "name": "Haolin Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2338226156"
        },
        {
          "name": "Yung-An Hsieh",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2000576666"
        },
        {
          "name": "Zhongyu Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376180006"
        },
        {
          "name": "Anthony Yezzi",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2247075756"
        },
        {
          "name": "Yi-Chang Tsai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373641611"
        }
      ]
    },
    {
      "id": "2508.10260",
      "title": "DINOMotion: advanced robust tissue motion tracking with DINOv2 in\n  2D-Cine MRI-guided radiotherapy",
      "authors": [
        "Soorena Salari",
        "Catherine Spino",
        "Laurie-Anne Pharand",
        "Fabienne Lathuiliere",
        "Hassan Rivaz",
        "Silvain Beriault",
        "Yiming Xiao"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate tissue motion tracking is critical to ensure treatment outcome and\nsafety in 2D-Cine MRI-guided radiotherapy. This is typically achieved by\nregistration of sequential images, but existing methods often face challenges\nwith large misalignments and lack of interpretability. In this paper, we\nintroduce DINOMotion, a novel deep learning framework based on DINOv2 with\nLow-Rank Adaptation (LoRA) layers for robust, efficient, and interpretable\nmotion tracking. DINOMotion automatically detects corresponding landmarks to\nderive optimal image registration, enhancing interpretability by providing\nexplicit visual correspondences between sequential images. The integration of\nLoRA layers reduces trainable parameters, improving training efficiency, while\nDINOv2's powerful feature representations offer robustness against large\nmisalignments. Unlike iterative optimization-based methods, DINOMotion directly\ncomputes image registration at test time. Our experiments on volunteer and\npatient datasets demonstrate its effectiveness in estimating both linear and\nnonlinear transformations, achieving Dice scores of 92.07% for the kidney,\n90.90% for the liver, and 95.23% for the lung, with corresponding Hausdorff\ndistances of 5.47 mm, 8.31 mm, and 6.72 mm, respectively. DINOMotion processes\neach scan in approximately 30ms and consistently outperforms state-of-the-art\nmethods, particularly in handling large misalignments. These results highlight\nits potential as a robust and interpretable solution for real-time motion\ntracking in 2D-Cine MRI-guided radiotherapy.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10260v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10260v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.357,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10264",
      "title": "MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating\n  Hallucinations in LVLMs",
      "authors": [
        "Haonan Ge",
        "Yiwei Wang",
        "Ming-Hsuan Yang",
        "Yujun Cai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have shown strong performance across\nmultimodal tasks. However, they often produce hallucinations -- text that is\ninconsistent with visual input, due to the limited ability to verify\ninformation in different regions of the image. To address this, we propose\nMulti-Region Fusion Decoding (MRFD), a training-free decoding method that\nimproves factual grounding by modeling inter-region consistency. MRFD\nidentifies salient regions using cross-attention, generates initial responses\nfor each, and computes reliability weights based on Jensen-Shannon Divergence\n(JSD) among the responses. These weights guide a consistency-aware fusion of\nper-region predictions, using region-aware prompts inspired by Chain-of-Thought\nreasoning. Experiments across multiple LVLMs and benchmarks show that MRFD\nsignificantly reduces hallucinations and improves response factuality without\nrequiring model updates.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10264v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10264v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.462,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.52,
      "distributed_training_score": 0.379,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a training-free decoding method for LVLMs to reduce hallucinations using attention mechanisms and consistency measures, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses Chain-of-Thought prompting for region-aware responses but does not involve diffusion models, iterative refinement processes, or holistic correction of reasoning paths as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10265",
      "title": "Why Cannot Large Language Models Ever Make True Correct Reasoning?",
      "authors": [
        "Jingde Cheng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "Recently, with the application progress of AIGC tools based on large language\nmodels (LLMs), led by ChatGPT, many AI experts and more non-professionals are\ntrumpeting the \"reasoning ability\" of the LLMs. The present author considers\nthat the so-called \"reasoning ability\" of LLMs are just illusions of those\npeople who with vague concepts. In fact, the LLMs can never have the true\nreasoning ability. This paper intents to explain that, because the essential\nlimitations of their working principle, the LLMs can never have the ability of\ntrue correct reasoning.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10265v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10265v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.532,
      "distributed_training_score": 0.3,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper critiques the reasoning capabilities of large language models (LLMs) based on their fundamental limitations, arguing that they lack true reasoning ability. However, it does not discuss diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. As the topic specifically requires models using diffusion for reasoning tasks, this paper has no connection to it.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10268",
      "title": "Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile\n  Phones",
      "authors": [
        "Yujie Zhao",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Although appearance-based point-of-gaze (PoG) estimation has improved, the\nestimators still struggle to generalize across individuals due to personal\ndifferences. Therefore, person-specific calibration is required for accurate\nPoG estimation. However, calibrated PoG estimators are often sensitive to head\npose variations. To address this, we investigate the key factors influencing\ncalibrated estimators and explore pose-robust calibration strategies.\nSpecifically, we first construct a benchmark, MobilePoG, which includes facial\nimages from 32 individuals focusing on designated points under either fixed or\ncontinuously changing head poses. Using this benchmark, we systematically\nanalyze how the diversity of calibration points and head poses influences\nestimation accuracy. Our experiments show that introducing a wider range of\nhead poses during calibration improves the estimator's ability to handle pose\nvariation. Building on this insight, we propose a dynamic calibration strategy\nin which users fixate on calibration points while moving their phones. This\nstrategy naturally introduces head pose variation during a user-friendly and\nefficient calibration process, ultimately producing a better calibrated PoG\nestimator that is less sensitive to head pose variations than those using\nconventional calibration strategies. Codes and datasets are available at our\nproject page.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10268v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10268v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.307,
      "distributed_training_score": 0.332,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10280",
      "title": "High Fidelity Text to Image Generation with Contrastive Alignment and\n  Structural Guidance",
      "authors": [
        "Danyi Gao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper addresses the performance bottlenecks of existing text-driven\nimage generation methods in terms of semantic alignment accuracy and structural\nconsistency. A high-fidelity image generation method is proposed by integrating\ntext-image contrastive constraints with structural guidance mechanisms. The\napproach introduces a contrastive learning module that builds strong\ncross-modal alignment constraints to improve semantic matching between text and\nimage. At the same time, structural priors such as semantic layout maps or edge\nsketches are used to guide the generator in spatial-level structural modeling.\nThis enhances the layout completeness and detail fidelity of the generated\nimages. Within the overall framework, the model jointly optimizes contrastive\nloss, structural consistency loss, and semantic preservation loss. A\nmulti-objective supervision mechanism is adopted to improve the semantic\nconsistency and controllability of the generated content. Systematic\nexperiments are conducted on the COCO-2014 dataset. Sensitivity analyses are\nperformed on embedding dimensions, text length, and structural guidance\nstrength. Quantitative metrics confirm the superior performance of the proposed\nmethod in terms of CLIP Score, FID, and SSIM. The results show that the method\neffectively bridges the gap between semantic alignment and structural fidelity\nwithout increasing computational complexity. It demonstrates a strong ability\nto generate semantically clear and structurally complete images, offering a\nviable technical path for joint text-image modeling and image generation.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10280v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10280v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.323,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on text-to-image generation using contrastive learning and structural guidance, with losses for semantic alignment and consistency. It does not involve human feedback, reward models, or reinforcement learning for model alignment, as required for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a method for text-to-image generation with contrastive alignment and structural guidance, but it does not adapt diffusion models for multi-step logical reasoning or Chain-of-Thought processes. It lacks any component for iterative refinement in logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10281",
      "title": "VIFSS: View-Invariant and Figure Skating-Specific Pose Representation\n  Learning for Temporal Action Segmentation",
      "authors": [
        "Ryota Tanaka",
        "Tomohiro Suzuki",
        "Keisuke Fujii"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Understanding human actions from videos plays a critical role across various\ndomains, including sports analytics. In figure skating, accurately recognizing\nthe type and timing of jumps a skater performs is essential for objective\nperformance evaluation. However, this task typically requires expert-level\nknowledge due to the fine-grained and complex nature of jump procedures. While\nrecent approaches have attempted to automate this task using Temporal Action\nSegmentation (TAS), there are two major limitations to TAS for figure skating:\nthe annotated data is insufficient, and existing methods do not account for the\ninherent three-dimensional aspects and procedural structure of jump actions. In\nthis work, we propose a new TAS framework for figure skating jumps that\nexplicitly incorporates both the three-dimensional nature and the semantic\nprocedure of jump movements. First, we propose a novel View-Invariant, Figure\nSkating-Specific pose representation learning approach (VIFSS) that combines\ncontrastive learning as pre-training and action classification as fine-tuning.\nFor view-invariant contrastive pre-training, we construct FS-Jump3D, the first\npublicly available 3D pose dataset specialized for figure skating jumps.\nSecond, we introduce a fine-grained annotation scheme that marks the ``entry\n(preparation)'' and ``landing'' phases, enabling TAS models to learn the\nprocedural structure of jumps. Extensive experiments demonstrate the\neffectiveness of our framework. Our method achieves over 92% F1@50 on\nelement-level TAS, which requires recognizing both jump types and rotation\nlevels. Furthermore, we show that view-invariant contrastive pre-training is\nparticularly effective when fine-tuning data is limited, highlighting the\npracticality of our approach in real-world scenarios.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10281v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10281v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.274,
      "distributed_training_score": 0.324,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10287",
      "title": "JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in\n  Robotics",
      "authors": [
        "Simindokht Jahangard",
        "Mehrzad Mohammadi",
        "Yi Shen",
        "Zhixi Cai",
        "Hamid Rezatofighi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in Vision-Language Models (VLMs) and large language models\n(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI\nagents like robots. However, existing visual reasoning benchmarks often suffer\nfrom several limitations: they lack a clear definition of reasoning complexity,\noffer have no control to generate questions over varying difficulty and task\ncustomization, and fail to provide structured, step-by-step reasoning\nannotations (workflows). To bridge these gaps, we formalize reasoning\ncomplexity, introduce an adaptive query engine that generates customizable\nquestions of varying complexity with detailed intermediate annotations, and\nextend the JRDB dataset with human-object interaction and geometric\nrelationship annotations to create JRDB-Reasoning, a benchmark tailored for\nvisual reasoning in human-crowded environments. Our engine and benchmark enable\nfine-grained evaluation of visual reasoning frameworks and dynamic assessment\nof visual-language models across reasoning levels.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10287v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10287v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.528,
      "distributed_training_score": 0.319,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a benchmark for visual reasoning using VLMs and LLMs, including formalizing reasoning complexity and generating step-by-step annotations. It does not mention or involve diffusion models, iterative refinement processes, or adapting diffusion for multi-step logical reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and extension of the JRDB dataset into JRDB-Reasoning, including new annotations for human-object interactions and geometric relationships, along with an adaptive query engine for benchmarking. This directly aligns with research on dataset creation, curation, and evaluation for AI applications in visual reasoning.",
      "llm_score_status": "completed",
      "summary": "This paper introduces JRDB-Reasoning, a benchmark designed to enhance visual reasoning evaluation in robotics by addressing limitations in existing benchmarks, such as the lack of reasoning complexity definition and customizable questions. The authors formalize reasoning complexity using spatio-temporal graphs, develop an adaptive query engine for generating questions with adjustable difficulty and step-by-step annotations, and enrich the JRDB dataset with human-object interaction and geometric relationship annotations, enabling fine-grained assessment of vision-language models and demonstrating the benchmark's potential through initial evaluations.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark with formalized reasoning complexity and an adaptive query engine, significantly advancing state-of-the-art visual reasoning evaluation by providing customizable, difficulty-graded tasks. This represents a substantial innovation in addressing gaps in existing datasets for embodied AI.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of computer vision and robotics, as it offers a specialized benchmark for visual reasoning that can improve model assessments. However, its influence may be limited to niche applications rather than widespread commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a strong and valuable contribution to visual reasoning benchmarks, making it essential for researchers in robotics and computer vision to understand its advancements. While not groundbreaking for all AI fields, it offers practical tools that warrant attention in relevant subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/128db2a586596da50a0532bbd2c9cd6da80223df",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 9,
      "average_h_index": 3.2,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Simindokht Jahangard",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/3457834"
        },
        {
          "name": "Mehrzad Mohammadi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375903658"
        },
        {
          "name": "Yi Shen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376088316"
        },
        {
          "name": "Zhixi Cai",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2322001163"
        },
        {
          "name": "Hamid Rezatofighi",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2307081678"
        }
      ]
    },
    {
      "id": "2508.10293",
      "title": "Promoting Efficient Reasoning with Verifiable Stepwise Reward",
      "authors": [
        "Chuhuai Yue",
        "Chengqi Dong",
        "Yinan Gao",
        "Hang He",
        "Jiajun Chai",
        "Guojun Yin",
        "Wei Lin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large reasoning models (LRMs) have recently achieved significant progress in\ncomplex reasoning tasks, aided by reinforcement learning with verifiable\nrewards. However, LRMs often suffer from overthinking, expending excessive\ncomputation on simple problems and reducing efficiency. Existing efficient\nreasoning methods typically require accurate task assessment to preset token\nbudgets or select reasoning modes, which limits their flexibility and\nreliability. In this work, we revisit the essence of overthinking and identify\nthat encouraging effective steps while penalizing ineffective ones is key to\nits solution. To this end, we propose a novel rule-based verifiable stepwise\nreward mechanism (VSRM), which assigns rewards based on the performance of\nintermediate states in the reasoning trajectory. This approach is intuitive and\nnaturally fits the step-by-step nature of reasoning tasks. We conduct extensive\nexperiments on standard mathematical reasoning benchmarks, including AIME24 and\nAIME25, by integrating VSRM with PPO and Reinforce++. Results show that our\nmethod achieves substantial output length reduction while maintaining original\nreasoning performance, striking an optimal balance between efficiency and\naccuracy. Further analysis of overthinking frequency and pass@k score before\nand after training demonstrates that our approach in deed effectively\nsuppresses ineffective steps and encourages effective reasoning, fundamentally\nalleviating the overthinking problem. All code will be released upon\nacceptance.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10293v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10293v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.445,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.538,
      "distributed_training_score": 0.352,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a rule-based verifiable stepwise reward mechanism (VSRM) for reinforcement learning, which does not involve training a reward model on human-ranked data or incorporating human feedback. Instead, rewards are assigned based on automated evaluation of intermediate states, making it distinct from RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses efficient reasoning in large reasoning models using reinforcement learning and stepwise rewards, with no reference to diffusion models, iterative refinement processes, or treating Chains-of-Thought as entities for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10294",
      "title": "A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method",
      "authors": [
        "Tao Huang",
        "Hongbo Pan",
        "Nanxi Zhou",
        "Shun Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "High-accuracy matching of multimodal optical images is the basis of geometric\nprocessing. However, the image matching accuracy is usually degraded by the\nnonlinear radiation and geometric deformation differences caused by different\nspectral responses. To address these problems, we proposed a phase consistency\nweighted least absolute deviation (PCWLAD) sub-pixel template matching method\nto improve the matching accuracy of multimodal optical images. This method\nconsists of two main steps: coarse matching with the structural similarity\nindex measure (SSIM) and fine matching with WLAD. In the coarse matching step,\nPCs are calculated without a noise filter to preserve the original structural\ndetails, and template matching is performed using the SSIM. In the fine\nmatching step, we applied the radiometric and geometric transformation models\nbetween two multimodal PC templates based on the coarse matching. Furthermore,\nmutual structure filtering is adopted in the model to mitigate the impact of\nnoise within the corresponding templates on the structural consistency, and the\nWLAD criterion is used to estimate the sub-pixel offset. To evaluate the\nperformance of PCWLAD, we created three types of image datasets: visible to\ninfrared Landsat images, visible to near-infrared close-range images, and\nvisible to infrared uncrewed aerial vehicle (UAV) images. PCWLAD outperformed\nexisting state-of-the-art eight methods in terms of correct matching rate (CMR)\nand root mean square error (RMSE) and reached an average matching accuracy of\napproximately 0.4 pixels across all three datasets. Our software and datasets\nare publicly available at https://github.com/huangtaocsu/PCWLAD.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10294v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10294v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.264,
      "distributed_training_score": 0.284,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10297",
      "title": "InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild",
      "authors": [
        "Yiyi Ma",
        "Yuanzhi Liang",
        "Xiu Li",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present Interleaved Learning for Motion Synthesis (InterSyn), a novel\nframework that targets the generation of realistic interaction motions by\nlearning from integrated motions that consider both solo and multi-person\ndynamics. Unlike previous methods that treat these components separately,\nInterSyn employs an interleaved learning strategy to capture the natural,\ndynamic interactions and nuanced coordination inherent in real-world scenarios.\nOur framework comprises two key modules: the Interleaved Interaction Synthesis\n(INS) module, which jointly models solo and interactive behaviors in a unified\nparadigm from a first-person perspective to support multiple character\ninteractions, and the Relative Coordination Refinement (REC) module, which\nrefines mutual dynamics and ensures synchronized motions among characters.\nExperimental results show that the motion sequences generated by InterSyn\nexhibit higher text-to-motion alignment and improved diversity compared with\nrecent methods, setting a new benchmark for robust and natural motion\nsynthesis. Additionally, our code will be open-sourced in the future to promote\nfurther research and development in this area.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10297v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10297v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.38,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a framework for text-to-motion synthesis, specifically generating realistic human motions using interleaved learning and refinement modules. While it mentions diffusion models in the context of motion generation (e.g., text-conditioned diffusion for diverse motions), it does not adapt diffusion for multi-step logical reasoning or Chain-of-Thought processes. The core contributions involve motion synthesis and coordination, not solving complex logical tasks, so there is no clear component for diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10298",
      "title": "SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic\n  Representation Learning",
      "authors": [
        "Weijian Mai",
        "Jiamin Wu",
        "Yu Zhu",
        "Zhouheng Yao",
        "Dongzhan Zhou",
        "Andrew F. Luo",
        "Qihao Zheng",
        "Wanli Ouyang",
        "Chunfeng Song"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Deciphering how visual stimuli are transformed into cortical responses is a\nfundamental challenge in computational neuroscience. This visual-to-neural\nmapping is inherently a one-to-many relationship, as identical visual inputs\nreliably evoke variable hemodynamic responses across trials, contexts, and\nsubjects. However, existing deterministic methods struggle to simultaneously\nmodel this biological variability while capturing the underlying functional\nconsistency that encodes stimulus information. To address these limitations, we\npropose SynBrain, a generative framework that simulates the transformation from\nvisual semantics to neural responses in a probabilistic and biologically\ninterpretable manner. SynBrain introduces two key components: (i) BrainVAE\nmodels neural representations as continuous probability distributions via\nprobabilistic learning while maintaining functional consistency through visual\nsemantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantic\ntransmission pathway, projecting visual semantics into the neural response\nmanifold to facilitate high-fidelity fMRI synthesis. Experimental results\ndemonstrate that SynBrain surpasses state-of-the-art methods in\nsubject-specific visual-to-fMRI encoding performance. Furthermore, SynBrain\nadapts efficiently to new subjects with few-shot data and synthesizes\nhigh-quality fMRI signals that are effective in improving data-limited\nfMRI-to-image decoding performance. Beyond that, SynBrain reveals functional\nconsistency across trials and subjects, with synthesized signals capturing\ninterpretable patterns shaped by biological neural variability. The code will\nbe made publicly available.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10298v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10298v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.357,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a probabilistic framework for visual-to-fMRI synthesis, focusing on modeling neural responses using BrainVAE and a Semantic-to-Neural Mapper. It references diffusion-based methods in prior work (e.g., MindSimulator) but does not incorporate diffusion models or iterative refinement for multi-step logical reasoning. The paper addresses visual-to-neural mapping, not complex logical tasks or Chain-of-Thought reasoning, so it lacks any relevant components for this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10299",
      "title": "Improving Learning of New Diseases through Knowledge-Enhanced\n  Initialization for Federated Adapter Tuning",
      "authors": [
        "Danni Peng",
        "Yuan Wang",
        "Kangning Cai",
        "Peiyan Ning",
        "Jiming Xu",
        "Yong Liu",
        "Rick Siow Mong Goh",
        "Qingsong Wei",
        "Huazhu Fu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In healthcare, federated learning (FL) is a widely adopted framework that\nenables privacy-preserving collaboration among medical institutions. With large\nfoundation models (FMs) demonstrating impressive capabilities, using FMs in FL\nthrough cost-efficient adapter tuning has become a popular approach. Given the\nrapidly evolving healthcare environment, it is crucial for individual clients\nto quickly adapt to new tasks or diseases by tuning adapters while drawing upon\npast experiences. In this work, we introduce Federated Knowledge-Enhanced\nInitialization (FedKEI), a novel framework that leverages cross-client and\ncross-task transfer from past knowledge to generate informed initializations\nfor learning new tasks with adapters. FedKEI begins with a global clustering\nprocess at the server to generalize knowledge across tasks, followed by the\noptimization of aggregation weights across clusters (inter-cluster weights) and\nwithin each cluster (intra-cluster weights) to personalize knowledge transfer\nfor each new task. To facilitate more effective learning of the inter- and\nintra-cluster weights, we adopt a bi-level optimization scheme that\ncollaboratively learns the global intra-cluster weights across clients and\noptimizes the local inter-cluster weights toward each client's task objective.\nExtensive experiments on three benchmark datasets of different modalities,\nincluding dermatology, chest X-rays, and retinal OCT, demonstrate FedKEI's\nadvantage in adapting to new diseases compared to state-of-the-art methods.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10299v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10299v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.407,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on federated learning (FL), which is a form of distributed training involving multiple clients (nodes) for collaborative model training without sharing data. It includes elements like aggregation of adapters and bi-level optimization across clients, aligning with distributed training concepts such as partitioning computation and model updates. However, the primary contribution is on knowledge-enhanced initialization for new tasks in healthcare, rather than innovating core distributed training techniques for acceleration or parallel computing. Thus, it is relevant but not central to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Federated Knowledge-Enhanced Initialization (FedKEI), a framework designed to improve federated learning for healthcare by enabling efficient adaptation to new diseases through informed initializations of adapters in foundation models. It achieves this by performing global clustering of task-specific modules at the server, optimizing inter- and intra-cluster weights via bi-level optimization to facilitate knowledge transfer across clients and tasks, and demonstrating superior performance on benchmark datasets for dermatology, chest X-rays, and retinal OCT compared to state-of-the-art methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing concepts like federated learning and adapter tuning with novel elements such as bi-level optimization for knowledge transfer, but it primarily refines rather than introduces entirely new problems or techniques.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in federated learning for healthcare, particularly for adapting to new tasks, due to its practical approach in medical imaging. However, its influence may remain confined to specific subfields rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to federated learning in healthcare, making it essential for researchers in medical AI to be aware of its methods and results. While innovative, it is not groundbreaking enough to be considered a must-read for all.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7c1cc98c74bafb5f82aab927f9c77e9df9c631d0",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 17,
      "average_h_index": 3.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Danni Peng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2313966658"
        },
        {
          "name": "Yuan Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2285070153"
        },
        {
          "name": "Kangning Cai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375211942"
        },
        {
          "name": "Peiyan Ning",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375209892"
        },
        {
          "name": "Jiming Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375244198"
        },
        {
          "name": "Yong Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2285469992"
        },
        {
          "name": "R. Goh",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/117628883"
        },
        {
          "name": "Qingsong Wei",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2285078682"
        },
        {
          "name": "Huazhu Fu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2258526406"
        }
      ]
    },
    {
      "id": "2508.10304",
      "title": "Yet another algorithmic bias: A Discursive Analysis of Large Language\n  Models Reinforcing Dominant Discourses on Gender and Race",
      "authors": [
        "Gustavo Bonil",
        "Simone Hashiguti",
        "Jhessica Silva",
        "João Gondim",
        "Helena Maia",
        "Nádia Silva",
        "Helio Pedrini",
        "Sandra Avila"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the advance of Artificial Intelligence (AI), Large Language Models\n(LLMs) have gained prominence and been applied in diverse contexts. As they\nevolve into more sophisticated versions, it is essential to assess whether they\nreproduce biases, such as discrimination and racialization, while maintaining\nhegemonic discourses. Current bias detection approaches rely mostly on\nquantitative, automated methods, which often overlook the nuanced ways in which\nbiases emerge in natural language. This study proposes a qualitative,\ndiscursive framework to complement such methods. Through manual analysis of\nLLM-generated short stories featuring Black and white women, we investigate\ngender and racial biases. We contend that qualitative methods such as the one\nproposed here are fundamental to help both developers and users identify the\nprecise ways in which biases manifest in LLM outputs, thus enabling better\nconditions to mitigate them. Results show that Black women are portrayed as\ntied to ancestry and resistance, while white women appear in self-discovery\nprocesses. These patterns reflect how language models replicate crystalized\ndiscursive representations, reinforcing essentialization and a sense of social\nimmobility. When prompted to correct biases, models offered superficial\nrevisions that maintained problematic meanings, revealing limitations in\nfostering inclusive narratives. Our results demonstrate the ideological\nfunctioning of algorithms and have significant implications for the ethical use\nand development of AI. The study reinforces the need for critical,\ninterdisciplinary approaches to AI design and deployment, addressing how\nLLM-generated discourses reflect and perpetuate inequalities.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10304v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10304v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.333,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on qualitative analysis of biases in existing LLM outputs and does not involve training, fine-tuning, or aligning AI models using human feedback or reinforcement learning techniques. It only discusses prompting models to correct biases, which is not equivalent to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper creates and analyzes a corpus of LLM-generated texts for bias evaluation, which touches on dataset collection and analysis, but its primary contribution is qualitative bias detection in LLMs, not the creation, benchmarking, or evaluation of datasets as a main focus.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10307",
      "title": "Efficient Image Denoising Using Global and Local Circulant\n  Representation",
      "authors": [
        "Zhaoming Kong",
        "Jiahuan Zhang",
        "Xiaowei Yang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The advancement of imaging devices and countless image data generated\neveryday impose an increasingly high demand on efficient and effective image\ndenoising. In this paper, we present a computationally simple denoising\nalgorithm, termed Haar-tSVD, aiming to explore the nonlocal self-similarity\nprior and leverage the connection between principal component analysis (PCA)\nand the Haar transform under circulant representation. We show that global and\nlocal patch correlations can be effectively captured through a unified\ntensor-singular value decomposition (t-SVD) projection with the Haar transform.\nThis results in a one-step, highly parallelizable filtering method that\neliminates the need for learning local bases to represent image patches,\nstriking a balance between denoising speed and performance. Furthermore, we\nintroduce an adaptive noise estimation scheme based on a CNN estimator and\neigenvalue analysis to enhance the robustness and adaptability of the proposed\nmethod. Experiments on different real-world denoising tasks validate the\nefficiency and effectiveness of Haar-tSVD for noise removal and detail\npreservation. Datasets, code and results are publicly available at\nhttps://github.com/ZhaomingKong/Haar-tSVD.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10307v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10307v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.251,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.299,
      "distributed_training_score": 0.305,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10308",
      "title": "ReviewRL: Towards Automated Scientific Review with RL",
      "authors": [
        "Sihang Zeng",
        "Kai Tian",
        "Kaiyan Zhang",
        "Yuru wang",
        "Junqi Gao",
        "Runze Liu",
        "Sa Yang",
        "Jingxuan Li",
        "Xinwei Long",
        "Jiaheng Ma",
        "Biqing Qi",
        "Bowen Zhou"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Peer review is essential for scientific progress but faces growing challenges\ndue to increasing submission volumes and reviewer fatigue. Existing automated\nreview approaches struggle with factual accuracy, rating consistency, and\nanalytical depth, often generating superficial or generic feedback lacking the\ninsights characteristic of high-quality human reviews. We introduce ReviewRL, a\nreinforcement learning framework for generating comprehensive and factually\ngrounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP\nretrieval-augmented context generation pipeline that incorporates relevant\nscientific literature, (2) supervised fine-tuning that establishes foundational\nreviewing capabilities, and (3) a reinforcement learning procedure with a\ncomposite reward function that jointly enhances review quality and rating\naccuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL\nsignificantly outperforms existing methods across both rule-based metrics and\nmodel-based quality assessments. ReviewRL establishes a foundational framework\nfor RL-driven automatic critique generation in scientific discovery,\ndemonstrating promising potential for future development in this domain. The\nimplementation of ReviewRL will be released at GitHub.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10308v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10308v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.499,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.345,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with a composite reward function to improve review quality, but it does not specify training a reward model on human-ranked data. While RL is involved, the lack of explicit human feedback means it does not fully align with RLHF definitions.",
      "weak_supervision_justification": "The paper mentions supervised fine-tuning for foundational capabilities but does not describe using programmatically generated, noisy labels or weak supervision techniques, focusing instead on RL and retrieval-augmented methods.",
      "diffusion_reasoning_justification": "The paper's framework relies on reinforcement learning and retrieval augmentation, with no mention of diffusion models, iterative refinement for reasoning, or treating Chain-of-Thought as a holistic entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10309",
      "title": "From Pixel to Mask: A Survey of Out-of-Distribution Segmentation",
      "authors": [
        "Wenjie Zhao",
        "Jia Li",
        "Yunhui Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Out-of-distribution (OoD) detection and segmentation have attracted growing\nattention as concerns about AI security rise. Conventional OoD detection\nmethods identify the existence of OoD objects but lack spatial localization,\nlimiting their usefulness in downstream tasks. OoD segmentation addresses this\nlimitation by localizing anomalous objects at pixel-level granularity. This\ncapability is crucial for safety-critical applications such as autonomous\ndriving, where perception modules must not only detect but also precisely\nsegment OoD objects, enabling targeted control actions and enhancing overall\nsystem robustness. In this survey, we group current OoD segmentation approaches\ninto four categories: (i) test-time OoD segmentation, (ii) outlier exposure for\nsupervised training, (iii) reconstruction-based methods, (iv) and approaches\nthat leverage powerful models. We systematically review recent advances in OoD\nsegmentation for autonomous-driving scenarios, identify emerging challenges,\nand discuss promising future research directions.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10309v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10309v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.366,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10315",
      "title": "A Vision-Language Pre-training Model-Guided Approach for Mitigating\n  Backdoor Attacks in Federated Learning",
      "authors": [
        "Keke Gai",
        "Dongjue Wang",
        "Jing Yu",
        "Liehuang Zhu",
        "Qi Wu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Existing backdoor defense methods in Federated Learning (FL) rely on the\nassumption of homogeneous client data distributions or the availability of a\nclean serve dataset, which limits the practicality and effectiveness. Defending\nagainst backdoor attacks under heterogeneous client data distributions while\npreserving model performance remains a significant challenge. In this paper, we\npropose a FL backdoor defense framework named CLIP-Fed, which leverages the\nzero-shot learning capabilities of vision-language pre-training models. By\nintegrating both pre-aggregation and post-aggregation defense strategies,\nCLIP-Fed overcomes the limitations of Non-IID imposed on defense effectiveness.\nTo address privacy concerns and enhance the coverage of the dataset against\ndiverse triggers, we construct and augment the server dataset using the\nmultimodal large language model and frequency analysis without any client\nsamples. To address class prototype deviations caused by backdoor samples and\neliminate the correlation between trigger patterns and target labels, CLIP-Fed\naligns the knowledge of the global model and CLIP on the augmented dataset\nusing prototype contrastive loss and Kullback-Leibler divergence. Extensive\nexperiments on representative datasets validate the effectiveness of CLIP-Fed.\nCompared to state-of-the-art methods, CLIP-Fed achieves an average reduction in\nASR, i.e., 2.03\\% on CIFAR-10 and 1.35\\% on CIFAR-10-LT, while improving\naverage MA by 7.92\\% and 0.48\\%, respectively.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10315v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10315v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.419,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on defending against backdoor attacks in Federated Learning using vision-language models like CLIP, involving model alignment and contrastive loss. It does not involve human feedback, reward models, or reinforcement learning for aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper is centered on Federated Learning, a form of distributed training, and proposes defense strategies like pre-aggregation clustering, which relate to handling distributed updates across clients. However, its main contribution is security enhancements rather than novel algorithms for partitioning data or accelerating training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces CLIP-Fed, a novel framework designed to mitigate backdoor attacks in Federated Learning (FL) by leveraging the zero-shot learning capabilities of vision-language pre-training models like CLIP, addressing limitations in existing methods that assume homogeneous data distributions or require clean datasets. The methodology involves pre-aggregation model clustering to filter malicious updates, constructing a privacy-preserving server dataset using multimodal large language models and frequency analysis, and post-aggregation alignment of the global model with CLIP via prototype contrastive loss and Kullback-Leibler divergence to correct class prototypes and eliminate backdoor correlations; experiments on datasets such as CIFAR-10 demonstrate significant reductions in attack success rates (e.g., 2.03% on CIFAR-10) and improvements in main accuracy.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative approach by being the first to use vision-language pre-training models for backdoor defense in FL, significantly advancing the state-of-the-art in handling heterogeneous data distributions and privacy concerns.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of FL security due to its effective defense mechanisms and experimental validations, though its influence may be limited to specific applications in AI robustness and privacy.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to FL backdoor defense research with practical innovations and solid results, making it essential for specialists in machine learning security but not broadly mandatory.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/51d4cf907bdab9411d7013e9e792e63258701afe",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 6,
      "average_h_index": 4.2,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Keke Gai",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2248168950"
        },
        {
          "name": "Dongjue Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2344635306"
        },
        {
          "name": "Jing Yu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2261262372"
        },
        {
          "name": "Liehuang Zhu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2288887868"
        },
        {
          "name": "Qi Wu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2265520519"
        }
      ]
    },
    {
      "id": "2508.10316",
      "title": "Integrating Reinforcement Learning with Visual Generative Models:\n  Foundations and Advances",
      "authors": [
        "Yuanzhi Liang",
        "Yijie Fang",
        "Rui Li",
        "Ziqi Ni",
        "Ruijie Su",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Generative models have made significant progress in synthesizing visual\ncontent, including images, videos, and 3D/4D structures. However, they are\ntypically trained with surrogate objectives such as likelihood or\nreconstruction loss, which often misalign with perceptual quality, semantic\naccuracy, or physical realism. Reinforcement learning (RL) offers a principled\nframework for optimizing non-differentiable, preference-driven, and temporally\nstructured objectives. Recent advances demonstrate its effectiveness in\nenhancing controllability, consistency, and human alignment across generative\ntasks. This survey provides a systematic overview of RL-based methods for\nvisual content generation. We review the evolution of RL from classical control\nto its role as a general-purpose optimization tool, and examine its integration\ninto image, video, and 3D/4D generation. Across these domains, RL serves not\nonly as a fine-tuning mechanism but also as a structural component for aligning\ngeneration with complex, high-level goals. We conclude with open challenges and\nfuture research directions at the intersection of RL and generative modeling.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10316v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10316v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.499,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.343,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses RL for human alignment, preference-based fine-tuning, and optimizing objectives that align with human perception and semantics, as seen in sections on enhancing controllability and realism. While it implies the use of feedback-aware learning and preferences, which are core to RLHF, it does not explicitly describe training a separate reward model on human-ranked data. Thus, it is relevant but not a direct match to the strict definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on integrating RL with visual generative models, including diffusion models, for tasks like image and video synthesis, but it does not involve adapting diffusion for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. There is no component for iterative refinement in a reasoning context, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This survey paper explores the integration of reinforcement learning (RL) with visual generative models, addressing the limitations of traditional training objectives like likelihood or reconstruction loss by using RL to optimize for perceptual quality, semantic accuracy, and physical realism in generating images, videos, and 3D/4D content. It reviews the evolution of RL from classical control to its applications in generative tasks, examines its role in enhancing controllability and alignment across domains, and highlights the rapid growth in research at this intersection, while concluding with open challenges and future directions.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by systematically synthesizing and combining existing ideas on integrating RL with generative models, offering a new overview that advances understanding in this area without introducing entirely novel techniques. However, as a survey, it primarily consolidates prior work rather than pioneering a completely new problem or method.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence a wide range of future research and applications in AI by providing a comprehensive overview of a rapidly growing field, potentially guiding developments in generative modeling across computer vision and beyond. Its documentation of exponential publication growth underscores its relevance and potential to shape strategic directions in visual content generation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution as a timely and comprehensive survey on an emerging topic, making it essential for researchers in generative AI to be aware of for staying updated on trends and applications. While not groundbreaking original research, its insights and synthesis warrant attention for those working at the intersection of RL and visual generation.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/548960e58f66c101264b62b7b6f99c2d69acaf58",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 1,
      "average_h_index": 0.2857142857142857,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yuanzhi Liang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2337074199"
        },
        {
          "name": "Yijie Fang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375954965"
        },
        {
          "name": "Rui Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376087325"
        },
        {
          "name": "Ziqi Ni",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375902027"
        },
        {
          "name": "Ruijie Su",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375903003"
        },
        {
          "name": "Chi Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376189934"
        },
        {
          "name": "Xuelong Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336880377"
        }
      ]
    },
    {
      "id": "2508.10332",
      "title": "Layer-Wise Analysis of Self-Supervised Representations for Age and\n  Gender Classification in Children's Speech",
      "authors": [
        "Abhijit Sinha",
        "Harishankar Kumar",
        "Mohit Joshi",
        "Hemant Kumar Kathania",
        "Shrikanth Narayanan",
        "Sudarsana Reddy Kadiri"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)",
        "cs.LG (Machine Learning)",
        "cs.SD (Sound)"
      ],
      "abstract": "Children's speech presents challenges for age and gender classification due\nto high variability in pitch, articulation, and developmental traits. While\nself-supervised learning (SSL) models perform well on adult speech tasks, their\nability to encode speaker traits in children remains underexplored. This paper\npresents a detailed layer-wise analysis of four Wav2Vec2 variants using the\nPFSTAR and CMU Kids datasets. Results show that early layers (1-7) capture\nspeaker-specific cues more effectively than deeper layers, which increasingly\nfocus on linguistic information. Applying PCA further improves classification,\nreducing redundancy and highlighting the most informative components. The\nWav2Vec2-large-lv60 model achieves 97.14% (age) and 98.20% (gender) on CMU\nKids; base-100h and large-lv60 models reach 86.05% and 95.00% on PFSTAR. These\nresults reveal how speaker traits are structured across SSL model depth and\nsupport more targeted, adaptive strategies for child-aware speech interfaces.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10332v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10332v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.313,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10333",
      "title": "ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot\n  Perceiver",
      "authors": [
        "Wenxuan Song",
        "Ziyang Zhou",
        "Han Zhao",
        "Jiayi Chen",
        "Pengxiang Ding",
        "Haodong Yan",
        "Yuxin Huang",
        "Feilong Tang",
        "Donglin Wang",
        "Haoang Li"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in Vision-Language-Action (VLA) models have enabled robotic\nagents to integrate multimodal understanding with action execution. However,\nour empirical analysis reveals that current VLAs struggle to allocate visual\nattention to target regions. Instead, visual attention is always dispersed. To\nguide the visual attention grounding on the correct target, we propose\nReconVLA, a reconstructive VLA model with an implicit grounding paradigm.\nConditioned on the model's visual outputs, a diffusion transformer aims to\nreconstruct the gaze region of the image, which corresponds to the target\nmanipulated objects. This process prompts the VLA model to learn fine-grained\nrepresentations and accurately allocate visual attention, thus effectively\nleveraging task-specific visual information and conducting precise\nmanipulation. Moreover, we curate a large-scale pretraining dataset comprising\nover 100k trajectories and 2 million data samples from open-source robotic\ndatasets, further boosting the model's generalization in visual reconstruction.\nExtensive experiments in simulation and the real world demonstrate the\nsuperiority of our implicit grounding method, showcasing its capabilities of\nprecise manipulation and generalization. Our project page is\nhttps://zionchow.github.io/ReconVLA/.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10333v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10333v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.456,
      "distributed_training_score": 0.342,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing a reconstructive Vision-Language-Action model using diffusion for visual attention and grounding in robotics, with pretraining on open-source datasets. There is no mention of human feedback, reward models, or reinforcement learning to align with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs a diffusion transformer for visual reconstruction to improve attention in robotic tasks, but it does not adapt diffusion for multi-step logical reasoning or treat a Chain-of-Thought as an entity for iterative refinement. The diffusion process is used for image generation, not solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10337",
      "title": "A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG\n  for Multimodal Question Answering",
      "authors": [
        "Chenliang Zhang",
        "Lin Wang",
        "Yuanyuan Lu",
        "Yusheng Qi",
        "Kexin Wang",
        "Peixu Hou",
        "Wenshi Chen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This paper describes the solutions of the Dianping-Trust-Safety team for the\nMETA CRAG-MM challenge. The challenge requires building a comprehensive\nretrieval-augmented generation system capable for multi-modal multi-turn\nquestion answering. The competition consists of three tasks: (1) answering\nquestions using structured data retrieved from an image-based mock knowledge\ngraph, (2) synthesizing information from both knowledge graphs and web search\nresults, and (3) handling multi-turn conversations that require context\nunderstanding and information aggregation from multiple sources. For Task 1,\nour solution is based on the vision large language model, enhanced by\nsupervised fine-tuning with knowledge distilled from GPT-4.1. We further\napplied curriculum learning strategies to guide reinforcement learning,\nresulting in improved answer accuracy and reduced hallucination. For Task 2 and\nTask 3, we additionally leveraged web search APIs to incorporate external\nknowledge, enabling the system to better handle complex queries and multi-turn\nconversations. Our approach achieved 1st place in Task 1 with a significant\nlead of 52.38\\%, and 3rd place in Task 3, demonstrating the effectiveness of\nthe integration of curriculum learning with reinforcement learning in our\ntraining pipeline.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10337v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10337v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.34,
      "datasets_score": 0.384,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes using reinforcement learning guided by curriculum learning and knowledge distillation from GPT-4.1, but it does not involve human feedback, such as training a reward model on human-ranked data. Since RLHF specifically requires human preferences, this approach does not qualify.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on vision large language models, supervised fine-tuning, curriculum learning, and reinforcement learning for multimodal QA, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10339",
      "title": "Concepts or Skills? Rethinking Instruction Selection for Multi-modal\n  Models",
      "authors": [
        "Andrew Bai",
        "Justin Cui",
        "Ruochen Wang",
        "Cho-Jui Hsieh"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Vision-language instruction tuning achieves two main purposes: learning\nvisual concepts and learning visual skills. In this paper, we found that\nvision-language benchmarks fall into the dichotomy of mainly benefiting from\ntraining on instructions with similar skills or visual concepts. Inspired by\nthe discovery, we designed a simple targeted training data selection method to\noptimize the performance of a given benchmark. We first extract the\nconcepts/skills from the benchmark, determine whether the benchmark\npredominantly benefits from similar concepts or skills, and finally select\ninstructions with the most matching concepts/skills. Experiments on 10+\nbenchmarks validate the effectiveness of our targeted data selection method,\nshowing +0.9\\% over the best existing baseline averaged over all benchmarks and\n+1.5\\% on the skill-focused subset. Our findings underscore the importance of\nrecognizing the inherent trade-off within instruction selection, which requires\nbalancing the acquisition of conceptual knowledge against visual skill.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10339v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10339v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.36,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on vision-language instruction tuning and data selection for multi-modal models, with no mention of human feedback, reward models, or reinforcement learning techniques. It is purely about supervised fine-tuning based on concepts and skills, not aligning models with human preferences.",
      "weak_supervision_justification": "The paper involves automated extraction and selection of instructions using parsing and taxonomy alignment, which could be seen as a form of programmatic data handling, but it does not primarily rely on noisy or imprecise label generation for training. Instead, it emphasizes targeted data selection for fine-tuning, not the core principles of weak supervision like using heuristic labels.",
      "diffusion_reasoning_justification": "The paper discusses instruction tuning for vision-language models and does not involve diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes. It is centered on concept/skill-based data selection, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10340",
      "title": "Multi-Agent Trust Region Policy Optimisation: A Joint Constraint\n  Approach",
      "authors": [
        "Chak Lam Shek",
        "Guangyao Shi",
        "Pratap Tokekar"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) requires coordinated and stable\npolicy updates among interacting agents. Heterogeneous-Agent Trust Region\nPolicy Optimization (HATRPO) enforces per-agent trust region constraints using\nKullback-Leibler (KL) divergence to stabilize training. However, assigning each\nagent the same KL threshold can lead to slow and locally optimal updates,\nespecially in heterogeneous settings. To address this limitation, we propose\ntwo approaches for allocating the KL divergence threshold across agents:\nHATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes\nthreshold assignment under global KL constraints, and HATRPO-G, a greedy\nalgorithm that prioritizes agents based on improvement-to-divergence ratio. By\nconnecting sequential policy optimization with constrained threshold\nscheduling, our approach enables more flexible and effective learning in\nheterogeneous-agent settings. Experimental results demonstrate that our methods\nsignificantly boost the performance of HATRPO, achieving faster convergence and\nhigher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and\nHATRPO-G achieve comparable improvements in final performance, each exceeding\n22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as\nreflected by its lower variance.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10340v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10340v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.465,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.414,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on multi-agent reinforcement learning and trust region policy optimization, specifically improving HATRPO through adaptive KL threshold allocation. It does not involve human feedback, such as training a reward model on human-ranked data or aligning AI models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses multi-agent systems with sequential policy updates to reduce gradient interference, which involves coordination among agents. However, it does not focus on distributed training techniques like parallel computing, data partitioning across nodes, or accelerating model training on multi-node systems; it is primarily about algorithmic improvements in MARL.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10345",
      "title": "Welfare-Centric Clustering",
      "authors": [
        "Claire Jie Zhang",
        "Seyed A. Esmaeili",
        "Jamie Morgenstern"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.DS (Data Structures and Algorithms)"
      ],
      "abstract": "Fair clustering has traditionally focused on ensuring equitable group\nrepresentation or equalizing group-specific clustering costs. However,\nDickerson et al. (2025) recently showed that these fairness notions may yield\nundesirable or unintuitive clustering outcomes and advocated for a\nwelfare-centric clustering approach that models the utilities of the groups. In\nthis work, we model group utilities based on both distances and proportional\nrepresentation and formalize two optimization objectives based on\nwelfare-centric clustering: the Rawlsian (Egalitarian) objective and the\nUtilitarian objective. We introduce novel algorithms for both objectives and\nprove theoretical guarantees for them. Empirical evaluations on multiple\nreal-world datasets demonstrate that our methods significantly outperform\nexisting fair clustering baselines.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10345v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10345v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.274,
      "distributed_training_score": 0.323,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10351",
      "title": "Glo-DMU: A Deep Morphometry Framework of Ultrastructural\n  Characterization in Glomerular Electron Microscopic Images",
      "authors": [
        "Zhentai Zhang",
        "Danyi Weng",
        "Guibin Zhang",
        "Xiang Chen",
        "Kaixing Long",
        "Jian Geng",
        "Yanmeng Lu",
        "Lei Zhang",
        "Zhitao Zhou",
        "Lei Cao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Complex and diverse ultrastructural features can indicate the type,\nprogression, and prognosis of kidney diseases. Recently, computational\npathology combined with deep learning methods has shown tremendous potential in\nadvancing automatic morphological analysis of glomerular ultrastructure.\nHowever, current research predominantly focuses on the recognition of\nindividual ultrastructure, which makes it challenging to meet practical\ndiagnostic needs. In this study, we propose the glomerular morphometry\nframework of ultrastructural characterization (Glo-DMU), which is grounded on\nthree deep models: the ultrastructure segmentation model, the glomerular\nfiltration barrier region classification model, and the electron-dense deposits\ndetection model. Following the conventional protocol of renal biopsy diagnosis,\nthis framework simultaneously quantifies the three most widely used\nultrastructural features: the thickness of glomerular basement membrane, the\ndegree of foot process effacement, and the location of electron-dense deposits.\nWe evaluated the 115 patients with 9 renal pathological types in real-world\ndiagnostic scenarios, demonstrating good consistency between automatic\nquantification results and morphological descriptions in the pathological\nreports. Glo-DMU possesses the characteristics of full automation, high\nprecision, and high throughput, quantifying multiple ultrastructural features\nsimultaneously, and providing an efficient tool for assisting renal\npathologists.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10351v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10351v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.249,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.325,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10356",
      "title": "Improving OCR for Historical Texts of Multiple Languages",
      "authors": [
        "Hylke Westerdijk",
        "Ben Blankenborg",
        "Khondoker Ittehadul Islam"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "This paper presents our methodology and findings from three tasks across\nOptical Character Recognition (OCR) and Document Layout Analysis using advanced\ndeep learning techniques. First, for the historical Hebrew fragments of the\nDead Sea Scrolls, we enhanced our dataset through extensive data augmentation\nand employed the Kraken and TrOCR models to improve character recognition. In\nour analysis of 16th to 18th-century meeting resolutions task, we utilized a\nConvolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for\nsemantic segmentation with a Bidirectional LSTM, incorporating confidence-based\npseudolabeling to refine our model. Finally, for modern English handwriting\nrecognition task, we applied a CRNN with a ResNet34 encoder, trained using the\nConnectionist Temporal Classification (CTC) loss function to effectively\ncapture sequential dependencies. This report offers valuable insights and\nsuggests potential directions for future research.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10356v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10356v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.375,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10358",
      "title": "What to Ask Next? Probing the Imaginative Reasoning of LLMs with\n  TurtleSoup Puzzles",
      "authors": [
        "Mengtao Zhou",
        "Sifan Wu",
        "Huan Zhang",
        "Qi Sima",
        "Bang Liu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We investigate the capacity of Large Language Models (LLMs) for imaginative\nreasoning--the proactive construction, testing, and revision of hypotheses in\ninformation-sparse environments. Existing benchmarks, often static or focused\non social deduction, fail to capture the dynamic, exploratory nature of this\nreasoning process. To address this gap, we introduce a comprehensive research\nframework based on the classic \"Turtle Soup\" game, integrating a benchmark, an\nagent, and an evaluation protocol. We present TurtleSoup-Bench, the first\nlarge-scale, bilingual, interactive benchmark for imaginative reasoning,\ncomprising 800 turtle soup puzzles sourced from both the Internet and expert\nauthors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'\nperformance in this setting. To evaluate reasoning quality, we develop a\nmulti-dimensional protocol measuring logical consistency, detail completion,\nand conclusion alignment. Experiments with leading LLMs reveal clear capability\nlimits, common failure patterns, and a significant performance gap compared to\nhumans. Our work offers new insights into LLMs' imaginative reasoning and\nestablishes a foundation for future research on exploratory agent behavior.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10358v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10358v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.541,
      "distributed_training_score": 0.316,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates LLMs' imaginative reasoning using iterative hypothesis testing in TurtleSoup puzzles, involving multi-step processes like question-asking and belief updating. However, it does not adapt or reference the iterative refinement process of diffusion models for logical tasks, nor does it treat reasoning paths as holistically corrected entities in a diffusion-like manner. Thus, it lacks any clear component of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10359",
      "title": "AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage\n  in STEM Imaging",
      "authors": [
        "Hao Wang",
        "Hongkui Zheng",
        "Kai He",
        "Abolfazl Razi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Scanning transmission electron microscopy (STEM) plays a critical role in\nmodern materials science, enabling direct imaging of atomic structures and\ntheir evolution under external interferences. However, interpreting\ntime-resolved STEM data remains challenging due to two entangled degradation\neffects: spatial drift caused by mechanical and thermal instabilities, and\nbeam-induced signal loss resulting from radiation damage. These factors distort\nboth geometry and intensity in complex, temporally correlated ways, making it\ndifficult for existing methods to explicitly separate their effects or model\nmaterial dynamics at atomic resolution. In this work, we present AtomDiffuser,\na time-aware degradation modeling framework that disentangles sample drift and\nradiometric attenuation by predicting an affine transformation and a spatially\nvarying decay map between any two STEM frames. Unlike traditional denoising or\nregistration pipelines, our method leverages degradation as a physically\nheuristic, temporally conditioned process, enabling interpretable structural\nevolutions across time. Trained on synthetic degradation processes,\nAtomDiffuser also generalizes well to real-world cryo-STEM data. It further\nsupports high-resolution degradation inference and drift alignment, offering\ntools for visualizing and quantifying degradation patterns that correlate with\nradiation-induced atomic instabilities.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10359v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10359v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.346,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces AtomDiffuser, a diffusion-variant model for STEM image degradation, which uses iterative refinement to predict transformations and decay maps. This shares the core mechanism of diffusion models (e.g., iterative denoising), but it applies it to physical imaging tasks like drift and damage modeling, not to multi-step logical reasoning or chain-of-thought processes for solving complex logical tasks. Thus, while there is a conceptual link to diffusion processes, it does not involve reasoning as specified in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10367",
      "title": "Contrast Sensitivity Function of Multimodal Vision-Language Models",
      "authors": [
        "Pablo Hernández-Cámara",
        "Alexandra Gomez-Villa",
        "Jose Manuel Jaén-Lorites",
        "Jorge Vila-Tomás",
        "Jesus Malo",
        "Valero Laparra"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Assessing the alignment of multimodal vision-language models~(VLMs) with\nhuman perception is essential to understand how they perceive low-level visual\nfeatures. A key characteristic of human vision is the contrast sensitivity\nfunction (CSF), which describes sensitivity to spatial frequency at\nlow-contrasts. Here, we introduce a novel behavioral psychophysics-inspired\nmethod to estimate the CSF of chat-based VLMs by directly prompting them to\njudge pattern visibility at different contrasts for each frequency. This\nmethodology is closer to the real experiments in psychophysics than the\npreviously reported. Using band-pass filtered noise images and a diverse set of\nprompts, we assess model responses across multiple architectures. We find that\nwhile some models approximate human-like CSF shape or magnitude, none fully\nreplicate both. Notably, prompt phrasing has a large effect on the responses,\nraising concerns about prompt stability. Our results provide a new framework\nfor probing visual sensitivity in multimodal models and reveal key gaps between\ntheir visual representations and human perception.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10367v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10367v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.304,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating the contrast sensitivity function in multimodal vision-language models using psychophysics-inspired prompting methods. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10370",
      "title": "eMamba: Efficient Acceleration Framework for Mamba Models in Edge\n  Computing",
      "authors": [
        "Jiyong Kim",
        "Jaeho Lee",
        "Jiahao Lin",
        "Alish Kanani",
        "Miao Sun",
        "Umit Y. Ogras",
        "Jaehyun Park"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "State Space Model (SSM)-based machine learning architectures have recently\ngained significant attention for processing sequential data. Mamba, a recent\nsequence-to-sequence SSM, offers competitive accuracy with superior\ncomputational efficiency compared to state-of-the-art transformer models. While\nthis advantage makes Mamba particularly promising for resource-constrained edge\ndevices, no hardware acceleration frameworks are currently optimized for\ndeploying it in such environments. This paper presents eMamba, a comprehensive\nend-to-end hardware acceleration framework explicitly designed for deploying\nMamba models on edge platforms. eMamba maximizes computational efficiency by\nreplacing complex normalization layers with lightweight hardware-aware\nalternatives and approximating expensive operations, such as SiLU activation\nand exponentiation, considering the target applications. Then, it performs an\napproximation-aware neural architecture search (NAS) to tune the learnable\nparameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10,\nand MARS, an open-source human pose estimation dataset, show eMamba achieves\ncomparable accuracy to state-of-the-art techniques using 1.63-19.9$\\times$\nfewer parameters. In addition, it generalizes well to large-scale natural\nlanguage tasks, demonstrating stable perplexity across varying sequence lengths\non the WikiText2 dataset. We also quantize and implement the entire eMamba\npipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm\ntechnology. Experimental results show 4.95-5.62$\\times$ lower latency and\n2.22-9.95$\\times$ higher throughput, with 4.77$\\times$ smaller area,\n9.84$\\times$ lower power, and 48.6$\\times$ lower energy consumption than\nbaseline solutions while maintaining competitive accuracy.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10370v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10370v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.456,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on hardware acceleration for Mamba models on edge devices, emphasizing inference efficiency, optimizations like quantization and approximations, and implementations on FPGA and ASIC. It does not address distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation during model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10382",
      "title": "Towards Spatially Consistent Image Generation: On Incorporating\n  Intrinsic Scene Properties into Diffusion Models",
      "authors": [
        "Hyundo Lee",
        "Suhyung Choi",
        "Byoung-Tak Zhang",
        "Inwoo Hwang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Image generation models trained on large datasets can synthesize high-quality\nimages but often produce spatially inconsistent and distorted images due to\nlimited information about the underlying structures and spatial layouts. In\nthis work, we leverage intrinsic scene properties (e.g., depth, segmentation\nmaps) that provide rich information about the underlying scene, unlike prior\napproaches that solely rely on image-text pairs or use intrinsics as\nconditional inputs. Our approach aims to co-generate both images and their\ncorresponding intrinsics, enabling the model to implicitly capture the\nunderlying scene structure and generate more spatially consistent and realistic\nimages. Specifically, we first extract rich intrinsic scene properties from a\nlarge image dataset with pre-trained estimators, eliminating the need for\nadditional scene information or explicit 3D representations. We then aggregate\nvarious intrinsic scene properties into a single latent variable using an\nautoencoder. Building upon pre-trained large-scale Latent Diffusion Models\n(LDMs), our method simultaneously denoises the image and intrinsic domains by\ncarefully sharing mutual information so that the image and intrinsic reflect\neach other without degrading image quality. Experimental results demonstrate\nthat our method corrects spatial inconsistencies and produces a more natural\nlayout of scenes while maintaining the fidelity and textual alignment of the\nbase model (e.g., Stable Diffusion).",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10382v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10382v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.608,
      "distributed_training_score": 0.328,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is enhancing diffusion models for image generation by incorporating intrinsic scene properties to improve spatial consistency, such as co-generating images and intrinsics. It does not involve adapting the diffusion process for multi-step logical reasoning, treating a 'Chain-of-Thought' as an entity, or solving complex logical tasks. Instead, it focuses on visual generation tasks, lacking any component for holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10383",
      "title": "Unlocking Robust Semantic Segmentation Performance via Label-only\n  Elastic Deformations against Implicit Label Noise",
      "authors": [
        "Yechan Kim",
        "Dongho Yoon",
        "Younkwan Lee",
        "Unse Fatima",
        "Hong Kook Kim",
        "Songjae Lee",
        "Sanga Park",
        "Jeong Ho Park",
        "Seonjong Kang",
        "Moongu Jeon"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While previous studies on image segmentation focus on handling severe (or\nexplicit) label noise, real-world datasets also exhibit subtle (or implicit)\nlabel imperfections. These arise from inherent challenges, such as ambiguous\nobject boundaries and annotator variability. Although not explicitly present,\nsuch mild and latent noise can still impair model performance. Typical data\naugmentation methods, which apply identical transformations to the image and\nits label, risk amplifying these subtle imperfections and limiting the model's\ngeneralization capacity. In this paper, we introduce NSegment+, a novel\naugmentation framework that decouples image and label transformations to\naddress such realistic noise for semantic segmentation. By introducing\ncontrolled elastic deformations only to segmentation labels while preserving\nthe original images, our method encourages models to focus on learning robust\nrepresentations of object structures despite minor label inconsistencies.\nExtensive experiments demonstrate that NSegment+ consistently improves\nperformance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in\naverage on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even\nwithout bells and whistles, highlighting the importance of addressing implicit\nlabel noise. These gains can be further amplified when combined with other\ntraining tricks, including CutMix and Label Smoothing.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10383v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10383v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.49,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.351,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on handling implicit label noise in semantic segmentation through a novel data augmentation framework (NSegment+), which involves perturbing labels to improve model robustness against subtle imperfections. This relates to weak supervision, as it deals with training on noisy or imprecise labels rather than perfect annotations. However, the paper primarily emphasizes augmentation techniques rather than programmatically generating labels from high-level sources, making it moderately relevant rather than a direct match.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces NSegment+, a novel data augmentation framework designed to enhance semantic segmentation by addressing implicit label noise, such as ambiguous boundaries and annotator variability, through elastic deformations applied only to labels while keeping images unchanged. The methodology involves stochastic per-sample deformations with scale-aware suppression to protect small objects, and experiments on datasets like Vaihingen, LoveDA, Cityscapes, and PASCAL VOC demonstrate significant mIoU improvements of up to +3.39, with further gains when combined with techniques like CutMix and Label Smoothing, underscoring the importance of robust handling of subtle label imperfections.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative approach by decoupling image and label transformations and applying elastic deformations specifically to labels to combat implicit noise, marking a significant advancement in semantic segmentation techniques. This generalization of elastic deformation to general tasks and focus on previously overlooked implicit noise sets it apart from existing methods.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a broad range of future research and applications in computer vision by improving model robustness against real-world label imperfections, as evidenced by consistent performance gains across diverse benchmarks. Its compatibility with other techniques suggests it could lead to widespread adoption in semantic segmentation pipelines.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, practical contribution to handling implicit label noise in semantic segmentation, making it essential for researchers working in computer vision to understand and potentially integrate into their work. While not revolutionary, its demonstrated effectiveness and innovations warrant attention.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8a2a4788d46b24eb8c8b0b5d171bf9c82142221f",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 6,
      "average_h_index": 1.6,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yechan Kim",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2119254169"
        },
        {
          "name": "Dongho Yoon",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2344138793"
        },
        {
          "name": "Younkwan Lee",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2109372904"
        },
        {
          "name": "Unse Fatima",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2142329657"
        },
        {
          "name": "Hong Kook Kim",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2287919981"
        },
        {
          "name": "Songjae Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375950900"
        },
        {
          "name": "Sanga Park",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376036647"
        },
        {
          "name": "Jeong Ho Park",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376082985"
        },
        {
          "name": "Seonjong Kang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375913277"
        },
        {
          "name": "Moongu Jeon",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2267645045"
        }
      ]
    },
    {
      "id": "2508.10391",
      "title": "LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and\n  Hierarchical Retrieval",
      "authors": [
        "Yaoze Zhang",
        "Rong Wu",
        "Pinlong Cai",
        "Xiaoman Wang",
        "Guohang Yan",
        "Song Mao",
        "Ding Wang",
        "Botian Shi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large\nLanguage Models by leveraging external knowledge, whereas the effectiveness is\noften compromised by the retrieval of contextually flawed or incomplete\ninformation. To address this, knowledge graph-based RAG methods have evolved\ntowards hierarchical structures, organizing knowledge into multi-level\nsummaries. However, these approaches still suffer from two critical,\nunaddressed challenges: high-level conceptual summaries exist as disconnected\n``semantic islands'', lacking the explicit relations needed for cross-community\nreasoning; and the retrieval process itself remains structurally unaware, often\ndegenerating into an inefficient flat search that fails to exploit the graph's\nrich topology. To overcome these limitations, we introduce LeanRAG, a framework\nthat features a deeply collaborative design combining knowledge aggregation and\nretrieval strategies. LeanRAG first employs a novel semantic aggregation\nalgorithm that forms entity clusters and constructs new explicit relations\namong aggregation-level summaries, creating a fully navigable semantic network.\nThen, a bottom-up, structure-guided retrieval strategy anchors queries to the\nmost relevant fine-grained entities and then systematically traverses the\ngraph's semantic pathways to gather concise yet contextually comprehensive\nevidence sets. The LeanRAG can mitigate the substantial overhead associated\nwith path retrieval on graphs and minimizes redundant information retrieval.\nExtensive experiments on four challenging QA benchmarks with different domains\ndemonstrate that LeanRAG significantly outperforming existing methods in\nresponse quality while reducing 46\\% retrieval redundancy. Code is available\nat: https://github.com/RaZzzyz/LeanRAG",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10391v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10391v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.351,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving Retrieval-Augmented Generation (RAG) using knowledge graphs, semantic aggregation, and hierarchical retrieval, with no mention of training models with human feedback, reward models, or reinforcement learning for alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a framework for RAG with hierarchical structures and retrieval strategies, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10397",
      "title": "PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce\n  Driver Distraction Detection",
      "authors": [
        "Haibin Sun",
        "Xinghui Song"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Driver distraction detection is essential for improving traffic safety and\nreducing road accidents. However, existing models often suffer from degraded\ngeneralization when deployed in real-world scenarios. This limitation primarily\narises from the few-shot learning challenge caused by the high cost of data\nannotation in practical environments, as well as the substantial domain shift\nbetween training datasets and target deployment conditions. To address these\nissues, we propose a Pose-driven Quality-controlled Data Augmentation Framework\n(PQ-DAF) that leverages a vision-language model for sample filtering to\ncost-effectively expand training data and enhance cross-domain robustness.\nSpecifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to\naccurately capture key driver pose features and synthesize diverse training\nexamples. A sample quality assessment module, built upon the CogVLM\nvision-language model, is then introduced to filter out low-quality synthetic\nsamples based on a confidence threshold, ensuring the reliability of the\naugmented dataset. Extensive experiments demonstrate that PQ-DAF substantially\nimproves performance in few-shot driver distraction detection, achieving\nsignificant gains in model generalization under data-scarce conditions.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10397v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10397v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.464,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.421,
      "datasets_score": 0.437,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on data augmentation using diffusion models and vision-language models for driver distraction detection, with no mention of reinforcement learning, human feedback, or training a reward model based on human preferences.",
      "weak_supervision_justification": "The paper employs programmatically generated synthetic data via diffusion models and filters it using a vision-language model, which aligns with weak supervision by reducing reliance on hand-labeled data. However, it does not explicitly address noisy label generation or traditional weak supervision techniques as its core focus.",
      "diffusion_reasoning_justification": "The paper uses diffusion models for generating synthetic images based on pose features, but it does not involve multi-step logical reasoning or treating a 'Chain-of-Thought' as an entity for iterative refinement.",
      "distributed_training_justification": "The paper does not discuss parallel computing, multi-node training, or partitioning data/computation across processors; it centers on data augmentation techniques for few-shot learning.",
      "datasets_justification": "The paper involves augmenting existing datasets for experiments and improving data scarcity, but its main contribution is the augmentation framework, not the creation, analysis, benchmarking, or curation of datasets.",
      "llm_score_status": "completed",
      "summary": "The paper introduces PQ-DAF, a framework aimed at improving driver distraction detection in data-scarce environments by generating high-quality synthetic data. It utilizes DWpose to extract driver poses, Progressive Conditional Diffusion Models to synthesize diverse images, and a CogVLM-based module to filter low-quality samples, with experiments showing significant enhancements in model generalization and performance under few-shot conditions.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like diffusion models and vision-language models for pose-driven data augmentation, offering a notable improvement in addressing few-shot learning for driver distraction detection. However, it does not introduce a entirely new problem or architecture, building instead on established methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision for automotive safety, as it addresses practical challenges like data scarcity and domain shift. While its influence may be limited to specific applications in driver monitoring, it could contribute to broader advancements in data augmentation techniques.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with practical implications for AI in traffic safety, making it worth reading for researchers in computer vision and artificial intelligence. However, it is not essential for those outside the specific domain of driver distraction detection.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/513583a9fdc2c73e33ed8d039c153683923b2d7d",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Haibin Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376131550"
        },
        {
          "name": "Xinghui Song",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375952398"
        }
      ]
    },
    {
      "id": "2508.10404",
      "title": "Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text\n  Generation",
      "authors": [
        "Huizhen Shu",
        "Xuying Li",
        "Qirui Wang",
        "Yuji Kosuga",
        "Mengqiu Tian",
        "Zhuo Li"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the rapid proliferation of Natural Language Processing (NLP), especially\nLarge Language Models (LLMs), generating adversarial examples to jailbreak LLMs\nremains a key challenge for understanding model vulnerabilities and improving\nrobustness. In this context, we propose a new black-box attack method that\nleverages the interpretability of large models. We introduce the Sparse Feature\nPerturbation Framework (SFPF), a novel approach for adversarial text generation\nthat utilizes sparse autoencoders to identify and manipulate critical features\nin text. After using the SAE model to reconstruct hidden layer representations,\nwe perform feature clustering on the successfully attacked texts to identify\nfeatures with higher activations. These highly activated features are then\nperturbed to generate new adversarial texts. This selective perturbation\npreserves the malicious intent while amplifying safety signals, thereby\nincreasing their potential to evade existing defenses. Our method enables a new\nred-teaming strategy that balances adversarial effectiveness with safety\nalignment. Experimental results demonstrate that adversarial texts generated by\nSFPF can bypass state-of-the-art defense mechanisms, revealing persistent\nvulnerabilities in current NLP systems.However, the method's effectiveness\nvaries across prompts and layers, and its generalizability to other\narchitectures and larger models remains to be validated.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10404v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10404v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.36,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using sparse autoencoders for generating adversarial text by identifying and perturbing features in NLP models, with no mention of diffusion models, iterative refinement processes, or adapting diffusion for logical reasoning tasks. It deals with adversarial attacks and feature manipulation, which are unrelated to the topic's emphasis on holistic Chain-of-Thought correction in diffusion-based systems.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10407",
      "title": "Translation of Text Embedding via Delta Vector to Suppress Strongly\n  Entangled Content in Text-to-Image Diffusion Models",
      "authors": [
        "Eunseo Koh",
        "Seunghoo Hong",
        "Tae-Young Kim",
        "Simon S. Woo",
        "Jae-Pil Heo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-Image (T2I) diffusion models have made significant progress in\ngenerating diverse high-quality images from textual prompts. However, these\nmodels still face challenges in suppressing content that is strongly entangled\nwith specific words. For example, when generating an image of \"Charlie\nChaplin\", a \"mustache\" consistently appears even if explicitly instructed not\nto include it, as the concept of \"mustache\" is strongly entangled with \"Charlie\nChaplin\". To address this issue, we propose a novel approach to directly\nsuppress such entangled content within the text embedding space of diffusion\nmodels. Our method introduces a delta vector that modifies the text embedding\nto weaken the influence of undesired content in the generated image, and we\nfurther demonstrate that this delta vector can be easily obtained through a\nzero-shot approach. Furthermore, we propose a Selective Suppression with Delta\nVector (SSDV) method to adapt delta vector into the cross-attention mechanism,\nenabling more effective suppression of unwanted content in regions where it\nwould otherwise be generated. Additionally, we enabled more precise suppression\nin personalized T2I models by optimizing delta vector, which previous baselines\nwere unable to achieve. Extensive experimental results demonstrate that our\napproach significantly outperforms existing methods, both in terms of\nquantitative and qualitative metrics.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10407v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10407v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.515,
      "distributed_training_score": 0.339,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves modifying text embeddings in text-to-image diffusion models to suppress unwanted content, focusing on image generation techniques. It does not involve adapting the diffusion process for multi-step logical reasoning, chain-of-thought processing, or solving complex logical tasks, which are core to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10409",
      "title": "AnalogSeeker: An Open-source Foundation Language Model for Analog\n  Circuit Design",
      "authors": [
        "Zihao Chen",
        "Ji Zhuang",
        "Jinyi Shen",
        "Xiaoyue Ke",
        "Xinyi Yang",
        "Mingjie Zhou",
        "Zhuoyao Du",
        "Xu Yan",
        "Zhouyang Wu",
        "Zhenyu Xu",
        "Jiangli Huang",
        "Li Shang",
        "Xuan Zeng",
        "Fan Yang"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we propose AnalogSeeker, an effort toward an open-source\nfoundation language model for analog circuit design, with the aim of\nintegrating domain knowledge and giving design assistance. To overcome the\nscarcity of data in this field, we employ a corpus collection strategy based on\nthe domain knowledge framework of analog circuits. High-quality, accessible\ntextbooks across relevant subfields are systematically curated and cleaned into\na textual domain corpus. To address the complexity of knowledge of analog\ncircuits, we introduce a granular domain knowledge distillation method. Raw,\nunlabeled domain corpus is decomposed into typical, granular learning nodes,\nwhere a multi-agent framework distills implicit knowledge embedded in\nunstructured text into question-answer data pairs with detailed reasoning\nprocesses, yielding a fine-grained, learnable dataset for fine-tuning. To\naddress the unexplored challenges in training analog circuit foundation models,\nwe explore and share our training methods through both theoretical analysis and\nexperimental validation. We finally establish a fine-tuning-centric training\nparadigm, customizing and implementing a neighborhood self-constrained\nsupervised fine-tuning algorithm. This approach enhances training outcomes by\nconstraining the perturbation magnitude between the model's output\ndistributions before and after training. In practice, we train the\nQwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04%\naccuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark,\nwith a 15.67% point improvement over the original model and is competitive with\nmainstream commercial models. Furthermore, AnalogSeeker also shows\neffectiveness in the downstream operational amplifier design task. AnalogSeeker\nis open-sourced at https://huggingface.co/analogllm/analogseeker for research\nuse.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10409v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10409v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.38,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces AnalogSeeker, a language model for analog circuit design, focusing on corpus collection, knowledge distillation via a multi-agent framework, and supervised fine-tuning with a neighborhood self-constrained algorithm. While it involves multi-step reasoning in knowledge extraction (e.g., QTSA format), there is no mention of diffusion models, iterative refinement processes, or adapting diffusion for logical tasks. Thus, it does not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10411",
      "title": "SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for\n  3D Lane Detection",
      "authors": [
        "Chaesong Park",
        "Eunbin Seo",
        "Jihyeon Hwang",
        "Jongwoo Lim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we introduce SC-Lane, a novel slope-aware and temporally\nconsistent heightmap estimation framework for 3D lane detection. Unlike\nprevious approaches that rely on fixed slope anchors, SC-Lane adaptively\ndetermines the fusion of slope-specific height features, improving robustness\nto diverse road geometries. To achieve this, we propose a Slope-Aware Adaptive\nFeature module that dynamically predicts the appropriate weights from image\ncues for integrating multi-slope representations into a unified heightmap.\nAdditionally, a Height Consistency Module enforces temporal coherence, ensuring\nstable and accurate height estimation across consecutive frames, which is\ncrucial for real-world driving scenarios. To evaluate the effectiveness of\nSC-Lane, we employ three standardized metrics-Mean Absolute Error(MAE), Root\nMean Squared Error (RMSE), and threshold-based accuracy-which, although common\nin surface and depth estimation, have been underutilized for road height\nassessment. Using the LiDAR-derived heightmap dataset introduced in prior work\n[20], we benchmark our method under these metrics, thereby establishing a\nrigorous standard for future comparisons. Extensive experiments on the OpenLane\nbenchmark demonstrate that SC-Lane significantly improves both height\nestimation and 3D lane detection, achieving state-of-the-art performance with\nan F-score of 64.3%, outperforming existing methods by a notable margin. For\ndetailed results and a demonstration video, please refer to our project\npage:https://parkchaesong.github.io/sclane/",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10411v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10411v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.303,
      "distributed_training_score": 0.319,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10414",
      "title": "MCP2OSC: Parametric Control by Natural Language",
      "authors": [
        "Yuan-Yi Fan"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Text prompts enable intuitive content creation but may fall short in\nachieving high precision for intricate tasks; knob or slider controls offer\nprecise adjustments at the cost of increased complexity. To address the gap\nbetween knobs and prompts, a new MCP (Model Context Protocol) server and a\nunique set of prompt design criteria are presented to enable exploring\nparametric OSC (OpenSoundControl) control by natural language prompts.\nDemonstrated by 14 practical QA examples with best practices and the\ngeneralized prompt templates, this study finds Claude integrated with the\nMCP2OSC server effective in generating OSC messages by natural language,\ninterpreting, searching, and visualizing OSC messages, validating and debugging\nOSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine\ncollaboration by leveraging LLM (Large Language Model) to handle intricate OSC\ndevelopment tasks, and by empowering human creativity with an intuitive\nlanguage interface featuring flexible precision controls: a prompt-based OSC\ntool. This study provides a novel perspective on the creative MCP application\nat the network protocol level by utilizing LLM's strength in directly\nprocessing and generating human-readable OSC messages. The results suggest its\npotential for a LLM-based universal control mechanism for multimedia devices.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10414v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10414v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.289,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10416",
      "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action\n  Navigation Model",
      "authors": [
        "Zhuoyuan Yu",
        "Yuxing Long",
        "Zihan Yang",
        "Chengyan Zeng",
        "Hongwei Fan",
        "Jiyao Zhang",
        "Hao Dong"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing vision-and-language navigation models often deviate from the correct\ntrajectory when executing instructions. However, these models lack effective\nerror correction capability, hindering their recovery from errors. To address\nthis challenge, we propose Self-correction Flywheel, a novel post-training\nparadigm. Instead of considering the model's error trajectories on the training\nset as a drawback, our paradigm emphasizes their significance as a valuable\ndata source. We have developed a method to identify deviations in these error\ntrajectories and devised innovative techniques to automatically generate\nself-correction data for perception and action. These self-correction data\nserve as fuel to power the model's continued training. The brilliance of our\nparadigm is revealed when we re-evaluate the model on the training set,\nuncovering new error trajectories. At this time, the self-correction flywheel\nbegins to spin. Through multiple flywheel iterations, we progressively enhance\nour monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE\nand RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success\nrates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%\nand 16.4%. Real robot tests in various indoor and outdoor environments\ndemonstrate \\method's superior capability of error correction, dynamic obstacle\navoidance, and long instruction following.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10416v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10416v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.311,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10419",
      "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning",
      "authors": [
        "Juyuan Wang",
        "Rongchen Zhao",
        "Wei Wei",
        "Yufeng Wang",
        "Mo Yu",
        "Jie Zhou",
        "Jin Xu",
        "Liyan Xu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10419v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10419v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.532,
      "distributed_training_score": 0.299,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces ComoRAG, a cognitive-inspired RAG framework that uses iterative reasoning cycles for long narrative comprehension, drawing from human brain processes like metacognitive regulation. However, it does not involve diffusion models, nor does it adapt the iterative refinement process of diffusion for multi-step logical reasoning. The core mechanisms focus on retrieval, memory consolidation, and probing queries, with no reference to treating Chain-of-Thought as a holistic entity via diffusion, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10423",
      "title": "MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for\n  Single Humanoid Robot Locomotion",
      "authors": [
        "Qi Liu",
        "Xiaopeng Zhang",
        "Mingshan Tan",
        "Shuaikang Ma",
        "Jinliang Ding",
        "Yanjie Li"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "This paper proposes a novel method to enhance locomotion for a single\nhumanoid robot through cooperative-heterogeneous multi-agent deep reinforcement\nlearning (MARL). While most existing methods typically employ single-agent\nreinforcement learning algorithms for a single humanoid robot or MARL\nalgorithms for multi-robot system tasks, we propose a distinct paradigm:\napplying cooperative-heterogeneous MARL to optimize locomotion for a single\nhumanoid robot. The proposed method, multi-agent reinforcement learning for\nsingle humanoid locomotion (MASH), treats each limb (legs and arms) as an\nindependent agent that explores the robot's action space while sharing a global\ncritic for cooperative learning. Experiments demonstrate that MASH accelerates\ntraining convergence and improves whole-body cooperation ability, outperforming\nconventional single-agent reinforcement learning methods. This work advances\nthe integration of MARL into single-humanoid-robot control, offering new\ninsights into efficient locomotion strategies.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10423v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10423v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.318,
      "distributed_training_score": 0.332,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a cooperative-heterogeneous multi-agent reinforcement learning method (MASH) for enhancing locomotion in a single humanoid robot by treating limbs as agents. It does not involve human feedback, such as training a reward model on human-ranked data or fine-tuning with human preferences, which are core to RLHF. Instead, it focuses on standard RL and MARL techniques without any human involvement in the learning process.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10424",
      "title": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer",
      "authors": [
        "Shanyuan Liu",
        "Jian Zhu",
        "Junda Lu",
        "Yue Gong",
        "Liuzhuozheng Li",
        "Bo Cheng",
        "Yuhang Ma",
        "Liebucha Wu",
        "Xiaoyu Wu",
        "Dawei Leng",
        "Yuhui Yin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10424v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10424v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.331,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on NanoControl, a framework for efficient controllable text-to-image generation using Diffusion Transformers, emphasizing parameter efficiency and control mechanisms like LoRA-style modules. It does not involve adapting diffusion processes for multi-step logical reasoning, Chain-of-Thought, or solving complex logical tasks; instead, it is centered on image synthesis and control, with no components for reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10425",
      "title": "HiRef: Leveraging Hierarchical Ontology and Network Refinement for\n  Robust Medication Recommendation",
      "authors": [
        "Yan Ting Chok",
        "Soyon Park",
        "Seungheun Baek",
        "Hajung Kim",
        "Junhyun Lee",
        "Jaewoo Kang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Medication recommendation is a crucial task for assisting physicians in\nmaking timely decisions from longitudinal patient medical records. However,\nreal-world EHR data present significant challenges due to the presence of\nrarely observed medical entities and incomplete records that may not fully\ncapture the clinical ground truth. While data-driven models trained on\nlongitudinal Electronic Health Records often achieve strong empirical\nperformance, they struggle to generalize under missing or novel conditions,\nlargely due to their reliance on observed co-occurrence patterns. To address\nthese issues, we propose Hierarchical Ontology and Network Refinement for\nRobust Medication Recommendation (HiRef), a unified framework that combines two\ncomplementary structures: (i) the hierarchical semantics encoded in curated\nmedical ontologies, and (ii) refined co-occurrence patterns derived from\nreal-world EHRs. We embed ontology entities in hyperbolic space, which\nnaturally captures tree-like relationships and enables knowledge transfer\nthrough shared ancestors, thereby improving generalizability to unseen codes.\nTo further improve robustness, we introduce a prior-guided sparse\nregularization scheme that refines the EHR co-occurrence graph by suppressing\nspurious edges while preserving clinically meaningful associations. Our model\nachieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and\nmaintains high accuracy under simulated unseen-code settings. Extensive\nexperiments with comprehensive ablation studies demonstrate HiRef's resilience\nto unseen medical codes, supported by in-depth analyses of the learned\nsparsified graph structure and medical code embeddings.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10425v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10425v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.296,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a framework for medication recommendation using hierarchical ontologies and EHR data refinement, emphasizing generalizability and robustness through embeddings and graph regularization. It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI models with human preferences, which are core to RLHF. Thus, there is no relevance to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10427",
      "title": "STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal\n  Reasoning in Urban Driving Scenes",
      "authors": [
        "Keishi Ishihara",
        "Kento Sasaki",
        "Tsubasa Takahashi",
        "Daiki Shiono",
        "Yu Yamaguchi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-Language Models (VLMs) have been applied to autonomous driving to\nsupport decision-making in complex real-world scenarios. However, their\ntraining on static, web-sourced image-text pairs fundamentally limits the\nprecise spatiotemporal reasoning required to understand and predict dynamic\ntraffic scenes. We address this critical gap with STRIDE-QA, a large-scale\nvisual question answering (VQA) dataset for physically grounded reasoning from\nan ego-centric perspective. Constructed from 100 hours of multi-sensor driving\ndata in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the\nlargest VQA dataset for spatiotemporal reasoning in urban driving, offering 16\nmillion QA pairs over 285K frames. Grounded by dense, automatically generated\nannotations including 3D bounding boxes, segmentation masks, and multi-object\ntracks, the dataset uniquely supports both object-centric and ego-centric\nreasoning through three novel QA tasks that require spatial localization and\ntemporal prediction. Our benchmarks demonstrate that existing VLMs struggle\nsignificantly, achieving near-zero scores on prediction consistency. In\ncontrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,\nachieving 55% success in spatial localization and 28% consistency in future\nmotion prediction, compared to near-zero scores from general-purpose VLMs.\nTherefore, STRIDE-QA establishes a comprehensive foundation for developing more\nreliable VLMs for safety-critical autonomous systems.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10427v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10427v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.34,
      "datasets_score": 0.453,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a new VQA dataset for spatiotemporal reasoning in autonomous driving and benchmarking VLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. Thus, it does not relate to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution is the creation, introduction, and benchmarking of a new large-scale dataset (STRIDE-QA) for VQA in autonomous driving, including details on data curation from multi-sensor sources and evaluation metrics, which directly aligns with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "STRIDE-QA introduces a large-scale visual question answering dataset derived from 100 hours of multi-sensor driving data in Tokyo, featuring 16 million QA pairs across 285,000 frames to enhance spatiotemporal reasoning for Vision-Language Models in autonomous driving. The dataset employs automated annotations like 3D bounding boxes and multi-object tracking to support three novel tasks—object-centric spatial QA, ego-centric spatial QA, and ego-centric spatiotemporal QA—demonstrating that fine-tuned VLMs achieve significant improvements in spatial localization and motion prediction compared to general-purpose models, thus addressing the limitations of static training data.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new large-scale dataset and novel QA tasks specifically for spatiotemporal reasoning in urban driving scenes, significantly advancing the state-of-the-art in VLM applications for autonomous systems.",
      "impact_score": "High",
      "impact_justification": "This work could broadly influence future research and development in autonomous driving by providing a robust foundation for training VLMs on dynamic, real-world scenarios, potentially leading to safer and more reliable commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative dataset and tasks that are valuable for advancing VLM research in autonomous driving, making it essential for specialists in the field but not universally critical.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6ee713cefdb770d9c038b9101087e579920f8627",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 3,
      "average_h_index": 1.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Keishi Ishihara",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/1576981539"
        },
        {
          "name": "Kento Sasaki",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2296026459"
        },
        {
          "name": "Tsubasa Takahashi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2334530296"
        },
        {
          "name": "Daiki Shiono",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375902391"
        },
        {
          "name": "Yu Yamaguchi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2334478867"
        }
      ]
    },
    {
      "id": "2508.10429",
      "title": "MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with\n  Verifiable Provenance",
      "authors": [
        "Yi Dong",
        "Yusuke Muraoka",
        "Scott Shi",
        "Yi Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present MM-Food-100K, a public 100,000-sample multimodal food intelligence\ndataset with verifiable provenance. It is a curated approximately 10% open\nsubset of an original 1.2 million, quality-accepted corpus of food images\nannotated for a wide range of information (such as dish name, region of\ncreation). The corpus was collected over six weeks from over 87,000\ncontributors using the Codatta contribution model, which combines community\nsourcing with configurable AI-assisted quality checks; each submission is\nlinked to a wallet address in a secure off-chain ledger for traceability, with\na full on-chain protocol on the roadmap. We describe the schema, pipeline, and\nQA, and validate utility by fine-tuning large vision-language models (ChatGPT\n5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning\nyields consistent gains over out-of-box baselines across standard metrics; we\nreport results primarily on the MM-Food-100K subset. We release MM-Food-100K\nfor publicly free access and retain approximately 90% for potential commercial\naccess with revenue sharing to contributors.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10429v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10429v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.353,
      "datasets_score": 0.511,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of MM-Food-100K, a new multimodal dataset for AI applications in food intelligence, which aligns directly with dataset creation. It details curation methodologies, including community sourcing, AI-assisted quality checks, and blockchain-based provenance, as well as benchmark evaluations through fine-tuning vision-language models and comparing performance metrics. This comprehensively covers aspects of creating, analyzing, and evaluating datasets for machine learning, making it highly relevant to the topic.",
      "llm_score_status": "completed",
      "summary": "The paper introduces MM-Food-100K, a publicly available dataset of 100,000 multimodal samples consisting of food images with annotations, derived from a larger 1.2 million corpus collected via a blockchain-based community sourcing model called Codatta, which ensures verifiable provenance through wallet-linked submissions and AI-assisted quality checks. The authors demonstrate the dataset's utility by fine-tuning large vision-language models like ChatGPT variants for tasks such as image-based nutrition prediction, achieving consistent performance improvements over baseline models, while retaining 90% of the corpus for potential commercial use with revenue sharing for contributors.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining community sourcing with blockchain for verifiable provenance in dataset creation, offering a new way to ensure data integrity and attribution in AI datasets, though it builds on existing concepts of crowdsourced data collection.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and utilized in subfields like computer vision and AI ethics, as it provides a valuable resource for training models on food-related tasks with enhanced provenance, potentially influencing future research on data sourcing and multimodal learning.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper represents a strong contribution by introducing a high-quality dataset with innovative features for verifiable data, making it essential for researchers focused on multimodal AI and data integrity, though not universally groundbreaking.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/626d141b2a40dbcc8684b7c53127dab80e2df24a",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yi Dong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376010773"
        },
        {
          "name": "Yusuke Muraoka",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375900999"
        },
        {
          "name": "Scott Shi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376063239"
        },
        {
          "name": "Yi Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376140904"
        }
      ]
    },
    {
      "id": "2508.10432",
      "title": "CRISP: Contrastive Residual Injection and Semantic Prompting for\n  Continual Video Instance Segmentation",
      "authors": [
        "Baichen Liu",
        "Qi Lyu",
        "Xudong Wang",
        "Jiahua Dong",
        "Lianqing Liu",
        "Zhi Han"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Continual video instance segmentation demands both the plasticity to absorb\nnew object categories and the stability to retain previously learned ones, all\nwhile preserving temporal consistency across frames. In this work, we introduce\nContrastive Residual Injection and Semantic Prompting (CRISP), an earlier\nattempt tailored to address the instance-wise, category-wise, and task-wise\nconfusion in continual video instance segmentation. For instance-wise learning,\nwe model instance tracking and construct instance correlation loss, which\nemphasizes the correlation with the prior query space while strengthening the\nspecificity of the current task query. For category-wise learning, we build an\nadaptive residual semantic prompt (ARSP) learning framework, which constructs a\nlearnable semantic residual prompt pool generated by category text and uses an\nadjustive query-prompt matching mechanism to build a mapping relationship\nbetween the query of the current task and the semantic residual prompt.\nMeanwhile, a semantic consistency loss based on the contrastive learning is\nintroduced to maintain semantic coherence between object queries and residual\nprompts during incremental training. For task-wise learning, to ensure the\ncorrelation at the inter-task level within the query space, we introduce a\nconcise yet powerful initialization strategy for incremental prompts. Extensive\nexperiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that\nCRISP significantly outperforms existing continual segmentation methods in the\nlong-term continual video instance segmentation task, avoiding catastrophic\nforgetting and effectively improving segmentation and classification\nperformance. The code is available at https://github.com/01upup10/CRISP.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10432v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10432v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.327,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10433",
      "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual\n  Mathematical Reasoning",
      "authors": [
        "Runqi Qiao",
        "Qiuna Tan",
        "Peiqing Yang",
        "Yanzi Wang",
        "Xiaowan Wang",
        "Enhui Wan",
        "Sitong Zhou",
        "Guanting Dong",
        "Yuchen Zeng",
        "Yida Xu",
        "Jie Wang",
        "Chong Sun",
        "Chen Li",
        "Honggang Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across various tasks, but still struggle with complex mathematical\nreasoning. Existing research primarily focuses on dataset construction and\nmethod optimization, often overlooking two critical aspects: comprehensive\nknowledge-driven design and model-centric data space modeling. In this paper,\nwe introduce We-Math 2.0, a unified system that integrates a structured\nmathematical knowledge system, model-centric data space modeling, and a\nreinforcement learning (RL)-based training paradigm to comprehensively enhance\nthe mathematical reasoning abilities of MLLMs. The key contributions of We-Math\n2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level\nhierarchical system encompassing 491 knowledge points and 1,819 fundamental\nprinciples. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a\ndataset that ensures broad conceptual coverage and flexibility through dual\nexpansion. Additionally, we define a three-dimensional difficulty space and\ngenerate 7 progressive variants per problem to build MathBook-Pro, a\nchallenging dataset for robust training. (3) MathBook-RL: We propose a\ntwo-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the\nmodel with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive\nAlignment RL, leveraging average-reward learning and dynamic data scheduling to\nachieve progressive alignment across difficulty levels. (4) MathBookEval: We\nintroduce a comprehensive benchmark covering all 491 knowledge points with\ndiverse reasoning step distributions. Experimental results show that\nMathBook-RL performs competitively with existing baselines on four widely-used\nbenchmarks and achieves strong results on MathBookEval, suggesting promising\ngeneralization in mathematical reasoning.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10433v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10433v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.496,
      "distributed_training_score": 0.348,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves a reinforcement learning-based framework (MathBook-RL) for enhancing mathematical reasoning in MLLMs, including knowledge systems, datasets, and fine-tuning. It does not mention or adapt diffusion models, iterative refinement processes, or treat Chain-of-Thought as a holistically corrected entity over multiple steps. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10435",
      "title": "Unpacking the Implicit Norm Dynamics of Sharpness-Aware Minimization in\n  Tensorized Models",
      "authors": [
        "Tianxiao Cao",
        "Kyohei Atarashi",
        "Hisashi Kashima"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Sharpness-Aware Minimization (SAM) has been proven to be an effective\noptimization technique for improving generalization in overparameterized\nmodels. While prior works have explored the implicit regularization of SAM in\nsimple two-core scale-invariant settings, its behavior in more general\ntensorized or scale-invariant models remains underexplored. In this work, we\nleverage scale-invariance to analyze the norm dynamics of SAM in general\ntensorized models. We introduce the notion of \\emph{Norm Deviation} as a global\nmeasure of core norm imbalance, and derive its evolution under SAM using\ngradient flow analysis. We show that SAM's implicit control of Norm Deviation\nis governed by the covariance between core norms and their gradient magnitudes.\nMotivated by these findings, we propose a simple yet effective method,\n\\emph{Deviation-Aware Scaling (DAS)}, which explicitly mimics this\nregularization behavior by scaling core norms in a data-adaptive manner. Our\nexperiments across tensor completion, noisy training, model compression, and\nparameter-efficient fine-tuning confirm that DAS achieves competitive or\nimproved performance over SAM, while offering reduced computational overhead.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10435v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10435v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.416,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on analyzing Sharpness-Aware Minimization (SAM) and its effects on norm dynamics in tensorized models, introducing concepts like Norm Deviation and proposing Deviation-Aware Scaling (DAS). It deals with optimization techniques for improving generalization in neural networks, with experiments on tasks like tensor completion and model compression. There is no mention of distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors or nodes, making it unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10436",
      "title": "Alternating Approach-Putt Models for Multi-Stage Speech Enhancement",
      "authors": [
        "Iksoon Jeong",
        "Kyung-Joong Kim",
        "Kang-Hun Ahn"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Speech enhancement using artificial neural networks aims to remove noise from\nnoisy speech signals while preserving the speech content. However, speech\nenhancement networks often introduce distortions to the speech signal, referred\nto as artifacts, which can degrade audio quality. In this work, we propose a\npost-processing neural network designed to mitigate artifacts introduced by\nspeech enhancement models. Inspired by the analogy of making a `Putt' after an\n`Approach' in golf, we name our model PuttNet. We demonstrate that alternating\nbetween a speech enhancement model and the proposed Putt model leads to\nimproved speech quality, as measured by perceptual quality scores (PESQ),\nobjective intelligibility (STOI), and background noise intrusiveness (CBAK)\nscores. Furthermore, we illustrate with graphical analysis why this alternating\nApproach outperforms repeated application of either model alone.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10436v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10436v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.304,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a multi-stage speech enhancement model that avoids diffusion models, focusing on audio processing to reduce artifacts. While it references diffusion models in prior literature, it does not adapt iterative refinement processes for logical reasoning tasks or treat chains of thought as entities for correction. Thus, it does not align with the topic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10445",
      "title": "DOD-SA: Infrared-Visible Decoupled Object Detection with Single-Modality\n  Annotations",
      "authors": [
        "Hang Jin",
        "Chenqiang Gao",
        "Junjie Guo",
        "Fangcen Liu",
        "Kanghui Tian",
        "Qinyao Chang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Infrared-visible object detection has shown great potential in real-world\napplications, enabling robust all-day perception by leveraging the\ncomplementary information of infrared and visible images. However, existing\nmethods typically require dual-modality annotations to output detection results\nfor both modalities during prediction, which incurs high annotation costs. To\naddress this challenge, we propose a novel infrared-visible Decoupled Object\nDetection framework with Single-modality Annotations, called DOD-SA. The\narchitecture of DOD-SA is built upon a Single- and Dual-Modality Collaborative\nTeacher-Student Network (CoSD-TSNet), which consists of a single-modality\nbranch (SM-Branch) and a dual-modality decoupled branch (DMD-Branch). The\nteacher model generates pseudo-labels for the unlabeled modality,\nsimultaneously supporting the training of the student model. The collaborative\ndesign enables cross-modality knowledge transfer from the labeled modality to\nthe unlabeled modality, and facilitates effective SM-to-DMD branch supervision.\nTo further improve the decoupling ability of the model and the pseudo-label\nquality, we introduce a Progressive and Self-Tuning Training Strategy (PaST)\nthat trains the model in three stages: (1) pretraining SM-Branch, (2) guiding\nthe learning of DMD-Branch by SM-Branch, and (3) refining DMD-Branch. In\naddition, we design a Pseudo Label Assigner (PLA) to align and pair labels\nacross modalities, explicitly addressing modality misalignment during training.\nExtensive experiments on the DroneVehicle dataset demonstrate that our method\noutperforms state-of-the-art (SOTA).",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10445v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10445v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.363,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10449",
      "title": "SkeySpot: Automating Service Key Detection for Digital Electrical Layout\n  Plans in the Construction Industry",
      "authors": [
        "Dhruv Dosi",
        "Rohit Meena",
        "Param Rajpura",
        "Yogesh Kumar Meena"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Legacy floor plans, often preserved only as scanned documents, remain\nessential resources for architecture, urban planning, and facility management\nin the construction industry. However, the lack of machine-readable floor plans\nrender large-scale interpretation both time-consuming and error-prone.\nAutomated symbol spotting offers a scalable solution by enabling the\nidentification of service key symbols directly from floor plans, supporting\nworkflows such as cost estimation, infrastructure maintenance, and regulatory\ncompliance. This work introduces a labelled Digitised Electrical Layout Plans\n(DELP) dataset comprising 45 scanned electrical layout plans annotated with\n2,450 instances across 34 distinct service key classes. A systematic evaluation\nframework is proposed using pretrained object detection models for DELP\ndataset. Among the models benchmarked, YOLOv8 achieves the highest performance\nwith a mean Average Precision (mAP) of 82.5\\%. Using YOLOv8, we develop\nSkeySpot, a lightweight, open-source toolkit for real-time detection,\nclassification, and quantification of electrical symbols. SkeySpot produces\nstructured, standardised outputs that can be scaled up for interoperable\nbuilding information workflows, ultimately enabling compatibility across\ndownstream applications and regulatory platforms. By lowering dependency on\nproprietary CAD systems and reducing manual annotation effort, this approach\nmakes the digitisation of electrical layouts more accessible to small and\nmedium-sized enterprises (SMEs) in the construction industry, while supporting\nbroader goals of standardisation, interoperability, and sustainability in the\nbuilt environment.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10449v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10449v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.362,
      "datasets_score": 0.393,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10450",
      "title": "From Images to Perception: Emergence of Perceptual Properties by\n  Reconstructing Images",
      "authors": [
        "Pablo Hernández-Cámara",
        "Jesus Malo",
        "Valero Laparra"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "A number of scientists suggested that human visual perception may emerge from\nimage statistics, shaping efficient neural representations in early vision. In\nthis work, a bio-inspired architecture that can accommodate several known facts\nin the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for\ndifferent tasks related to image reconstruction: autoencoding, denoising,\ndeblurring, and sparsity regularization. Our results show that the encoder\nstage (V1-like layer) consistently exhibits the highest correlation with human\nperceptual judgments on image distortion despite not using perceptual\ninformation in the initialization or training. This alignment exhibits an\noptimum for moderate noise, blur and sparsity. These findings suggest that the\nvisual system may be tuned to remove those particular levels of distortion with\nthat level of sparsity and that biologically inspired models can learn\nperceptual metrics without human supervision.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10450v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10450v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.283,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses a bio-inspired architecture (PerceptNet) for image reconstruction tasks like autoencoding, denoising, and deblurring, aiming to align with human perceptual judgments. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning on a Chain-of-Thought. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10453",
      "title": "Trajectory-aware Shifted State Space Models for Online Video\n  Super-Resolution",
      "authors": [
        "Qiang Zhu",
        "Xiandong Meng",
        "Yuxian Jiang",
        "Fan Zhang",
        "David Bull",
        "Shuyuan Zhu",
        "Bing Zeng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Online video super-resolution (VSR) is an important technique for many\nreal-world video processing applications, which aims to restore the current\nhigh-resolution video frame based on temporally previous frames. Most of the\nexisting online VSR methods solely employ one neighboring previous frame to\nachieve temporal alignment, which limits long-range temporal modeling of\nvideos. Recently, state space models (SSMs) have been proposed with linear\ncomputational complexity and a global receptive field, which significantly\nimprove computational efficiency and performance. In this context, this paper\npresents a novel online VSR method based on Trajectory-aware Shifted SSMs\n(TS-Mamba), leveraging both long-term trajectory modeling and low-complexity\nMamba to achieve efficient spatio-temporal information aggregation.\nSpecifically, TS-Mamba first constructs the trajectories within a video to\nselect the most similar tokens from the previous frames. Then, a\nTrajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed\nshifted SSMs blocks is employed to aggregate the selected tokens. The shifted\nSSMs blocks are designed based on Hilbert scannings and corresponding shift\noperations to compensate for scanning losses and strengthen the spatial\ncontinuity of Mamba. Additionally, we propose a trajectory-aware loss function\nto supervise the trajectory generation, ensuring the accuracy of token\nselection when training our model. Extensive experiments on three widely used\nVSR test datasets demonstrate that compared with six online VSR benchmark\nmodels, our TS-Mamba achieves state-of-the-art performance in most cases and\nover 22.7\\% complexity reduction (in MACs). The source code for TS-Mamba will\nbe available at https://github.com.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10453v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10453v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.367,
      "datasets_score": 0.268,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10455",
      "title": "RealAC: A Domain-Agnostic Framework for Realistic and Actionable\n  Counterfactual Explanations",
      "authors": [
        "Asiful Arefeen",
        "Shovito Barua Soumma",
        "Hassan Ghasemzadeh"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Counterfactual explanations provide human-understandable reasoning for\nAI-made decisions by describing minimal changes to input features that would\nalter a model's prediction. To be truly useful in practice, such explanations\nmust be realistic and feasible -- they should respect both the underlying data\ndistribution and user-defined feasibility constraints. Existing approaches\noften enforce inter-feature dependencies through rigid, hand-crafted\nconstraints or domain-specific knowledge, which limits their generalizability\nand ability to capture complex, nonlinear relations inherent in data. Moreover,\nthey rarely accommodate user-specified preferences and suggest explanations\nthat are causally implausible or infeasible to act upon. We introduce RealAC, a\ndomain-agnostic framework for generating realistic and actionable\ncounterfactuals. RealAC automatically preserves complex inter-feature\ndependencies without relying on explicit domain knowledge -- by aligning the\njoint distributions of feature pairs between factual and counterfactual\ninstances. The framework also allows end-users to ``freeze'' attributes they\ncannot or do not wish to change by suppressing change in frozen features during\noptimization. Evaluations on three synthetic and two real datasets demonstrate\nthat RealAC balances realism with actionability. Our method outperforms\nstate-of-the-art baselines and Large Language Model-based counterfactual\ngeneration techniques in causal edge score, dependency preservation score, and\nIM1 realism metric and offers a solution for causality-aware and user-centric\ncounterfactual generation.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10455v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10455v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.459,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.345,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on generating realistic counterfactual explanations for AI decisions using optimization techniques, without involving reinforcement learning, human feedback for training a reward model, or fine-tuning based on human-ranked data. It only incorporates user preferences as constraints in optimization, which does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a framework for counterfactual explanations that uses mutual information minimization for dependency preservation, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning on a Chain-of-Thought. It does not adapt diffusion techniques for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10457",
      "title": "Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head\n  Vision Transformers",
      "authors": [
        "Hanna Herasimchyk",
        "Robin Labryga",
        "Tomislav Prusina"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.IR (Information Retrieval)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We present a multi-head vision transformer approach for multi-label plant\nspecies prediction in vegetation plot images, addressing the PlantCLEF 2025\nchallenge. The task involves training models on single-species plant images\nwhile testing on multi-species quadrat images, creating a drastic domain shift.\nOur methodology leverages a pre-trained DINOv2 Vision Transformer Base\n(ViT-B/14) backbone with multiple classification heads for species, genus, and\nfamily prediction, utilizing taxonomic hierarchies. Key contributions include\nmulti-scale tiling to capture plants at different scales, dynamic threshold\noptimization based on mean prediction length, and ensemble strategies through\nbagging and Hydra model architectures. The approach incorporates various\ninference techniques including image cropping to remove non-plant artifacts,\ntop-n filtering for prediction constraints, and logit thresholding strategies.\nExperiments were conducted on approximately 1.4 million training images\ncovering 7,806 plant species. Results demonstrate strong performance, making\nour submission 3rd best on the private leaderboard. Our code is available at\nhttps://github.com/geranium12/plant-clef-2025/tree/v1.0.0.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10457v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10457v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.353,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10461",
      "title": "X-Node: Self-Explanation is All We Need",
      "authors": [
        "Prajit Sengupta",
        "Islem Rekik"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Graph neural networks (GNNs) have achieved state-of-the-art results in\ncomputer vision and medical image classification tasks by capturing structural\ndependencies across data instances. However, their decision-making remains\nlargely opaque, limiting their trustworthiness in high-stakes clinical\napplications where interpretability is essential. Existing explainability\ntechniques for GNNs are typically post-hoc and global, offering limited insight\ninto individual node decisions or local reasoning. We introduce X-Node, a\nself-explaining GNN framework in which each node generates its own explanation\nas part of the prediction process. For every node, we construct a structured\ncontext vector encoding interpretable cues such as degree, centrality,\nclustering, feature saliency, and label agreement within its local topology. A\nlightweight Reasoner module maps this context into a compact explanation\nvector, which serves three purposes: (1) reconstructing the node's latent\nembedding via a decoder to enforce faithfulness, (2) generating a natural\nlanguage explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)\nguiding the GNN itself via a \"text-injection\" mechanism that feeds explanations\nback into the message-passing pipeline. We evaluate X-Node on two graph\ndatasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,\nand GIN backbones. Our results show that X-Node maintains competitive\nclassification accuracy while producing faithful, per-node explanations.\nRepository: https://github.com/basiralab/X-Node.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10461v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10461v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.318,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces X-Node, a self-explaining GNN framework that focuses on generating per-node explanations using local topology and integrating them into the prediction process. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no component related to treating a 'Chain-of-Thought' as a single entity for holistic correction, making the paper unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10464",
      "title": "SingleStrip: learning skull-stripping from a single labeled example",
      "authors": [
        "Bella Specktor-Fadida",
        "Malte Hoffmann"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Deep learning segmentation relies heavily on labeled data, but manual\nlabeling is laborious and time-consuming, especially for volumetric images such\nas brain magnetic resonance imaging (MRI). While recent domain-randomization\ntechniques alleviate the dependency on labeled data by synthesizing diverse\ntraining images from label maps, they offer limited anatomical variability when\nvery few label maps are available. Semi-supervised self-training addresses\nlabel scarcity by iteratively incorporating model predictions into the training\nset, enabling networks to learn from unlabeled data. In this work, we combine\ndomain randomization with self-training to train three-dimensional\nskull-stripping networks using as little as a single labeled example. First, we\nautomatically bin voxel intensities, yielding labels we use to synthesize\nimages for training an initial skull-stripping model. Second, we train a\nconvolutional autoencoder (AE) on the labeled example and use its\nreconstruction error to assess the quality of brain masks predicted for\nunlabeled data. Third, we select the top-ranking pseudo-labels to fine-tune the\nnetwork, achieving skull-stripping performance on out-of-distribution data that\napproaches models trained with more labeled images. We compare AE-based ranking\nto consistency-based ranking under test-time augmentation, finding that the AE\napproach yields a stronger correlation with segmentation accuracy. Our results\nhighlight the potential of combining domain randomization and AE-based quality\ncontrol to enable effective semi-supervised segmentation from extremely limited\nlabeled data. This strategy may ease the labeling burden that slows progress in\nstudies involving new anatomical structures or emerging imaging techniques.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10464v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10464v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.467,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.352,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves training a skull-stripping network using a single labeled example by synthesizing images and generating pseudo-labels through self-training and domain randomization. This aligns closely with weak supervision, as it programmatically creates large quantities of training labels (e.g., pseudo-labels from model predictions) from noisy or imprecise sources, reducing reliance on hand-labeled data. The use of automated quality control to select reliable pseudo-labels further emphasizes this approach, making the paper a strong fit for the topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces SingleStrip, a semi-supervised method for training three-dimensional skull-stripping networks using only a single labeled brain MRI example. The approach combines domain randomization to synthesize diverse training images from the labeled data and self-training with an autoencoder to select high-quality pseudo-labels from unlabeled data, enabling the network to achieve robust segmentation performance comparable to models trained on larger datasets. Key findings highlight that autoencoder-based quality control outperforms consistency-based ranking, demonstrating the potential to significantly reduce labeling efforts in medical image segmentation for new anatomies or modalities.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of domain randomization and self-training with autoencoder-based quality control to address label scarcity in skull-stripping, offering a notable improvement over existing methods by enabling effective training with just one labeled example.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in medical image segmentation subfields due to its practical approach for handling limited labeled data, potentially influencing research on emerging imaging techniques or new anatomical structures.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to semi-supervised learning in computer vision, particularly for researchers dealing with label-scarce medical imaging tasks, as its methods could inspire efficient segmentation strategies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8b17e416ac2b9240846f9dfec353864406fae973",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 15,
      "average_h_index": 9.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Bella Specktor-Fadida",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2130621531"
        },
        {
          "name": "Malte Hoffmann",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/3147257"
        }
      ]
    },
    {
      "id": "2508.10467",
      "title": "FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over\n  Scholarly Knowledge Graphs",
      "authors": [
        "Xueli Pan",
        "Victor de Boer",
        "Jacco van Ossenbruggen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.DL (Digital Libraries)"
      ],
      "abstract": "Question answering over Scholarly Knowledge Graphs (SKGs) remains a\nchallenging task due to the complexity of scholarly content and the intricate\nstructure of these graphs. Large Language Model (LLM) approaches could be used\nto translate natural language questions (NLQs) into SPARQL queries; however,\nthese LLM-based approaches struggle with SPARQL query generation due to limited\nexposure to SKG-specific content and the underlying schema. We identified two\nmain types of errors in the LLM-generated SPARQL queries: (i) structural\ninconsistencies, such as missing or redundant triples in the queries, and (ii)\nsemantic inaccuracies, where incorrect entities or properties are shown in the\nqueries despite a correct query structure. To address these issues, we propose\nFIRESPARQL, a modular framework that supports fine-tuned LLMs as a core\ncomponent, with optional context provided via retrieval-augmented generation\n(RAG) and a SPARQL query correction layer. We evaluate the framework on the\nSciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,\none-shot, fine-tuning, and fine-tuning with RAG) and compare the performance\nwith baseline and state-of-the-art approaches. We measure query accuracy using\nBLEU and ROUGE metrics, and query result accuracy using relaxed exact\nmatch(RelaxedEM), with respect to the gold standards containing the NLQs,\nSPARQL queries, and the results of the queries. Experimental results\ndemonstrate that fine-tuning achieves the highest overall performance, reaching\n0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the\ntest set.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10467v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10467v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.376,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper proposes a framework for SPARQL query generation using fine-tuned LLMs, which involves standard supervised fine-tuning on a benchmark dataset. It does not discuss or utilize weak supervision techniques, such as programmatically generating noisy labels, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "The paper's framework relies on LLMs, RAG, and a SPARQL correction layer for query generation, but it does not incorporate diffusion models or iterative refinement processes for multi-step logical reasoning. There is no mention of treating a chain-of-thought as a single entity for holistic correction via diffusion, so it does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10469",
      "title": "Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human\n  Action Recognition",
      "authors": [
        "Maimunatu Tunau",
        "Vincent Gbouna Zakka",
        "Zhuangzhuang Dai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Human Action Recognition (HAR) plays a crucial role in healthcare, fitness\ntracking, and ambient assisted living technologies. While traditional vision\nbased HAR systems are effective, they pose privacy concerns. mmWave radar\nsensors offer a privacy preserving alternative but present challenges due to\nthe sparse and noisy nature of their point cloud data. In the literature, three\nprimary data processing methods: Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering\nhave been widely used to improve the quality and continuity of radar data.\nHowever, a comprehensive evaluation of these methods, both individually and in\ncombination, remains lacking. This paper addresses that gap by conducting a\ndetailed performance analysis of the three methods using the MiliPoint dataset.\nWe evaluate each method individually, all possible pairwise combinations, and\nthe combination of all three, assessing both recognition accuracy and\ncomputational cost. Furthermore, we propose targeted enhancements to the\nindividual methods aimed at improving accuracy. Our results provide crucial\ninsights into the strengths and trade-offs of each method and their\nintegrations, guiding future work on mmWave based HAR systems",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10469v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10469v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.315,
      "distributed_training_score": 0.351,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10473",
      "title": "STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS\n  Diagnosis in Multi-center Histopathology Images",
      "authors": [
        "Liangrui Pan",
        "xiaoyu Li",
        "Guang Zhu",
        "Guanting Li",
        "Ruixin Wang",
        "Jiadi Luo",
        "Yaning Yang",
        "Liang qingchun",
        "Shaoliang Peng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Spread through air spaces (STAS) constitutes a novel invasive pattern in lung\nadenocarcinoma (LUAD), associated with tumor recurrence and diminished survival\nrates. However, large-scale STAS diagnosis in LUAD remains a labor-intensive\nendeavor, compounded by the propensity for oversight and misdiagnosis due to\nits distinctive pathological characteristics and morphological features.\nConsequently, there is a pressing clinical imperative to leverage deep learning\nmodels for STAS diagnosis. This study initially assembled histopathological\nimages from STAS patients at the Second Xiangya Hospital and the Third Xiangya\nHospital of Central South University, alongside the TCGA-LUAD cohort. Three\nsenior pathologists conducted cross-verification annotations to construct the\nSTAS-SXY, STAS-TXY, and STAS-TCGA datasets. We then propose a multi-pattern\nattention-aware multiple instance learning framework, named STAMP, to analyze\nand diagnose the presence of STAS across multi-center histopathology images.\nSpecifically, the dual-branch architecture guides the model to learn\nSTAS-associated pathological features from distinct semantic spaces.\nTransformer-based instance encoding and a multi-pattern attention aggregation\nmodules dynamically selects regions closely associated with STAS pathology,\nsuppressing irrelevant noise and enhancing the discriminative power of global\nrepresentations. Moreover, a similarity regularization constraint prevents\nfeature redundancy across branches, thereby improving overall diagnostic\naccuracy. Extensive experiments demonstrated that STAMP achieved competitive\ndiagnostic results on STAS-SXY, STAS-TXY and STAS-TCGA, with AUCs of 0.8058,\n0.8017, and 0.7928, respectively, surpassing the clinical level.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10473v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10473v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.362,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10480",
      "title": "Pinet: Optimizing hard-constrained neural networks with orthogonal\n  projection layers",
      "authors": [
        "Panagiotis D. Grontas",
        "Antonio Terpin",
        "Efe C. Balta",
        "Raffaello D'Andrea",
        "John Lygeros"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.OC (Optimization and Control)"
      ],
      "abstract": "We introduce an output layer for neural networks that ensures satisfaction of\nconvex constraints. Our approach, $\\Pi$net, leverages operator splitting for\nrapid and reliable projections in the forward pass, and the implicit function\ntheorem for backpropagation. We deploy $\\Pi$net as a feasible-by-design\noptimization proxy for parametric constrained optimization problems and obtain\nmodest-accuracy solutions faster than traditional solvers when solving a single\nproblem, and significantly faster for a batch of problems. We surpass\nstate-of-the-art learning approaches in terms of training time, solution\nquality, and robustness to hyperparameter tuning, while maintaining similar\ninference times. Finally, we tackle multi-vehicle motion planning with\nnon-convex trajectory preferences and provide $\\Pi$net as a GPU-ready package\nimplemented in JAX with effective tuning heuristics.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10480v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10480v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.282,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.358,
      "datasets_score": 0.225,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10486",
      "title": "SEQ-GPT: LLM-assisted Spatial Query via Example",
      "authors": [
        "Ivan Khai Ze Lim",
        "Ningyi Liao",
        "Yiming Yang",
        "Gerald Wei Yong Yip",
        "Siqiang Luo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Contemporary spatial services such as online maps predominantly rely on user\nqueries for location searches. However, the user experience is limited when\nperforming complex tasks, such as searching for a group of locations\nsimultaneously. In this study, we examine the extended scenario known as\nSpatial Exemplar Query (SEQ), where multiple relevant locations are jointly\nsearched based on user-specified examples. We introduce SEQ-GPT, a spatial\nquery system powered by Large Language Models (LLMs) towards more versatile SEQ\nsearch using natural language. The language capabilities of LLMs enable unique\ninteractive operations in the SEQ process, including asking users to clarify\nquery details and dynamically adjusting the search based on user feedback. We\nalso propose a tailored LLM adaptation pipeline that aligns natural language\nwith structured spatial data and queries through dialogue synthesis and\nmulti-model cooperation. SEQ-GPT offers an end-to-end demonstration for\nbroadening spatial search with realistic data and application scenarios.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10486v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10486v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.339,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of SEQ-GPT, a system using Large Language Models (LLMs) for spatial queries, including natural language interaction and dynamic search adjustments. It does not involve diffusion models, iterative refinement processes for logical tasks, or treating a Chain-of-Thought as a single entity for holistic correction. Thus, it lacks any components of multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10490",
      "title": "On the Complexity-Faithfulness Trade-off of Gradient-Based Explanations",
      "authors": [
        "Amir Mehrpanah",
        "Matteo Gamba",
        "Kevin Smith",
        "Hossein Azizpour"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "ReLU networks, while prevalent for visual data, have sharp transitions,\nsometimes relying on individual pixels for predictions, making vanilla\ngradient-based explanations noisy and difficult to interpret. Existing methods,\nsuch as GradCAM, smooth these explanations by producing surrogate models at the\ncost of faithfulness. We introduce a unifying spectral framework to\nsystematically analyze and quantify smoothness, faithfulness, and their\ntrade-off in explanations. Using this framework, we quantify and regularize the\ncontribution of ReLU networks to high-frequency information, providing a\nprincipled approach to identifying this trade-off. Our analysis characterizes\nhow surrogate-based smoothing distorts explanations, leading to an\n``explanation gap'' that we formally define and measure for different post-hoc\nmethods. Finally, we validate our theoretical findings across different design\nchoices, datasets, and ablations.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10490v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10490v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.336,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on analyzing and improving gradient-based explanations for ReLU networks in computer vision, focusing on the trade-off between complexity and faithfulness using a spectral framework. It does not involve diffusion models, iterative refinement processes, or any adaptation for solving complex logical tasks or Chain-of-Thought reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10491",
      "title": "Contrastive ECOC: Learning Output Codes for Adversarial Defense",
      "authors": [
        "Che-Yu Chou",
        "Hung-Hsuan Chen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.IT (Information Theory)",
        "math.IT (Information Theory)"
      ],
      "abstract": "Although one-hot encoding is commonly used for multiclass classification, it\nis not always the most effective encoding mechanism. Error Correcting Output\nCodes (ECOC) address multiclass classification by mapping each class to a\nunique codeword used as a label. Traditional ECOC methods rely on manually\ndesigned or randomly generated codebooks, which are labor-intensive and may\nyield suboptimal, dataset-agnostic results. This paper introduces three models\nfor automated codebook learning based on contrastive learning, allowing\ncodebooks to be learned directly and adaptively from data. Across four\ndatasets, our proposed models demonstrate superior robustness to adversarial\nattacks compared to two baselines. The source is available at\nhttps://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10491v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10491v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.308,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10492",
      "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis\n  Driven by a Large Language Model",
      "authors": [
        "Shicheng Xu",
        "Xin Huang",
        "Zihao Wei",
        "Liang Pang",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Full-process clinical diagnosis in the real world encompasses the entire\ndiagnostic workflow that begins with only an ambiguous chief complaint. While\nartificial intelligence (AI), particularly large language models (LLMs), is\ntransforming clinical diagnosis, its role remains largely as an assistant to\nphysicians. This AI-assisted working pattern makes AI can only answer specific\nmedical questions at certain parts within the diagnostic process, but lack the\nability to drive the entire diagnostic process starting from an ambiguous\ncomplaint, which still relies heavily on human physicians. This gap limits AI's\nability to fully reduce physicians' workload and enhance diagnostic efficiency.\nTo address this, we propose a paradigm shift that reverses the relationship\nbetween physicians and AI: repositioning AI as the primary director, with\nphysicians serving as its assistants. So we present DxDirector-7B, an LLM\nendowed with advanced deep thinking capabilities, enabling it to drive the\nfull-process diagnosis with minimal physician involvement. Furthermore,\nDxDirector-7B establishes a robust accountability framework for misdiagnoses,\ndelineating responsibility between AI and human physicians. In evaluations\nacross rare, complex, and real-world cases under full-process diagnosis\nsetting, DxDirector-7B not only achieves significant superior diagnostic\naccuracy but also substantially reduces physician workload than\nstate-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained\nanalyses across multiple clinical departments and tasks validate its efficacy,\nwith expert evaluations indicating its potential to serve as a viable\nsubstitute for medical specialists. These findings mark a new era where AI,\ntraditionally a physicians' assistant, now drives the entire diagnostic process\nto drastically reduce physicians' workload, indicating an efficient and\naccurate diagnostic solution.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10492v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10492v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.385,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an LLM designed for full-process clinical diagnosis, emphasizing its role in driving the diagnostic workflow with minimal physician involvement. However, there is no mention of training the model using human feedback, a reward model, or reinforcement learning techniques to align with human preferences. The physician interaction described is operational (e.g., providing inputs during diagnosis), not part of the training process.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes DxDirector-7B as performing step-by-step, iterative reasoning in clinical diagnosis, similar to a chain-of-thought process, which could loosely resemble iterative refinement. However, it does not explicitly use or adapt diffusion models for multi-step logical reasoning or holistic correction of reasoning paths, focusing instead on general LLM capabilities without referencing diffusion-based techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10494",
      "title": "A Unified Multi-Agent Framework for Universal Multimodal Understanding\n  and Generation",
      "authors": [
        "Jiulin Li",
        "Ping Huang",
        "Yexin Li",
        "Shuo Chen",
        "Juewen Hu",
        "Ye Tian"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Real-world multimodal applications often require any-to-any capabilities,\nenabling both understanding and generation across modalities including text,\nimage, audio, and video. However, integrating the strengths of autoregressive\nlanguage models (LLMs) for reasoning and diffusion models for high-fidelity\ngeneration remains challenging. Existing approaches rely on rigid pipelines or\ntightly coupled architectures, limiting flexibility and scalability. We propose\nMAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that\nunifies multimodal understanding and generation via two decoupled phases:\nCognition and Deliberation. MAGUS enables symbolic multi-agent collaboration\nwithin a shared textual workspace. In the Cognition phase, three\nrole-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -\nengage in collaborative dialogue to perform structured understanding and\nplanning. The Deliberation phase incorporates a Growth-Aware Search mechanism\nthat orchestrates LLM-based reasoning and diffusion-based generation in a\nmutually reinforcing manner. MAGUS supports plug-and-play extensibility,\nscalable any-to-any modality conversion, and semantic alignment - all without\nthe need for joint training. Experiments across multiple benchmarks, including\nimage, video, and audio generation, as well as cross-modal instruction\nfollowing, demonstrate that MAGUS outperforms strong baselines and\nstate-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the\npowerful closed-source model GPT-4o.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10494v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10494v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.532,
      "distributed_training_score": 0.366,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MAGUS, which integrates diffusion models for generation tasks in the Deliberation phase, using a Growth-Aware Search mechanism to combine LLM-based reasoning with diffusion-based output. However, diffusion is primarily used for high-fidelity modality-specific generation (e.g., images, videos), not for adapting its iterative refinement process to solve complex logical tasks or holistically correct a Chain-of-Thought. The reasoning is handled by LLM agents in the Cognition phase, making the paper's contribution only indirectly related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10498",
      "title": "TweezeEdit: Consistent and Efficient Image Editing with Path\n  Regularization",
      "authors": [
        "Jianda Mao",
        "Kaibo Wang",
        "Yang Xiang",
        "Kani Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Large-scale pre-trained diffusion models empower users to edit images through\ntext guidance. However, existing methods often over-align with target prompts\nwhile inadequately preserving source image semantics. Such approaches generate\ntarget images explicitly or implicitly from the inversion noise of the source\nimages, termed the inversion anchors. We identify this strategy as suboptimal\nfor semantic preservation and inefficient due to elongated editing paths. We\npropose TweezeEdit, a tuning- and inversion-free framework for consistent and\nefficient image editing. Our method addresses these limitations by regularizing\nthe entire denoising path rather than relying solely on the inversion anchors,\nensuring source semantic retention and shortening editing paths. Guided by\ngradient-driven regularization, we efficiently inject target prompt semantics\nalong a direct path using a consistency model. Extensive experiments\ndemonstrate TweezeEdit's superior performance in semantic preservation and\ntarget alignment, outperforming existing methods. Remarkably, it requires only\n12 steps (1.6 seconds per edit), underscoring its potential for real-time\napplications.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10498v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10498v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.295,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on image editing using diffusion models, specifically proposing TweezeEdit to regularize denoising paths for better semantic preservation and efficiency. While it involves the iterative refinement process of diffusion models, this is applied to generative image tasks, not to solving complex logical tasks or treating a 'Chain-of-Thought' as a single entity for multi-step reasoning. There is no component for logical reasoning, making the paper unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10501",
      "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and\n  Adaptive Chest X-Ray Reasoning",
      "authors": [
        "Yushi Feng",
        "Junye Du",
        "Yingying Hong",
        "Qifan Wang",
        "Lequan Yu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Existing tool-augmented agentic systems are limited in the real world by (i)\nblack-box reasoning steps that undermine trust of decision-making and pose\nsafety risks, (ii) poor multimodal integration, which is inherently critical\nfor healthcare tasks, and (iii) rigid and computationally inefficient agentic\npipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the\nfirst multimodal framework to address these challenges in the context of Chest\nX-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a\nmulti-tool graph, yielding decision paths annotated with interpretable\nprobabilities. Given the complex CXR reasoning task with multimodal medical\ndata, PASS leverages its learned task-conditioned distribution over the agentic\nsupernet. Thus, it adaptively selects the most suitable tool at each supernet\nlayer, offering probability-annotated trajectories for post-hoc audits and\ndirectly enhancing medical AI safety. PASS also continuously compresses salient\nfindings into an evolving personalized memory, while dynamically deciding\nwhether to deepen its reasoning path or invoke an early exit for efficiency. To\noptimize a Pareto frontier balancing performance and cost, we design a novel\nthree-stage training procedure, including expert knowledge warm-up, contrastive\npath-ranking, and cost-aware reinforcement learning. To facilitate rigorous\nevaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,\nsafety-critical, free-form CXR reasoning. Experiments across various benchmarks\nvalidate that PASS significantly outperforms strong baselines in multiple\nmetrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,\npushing a new paradigm shift towards interpretable, adaptive, and multimodal\nmedical agentic systems.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10501v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10501v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.358,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is PASS, a framework for probabilistic agentic supernet sampling in multimodal Chest X-Ray reasoning, which involves adaptive workflow sampling, a probabilistic controller, and reinforcement learning-based training. It does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for complex logical tasks. Thus, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10504",
      "title": "Advances in Logic-Based Entity Resolution: Enhancing ASPEN with Local\n  Merges and Optimality Criteria",
      "authors": [
        "Zhliang Xiang",
        "Meghyn Bienvenu",
        "Gianluca Cima",
        "Víctor Gutiérrez-Basulto",
        "Yazmín Ibáñez-García"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we present ASPEN+, which extends an existing ASP-based system,\nASPEN,for collective entity resolution with two important functionalities:\nsupport for local merges and new optimality criteria for preferred solutions.\nIndeed, ASPEN only supports so-called global merges of entity-referring\nconstants (e.g. author ids), in which all occurrences of matched constants are\ntreated as equivalent and merged accordingly. However, it has been argued that\nwhen resolving data values, local merges are often more appropriate, as e.g.\nsome instances of 'J. Lee' may refer to 'Joy Lee', while others should be\nmatched with 'Jake Lee'. In addition to allowing such local merges, ASPEN+\noffers new optimality criteria for selecting solutions, such as minimizing rule\nviolations or maximising the number of rules supporting a merge. Our main\ncontributions are thus (1) the formalisation and computational analysis of\nvarious notions of optimal solution, and (2) an extensive experimental\nevaluation on real-world datasets, demonstrating the effect of local merges and\nthe new optimality criteria on both accuracy and runtime.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10504v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10504v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.289,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.291,
      "distributed_training_score": 0.292,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10507",
      "title": "Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian\n  Splatting",
      "authors": [
        "Zheng Zhou",
        "Jia-Chen Zhang",
        "Yu-Jie Xiong",
        "Chun-Ming Xia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in 3D Gaussian splatting have significantly improved\nreal-time novel view synthesis, yet insufficient geometric constraints during\nscene optimization often result in blurred reconstructions of fine-grained\ndetails, particularly in regions with high-frequency textures and sharp\ndiscontinuities. To address this, we propose a comprehensive optimization\nframework integrating multisample anti-aliasing (MSAA) with dual geometric\nconstraints. Our system computes pixel colors through adaptive blending of\nquadruple subsamples, effectively reducing aliasing artifacts in high-frequency\ncomponents. The framework introduces two constraints: (a) an adaptive weighting\nstrategy that prioritizes under-reconstructed regions through dynamic gradient\nanalysis, and (b) gradient differential constraints enforcing geometric\nregularization at object boundaries. This targeted optimization enables the\nmodel to allocate computational resources preferentially to critical regions\nrequiring refinement while maintaining global consistency. Extensive\nexperimental evaluations across multiple benchmarks demonstrate that our method\nachieves state-of-the-art performance in detail preservation, particularly in\npreserving high-frequency textures and sharp discontinuities, while maintaining\nreal-time rendering efficiency. Quantitative metrics and perceptual studies\nconfirm statistically significant improvements over baseline approaches in both\nstructural similarity (SSIM) and perceptual quality (LPIPS).",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10507v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10507v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.35,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10509",
      "title": "A Segmentation-driven Editing Method for Bolt Defect Augmentation and\n  Detection",
      "authors": [
        "Yangjie Xiao",
        "Ke Zhang",
        "Jiacun Wang",
        "Xin Sheng",
        "Yurong Guo",
        "Meijuan Chen",
        "Zehua Ren",
        "Zhaoye Zheng",
        "Zhenbing Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Bolt defect detection is critical to ensure the safety of transmission lines.\nHowever, the scarcity of defect images and imbalanced data distributions\nsignificantly limit detection performance. To address this problem, we propose\na segmentationdriven bolt defect editing method (SBDE) to augment the dataset.\nFirst, a bolt attribute segmentation model (Bolt-SAM) is proposed, which\nenhances the segmentation of complex bolt attributes through the CLAHE-FFT\nAdapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality\nmasks for subsequent editing tasks. Second, a mask optimization module (MOD) is\ndesigned and integrated with the image inpainting model (LaMa) to construct the\nbolt defect attribute editing model (MOD-LaMa), which converts normal bolts\ninto defective ones through attribute editing. Finally, an editing recovery\naugmentation (ERA) strategy is proposed to recover and put the edited defect\nbolts back into the original inspection scenes and expand the defect detection\ndataset. We constructed multiple bolt datasets and conducted extensive\nexperiments. Experimental results demonstrate that the bolt defect images\ngenerated by SBDE significantly outperform state-of-the-art image editing\nmodels, and effectively improve the performance of bolt defect detection, which\nfully verifies the effectiveness and application potential of the proposed\nmethod. The code of the project is available at\nhttps://github.com/Jay-xyj/SBDE.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10509v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10509v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.279,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.344,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10522",
      "title": "EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba",
      "authors": [
        "Quang Nguyen",
        "Nhat Le",
        "Baoru Huang",
        "Minh Nhat Vu",
        "Chengcheng Tang",
        "Van Nguyen",
        "Ngan Le",
        "Thieu Vo",
        "Anh Nguyen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Estimating human dance motion is a challenging task with various industrial\napplications. Recently, many efforts have focused on predicting human dance\nmotion using either egocentric video or music as input. However, the task of\njointly estimating human motion from both egocentric video and music remains\nlargely unexplored. In this paper, we aim to develop a new method that predicts\nhuman dance motion from both egocentric video and music. In practice, the\negocentric view often obscures much of the body, making accurate full-pose\nestimation challenging. Additionally, incorporating music requires the\ngenerated head and body movements to align well with both visual and musical\ninputs. We first introduce EgoAIST++, a new large-scale dataset that combines\nboth egocentric views and music with more than 36 hours of dancing motion.\nDrawing on the success of diffusion models and Mamba on modeling sequences, we\ndevelop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly\ncaptures the skeleton structure of the human body. We illustrate that our\napproach is theoretically supportive. Intensive experiments show that our\nmethod clearly outperforms state-of-the-art approaches and generalizes\neffectively to real-world data.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10522v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10522v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.276,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.269,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10523",
      "title": "Reasoning in Computer Vision: Taxonomy, Models, Tasks, and Methodologies",
      "authors": [
        "Ayushman Sarkar",
        "Mohd Yamani Idna Idris",
        "Zhenyu Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual reasoning is critical for a wide range of computer vision tasks that\ngo beyond surface-level object detection and classification. Despite notable\nadvances in relational, symbolic, temporal, causal, and commonsense reasoning,\nexisting surveys often address these directions in isolation, lacking a unified\nanalysis and comparison across reasoning types, methodologies, and evaluation\nprotocols. This survey aims to address this gap by categorizing visual\nreasoning into five major types (relational, symbolic, temporal, causal, and\ncommonsense) and systematically examining their implementation through\narchitectures such as graph-based models, memory networks, attention\nmechanisms, and neuro-symbolic systems. We review evaluation protocols designed\nto assess functional correctness, structural consistency, and causal validity,\nand critically analyze their limitations in terms of generalizability,\nreproducibility, and explanatory power. Beyond evaluation, we identify key open\nchallenges in visual reasoning, including scalability to complex scenes, deeper\nintegration of symbolic and neural paradigms, the lack of comprehensive\nbenchmark datasets, and reasoning under weak supervision. Finally, we outline a\nforward-looking research agenda for next-generation vision systems, emphasizing\nthat bridging perception and reasoning is essential for building transparent,\ntrustworthy, and cross-domain adaptive AI systems, particularly in critical\ndomains such as autonomous driving and medical diagnostics.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10523v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10523v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.529,
      "distributed_training_score": 0.309,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper is a survey on visual reasoning in computer vision, focusing on taxonomy, models, tasks, and methodologies such as graph-based models, memory networks, attention mechanisms, and neuro-symbolic systems. It does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, there is no connection to the topic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10528",
      "title": "Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale\n  Grounded Dataset",
      "authors": [
        "Ziye Deng",
        "Ruihan He",
        "Jiaxiang Liu",
        "Yuan Wang",
        "Zijie Meng",
        "Songtao Jiang",
        "Yong Xie",
        "Zuozhu Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Medical image grounding aims to align natural language phrases with specific\nregions in medical images, serving as a foundational task for intelligent\ndiagnosis, visual question answering (VQA), and automated report generation\n(MRG). However, existing research is constrained by limited modality coverage,\ncoarse-grained annotations, and the absence of a unified, generalizable\ngrounding framework. To address these challenges, we construct a large-scale\nmedical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level\nannotations across seven imaging modalities, covering diverse anatomical\nstructures and pathological findings. The dataset supports both segmentation\nand grounding tasks with hierarchical region labels, ranging from organ-level\nboundaries to fine-grained lesions. Based on this foundation, we propose\nMed-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather\nthan relying on explicitly designed expert modules, Med-GLIP implicitly\nacquires hierarchical semantic understanding from diverse training data --\nenabling it to recognize multi-granularity structures, such as distinguishing\nlungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP\nconsistently outperforms state-of-the-art baselines across multiple grounding\nbenchmarks. Furthermore, integrating its spatial outputs into downstream tasks,\nincluding medical VQA and report generation, leads to substantial performance\ngains. Our dataset will be released soon.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10528v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10528v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.366,
      "datasets_score": 0.414,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on medical language-image pre-training and a grounding framework, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not adapt diffusion for tasks like chain-of-thought correction, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and curation of a new large-scale dataset, Med-GLIP-5M, with over 5.3 million annotations across multiple modalities. It details dataset construction, annotation types, and its application to benchmarking grounding tasks, directly aligning with research on dataset creation and evaluation for AI.",
      "llm_score_status": "completed",
      "summary": "The paper introduces Med-GLIP-5M, a large-scale dataset with over 5.3 million region-level annotations across seven medical imaging modalities, to overcome the limitations of existing medical image grounding resources. It proposes the Med-GLIP framework, a modality-aware model trained on this dataset, which achieves superior performance in grounding tasks and enhances downstream applications such as visual question answering and medical report generation by enabling precise language-image alignment.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new large-scale dataset and a modality-aware framework that significantly advances the state-of-the-art in medical image grounding by addressing data scarcity and cross-modal challenges. This represents a substantial innovation over existing methods, which are limited in scale and diversity.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence a wide range of future research in medical AI, including vision-language models and clinical applications, due to its comprehensive dataset and demonstrated improvements in downstream tasks. Its release could standardize benchmarks and foster advancements in intelligent diagnosis and report generation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to medical image grounding, offering essential insights and resources for researchers in AI and computer vision, though it may not be groundbreaking for all audiences. It is particularly relevant for those working on multimodal medical tasks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0c3c2f8af061ed4dccdf901fe3993e3a4baaebd0",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 5,
      "average_h_index": 2.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ziye Deng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2305094794"
        },
        {
          "name": "Ruihan He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376214825"
        },
        {
          "name": "Jiaxiang Liu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2297255114"
        },
        {
          "name": "Yuan Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2356766936"
        },
        {
          "name": "Zijie Meng",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2278829306"
        },
        {
          "name": "Songtao Jiang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2295676633"
        },
        {
          "name": "Yong Xie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375956576"
        },
        {
          "name": "Zuozhu Liu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2328017438"
        }
      ]
    },
    {
      "id": "2508.10530",
      "title": "Diversity First, Quality Later: A Two-Stage Assumption for Language\n  Model Alignment",
      "authors": [
        "Zetian Sun",
        "Dongfang Li",
        "Baotian Hu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The alignment of language models (LMs) with human preferences is critical for\nbuilding reliable AI systems. The problem is typically framed as optimizing an\nLM policy to maximize the expected reward that reflects human preferences.\nRecently, Direct Preference Optimization (DPO) was proposed as a LM alignment\nmethod that directly optimize the policy from static preference data, and\nfurther improved by incorporating on-policy sampling (i.e., preference\ncandidates generated during the training loop) for better LM alignment.\nHowever, we show on-policy data is not always optimal, with systematic\neffectiveness difference emerging between static and on-policy preference\ncandidates. For example, on-policy data can result in a 3$\\times$ effectiveness\ncompared with static data for Llama-3, and a 0.4$\\times$ effectiveness for\nZephyr. To explain the phenomenon, we propose the alignment stage assumption,\nwhich divides the alignment process into two distinct stages: the preference\ninjection stage, which benefits from diverse data, and the preference\nfine-tuning stage, which favors high-quality data. Through theoretical and\nempirical analysis, we characterize these stages and propose an effective\nalgorithm to identify the boundaries between them. We perform experiments on 5\nmodels (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,\nSLiC-HF) to show the generalizability of alignment stage assumption and\nboundary measurement.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10530v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10530v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.543,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.413,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses LM alignment methods like DPO, which is an alternative to RLHF, and compares on-policy and off-policy approaches that stem from RL-based techniques. It builds on concepts from RLHF by integrating on-policy sampling for better alignment, but focuses more on preference optimization than directly implementing RLHF.",
      "weak_supervision_justification": "The paper uses preference data, which may be noisy or from static sources, but it does not emphasize programmatically generating labels. It primarily focuses on optimizing alignment with existing data rather than weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes; it centers on language model alignment and preference optimization.",
      "distributed_training_justification": "The paper does not address distributed systems, parallel computing, or multi-node training; its focus is solely on algorithmic aspects of LM alignment.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates the alignment of language models with human preferences by proposing a two-stage assumption: an initial preference injection stage that benefits from diverse data and a subsequent preference fine-tuning stage that requires high-quality data. Through theoretical analysis and experiments on five models (Llama-3, Zephyr, Phi-2, Qwen, Pythia) and two alignment methods (DPO and SLiC-HF), the authors demonstrate varying effectiveness of on-policy versus off-policy data across stages, introduce a boundary measurement algorithm to identify stage transitions, and provide theoretical insights into why diversity aids in approximating ground-truth preferences while quality enhances fine-tuning.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel two-stage alignment assumption and a boundary measurement algorithm, which provide a new framework for understanding and optimizing language model alignment, significantly advancing beyond existing methods like DPO. This represents a meaningful shift in how preference data is utilized during training, offering fresh insights into dynamic data requirements.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research in language model alignment by guiding data selection strategies, potentially improving efficiency and performance across various AI applications. Its empirical validation on multiple models and methods suggests broad applicability, making it likely to be cited and built upon in the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights and a practical algorithm for LM alignment, making it a significant contribution that researchers in AI and computational language should be aware of. While not revolutionary, its systematic approach and empirical support warrant attention for those working on model alignment techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/da30931a1d7bfaddc0a47d00bd5a8600c4ad3e96",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 8,
      "average_h_index": 7.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Zetian Sun",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2265614778"
        },
        {
          "name": "Dongfang Li",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2265618386"
        },
        {
          "name": "Baotian Hu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2285172247"
        }
      ]
    },
    {
      "id": "2508.10539",
      "title": "Improving Value-based Process Verifier via Low-Cost Variance Reduction",
      "authors": [
        "Zetian Sun",
        "Dongfang Li",
        "Baotian Hu",
        "Min Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable success in a wide range\nof tasks. However, their reasoning capabilities, particularly in complex\ndomains like mathematics, remain a significant challenge. Value-based process\nverifiers, which estimate the probability of a partial reasoning chain leading\nto a correct solution, are a promising approach for improving reasoning.\nNevertheless, their effectiveness is often hindered by estimation error in\ntheir training annotations, a consequence of the limited number of Monte Carlo\n(MC) samples feasible due to the high cost of LLM inference. In this paper, we\nidentify that the estimation error primarily arises from high variance rather\nthan bias, and the MC estimator is a Minimum Variance Unbiased Estimator\n(MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte\n\\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased\nestimator by linearly combining the MC estimators from the current and\nsubsequent steps. Theoretically, we show that our method leads to a predictable\nreduction in variance, while maintaining an unbiased estimation without\nadditional LLM inference cost. We also perform empirical experiments on the\nMATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.\nNotably, ComMCS outperforms regression-based optimization method by 2.8 points,\nthe non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32\nsampling experiment.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10539v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10539v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.362,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving value-based process verifiers for LLMs using Monte Carlo sampling and variance reduction techniques, such as ComMCS, without any reference to diffusion models or iterative refinement processes. It does not adapt diffusion for multi-step logical reasoning or treat reasoning chains holistically through diffusion-based methods, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10542",
      "title": "GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For\n  Salient Object Detection in Optical Remote Sensing Images",
      "authors": [
        "Mengyu Ren",
        "Yutong Li",
        "Hua Li",
        "Runmin Cong",
        "Sam Kwong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Salient object detection (SOD) in optical remote sensing images (ORSIs) faces\nnumerous challenges, including significant variations in target scales and low\ncontrast between targets and the background. Existing methods based on vision\ntransformers (ViTs) and convolutional neural networks (CNNs) architectures aim\nto leverage both global and local features, but the difficulty in effectively\nintegrating these heterogeneous features limits their overall performance. To\novercome these limitations, we propose a graph-enhanced contextual and regional\nperception network (GCRPNet), which builds upon the Mamba architecture to\nsimultaneously capture long-range dependencies and enhance regional feature\nrepresentation. Specifically, we employ the visual state space (VSS) encoder to\nextract multi-scale features. To further achieve deep guidance and enhancement\nof these features, we first design a difference-similarity guided hierarchical\ngraph attention module (DS-HGAM). This module strengthens cross-layer\ninteraction capabilities between features of different scales while enhancing\nthe model's structural perception,allowing it to distinguish between foreground\nand background more effectively. Then, we design the LEVSS block as the decoder\nof GCRPNet. This module integrates our proposed adaptive scanning strategy and\nmulti-granularity collaborative attention enhancement module (MCAEM). It\nperforms adaptive patch scanning on feature maps processed via multi-scale\nconvolutions, thereby capturing rich local region information and enhancing\nMamba's local modeling capability. Extensive experimental results demonstrate\nthat the proposed model achieves state-of-the-art performance, validating its\neffectiveness and superiority.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10542v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10542v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.335,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10548",
      "title": "Stabilizing Long-term Multi-turn Reinforcement Learning with Gated\n  Rewards",
      "authors": [
        "Zetian Sun",
        "Dongfang Li",
        "Zhuoen Chen",
        "Yuhuai Qin",
        "Baotian Hu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a\nsignificant challenge, while existing outcome-based reward shaping struggles to\ndefine meaningful immediate rewards without introducing bias or requiring\nexplicit task decomposition. Alternatively, verification-based reward shaping\nuses stepwise critics, but misalignment between immediate rewards and long-term\nobjectives can lead to reward hacking and suboptimal policies. In this work, we\naddress this problem in the context of software engineering (SWE) tasks, where\nmulti-turn reasoning and rule-based verification are critical. We introduce the\nSWE-oriented RL Framework, a unified system supporting multi-turn interaction,\ndocker-based execution, and customizable reward functions. Additionally, we\npropose Gated Reward Accumulation (G-RA), a novel method that accumulates\nimmediate rewards only when high-level (long-term) rewards meet a predefined\nthreshold, ensuring stable RL optimization. Experiments on SWE-bench Verified\nand kBench demonstrate that G-RA leads to an increase in completion rates\n(47.6\\% \\rightarrow 93.8\\% and 22.0\\% \\rightarrow 86.0\\%) and modification\nrates (19.6\\% \\rightarrow 23.8\\% and 12.0\\% \\rightarrow 42.0\\%), while avoiding\npolicy degradation caused by reward misalignment. Our findings highlight the\nimportance of balanced reward accumulation in long-horizon RL and provide a\npractical solution.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10548v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10548v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.481,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.385,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on reinforcement learning with rule-based and verification-based rewards for software engineering tasks, without involving human feedback, human-ranked data, or a reward model trained on human preferences. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a reinforcement learning framework and reward accumulation method for multi-turn tasks, but it does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10549",
      "title": "PSScreen: Partially Supervised Multiple Retinal Disease Screening",
      "authors": [
        "Boyi Zheng",
        "Qing Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Leveraging multiple partially labeled datasets to train a model for multiple\nretinal disease screening reduces the reliance on fully annotated datasets, but\nremains challenging due to significant domain shifts across training datasets\nfrom various medical sites, and the label absent issue for partial classes. To\nsolve these challenges, we propose PSScreen, a novel Partially Supervised\nmultiple retinal disease Screening model. Our PSScreen consists of two streams\nand one learns deterministic features and the other learns probabilistic\nfeatures via uncertainty injection. Then, we leverage the textual guidance to\ndecouple two types of features into disease-wise features and align them via\nfeature distillation to boost the domain generalization ability. Meanwhile, we\nemploy pseudo label consistency between two streams to address the label absent\nissue and introduce a self-distillation to transfer task-relevant semantics\nabout known classes from the deterministic to the probabilistic stream to\nfurther enhance the detection performances. Experiments show that our PSScreen\nsignificantly enhances the detection performances on six retinal diseases and\nthe normal state averagely and achieves state-of-the-art results on both\nin-domain and out-of-domain datasets. Codes are available at\nhttps://github.com/boyiZheng99/PSScreen.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10549v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10549v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.439,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.392,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, PSScreen, employs partially supervised learning on datasets with missing labels, using techniques like pseudo label consistency to generate labels programmatically. This directly aligns with weak supervision, as it relies on noisy or imprecise sources (e.g., pseudo labels from the model) to train the model, reducing the need for fully annotated data and addressing label absence issues.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"PSScreen: Partially Supervised Multiple Retinal Disease Screening,\" addresses the challenges of training a model for multiple retinal diseases using partially labeled datasets from various domains, which involve domain shifts and missing labels. It proposes PSScreen, a two-stream network that learns deterministic and probabilistic features, aligns them via textual guidance and feature distillation for domain generalization, and employs pseudo label consistency and self-distillation to handle label absences, ultimately achieving state-of-the-art performance on both in-domain and out-of-domain datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like two-stream networks and uncertainty injection to address domain shifts and partial labels in retinal disease screening, marking a notable improvement for this specific application. However, it builds on established concepts rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of medical image analysis for retinal diseases due to its enhanced domain generalization and practical approach to partial supervision. Nonetheless, its influence may remain confined to specialized applications in computer vision for healthcare rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to AI-driven medical screening by tackling real-world challenges like domain shifts, making it essential for researchers in computer vision and retinal disease detection. While not groundbreaking across all domains, its methodological innovations and empirical results warrant attention from relevant experts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/466cc64e5138b15342ae09ff0278facdfb53d9e4",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Boyi Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376043823"
        },
        {
          "name": "Qing Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376303370"
        }
      ]
    },
    {
      "id": "2508.10552",
      "title": "When Language Overrules: Revealing Text Dominance in Multimodal Large\n  Language Models",
      "authors": [
        "Huyu Wu",
        "Meng Tang",
        "Xinhan Zheng",
        "Haiyun Jiang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across a diverse range of multimodal tasks. However, these models\nsuffer from a core problem known as text dominance: they depend heavily on text\nfor their inference, while underutilizing other modalities. While prior work\nhas acknowledged this phenomenon in vision-language tasks, often attributing it\nto data biases or model architectures. In this paper, we conduct the first\nsystematic investigation of text dominance across diverse data modalities,\nincluding images, videos, audio, time-series, and graphs. To measure this\nimbalance, we propose two evaluation metrics: the Modality Dominance Index\n(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis\nreveals that text dominance is both significant and pervasive across all tested\nmodalities. Our in-depth analysis identifies three underlying causes: attention\ndilution from severe token redundancy in non-textual modalities, the influence\nof fusion architecture design, and task formulations that implicitly favor\ntextual inputs. Furthermore, we propose a simple token compression method that\neffectively rebalances model attention. Applying this method to LLaVA-7B, for\ninstance, drastically reduces its MDI from 10.23 to a well-balanced value of\n0.86. Our analysis and methodological framework offer a foundation for the\ndevelopment of more equitable and comprehensive multimodal language models.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10552v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10552v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.377,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on text dominance in Multimodal Large Language Models (MLLMs), analyzing attention mechanisms, metrics like MDI and AEI, and proposing token compression to balance modalities. It does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10554",
      "title": "AR Surgical Navigation with Surface Tracing: Comparing In-Situ\n  Visualization with Tool-Tracking Guidance for Neurosurgical Applications",
      "authors": [
        "Marc J. Fischer",
        "Jeffrey Potts",
        "Gabriel Urreola",
        "Dax Jones",
        "Paolo Palmisciano",
        "E. Bradley Strong",
        "Branden Cord",
        "Andrew D. Hernandez",
        "Julia D. Sharma",
        "E. Brandon Strong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Augmented Reality (AR) surgical navigation systems are emerging as the next\ngeneration of intraoperative surgical guidance, promising to overcome\nlimitations of traditional navigation systems. However, known issues with AR\ndepth perception due to vergence-accommodation conflict and occlusion handling\nlimitations of the currently commercially available display technology present\nacute challenges in surgical settings where precision is paramount. This study\npresents a novel methodology for utilizing AR guidance to register anatomical\ntargets and provide real-time instrument navigation using placement of\nsimulated external ventricular drain catheters on a phantom model as the\nclinical scenario. The system registers target positions to the patient through\na novel surface tracing method and uses real-time infrared tool tracking to aid\nin catheter placement, relying only on the onboard sensors of the Microsoft\nHoloLens 2. A group of intended users performed the procedure of simulated\ninsertions under two AR guidance conditions: static in-situ visualization,\nwhere planned trajectories are overlaid directly onto the patient anatomy, and\nreal-time tool-tracking guidance, where live feedback of the catheter's pose is\nprovided relative to the plan. Following the insertion tests, computed\ntomography scans of the phantom models were acquired, allowing for evaluation\nof insertion accuracy, target deviation, angular error, and depth precision.\nSystem Usability Scale surveys assessed user experience and cognitive workload.\nTool-tracking guidance improved performance metrics across all accuracy\nmeasures and was preferred by users in subjective evaluations. A free copy of\nthis paper and all supplemental materials are available at\nhttps://bit.ly/45l89Hq.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10554v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10554v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.276,
      "diffusion_reasoning_score": 0.273,
      "distributed_training_score": 0.258,
      "datasets_score": 0.237,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10556",
      "title": "Retrieval-Augmented Prompt for OOD Detection",
      "authors": [
        "Ruisong Han",
        "Zongbo Han",
        "Jiahao Zhang",
        "Mingyue Cheng",
        "Changqing Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Out-of-Distribution (OOD) detection is crucial for the reliable deployment of\nmachine learning models in-the-wild, enabling accurate identification of test\nsamples that differ from the training data distribution. Existing methods rely\non auxiliary outlier samples or in-distribution (ID) data to generate outlier\ninformation for training, but due to limited outliers and their mismatch with\nreal test OOD samples, they often fail to provide sufficient semantic\nsupervision, leading to suboptimal performance. To address this, we propose a\nnovel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP\naugments a pre-trained vision-language model's prompts by retrieving external\nknowledge, offering enhanced semantic supervision for OOD detection. During\ntraining, RAP retrieves descriptive words for outliers based on joint\nsimilarity with external textual knowledge and uses them to augment the model's\nOOD prompts. During testing, RAP dynamically updates OOD prompts in real-time\nbased on the encountered OOD samples, enabling the model to rapidly adapt to\nthe test environment. Our extensive experiments demonstrate that RAP achieves\nstate-of-the-art performance on large-scale OOD detection benchmarks. For\nexample, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the\naverage FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous\nmethods. Additionally, comprehensive ablation studies validate the\neffectiveness of each module and the underlying motivations of our approach.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10556v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10556v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.348,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on OOD detection using retrieval-augmented prompts from external knowledge, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper uses external knowledge and a small number of ID samples to programmatically generate supervision for OOD prompts, which shares some similarities with weak supervision by relying on noisy or derived labels, but its main contribution is OOD detection rather than weak supervision methodologies.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement, or multi-step logical reasoning; it centers on retrieval-augmented prompts for OOD detection in vision-language models.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10557",
      "title": "PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D\n  Perception Tasks",
      "authors": [
        "Xinhao Wang",
        "Zhiwei Lin",
        "Zhongyu Xia",
        "Yongtao Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)\nrepresent two mainstream model quantization approaches. However, PTQ often\nleads to unacceptable performance degradation in quantized models, while QAT\nimposes substantial GPU memory requirements and extended training time due to\nweight fine-tuning. In this paper, we propose PTQAT, a novel general hybrid\nquantization algorithm for the efficient deployment of 3D perception networks.\nTo address the speed accuracy trade-off between PTQ and QAT, our method selects\ncritical layers for QAT fine-tuning and performs PTQ on the remaining layers.\nContrary to intuition, fine-tuning the layers with smaller output discrepancies\nbefore and after quantization, rather than those with larger discrepancies,\nactually leads to greater improvements in the model's quantization accuracy.\nThis means we better compensate for quantization errors during their\npropagation, rather than addressing them at the point where they occur. The\nproposed PTQAT achieves similar performance to QAT with more efficiency by\nfreezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal\nquantization method that supports various quantization bit widths (4 bits) as\nwell as different model architectures, including CNNs and Transformers. The\nexperimental results on nuScenes across diverse 3D perception tasks, including\nobject detection, semantic segmentation, and occupancy prediction, show that\nour method consistently outperforms QAT-only baselines. Notably, it achieves\n0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains\nin semantic segmentation and occupancy prediction while fine-tuning fewer\nweights.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10557v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10557v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.456,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a hybrid quantization algorithm (PTQAT) for 3D perception tasks, focusing on combining post-training quantization and quantization-aware training to improve model efficiency and accuracy without hardware-specific optimizations. It does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors. The core elements of the paper, such as layer selection for fine-tuning and error propagation in quantization, are unrelated to accelerating training via distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10559",
      "title": "Fake Speech Wild: Detecting Deepfake Speech on Social Media Platform",
      "authors": [
        "Yuankun Xie",
        "Ruibo Fu",
        "Xiaopeng Wang",
        "Zhiyong Wang",
        "Ya Li",
        "Zhengqi Wen",
        "Haonnan Cheng",
        "Long Ye"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid advancement of speech generation technology has led to the\nwidespread proliferation of deepfake speech across social media platforms.\nWhile deepfake audio countermeasures (CMs) achieve promising results on public\ndatasets, their performance degrades significantly in cross-domain scenarios.\nTo advance CMs for real-world deepfake detection, we first propose the Fake\nSpeech Wild (FSW) dataset, which includes 254 hours of real and deepfake audio\nfrom four different media platforms, focusing on social media. As CMs, we\nestablish a benchmark using public datasets and advanced selfsupervised\nlearning (SSL)-based CMs to evaluate current CMs in real-world scenarios. We\nalso assess the effectiveness of data augmentation strategies in enhancing CM\nrobustness for detecting deepfake speech on social media. Finally, by\naugmenting public datasets and incorporating the FSW training set, we\nsignificantly advanced real-world deepfake audio detection performance,\nachieving an average equal error rate (EER) of 3.54% across all evaluation\nsets.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10559v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10559v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.372,
      "datasets_score": 0.419,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on creating a new dataset for deepfake speech detection and benchmarking countermeasures, but it does not involve weak supervision techniques. It relies on verified data from social accounts, manual verification, and public datasets with presumably accurate labels, rather than programmatically generating noisy or imprecise labels for training models.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and curation of the Fake Speech Wild (FSW) dataset, including its collection from social media platforms, segmentation using Voice Activity Detection, and verification processes. It also involves benchmarking and evaluating this dataset alongside public ones for deepfake detection, directly aligning with research on dataset creation, analysis, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of deepfake speech detection on social media by introducing the Fake Speech Wild (FSW) dataset, which comprises 254 hours of real and deepfake audio from four platforms, to evaluate and improve countermeasures (CMs) in cross-domain scenarios. The authors benchmark existing CMs using public datasets and self-supervised learning techniques, assess the effectiveness of data augmentation for robustness, and demonstrate significant enhancements in detection performance, achieving an average equal error rate (EER) of 3.54% through joint training with augmented datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the FSW dataset tailored for Chinese social media and exploring data augmentation to enhance existing deepfake detection methods, addressing limitations in prior work like ITW. While it advances the state-of-the-art in real-world scenarios, it primarily builds on established techniques rather than introducing entirely new problems or architectures.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in deepfake detection by providing a new dataset and improved methods for cross-domain scenarios, potentially leading to more robust applications in social media security. However, its impact may be confined to the subfield of audio deepfake countermeasures rather than broader AI or commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a valuable contribution with a new dataset and practical enhancements to deepfake detection, making it important for researchers focused on AI and sound security. While not essential for all, it provides insights that could guide ongoing efforts in this area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/89dcb7fd65bfbd99f104512939f528b4c3719e08",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 25,
      "average_h_index": 8.0,
      "notable_authors_count": 5,
      "author_h_indexes": [
        {
          "name": "Yuankun Xie",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2038455251"
        },
        {
          "name": "Ruibo Fu",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/3418514"
        },
        {
          "name": "Xiaopeng Wang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2237583864"
        },
        {
          "name": "Zhiyong Wang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2237589153"
        },
        {
          "name": "Ya Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375958782"
        },
        {
          "name": "Zhengqi Wen",
          "h_index": 25,
          "profile_url": "https://www.semanticscholar.org/author/1718662"
        },
        {
          "name": "Haonnan Cheng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2304905343"
        },
        {
          "name": "Long Ye",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2300337196"
        }
      ]
    },
    {
      "id": "2508.10566",
      "title": "HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head\n  Synthesis",
      "authors": [
        "Shiyu Liu",
        "Kui Jiang",
        "Xianming Liu",
        "Hongxun Yao",
        "Xiaocheng Feng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Audio-driven talking head video generation enhances user engagement in\nhuman-computer interaction. However, current methods frequently produce videos\nwith motion blur and lip jitter, primarily due to their reliance on implicit\nmodeling of audio-facial motion correlations--an approach lacking explicit\narticulatory priors (i.e., anatomical guidance for speech-related facial\nmovements). To overcome this limitation, we propose HM-Talker, a novel\nframework for generating high-fidelity, temporally coherent talking heads.\nHM-Talker leverages a hybrid motion representation combining both implicit and\nexplicit motion cues. Explicit cues use Action Units (AUs), anatomically\ndefined facial muscle movements, alongside implicit features to minimize\nphoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement\nModule (CMDM) extracts complementary implicit/explicit motion features while\npredicting AUs directly from audio input aligned to visual cues. To mitigate\nidentity-dependent biases in explicit features and enhance cross-subject\ngeneralization, we introduce the Hybrid Motion Modeling Module (HMMM). This\nmodule dynamically merges randomly paired implicit/explicit features, enforcing\nidentity-agnostic learning. Together, these components enable robust lip\nsynchronization across diverse identities, advancing personalized talking head\nsynthesis. Extensive experiments demonstrate HM-Talker's superiority over\nstate-of-the-art methods in visual quality and lip-sync accuracy.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10566v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10566v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.295,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on audio-driven talking head synthesis using hybrid motion modeling with explicit and implicit cues, such as Action Units and neural representations, to improve facial animation and lip synchronization. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning processes. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10567",
      "title": "SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous\n  Driving",
      "authors": [
        "Philipp Wolters",
        "Johannes Gilg",
        "Torben Teepe",
        "Gerhard Rigoll"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "End-to-end autonomous driving systems promise stronger performance through\nunified optimization of perception, motion forecasting, and planning. However,\nvision-based approaches face fundamental limitations in adverse weather\nconditions, partial occlusions, and precise velocity estimation - critical\nchallenges in safety-sensitive scenarios where accurate motion understanding\nand long-horizon trajectory prediction are essential for collision avoidance.\nTo address these limitations, we propose SpaRC-AD, a query-based end-to-end\ncamera-radar fusion framework for planning-oriented autonomous driving. Through\nsparse 3D feature alignment, and doppler-based velocity estimation, we achieve\nstrong 3D scene representations for refinement of agent anchors, map polylines\nand motion modelling. Our method achieves strong improvements over the\nstate-of-the-art vision-only baselines across multiple autonomous driving\ntasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA),\nonline mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory\nplanning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal\nconsistency on multiple challenging benchmarks, including real-world open-loop\nnuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We\nshow the effectiveness of radar-based fusion in safety-critical scenarios where\naccurate motion understanding and long-horizon trajectory prediction are\nessential for collision avoidance. The source code of all experiments is\navailable at https://phi-wol.github.io/sparcad/",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10567v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10567v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.357,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10568",
      "title": "Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote\n  Sensing Change Detection",
      "authors": [
        "Humza Naveed",
        "Xina Zeng",
        "Mitch Bryson",
        "Nagita Mehrseresht"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Foundational models have achieved significant success in diverse domains of\ncomputer vision. They learn general representations that are easily\ntransferable to tasks not seen during training. One such foundational model is\nSegment anything model (SAM), which can accurately segment objects in images.\nWe propose adapting the SAM encoder via fine-tuning for remote sensing change\ndetection (RSCD) along with spatial-temporal feature enhancement (STFE) and\nmulti-scale decoder fusion (MSDF) to detect changes robustly at multiple\nscales. Additionally, we propose a novel cross-entropy masking (CEM) loss to\nhandle high class imbalance in change detection datasets. Our method\noutperforms state-of-the-art (SOTA) methods on four change detection datasets,\nLevir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement on\na large complex S2Looking dataset. The code is available at:\nhttps://github.com/humza909/SAM-CEM-CD",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10568v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10568v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.298,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.321,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10572",
      "title": "Towards Agentic AI for Multimodal-Guided Video Object Segmentation",
      "authors": [
        "Tuyen Tran",
        "Thao Minh Le",
        "Truyen Tran"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Referring-based Video Object Segmentation is a multimodal problem that\nrequires producing fine-grained segmentation results guided by external cues.\nTraditional approaches to this task typically involve training specialized\nmodels, which come with high computational complexity and manual annotation\neffort. Recent advances in vision-language foundation models open a promising\ndirection toward training-free approaches. Several studies have explored\nleveraging these general-purpose models for fine-grained segmentation,\nachieving performance comparable to that of fully supervised, task-specific\nmodels. However, existing methods rely on fixed pipelines that lack the\nflexibility needed to adapt to the dynamic nature of the task. To address this\nlimitation, we propose Multi-Modal Agent, a novel agentic system designed to\nsolve this task in a more flexible and adaptive manner. Specifically, our\nmethod leverages the reasoning capabilities of large language models (LLMs) to\ngenerate dynamic workflows tailored to each input. This adaptive procedure\niteratively interacts with a set of specialized tools designed for low-level\ntasks across different modalities to identify the target object described by\nthe multimodal cues. Our agentic approach demonstrates clear improvements over\nprior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10572v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10572v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.476,
      "distributed_training_score": 0.329,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on an agentic system using large language models (LLMs) for dynamic workflows in multimodal video object segmentation, involving multi-step reasoning and tool interactions. However, it does not involve diffusion models or adapt the iterative refinement process of diffusion for logical tasks. There is no mention of treating a Chain-of-Thought as a holistic entity for correction via diffusion-like mechanisms, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10576",
      "title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware\n  Responses through Reasoning MLLMs",
      "authors": [
        "Zheng Qin",
        "Ruobing Zheng",
        "Yabing Wang",
        "Tianqi Li",
        "Yi Yuan",
        "Jingdong Chen",
        "Le Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) show immense promise for\nachieving truly human-like interactions, progress is hindered by the lack of\nfine-grained evaluation frameworks for human-centered scenarios, encompassing\nboth the understanding of complex human intentions and the provision of\nempathetic, context-aware responses. Here we introduce HumanSense, a\ncomprehensive benchmark designed to evaluate the human-centered perception and\ninteraction capabilities of MLLMs, with a particular focus on deep\nunderstanding of extended multimodal contexts and the formulation of rational\nfeedback. Our evaluation reveals that leading MLLMs still have considerable\nroom for improvement, particularly for advanced interaction-oriented tasks.\nSupplementing visual input with audio and text information yields substantial\nimprovements, and Omni-modal models show advantages on these tasks.\nFurthermore, we argue that appropriate feedback stems from a contextual\nanalysis of the interlocutor's needs and emotions, with reasoning ability\nserving as the key to unlocking it. Accordingly, we employ a multi-stage,\nmodality-progressive reinforcement learning to enhance the reasoning abilities\nof an Omni model, achieving substantial gains on evaluation results.\nAdditionally, we observe that successful reasoning processes exhibit highly\nconsistent thought patterns. By designing corresponding prompts, we also\nenhance the performance of non-reasoning models in a training-free manner.\nProject page:\n\\textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10576v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10576v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.509,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.495,
      "distributed_training_score": 0.359,
      "datasets_score": 0.413,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper employs a multi-stage, omni-modal reinforcement learning approach to enhance an Omni model's reasoning abilities, which aligns somewhat with RLHF concepts. However, it does not explicitly mention using human-ranked data or a separate reward model trained on human feedback, making it only moderately relevant rather than a direct match.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning and reasoning in Multimodal Large Language Models but does not mention or adapt diffusion models for iterative refinement or chain-of-thought processes, so it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces the HumanSense benchmark, which involves creating, curating, and evaluating a new dataset of 3,882 questions from real-world records for assessing MLLMs in human-centered scenarios, directly aligning with research on datasets, benchmarks, and evaluations.",
      "llm_score_status": "completed",
      "summary": "The paper introduces HumanSense, a comprehensive benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on human-centered tasks, including multimodal perception, contextual understanding, and empathetic response generation in interactive scenarios. It assesses leading MLLMs using 3,882 real-world questions across 15 tests, reveals limitations in current models especially for advanced interactions, demonstrates improvements from combining modalities like audio and text with visual input, and proposes a multi-stage, modality-progressive reinforcement learning method to enhance reasoning abilities, achieving significant gains while also improving non-reasoning models via designed prompts.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new benchmark for fine-grained evaluation of MLLMs in human-centered scenarios, which cleverly combines existing multimodal techniques to address a gap in current frameworks, though it does not introduce an entirely new problem or architecture. This makes it a clever adaptation rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of MLLMs and human-AI interaction, as it provides a new benchmark and methods for improvement that could guide future research. However, its influence may be limited to specific applications in AI evaluation and multimodal processing rather than broader commercial or general AI advancements.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by addressing a key gap in evaluating human-centered AI interactions and proposing practical enhancements, making it essential for researchers in MLLMs and AI ethics. While not revolutionary, its insights and benchmark could significantly inform ongoing developments in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f81c3dd52c1b8a8395f403fa556f39cfca0ece5e",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 4,
      "average_h_index": 2.142857142857143,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zheng Qin",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2267257380"
        },
        {
          "name": "Ruobing Zheng",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2287848575"
        },
        {
          "name": "Yabing Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2304518993"
        },
        {
          "name": "Tianqi Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2288678402"
        },
        {
          "name": "Yi Yuan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374245841"
        },
        {
          "name": "Jingdong Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2358405278"
        },
        {
          "name": "Le Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2296842533"
        }
      ]
    },
    {
      "id": "2508.10582",
      "title": "EvTurb: Event Camera Guided Turbulence Removal",
      "authors": [
        "Yixing Liu",
        "Minggui Teng",
        "Yifei Xia",
        "Peiqi Duan",
        "Boxin Shi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Atmospheric turbulence degrades image quality by introducing blur and\ngeometric tilt distortions, posing significant challenges to downstream\ncomputer vision tasks. Existing single-image and multi-frame methods struggle\nwith the highly ill-posed nature of this problem due to the compositional\ncomplexity of turbulence-induced distortions. To address this, we propose\nEvTurb, an event guided turbulence removal framework that leverages high-speed\nevent streams to decouple blur and tilt effects. EvTurb decouples blur and tilt\neffects by modeling event-based turbulence formation, specifically through a\nnovel two-step event-guided network: event integrals are first employed to\nreduce blur in the coarse outputs. This is followed by employing a variance\nmap, derived from raw event streams, to eliminate the tilt distortion for the\nrefined outputs. Additionally, we present TurbEvent, the first real-captured\ndataset featuring diverse turbulence scenarios. Experimental results\ndemonstrate that EvTurb surpasses state-of-the-art methods while maintaining\ncomputational efficiency.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10582v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10582v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.269,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.303,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10594",
      "title": "FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly\n  Detection",
      "authors": [
        "Yunfeng Zhao",
        "Yixin Liu",
        "Shiyuan Li",
        "Qingfeng Chen",
        "Yu Zheng",
        "Shirui Pan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Graph Anomaly Detection (GAD) aims to identify nodes that deviate from the\nmajority within a graph, playing a crucial role in applications such as social\nnetworks and e-commerce. Despite the current advancements in deep\nlearning-based GAD, existing approaches often suffer from high deployment costs\nand poor scalability due to their complex and resource-intensive training\nprocesses. Surprisingly, our empirical findings suggest that the training phase\nof deep GAD methods, commonly perceived as crucial, may actually contribute\nless to anomaly detection performance than expected. Inspired by this, we\npropose FreeGAD, a novel training-free yet effective GAD method. Specifically,\nit leverages an affinity-gated residual encoder to generate anomaly-aware\nrepresentations. Meanwhile, FreeGAD identifies anchor nodes as pseudo-normal\nand anomalous guides, followed by calculating anomaly scores through\nanchor-guided statistical deviations. Extensive experiments demonstrate that\nFreeGAD achieves superior anomaly detection performance, efficiency, and\nscalability on multiple benchmark datasets from diverse domains, without any\ntraining or iterative optimization.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10594v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10594v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.407,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper introduces FreeGAD, a training-free method for graph anomaly detection, which eliminates the need for any training process. It discusses the inefficiencies of traditional deep learning-based methods but does not address distributed training, parallel computing, or strategies for accelerating model training across multiple nodes. Since the core contribution avoids training entirely, it has no connection to topics involving distributed or parallel training techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10595",
      "title": "On Spectral Properties of Gradient-based Explanation Methods",
      "authors": [
        "Amir Mehrpanah",
        "Erik Englesson",
        "Hossein Azizpour"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Understanding the behavior of deep networks is crucial to increase our\nconfidence in their results. Despite an extensive body of work for explaining\ntheir predictions, researchers have faced reliability issues, which can be\nattributed to insufficient formalism. In our research, we adopt novel\nprobabilistic and spectral perspectives to formally analyze explanation\nmethods. Our study reveals a pervasive spectral bias stemming from the use of\ngradient, and sheds light on some common design choices that have been\ndiscovered experimentally, in particular, the use of squared gradient and input\nperturbation. We further characterize how the choice of perturbation\nhyperparameters in explanation methods, such as SmoothGrad, can lead to\ninconsistent explanations and introduce two remedies based on our proposed\nformalism: (i) a mechanism to determine a standard perturbation scale, and (ii)\nan aggregation method which we call SpectralLens. Finally, we substantiate our\ntheoretical results through quantitative evaluations.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10595v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10595v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.3,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines spectral properties of gradient-based explanation methods for deep networks, focusing on probabilistic and spectral analysis to address inconsistencies in explanations. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10599",
      "title": "MSRS: Adaptive Multi-Subspace Representation Steering for Attribute\n  Alignment in Large Language Models",
      "authors": [
        "Xinyan Jiang",
        "Lin Zhang",
        "Jiayi Zhang",
        "Qingsong Yang",
        "Guimin Hu",
        "Di Wang",
        "Lijie Hu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Activation steering offers a promising approach to controlling the behavior\nof Large Language Models by directly manipulating their internal activations.\nHowever, most existing methods struggle to jointly steer multiple attributes,\noften resulting in interference and undesirable trade-offs. To address this\nchallenge, we propose Multi-Subspace Representation Steering (MSRS), a novel\nframework for effective multi-attribute steering via subspace representation\nfine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal\nsubspaces to each attribute, isolating their influence within the model's\nrepresentation space. MSRS also incorporates a hybrid subspace composition\nstrategy: it combines attribute-specific subspaces for unique steering\ndirections with a shared subspace for common steering directions. A dynamic\nweighting function learns to efficiently integrate these components for precise\ncontrol. During inference, MSRS introduces a token-level steering mechanism\nthat dynamically identifies and intervenes on the most semantically relevant\ntokens, enabling fine-grained behavioral modulation. Experimental results show\nthat MSRS significantly reduces attribute conflicts, surpasses existing methods\nacross a range of attributes, and generalizes effectively to diverse downstream\ntasks.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10599v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10599v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.364,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces MSRS, a method for steering large language models by manipulating internal activations and using subspace representation fine-tuning to handle multiple attributes. It does not involve human feedback, training a reward model, or using reinforcement learning to fine-tune the model based on human preferences. Instead, it focuses on post-training activation adjustments, which is unrelated to the core elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10600",
      "title": "Towards Powerful and Practical Patch Attacks for 2D Object Detection in\n  Autonomous Driving",
      "authors": [
        "Yuxin Cao",
        "Yedi Zhang",
        "Wentao He",
        "Yifan Liao",
        "Yan Xiao",
        "Chang Li",
        "Zhiyong Huang",
        "Jin Song Dong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Learning-based autonomous driving systems remain critically vulnerable to\nadversarial patches, posing serious safety and security risks in their\nreal-world deployment. Black-box attacks, notable for their high attack success\nrate without model knowledge, are especially concerning, with their\ntransferability extensively studied to reduce computational costs compared to\nquery-based attacks. Previous transferability-based black-box attacks typically\nadopt mean Average Precision (mAP) as the evaluation metric and design training\nloss accordingly. However, due to the presence of multiple detected bounding\nboxes and the relatively lenient Intersection over Union (IoU) thresholds, the\nattack effectiveness of these approaches is often overestimated, resulting in\nreduced success rates in practical attacking scenarios. Furthermore, patches\ntrained on low-resolution data often fail to maintain effectiveness on\nhigh-resolution images, limiting their transferability to autonomous driving\ndatasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch\nAttack framework for 2D object detection in autonomous driving, specifically\noptimized for high-resolution datasets. First, we introduce a novel metric,\nPractical Attack Success Rate (PASR), to more accurately quantify attack\neffectiveness with greater relevance for pedestrian safety. Second, we present\na tailored Localization-Confidence Suppression Loss (LCSL) to improve attack\ntransferability under PASR. Finally, to maintain the transferability for\nhigh-resolution datasets, we further incorporate the Probabilistic\nScale-Preserving Padding (PSPP) into the patch attack pipeline as a data\npreprocessing step. Extensive experiments show that P$^3$A outperforms\nstate-of-the-art attacks on unseen models and unseen high-resolution datasets,\nboth under the proposed practical IoU-based evaluation metric and the previous\nmAP-based metrics.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10600v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10600v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.337,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10605",
      "title": "DIVA-VQA: Detecting Inter-frame Variations in UGC Video Quality",
      "authors": [
        "Xinyi Wang",
        "Angeliki Katsenou",
        "David Bull"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "The rapid growth of user-generated (video) content (UGC) has driven increased\ndemand for research on no-reference (NR) perceptual video quality assessment\n(VQA). NR-VQA is a key component for large-scale video quality monitoring in\nsocial media and streaming applications where a pristine reference is not\navailable. This paper proposes a novel NR-VQA model based on spatio-temporal\nfragmentation driven by inter-frame variations. By leveraging these inter-frame\ndifferences, the model progressively analyses quality-sensitive regions at\nmultiple levels: frames, patches, and fragmented frames. It integrates frames,\nfragmented residuals, and fragmented frames aligned with residuals to\neffectively capture global and local information. The model extracts both 2D\nand 3D features in order to characterize these spatio-temporal variations.\nExperiments conducted on five UGC datasets and against state-of-the-art models\nranked our proposed method among the top 2 in terms of average rank correlation\n(DIVA-VQA-L: 0.898 and DIVA-VQA-B: 0.886). The improved performance is offered\nat a low runtime complexity, with DIVA-VQA-B ranked top and DIVA-VQA-L third on\naverage compared to the fastest existing NR-VQA method. Code and models are\npublicly available at: https://github.com/xinyiW915/DIVA-VQA.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10605v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10605v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.334,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a no-reference video quality assessment model (DIVA-VQA) that analyzes inter-frame variations using spatio-temporal features, CNNs, and Transformers. It does not involve diffusion models, iterative refinement for logical tasks, or any form of Chain-of-Thought reasoning, focusing instead on perceptual video quality metrics.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10616",
      "title": "Fourier-Guided Attention Upsampling for Image Super-Resolution",
      "authors": [
        "Daejune Choi",
        "Youchan No",
        "Jinhyung Lee",
        "Duksu Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose Frequency-Guided Attention (FGA), a lightweight upsampling module\nfor single image super-resolution. Conventional upsamplers, such as Sub-Pixel\nConvolution, are efficient but frequently fail to reconstruct high-frequency\ndetails and introduce aliasing artifacts. FGA addresses these issues by\nintegrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for\npositional frequency encoding, (2) a cross-resolution Correlation Attention\nLayer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for\nspectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently\nenhances performance across five diverse super-resolution backbones in both\nlightweight and full-capacity scenarios. Experimental results demonstrate\naverage PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by\nup to 29%, particularly evident on texture-rich datasets. Visual and spectral\nevaluations confirm FGA's effectiveness in reducing aliasing and preserving\nfine details, establishing it as a practical, scalable alternative to\ntraditional upsampling methods.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10616v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10616v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.332,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10617",
      "title": "FIND-Net -- Fourier-Integrated Network with Dictionary Kernels for Metal\n  Artifact Reduction",
      "authors": [
        "Farid Tasharofi",
        "Fuxin Fan",
        "Melika Qahqaie",
        "Mareike Thies",
        "Andreas Maier"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Metal artifacts, caused by high-density metallic implants in computed\ntomography (CT) imaging, severely degrade image quality, complicating diagnosis\nand treatment planning. While existing deep learning algorithms have achieved\nnotable success in Metal Artifact Reduction (MAR), they often struggle to\nsuppress artifacts while preserving structural details. To address this\nchallenge, we propose FIND-Net (Fourier-Integrated Network with Dictionary\nKernels), a novel MAR framework that integrates frequency and spatial domain\nprocessing to achieve superior artifact suppression and structural\npreservation. FIND-Net incorporates Fast Fourier Convolution (FFC) layers and\ntrainable Gaussian filtering, treating MAR as a hybrid task operating in both\nspatial and frequency domains. This approach enhances global contextual\nunderstanding and frequency selectivity, effectively reducing artifacts while\nmaintaining anatomical structures. Experiments on synthetic datasets show that\nFIND-Net achieves statistically significant improvements over state-of-the-art\nMAR methods, with a 3.07% MAE reduction, 0.18% SSIM increase, and 0.90% PSNR\nimprovement, confirming robustness across varying artifact complexities.\nFurthermore, evaluations on real-world clinical CT scans confirm FIND-Net's\nability to minimize modifications to clean anatomical regions while effectively\nsuppressing metal-induced distortions. These findings highlight FIND-Net's\npotential for advancing MAR performance, offering superior structural\npreservation and improved clinical applicability. Code is available at\nhttps://github.com/Farid-Tasharofi/FIND-Net",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10617v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10617v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.341,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10631",
      "title": "Increasing the Utility of Synthetic Images through Chamfer Guidance",
      "authors": [
        "Nicola Dall'Asen",
        "Xiaofeng Zhang",
        "Reyhane Askari Hemmat",
        "Melissa Hall",
        "Jakob Verbeek",
        "Adriana Romero-Soriano",
        "Michal Drozdzal"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Conditional image generative models hold considerable promise to produce\ninfinite amounts of synthetic training data. Yet, recent progress in generation\nquality has come at the expense of generation diversity, limiting the utility\nof these models as a source of synthetic training data. Although guidance-based\napproaches have been introduced to improve the utility of generated data by\nfocusing on quality or diversity, the (implicit or explicit) utility functions\noftentimes disregard the potential distribution shift between synthetic and\nreal data. In this work, we introduce Chamfer Guidance: a training-free\nguidance approach which leverages a handful of real exemplar images to\ncharacterize the quality and diversity of synthetic data. We show that by\nleveraging the proposed Chamfer Guidance, we can boost the diversity of the\ngenerations w.r.t. a dataset of real images while maintaining or improving the\ngeneration quality on ImageNet-1k and standard geo-diversity benchmarks. Our\napproach achieves state-of-the-art few-shot performance with as little as 2\nexemplar real images, obtaining 96.4\\% in terms of precision, and 86.4\\% in\nterms of distributional coverage, which increase to 97.5\\% and 92.7\\%,\nrespectively, when using 32 real images. We showcase the benefits of the\nChamfer Guidance generation by training downstream image classifiers on\nsynthetic data, achieving accuracy boost of up to 15\\% for in-distribution over\nthe baselines, and up to 16\\% in out-of-distribution. Furthermore, our approach\ndoes not require using the unconditional model, and thus obtains a 31\\%\nreduction in FLOPs w.r.t. classifier-free-guidance-based approaches at sampling\ntime.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10631v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10631v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.365,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on generating synthetic images to reduce distribution shifts for training downstream models, which could indirectly support weak supervision by providing noisy or programmatically generated data alternatives to hand-labeled data. However, its main contribution is in improving image generation techniques, not in the core aspects of weak supervision like using labeling functions or heuristics for label creation.",
      "diffusion_reasoning_justification": "The paper uses diffusion models for image generation and iterative refinement of synthetic data, but it does not involve adapting diffusion processes for multi-step logical reasoning, chain-of-thought correction, or solving complex logical tasks. It is solely focused on visual data generation, lacking any component of reasoning or holistic logical refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10635",
      "title": "ChatENV: An Interactive Vision-Language Model for Sensor-Guided\n  Environmental Monitoring and Scenario Simulation",
      "authors": [
        "Hosam Elgendy",
        "Ahmed Sharshar",
        "Ahmed Aboeitta",
        "Mohsen Guizani"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Understanding environmental changes from aerial imagery is vital for climate\nresilience, urban planning, and ecosystem monitoring. Yet, current vision\nlanguage models (VLMs) overlook causal signals from environmental sensors, rely\non single-source captions prone to stylistic bias, and lack interactive\nscenario-based reasoning. We present ChatENV, the first interactive VLM that\njointly reasons over satellite image pairs and real-world sensor data. Our\nframework: (i) creates a 177k-image dataset forming 152k temporal pairs across\n62 land-use classes in 197 countries with rich sensor metadata (e.g.,\ntemperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 for\nstylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL using\nefficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENV\nachieves strong performance in temporal and \"what-if\" reasoning (e.g., BERT-F1\n0.903) and rivals or outperforms state-of-the-art temporal models, while\nsupporting interactive scenario-based analysis. This positions ChatENV as a\npowerful tool for grounded, sensor-aware environmental monitoring.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10635v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10635v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.31,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces ChatENV, a Vision-Language Model fine-tuned with LoRA for environmental monitoring and interactive reasoning over images and sensor data. It focuses on tasks like change detection and \"what-if\" scenarios but does not involve diffusion-based methods, iterative refinement processes, or any adaptation of diffusion models for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10637",
      "title": "Processing and acquisition traces in visual encoders: What does CLIP\n  know about your camera?",
      "authors": [
        "Ryan Ramos",
        "Vladan Stojnić",
        "Giorgos Kordopatis-Zilos",
        "Yuta Nakashima",
        "Giorgos Tolias",
        "Noa Garcia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Prior work has analyzed the robustness of visual encoders to image\ntransformations and corruptions, particularly in cases where such alterations\nare not seen during training. When this occurs, they introduce a form of\ndistribution shift at test time, often leading to performance degradation. The\nprimary focus has been on severe corruptions that, when applied aggressively,\ndistort useful signals necessary for accurate semantic predictions.\n  We take a different perspective by analyzing parameters of the image\nacquisition process and transformations that may be subtle or even\nimperceptible to the human eye. We find that such parameters are systematically\nencoded in the learned visual representations and can be easily recovered. More\nstrikingly, their presence can have a profound impact, either positively or\nnegatively, on semantic predictions. This effect depends on whether there is a\nstrong correlation or anti-correlation between semantic labels and these\nacquisition-based or processing-based labels. Our code and data are available\nat: https://github.com/ryan-caesar-ramos/visual-encoder-traces",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10637v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10637v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.39,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on analyzing the robustness of pre-trained visual encoders to image acquisition and processing parameters, rather than methods for training models. It does not discuss programmatically generating labels, using noisy supervision, or any form of weak supervision in the training process. Therefore, it has no direct connection to weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper examines visual encoders and their sensitivity to image parameters, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It is centered on computer vision representation analysis, not adapting diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10643",
      "title": "Lameness detection in dairy cows using pose estimation and bidirectional\n  LSTMs",
      "authors": [
        "Helena Russello",
        "Rik van der Tol",
        "Eldert J. van Henten",
        "Gert Kootstra"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This study presents a lameness detection approach that combines pose\nestimation and Bidirectional Long-Short-Term Memory (BLSTM) neural networks.\nCombining pose-estimation and BLSTMs classifier offers the following\nadvantages: markerless pose-estimation, elimination of manual feature\nengineering by learning temporal motion features from the keypoint\ntrajectories, and working with short sequences and small training datasets.\nMotion sequences of nine keypoints (located on the cows' hooves, head and back)\nwere extracted from videos of walking cows with the T-LEAP pose estimation\nmodel. The trajectories of the keypoints were then used as an input to a BLSTM\nclassifier that was trained to perform binary lameness classification. Our\nmethod significantly outperformed an established method that relied on\nmanually-designed locomotion features: our best architecture achieved a\nclassification accuracy of 85%, against 80% accuracy for the feature-based\napproach. Furthermore, we showed that our BLSTM classifier could detect\nlameness with as little as one second of video data.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10643v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10643v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.299,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10645",
      "title": "SemPT: Semantic Prompt Tuning for Vision-Language Models",
      "authors": [
        "Xiao Shi",
        "Yangjun Ou",
        "Zhenzhong Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual transfer learning for unseen categories presents an active research\ntopic yet a challenging task, due to the inherent conflict between preserving\ncategory-specific representations and acquiring transferable knowledge.\nVision-Language Models (VLMs) pre-trained on large amounts of image-text pairs\noffer a promising solution. However, existing prompt tuning methods rely on\nsparse category labels or disparate LLM-generated descriptions, which fragment\nknowledge representation and hinder transferability. To address this\nlimitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that\ntackles the generalization challenge by leveraging shared attribute-level\nknowledge across categories. Specifically, SemPT adopts a two-step prompting\nstrategy to guide LLM in extracting shared visual attributes and generating\nattribute-level descriptions, capturing transferable semantic cues beyond\nlabels while ensuring coherent structure. Then, visually guided weighting is\napplied to the embeddings of attribute-level descriptions to reduce noise from\nirrelevant attributes and enhance the text embeddings. Additionally, image\nembeddings are jointly aligned with both label and attribute-enhanced text\nembeddings, balancing discrimination for seen categories and transferability to\nunseen ones. Considering the availability of category exposure, our inference\ndynamically selects between standard label embeddings for seen categories and\nattribute-enhanced embeddings for unseen ones to ensure effective adaptation.\nExtensive experiments on 15 benchmark datasets demonstrate that SemPT achieves\nstate-of-the-art performance across various settings, including base-to-novel\ngeneralization, cross-dataset transfer, cross-domain transfer, and few-shot\nlearning.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10645v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10645v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.385,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper employs LLMs to generate attribute-level descriptions and shared visual attributes from category names, which programmatically creates training signals without relying on hand-labeled data, aligning with weak supervision concepts. However, weak supervision is not the primary focus; the main contribution is on prompt tuning for VLMs, making it moderately relevant rather than central.",
      "diffusion_reasoning_justification": "The paper focuses on semantic prompt tuning for vision-language models, involving attribute extraction and embedding alignment, with no mention or use of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Semantic Prompt Tuning (SemPT), a framework aimed at improving the transfer learning capabilities of Vision-Language Models (VLMs) by leveraging shared attribute-level knowledge to address the conflict between preserving category-specific representations and enabling generalization to unseen categories. It employs a two-step prompting strategy to extract shared visual attributes and generate attribute-level descriptions, applies visually guided weighting to enhance text embeddings, and uses a dual-branch supervision scheme to align image embeddings with both label and attribute-enhanced text embeddings, while adapting inference based on category exposure. Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves state-of-the-art performance in base-to-novel generalization, cross-dataset transfer, cross-domain transfer, and few-shot learning, showcasing its effectiveness and versatility.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique with the SemPT framework, which innovatively uses a two-step prompting strategy to model shared attributes for better transferability in VLMs, significantly advancing beyond existing prompt tuning methods.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in VLMs and transfer learning due to its state-of-the-art results across diverse benchmarks and its ability to enhance generalization.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality and innovative contribution to computer vision and AI, making it essential for researchers focused on VLMs and transfer learning to be aware of its advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9e083d3e19d41de8c96d15a181efa674e93425d7",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 7,
      "average_h_index": 2.3333333333333335,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Xiao Shi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2348787169"
        },
        {
          "name": "Yangjun Ou",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2057924944"
        },
        {
          "name": "Zhenzhong Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376038148"
        }
      ]
    },
    {
      "id": "2508.10646",
      "title": "SPHENIC: Topology-Informed Multi-View Clustering for Spatial\n  Transcriptomics",
      "authors": [
        "Chenkai Guo",
        "Yikai Zhu",
        "Jing Yangum",
        "Renxiang Guan",
        "Por Lip Yee",
        "Guangdun Peng",
        "Dayu Hu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "By incorporating spatial location information, spatial-transcriptomics\nclustering yields more comprehensive insights into cell subpopulation\nidentification. Despite recent progress, existing methods have at least two\nlimitations: (i) topological learning typically considers only representations\nof individual cells or their interaction graphs; however, spatial\ntranscriptomic profiles are often noisy, making these approaches vulnerable to\nlow-quality topological signals, and (ii) insufficient modeling of spatial\nneighborhood information leads to low-quality spatial embeddings. To address\nthese limitations, we propose SPHENIC, a novel Spatial Persistent Homology\nEnhanced Neighborhood Integrative Clustering method. Specifically, SPHENIC\nincorporates invariant topological features into the clustering network to\nachieve stable representation learning. Additionally, to construct high-quality\nspatial embeddings that reflect the true cellular distribution, we design the\nSpatial Constraint and Distribution Optimization Module (SCDOM). This module\nincreases the similarity between a cell's embedding and those of its spatial\nneighbors, decreases similarity with non-neighboring cells, and thereby\nproduces clustering-friendly spatial embeddings. Extensive experiments on 14\nbenchmark spatial transcriptomic slices demonstrate that SPHENIC achieves\nsuperior performance on the spatial clustering task, outperforming existing\nstate-of-the-art methods by 3.31%-6.54% over the best alternative.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10646v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10646v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.343,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10649",
      "title": "Geospatial Diffusion for Land Cover Imperviousness Change Forecasting",
      "authors": [
        "Debvrat Varshney",
        "Vibhas Vats",
        "Bhartendu Pandey",
        "Christa Brelsford",
        "Philipe Dias"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Land cover, both present and future, has a significant effect on several\nimportant Earth system processes. For example, impervious surfaces heat up and\nspeed up surface water runoff and reduce groundwater infiltration, with\nconcomitant effects on regional hydrology and flood risk. While regional Earth\nSystem models have increasing skill at forecasting hydrologic and atmospheric\nprocesses at high resolution in future climate scenarios, our ability to\nforecast land-use and land-cover change (LULC), a critical input to risk and\nconsequences assessment for these scenarios, has lagged behind. In this paper,\nwe propose a new paradigm exploiting Generative AI (GenAI) for land cover\nchange forecasting by framing LULC forecasting as a data synthesis problem\nconditioned on historical and auxiliary data-sources. We discuss desirable\nproperties of generative models that fundament our research premise, and\ndemonstrate the feasibility of our methodology through experiments on\nimperviousness forecasting using historical data covering the entire\nconterminous United States. Specifically, we train a diffusion model for\ndecadal forecasting of imperviousness and compare its performance to a baseline\nthat assumes no change at all. Evaluation across 12 metropolitan areas for a\nyear held-out during training indicate that for average resolutions $\\geq\n0.7\\times0.7km^2$ our model yields MAE lower than such a baseline. This finding\ncorroborates that such a generative model can capture spatiotemporal patterns\nfrom historical data that are significant for projecting future change.\nFinally, we discuss future research to incorporate auxiliary information on\nphysical properties about the Earth, as well as supporting simulation of\ndifferent scenarios by means of driver variables.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10649v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10649v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.56,
      "distributed_training_score": 0.338,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for generating and synthesizing future land cover data based on historical patterns, which involves iterative refinement processes typical of diffusion models. However, it focuses on geospatial data forecasting and does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks as defined in the topic. Thus, while diffusion models are employed, the application is not aligned with the topic's emphasis on reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10655",
      "title": "Serial Over Parallel: Learning Continual Unification for Multi-Modal\n  Visual Object Tracking and Benchmarking",
      "authors": [
        "Zhangyong Tang",
        "Tianyang Xu",
        "Xuefeng Zhu",
        "Chunyang Cheng",
        "Tao Zhou",
        "Xiaojun Wu",
        "Josef Kittler"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws\nincreasing attention due to the complementary nature of different modalities in\nbuilding robust tracking systems. Existing practices mix all data sensor types\nin a single training procedure, structuring a parallel paradigm from the\ndata-centric perspective and aiming for a global optimum on the joint\ndistribution of the involved tasks. However, the absence of a unified benchmark\nwhere all types of data coexist forces evaluations on separated benchmarks,\ncausing \\textit{inconsistency} between training and testing, thus leading to\nperformance \\textit{degradation}. To address these issues, this work advances\nin two aspects: \\ding{182} A unified benchmark, coined as UniBench300, is\nintroduced to bridge the inconsistency by incorporating multiple task data,\nreducing inference passes from three to one and cutting time consumption by\n27\\%. \\ding{183} The unification process is reformulated in a serial format,\nprogressively integrating new tasks. In this way, the performance degradation\ncan be specified as knowledge forgetting of previous tasks, which naturally\naligns with the philosophy of continual learning (CL), motivating further\nexploration of injecting CL into the unification process. Extensive experiments\nconducted on two baselines and four benchmarks demonstrate the significance of\nUniBench300 and the superiority of CL in supporting a stable unification\nprocess. Moreover, while conducting dedicated analyses, the performance\ndegradation is found to be negatively correlated with network capacity.\nAdditionally, modality discrepancies contribute to varying degradation levels\nacross tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for\nfuture multi-modal vision research. Source codes and the proposed benchmark is\navailable at \\textit{https://github.com/Zhangyong-Tang/UniBench300}.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10655v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10655v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.384,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10666",
      "title": "Deep Learning in Classical and Quantum Physics",
      "authors": [
        "Timothy Heightman",
        "Marcin Płodzień"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)",
        "cs.NE (Neural and Evolutionary Computing)"
      ],
      "abstract": "Scientific progress is tightly coupled to the emergence of new research\ntools. Today, machine learning (ML)-especially deep learning (DL)-has become a\ntransformative instrument for quantum science and technology. Owing to the\nintrinsic complexity of quantum systems, DL enables efficient exploration of\nlarge parameter spaces, extraction of patterns from experimental data, and\ndata-driven guidance for research directions. These capabilities already\nsupport tasks such as refining quantum control protocols and accelerating the\ndiscovery of materials with targeted quantum properties, making ML/DL literacy\nan essential skill for the next generation of quantum scientists. At the same\ntime, DL's power brings risks: models can overfit noisy data, obscure causal\nstructure, and yield results with limited physical interpretability.\nRecognizing these limitations and deploying mitigation strategies is crucial\nfor scientific rigor. These lecture notes provide a comprehensive,\ngraduate-level introduction to DL for quantum applications, combining\nconceptual exposition with hands-on examples. Organized as a progressive\nsequence, they aim to equip readers to decide when and how to apply DL\neffectively, to understand its practical constraints, and to adapt AI methods\nresponsibly to problems across quantum physics, chemistry, and engineering.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10666v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10666v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.397,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10667",
      "title": "AddressVLM: Cross-view Alignment Tuning for Image Address Localization\n  using Large Vision-Language Models",
      "authors": [
        "Shixiong Xu",
        "Chenghao Zhang",
        "Lubin Fan",
        "Yuan Zhou",
        "Bin Fan",
        "Shiming Xiang",
        "Gaofeng Meng",
        "Jieping Ye"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large visual language models (LVLMs) have demonstrated impressive performance\nin coarse-grained geo-localization at the country or city level, but they\nstruggle with fine-grained street-level localization within urban areas. In\nthis paper, we explore integrating city-wide address localization capabilities\ninto LVLMs, facilitating flexible address-related question answering using\nstreet-view images. A key challenge is that the street-view visual\nquestion-and-answer (VQA) data provides only microscopic visual cues, leading\nto subpar performance in fine-tuned models. To tackle this issue, we\nincorporate perspective-invariant satellite images as macro cues and propose\ncross-view alignment tuning including a satellite-view and street-view image\ngrafting mechanism, along with an automatic label generation mechanism. Then\nLVLM's global understanding of street distribution is enhanced through\ncross-view matching. Our proposed model, named AddressVLM, consists of\ntwo-stage training protocols: cross-view alignment tuning and address\nlocalization tuning. Furthermore, we have constructed two street-view VQA\ndatasets based on image address localization datasets from Pittsburgh and San\nFrancisco. Qualitative and quantitative evaluations demonstrate that AddressVLM\noutperforms counterpart LVLMs by over 9% and 12% in average address\nlocalization accuracy on these two datasets, respectively.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10667v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10667v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.39,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily addresses cross-view alignment tuning for image address localization using Large Vision-Language Models, focusing on integrating satellite and street-view images for better geo-localization. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10669",
      "title": "STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in\n  Conversational Recommendation",
      "authors": [
        "Zhenye Yang",
        "Jinpeng Chen",
        "Huan Li",
        "Xiongnan Jin",
        "Xuanyang Li",
        "Junwei Zhang",
        "Hongbo Gao",
        "Kaimin Wei",
        "Senzhang Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Conversational recommender systems (CRSs) aim to proactively capture user\npreferences through natural language dialogue and recommend high-quality items.\nTo achieve this, CRS gathers user preferences via a dialog module and builds\nuser profiles through a recommendation module to generate appropriate\nrecommendations. However, existing CRS faces challenges in capturing the deep\nsemantics of user preferences and dialogue context. In particular, the\nefficient integration of external knowledge graph (KG) information into\ndialogue generation and recommendation remains a pressing issue. Traditional\napproaches typically combine KG information directly with dialogue content,\nwhich often struggles with complex semantic relationships, resulting in\nrecommendations that may not align with user expectations.\n  To address these challenges, we introduce STEP, a conversational recommender\ncentered on pre-trained language models that combines curriculum-guided\ncontext-knowledge fusion with lightweight task-specific prompt tuning. At its\nheart, an F-Former progressively aligns the dialogue context with\nknowledge-graph entities through a three-stage curriculum, thus resolving\nfine-grained semantic mismatches. The fused representation is then injected\ninto the frozen language model via two minimal yet adaptive prefix prompts: a\nconversation prefix that steers response generation toward user intent and a\nrecommendation prefix that biases item ranking toward knowledge-consistent\ncandidates. This dual-prompt scheme allows the model to share cross-task\nsemantics while respecting the distinct objectives of dialogue and\nrecommendation. Experimental results show that STEP outperforms mainstream\nmethods in the precision of recommendation and dialogue quality in two public\ndatasets.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10669v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10669v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.319,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces STEP, a conversational recommender system using curriculum learning and prompt tuning to fuse dialogue context with knowledge graphs. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning on a 'Chain-of-Thought'. The core contributions are in semantic alignment and recommendation, with no components related to diffusion-based techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10672",
      "title": "Hybrid Generative Fusion for Efficient and Privacy-Preserving Face\n  Recognition Dataset Generation",
      "authors": [
        "Feiran Li",
        "Qianqian Xu",
        "Shilong Bao",
        "Boyu Han",
        "Zhiyong Yang",
        "Qingming Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we present our approach to the DataCV ICCV Challenge, which\ncenters on building a high-quality face dataset to train a face recognition\nmodel. The constructed dataset must not contain identities overlapping with any\nexisting public face datasets. To handle this challenge, we begin with a\nthorough cleaning of the baseline HSFace dataset, identifying and removing\nmislabeled or inconsistent identities through a Mixture-of-Experts (MoE)\nstrategy combining face embedding clustering and GPT-4o-assisted verification.\nWe retain the largest consistent identity cluster and apply data augmentation\nup to a fixed number of images per identity. To further diversify the dataset,\nwe generate synthetic identities using Stable Diffusion with prompt\nengineering. As diffusion models are computationally intensive, we generate\nonly one reference image per identity and efficiently expand it using Vec2Face,\nwhich rapidly produces 49 identity-consistent variants. This hybrid approach\nfuses GAN-based and diffusion-based samples, enabling efficient construction of\na diverse and high-quality dataset. To address the high visual similarity among\nsynthetic identities, we adopt a curriculum learning strategy by placing them\nearly in the training schedule, allowing the model to progress from easier to\nharder samples. Our final dataset contains 50 images per identity, and all\nnewly generated identities are checked with mainstream face datasets to ensure\nno identity leakage. Our method achieves \\textbf{1st place} in the competition,\nand experimental results show that our dataset improves model performance\nacross 10K, 20K, and 100K identity scales. Code is available at\nhttps://github.com/Ferry-Li/datacv_fr.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10672v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10672v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.376,
      "datasets_score": 0.418,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and curation of a new, high-quality face recognition dataset using techniques like data cleaning, synthetic identity generation with Stable Diffusion and Vec2Face, and verification to ensure no identity overlap. This directly aligns with research on dataset creation, curation methodologies, and evaluation, as it introduces a new dataset, analyzes its quality through cleaning and augmentation, and benchmarks its effectiveness in improving face recognition model performance across different scales.",
      "llm_score_status": "completed",
      "summary": "This paper presents a hybrid approach for generating a high-quality, privacy-preserving face recognition dataset by first cleaning the HSFace dataset using a Mixture-of-Experts strategy involving face embedding clustering and GPT-4o verification to retain consistent identities, then augmenting them and generating new synthetic identities with Stable Diffusion for reference images and Vec2Face for variants to ensure diversity and consistency. The methodology addresses challenges like identity leakage and computational cost by employing curriculum learning to structure training, resulting in a dataset that achieved 1st place in the DataCV ICCV Challenge and improved face recognition model performance across 10K, 20K, and 100K identity scales.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper combines existing techniques like Mixture-of-Experts, Stable Diffusion, and curriculum learning in a novel way to address privacy-preserving dataset generation, representing a clever improvement on known methods rather than a completely new architecture. While it advances the field by solving a specific challenge effectively, it does not introduce a fundamentally new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in synthetic dataset generation for face recognition, as demonstrated by its competition success, but its applicability may remain confined to specific subfields like privacy-focused AI. It provides practical insights that could be built upon, though not broadly transformative across all areas of computer vision.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative methods for dataset creation that address real-world privacy concerns, making it valuable for researchers in computer vision and AI ethics. However, its focus on a specific challenge means it's not essential for all readers beyond those directly involved in face recognition or synthetic data generation.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8ad63d14dc6cba04b49e5ecaf695e642fb600e9a",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 25,
      "average_h_index": 9.833333333333334,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Feiran Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2301478538"
        },
        {
          "name": "Qianqian Xu",
          "h_index": 25,
          "profile_url": "https://www.semanticscholar.org/author/34679664"
        },
        {
          "name": "Shilong Bao",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/3172846"
        },
        {
          "name": "Boyu Han",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2301181102"
        },
        {
          "name": "Zhiyong Yang",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/16546250"
        },
        {
          "name": "Qingming Huang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2279863252"
        }
      ]
    },
    {
      "id": "2508.10678",
      "title": "HyperTea: A Hypergraph-based Temporal Enhancement and Alignment Network\n  for Moving Infrared Small Target Detection",
      "authors": [
        "Zhaoyuan Qi",
        "Weihua Gao",
        "Wenlong Niu",
        "Jie Tang",
        "Yun Li",
        "Xiaodong Peng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In practical application scenarios, moving infrared small target detection\n(MIRSTD) remains highly challenging due to the target's small size, weak\nintensity, and complex motion pattern. Existing methods typically only model\nlow-order correlations between feature nodes and perform feature extraction and\nenhancement within a single temporal scale. Although hypergraphs have been\nwidely used for high-order correlation learning, they have received limited\nattention in MIRSTD. To explore the potential of hypergraphs and enhance\nmulti-timescale feature representation, we propose HyperTea, which integrates\nglobal and local temporal perspectives to effectively model high-order\nspatiotemporal correlations of features. HyperTea consists of three modules:\nthe global temporal enhancement module (GTEM) realizes global temporal context\nenhancement through semantic aggregation and propagation; the local temporal\nenhancement module (LTEM) is designed to capture local motion patterns between\nadjacent frames and then enhance local temporal context; additionally, we\nfurther develop a temporal alignment module (TAM) to address potential\ncross-scale feature misalignment. To our best knowledge, HyperTea is the first\nwork to integrate convolutional neural networks (CNNs), recurrent neural\nnetworks (RNNs), and hypergraph neural networks (HGNNs) for MIRSTD,\nsignificantly improving detection performance. Experiments on DAUB and IRDST\ndemonstrate its state-of-the-art (SOTA) performance. Our source codes are\navailable at https://github.com/Lurenjia-LRJ/HyperTea.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10678v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10678v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.36,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10680",
      "title": "Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural\n  Representation for Robust Fetal T2 Mapping",
      "authors": [
        "Busra Bulut",
        "Maik Dannecker",
        "Thomas Sanchez",
        "Sara Neves Silva",
        "Vladyslav Zalevskyi",
        "Steven Jia",
        "Jean-Baptiste Ledoux",
        "Guillaume Auzias",
        "François Rousseau",
        "Jana Hutter",
        "Daniel Rueckert",
        "Meritxell Bach Cuadra"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "T2 mapping in fetal brain MRI has the potential to improve characterization\nof the developing brain, especially at mid-field (0.55T), where T2 decay is\nslower. However, this is challenging as fetal MRI acquisition relies on\nmultiple motion-corrupted stacks of thick slices, requiring slice-to-volume\nreconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently,\nT2 mapping involves repeated acquisitions of these stacks at each echo time\n(TE), leading to long scan times and high sensitivity to motion. We tackle this\nchallenge with a method that jointly reconstructs data across TEs, addressing\nsevere motion. Our approach combines implicit neural representations with a\nphysics-informed regularization that models T2 decay, enabling information\nsharing across TEs while preserving anatomical and quantitative T2 fidelity. We\ndemonstrate state-of-the-art performance on simulated fetal brain and in vivo\nadult datasets with fetal-like motion. We also present the first in vivo fetal\nT2 mapping results at 0.55T. Our study shows potential for reducing the number\nof stacks per TE in T2 mapping by leveraging anatomical redundancy.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10680v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10680v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.342,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10681",
      "title": "IADGPT: Unified LVLM for Few-Shot Industrial Anomaly Detection,\n  Localization, and Reasoning via In-Context Learning",
      "authors": [
        "Mengyang Zhao",
        "Teng Fu",
        "Haiyang Yu",
        "Ke Niu",
        "Bin Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Few-Shot Industrial Anomaly Detection (FS-IAD) has important applications in\nautomating industrial quality inspection. Recently, some FS-IAD methods based\non Large Vision-Language Models (LVLMs) have been proposed with some\nachievements through prompt learning or fine-tuning. However, existing LVLMs\nfocus on general tasks but lack basic industrial knowledge and reasoning\ncapabilities related to FS-IAD, making these methods far from specialized human\nquality inspectors. To address these challenges, we propose a unified\nframework, IADGPT, designed to perform FS-IAD in a human-like manner, while\nalso handling associated localization and reasoning tasks, even for diverse and\nnovel industrial products. To this end, we introduce a three-stage progressive\ntraining strategy inspired by humans. Specifically, the first two stages\ngradually guide IADGPT in acquiring fundamental industrial knowledge and\ndiscrepancy awareness. In the third stage, we design an in-context\nlearning-based training paradigm, enabling IADGPT to leverage a few-shot image\nas the exemplars for improved generalization to novel products. In addition, we\ndesign a strategy that enables IADGPT to output image-level and pixel-level\nanomaly scores using the logits output and the attention map, respectively, in\nconjunction with the language output to accomplish anomaly reasoning. To\nsupport our training, we present a new dataset comprising 100K images across\n400 diverse industrial product categories with extensive attribute-level\ntextual annotations. Experiments indicate IADGPT achieves considerable\nperformance gains in anomaly detection and demonstrates competitiveness in\nanomaly localization and reasoning. We will release our dataset in\ncamera-ready.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10681v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10681v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.403,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a unified LVLM framework for few-shot industrial anomaly detection, localization, and reasoning using in-context learning and progressive training. It does not involve diffusion models, iterative refinement processes, or any multi-step logical reasoning akin to a Chain-of-Thought treated as a single entity for holistic correction. There is no mention of diffusion-based components.",
      "distributed_training_justification": "The paper discusses a progressive training strategy for an LVLM but does not address distributed training, parallel computing, multi-node machine learning, or any methods for partitioning data, architecture, or computation across processors or nodes. The focus is on model design and dataset construction, not on acceleration techniques for training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10687",
      "title": "Continuous Bangla Sign Language Translation: Mitigating the Expense of\n  Gloss Annotation with the Assistance of Graph",
      "authors": [
        "Safaeid Hossain Arib",
        "Rabeya Akter",
        "Sejuti Rahman"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Millions of individuals worldwide are affected by deafness and hearing\nimpairment. Sign language serves as a sophisticated means of communication for\nthe deaf and hard of hearing. However, in societies that prioritize spoken\nlanguages, sign language often faces underestimation, leading to communication\nbarriers and social exclusion. The Continuous Bangla Sign Language Translation\nproject aims to address this gap by enhancing translation methods. While recent\napproaches leverage transformer architecture for state-of-the-art results, our\nmethod integrates graph-based methods with the transformer architecture. This\nfusion, combining transformer and STGCN-LSTM architectures, proves more\neffective in gloss-free translation. Our contributions include architectural\nfusion, exploring various fusion strategies, and achieving a new\nstate-of-the-art performance on diverse sign language datasets, namely\nRWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach\ndemonstrates superior performance compared to current translation outcomes\nacross all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,\n2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in\nRWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce\nbenchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a\nbenchmark for future research, emphasizing the importance of gloss-free\ntranslation to improve communication accessibility for the deaf and hard of\nhearing.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10687v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10687v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.301,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10688",
      "title": "Novel View Synthesis using DDIM Inversion",
      "authors": [
        "Sehajdeep SIngh",
        "A V Subramanyam"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Synthesizing novel views from a single input image is a challenging task. It\nrequires extrapolating the 3D structure of a scene while inferring details in\noccluded regions, and maintaining geometric consistency across viewpoints. Many\nexisting methods must fine-tune large diffusion backbones using multiple views\nor train a diffusion model from scratch, which is extremely expensive.\nAdditionally, they suffer from blurry reconstruction and poor generalization.\nThis gap presents the opportunity to explore an explicit lightweight view\ntranslation framework that can directly utilize the high-fidelity generative\ncapabilities of a pretrained diffusion model while reconstructing a scene from\na novel view. Given the DDIM-inverted latent of a single input image, we employ\na camera pose-conditioned translation U-Net, TUNet, to predict the inverted\nlatent corresponding to the desired target view. However, the image sampled\nusing the predicted latent may result in a blurry reconstruction. To this end,\nwe propose a novel fusion strategy that exploits the inherent noise correlation\nstructure observed in DDIM inversion. The proposed fusion strategy helps\npreserve the texture and fine-grained details. To synthesize the novel view, we\nuse the fused latent as the initial condition for DDIM sampling, leveraging the\ngenerative prior of the pretrained diffusion model. Extensive experiments on\nMVImgNet demonstrate that our method outperforms existing methods.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10688v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10688v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.276,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.507,
      "distributed_training_score": 0.308,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for novel view synthesis using DDIM inversion and a translation U-Net, focusing on generative image tasks in computer vision. It leverages diffusion models for iterative denoising and latent space manipulation to generate new views, but does not involve adapting diffusion for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. As there is no clear component for logical reasoning, the paper does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10695",
      "title": "Learning from Natural Language Feedback for Personalized Question\n  Answering",
      "authors": [
        "Alireza Salemi",
        "Hamed Zamani"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Personalization is crucial for enhancing both the effectiveness and user\nsatisfaction of language technologies, particularly in information-seeking\ntasks like question answering. Current approaches for personalizing large\nlanguage models (LLMs) often rely on retrieval-augmented generation (RAG),\nfollowed by reinforcement learning with scalar reward signals to teach models\nhow to use retrieved personal context. We believe that these scalar rewards\nsometimes provide weak, non-instructive feedback, limiting learning efficiency\nand personalization quality. We introduce VAC, a novel framework for\npersonalized response generation that replaces scalar rewards with natural\nlanguage feedback (NLF) that are generated conditioned on the user profiles and\nthe question narratives. NLF serves as a rich and actionable supervision\nsignal, allowing the policy model to iteratively refine its outputs and\ninternalize effective personalization strategies. Training alternates between\noptimizing the feedback model and fine-tuning the policy model on the improved\nresponses, resulting in a policy model that no longer requires feedback at\ninference. Evaluation on the LaMP-QA benchmark that consists of three diverse\ndomains demonstrates consistent and significant improvements over the\nstate-of-the-art results. Human evaluations further confirm the superior\nquality of the generated responses. These results demonstrate that NLF provides\nmore effective signals for optimizing personalized question answering.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10695v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10695v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.521,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.301,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper builds on reinforcement learning concepts by replacing scalar rewards with natural language feedback (NLF) to refine model outputs, which is somewhat aligned with RLHF's goal of aligning models with preferences. However, it does not use human-ranked data to train a reward model; instead, NLF is generated programmatically by a feedback model based on user profiles, making it less directly tied to human feedback.",
      "weak_supervision_justification": "The paper's framework uses programmatically generated natural language feedback (NLF) as a supervision signal, derived from high-level sources like user profiles and question narratives, which aligns closely with weak supervision by providing noisy or imprecise labels to train models iteratively without relying on perfectly hand-labeled data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces the VAC framework, which enhances personalized question answering in large language models by replacing traditional scalar rewards with natural language feedback (NLF) generated from user profiles and question narratives, providing richer supervision for iterative model refinement. The methodology involves alternately training a feedback model to generate actionable NLF and fine-tuning a policy model to produce improved responses directly, eliminating the need for feedback during inference. Evaluations on the LaMP-QA benchmark across three domains show significant improvements in response quality and efficiency over state-of-the-art baselines, with human assessments confirming the superior personalization and relevance of generated outputs.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework, VAC, that uses natural language feedback instead of scalar rewards, representing a significant advancement in personalized language model training by offering explicit, actionable guidance for improvements. This approach addresses limitations in existing methods and could redefine how personalization is achieved in question answering systems.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and applications in personalized AI, particularly in information retrieval and language technologies, due to its demonstrated improvements in efficiency and user satisfaction. With publicly released code, it is likely to be adopted and built upon, fostering advancements in human-centered systems.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality, innovative contribution to personalized question answering that provides clear methodological advancements and empirical evidence, making it valuable for researchers in AI and NLP to understand and build upon. While impactful, it may not be essential for all readers outside these specific subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5add284a495ed02a76a38259c090438f77b185d9",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 10,
      "average_h_index": 9.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Alireza Salemi",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2073044451"
        },
        {
          "name": "Hamed Zamani",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2295731593"
        }
      ]
    },
    {
      "id": "2508.10701",
      "title": "REFN: A Reinforcement-Learning-From-Network Framework against\n  1-day/n-day Exploitations",
      "authors": [
        "Tianlong Yu",
        "Lihong Liu",
        "Ziyi Zhou",
        "Fudu Xing",
        "Kailong Wang",
        "Yang Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The exploitation of 1 day or n day vulnerabilities poses severe threats to\nnetworked devices due to massive deployment scales and delayed patching\n(average Mean Time To Patch exceeds 60 days). Existing defenses, including host\nbased patching and network based filtering, are inadequate due to limited\nscalability across diverse devices, compatibility issues especially with\nembedded or legacy systems, and error prone deployment process (manual patch\nvalidation). To address these issues, we introduce REFN (Reinforcement Learning\nFrom Network), a novel framework that trains Large Language Models (LLMs) to\nautonomously generate network filters to prevent 1 day or n day exploitations.\nREFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven\nby online network rewards instead of traditional Human Feedback (RLHF). REFN\nguarantees compatibility via unified deployment on edge security gateways\n(Amazon Eero). REFN provides robustness via online validation using real\nnetwork traffic. Crucially, REFN addresses three core challenges in training\nLLMs for exploit prevention: 1) expanding current LLMs limited vulnerability\nfixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging\ncurrent LLMs language to network gaps through an RL From VNF Pipeline that\ntranslates language context (vulnerability description) into network\nenforcement, 3) addressing the LLM hallucination and non determinism via the\nOnline Agentic Validation that penalizes erroneous outputs. Evaluated across 22\nfamilies of 1 day or n day exploits, REFN demonstrates effectiveness (21.1\npercent higher accuracy than alternatives), efficiency (Mean Time To Patch of\n3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an\ninitial step toward training LLMs to rapidly prevent massive scale 1 day or n\nday exploitations.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10701v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10701v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.48,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.405,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper explicitly states that REFN uses Reinforcement Learning driven by real-time network rewards, not human feedback, distinguishing it from RLHF. There is no involvement of human-ranked data or a reward model based on human preferences, making the paper's contribution unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. Its focus is on RL for generating network filters, with no components resembling diffusion-based approaches.",
      "distributed_training_justification": "The paper discusses scalability of the REFN framework to 10K devices, which could imply some distributed deployment aspects, but it does not focus on distributed training techniques, parallel computing, or multi-node machine learning algorithms for model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10703",
      "title": "GenOM: Ontology Matching with Description Generation and Large Language\n  Model",
      "authors": [
        "Yiping Song",
        "Jiaoyan Chen",
        "Renate A. Schmidt"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Ontology matching (OM) plays an essential role in enabling semantic\ninteroperability and integration across heterogeneous knowledge sources,\nparticularly in the biomedical domain which contains numerous complex concepts\nrelated to diseases and pharmaceuticals. This paper introduces GenOM, a large\nlanguage model (LLM)-based ontology alignment framework, which enriches the\nsemantic representations of ontology concepts via generating textual\ndefinitions, retrieves alignment candidates with an embedding model, and\nincorporates exact matching-based tools to improve precision. Extensive\nexperiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often\nachieve competitive performance, surpassing many baselines including\ntraditional OM systems and recent LLM-based methods. Further ablation studies\nconfirm the effectiveness of semantic enrichment and few-shot prompting,\nhighlighting the framework's robustness and adaptability.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10703v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10703v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.319,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on an ontology matching framework using large language models for semantic enrichment, candidate retrieval, and alignment, without any mention of diffusion models or iterative refinement processes. It does not involve adapting diffusion for multi-step logical reasoning or treating a chain-of-thought as a single entity for correction, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10704",
      "title": "Beyond conventional vision: RGB-event fusion for robust object detection\n  in dynamic traffic scenarios",
      "authors": [
        "Zhanwen Liu",
        "Yujing Sun",
        "Yang Wang",
        "Nan Yang",
        "Shengbo Eben Li",
        "Xiangmo Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The dynamic range limitation of conventional RGB cameras reduces global\ncontrast and causes loss of high-frequency details such as textures and edges\nin complex traffic environments (e.g., nighttime driving, tunnels), hindering\ndiscriminative feature extraction and degrading frame-based object detection.\nTo address this, we integrate a bio-inspired event camera with an RGB camera to\nprovide high dynamic range information and propose a motion cue fusion network\n(MCFNet), which achieves optimal spatiotemporal alignment and adaptive\ncross-modal feature fusion under challenging lighting. Specifically, an event\ncorrection module (ECM) temporally aligns asynchronous event streams with image\nframes via optical-flow-based warping, jointly optimized with the detection\nnetwork to learn task-aware event representations. The event dynamic upsampling\nmodule (EDUM) enhances spatial resolution of event frames to match image\nstructures, ensuring precise spatiotemporal alignment. The cross-modal mamba\nfusion module (CMM) uses adaptive feature fusion with a novel interlaced\nscanning mechanism, effectively integrating complementary information for\nrobust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD\ndatasets demonstrate that MCFNet significantly outperforms existing methods in\nvarious poor lighting and fast moving traffic scenarios. Notably, on the\nDSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best\nexisting methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The\ncode is available at https://github.com/Charm11492/MCFNet.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10704v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10704v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.334,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on RGB-event fusion for object detection in dynamic traffic scenarios, proposing a network for spatiotemporal alignment and feature fusion. It does not involve diffusion models, iterative refinement processes, or any mechanism for multi-step logical reasoning or Chain-of-Thought correction. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10710",
      "title": "CountCluster: Training-Free Object Quantity Guidance with\n  Cross-Attention Map Clustering for Text-to-Image Generation",
      "authors": [
        "Joohyeon Lee",
        "Jin-Seop Lee",
        "Jee-Hyong Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion-based text-to-image generation models have demonstrated strong\nperformance in terms of image quality and diversity. However, they still\nstruggle to generate images that accurately reflect the number of objects\nspecified in the input prompt. Several approaches have been proposed that rely\non either external counting modules for iterative refinement or quantity\nrepresentations derived from learned tokens or latent features. However, they\nstill have limitations in accurately reflecting the specified number of objects\nand overlook an important structural characteristic--The number of object\ninstances in the generated image is largely determined in the early timesteps\nof the denoising process. To correctly reflect the object quantity for image\ngeneration, the highly activated regions in the object cross-attention map at\nthe early timesteps should match the input object quantity, while each region\nshould be clearly separated. To address this issue, we propose\n\\textit{CountCluster}, a method that guides the object cross-attention map to\nbe clustered according to the specified object count in the input, without\nrelying on any external tools or additional training. The proposed method\npartitions the object cross-attention map into $k$ clusters at inference time\nbased on attention scores, defines an ideal distribution in which each cluster\nis spatially well-separated, and optimizes the latent to align with this target\ndistribution. Our method achieves an average improvement of 18.5\\%p in object\ncount accuracy compared to existing methods, and demonstrates superior quantity\ncontrol performance across a variety of prompts. Code will be released at:\nhttps://github.com/JoohyeonL22/CountCluster .",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10710v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10710v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.259,
      "weak_supervision_score": 0.294,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.304,
      "datasets_score": 0.236,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10711",
      "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous\n  Tokens at Scale",
      "authors": [
        "NextStep Team",
        "Chunrui Han",
        "Guopeng Li",
        "Jingwei Wu",
        "Quan Sun",
        "Yan Cai",
        "Yuang Peng",
        "Zheng Ge",
        "Deyu Zhou",
        "Haomiao Tang",
        "Hongyu Zhou",
        "Kenkun Liu",
        "Ailin Huang",
        "Bin Wang",
        "Changxin Miao",
        "Deshan Sun",
        "En Yu",
        "Fukun Yin",
        "Gang Yu",
        "Hao Nie",
        "Haoran Lv",
        "Hanpeng Hu",
        "Jia Wang",
        "Jian Zhou",
        "Jianjian Sun",
        "Kaijun Tan",
        "Kang An",
        "Kangheng Lin",
        "Liang Zhao",
        "Mei Chen",
        "Peng Xing",
        "Rui Wang",
        "Shiyu Liu",
        "Shutao Xia",
        "Tianhao You",
        "Wei Ji",
        "Xianfang Zeng",
        "Xin Han",
        "Xuelin Zhang",
        "Yana Wei",
        "Yanming Xu",
        "Yimin Jiang",
        "Yingming Wang",
        "Yu Zhou",
        "Yucheng Han",
        "Ziyang Meng",
        "Binxing Jiao",
        "Daxin Jiang",
        "Xiangyu Zhang",
        "Yibo Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Prevailing autoregressive (AR) models for text-to-image generation either\nrely on heavy, computationally-intensive diffusion models to process continuous\nimage tokens, or employ vector quantization (VQ) to obtain discrete tokens with\nquantization loss. In this paper, we push the autoregressive paradigm forward\nwith NextStep-1, a 14B autoregressive model paired with a 157M flow matching\nhead, training on discrete text tokens and continuous image tokens with\nnext-token prediction objectives. NextStep-1 achieves state-of-the-art\nperformance for autoregressive models in text-to-image generation tasks,\nexhibiting strong capabilities in high-fidelity image synthesis. Furthermore,\nour method shows strong performance in image editing, highlighting the power\nand versatility of our unified approach. To facilitate open research, we will\nrelease our code and models to the community.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10711v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10711v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.484,
      "distributed_training_score": 0.379,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces an autoregressive model for text-to-image generation using flow matching for continuous image tokens, without any adaptation of diffusion processes for logical reasoning tasks. It does not involve iterative refinement for solving complex logical problems, chain-of-thought processes, or multi-step reasoning, focusing instead on image synthesis and editing.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10712",
      "title": "Lightweight CNNs for Embedded SAR Ship Target Detection and\n  Classification",
      "authors": [
        "Fabian Kresse",
        "Georgios Pilikos",
        "Mario Azcueta",
        "Nicolas Floury"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Synthetic Aperture Radar (SAR) data enables large-scale surveillance of\nmaritime vessels. However, near-real-time monitoring is currently constrained\nby the need to downlink all raw data, perform image focusing, and subsequently\nanalyze it on the ground. On-board processing to generate higher-level products\ncould reduce the data volume that needs to be downlinked, alleviating bandwidth\nconstraints and minimizing latency. However, traditional image focusing and\nprocessing algorithms face challenges due to the satellite's limited memory,\nprocessing power, and computational resources. This work proposes and evaluates\nneural networks designed for real-time inference on unfocused SAR data acquired\nin Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our\nresults demonstrate the feasibility of using one of our models for on-board\nprocessing and deployment on an FPGA. Additionally, by investigating a binary\nclassification task between ships and windmills, we demonstrate that target\nclassification is possible.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10712v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10712v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.372,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10713",
      "title": "Electromagnetic Simulations of Antennas on GPUs for Machine Learning\n  Applications",
      "authors": [
        "Murat Temiz",
        "Vemund Bakken"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study proposes an antenna simulation framework powered by graphics\nprocessing units (GPUs) based on an open-source electromagnetic (EM) simulation\nsoftware (gprMax) for machine learning applications of antenna design and\noptimization. Furthermore, it compares the simulation results with those\nobtained through commercial EM software. The proposed software framework for\nmachine learning and surrogate model applications will produce antenna data\nsets consisting of a large number of antenna simulation results using GPUs.\nAlthough machine learning methods can attain the optimum solutions for many\nproblems, they are known to be data-hungry and require a great deal of samples\nfor the training stage of the algorithms. However, producing a sufficient\nnumber of training samples in EM applications within a limited time is\nchallenging due to the high computational complexity of EM simulations.\nTherefore, GPUs are utilized in this study to simulate a large number of\nantennas with predefined or random antenna shape parameters to produce data\nsets. Moreover, this study also compares various machine learning and deep\nlearning models in terms of antenna parameter estimation performance. This\nstudy demonstrates that an entry-level GPU substantially outperforms a high-end\nCPU in terms of computational performance, while a high-end gaming GPU can\nachieve around 18 times more computational performance compared to a high-end\nCPU. Moreover, it is shown that the open-source EM simulation software can\ndeliver similar results to those obtained via commercial software in the\nsimulation of microstrip antennas when the spatial resolution of the\nsimulations is sufficiently fine.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10713v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10713v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.28,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.415,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on using GPUs to accelerate electromagnetic simulations for generating datasets for machine learning applications, which involves parallel computing elements. However, it does not address distributed training of machine learning models, such as partitioning data or models across multiple nodes or processors, making it only loosely related to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10716",
      "title": "Revisiting Cross-View Localization from Image Matching",
      "authors": [
        "Panwang Xia",
        "Qiong Wu",
        "Lei Yu",
        "Yi Liu",
        "Mingtao Xiong",
        "Lei Liang",
        "Yongjun Zhang",
        "Yi Wan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cross-view localization aims to estimate the 3 degrees of freedom pose of a\nground-view image by registering it to aerial or satellite imagery. It is\nessential in GNSS-denied environments such as urban canyons and disaster zones.\nExisting methods either regress poses directly or align features in a shared\nbird's-eye view (BEV) space, both built upon accurate spatial correspondences\nbetween perspectives. However, these methods fail to establish strict\ncross-view correspondences, yielding only coarse or geometrically inconsistent\nmatches. Consequently, fine-grained image matching between ground and aerial\nviews remains an unsolved problem, which in turn constrains the\ninterpretability of localization results. In this paper, we revisit cross-view\nlocalization from the perspective of cross-view image matching and propose a\nnovel framework that improves both matching and localization. Specifically, we\nintroduce a Surface Model to model visible regions for accurate BEV projection,\nand a SimRefiner module to refine the similarity matrix through local-global\nresidual correction, eliminating the reliance on post-processing like RANSAC.\nTo further support research in this area, we introduce CVFM, the first\nbenchmark with 32,509 cross-view image pairs annotated with pixel-level\ncorrespondences. Extensive experiments demonstrate that our approach\nsubstantially improves both localization accuracy and image matching quality,\nsetting new baselines under extreme viewpoint disparity.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10716v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10716v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.317,
      "distributed_training_score": 0.32,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10719",
      "title": "Exploiting Discriminative Codebook Prior for Autoregressive Image\n  Generation",
      "authors": [
        "Longxiang Tang",
        "Ruihang Chu",
        "Xiang Wang",
        "Yujin Han",
        "Pingyu Wu",
        "Chunming He",
        "Yingya Zhang",
        "Shiwei Zhang",
        "Jiaya Jia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Advanced discrete token-based autoregressive image generation systems first\ntokenize images into sequences of token indices with a codebook, and then model\nthese sequences in an autoregressive paradigm. While autoregressive generative\nmodels are trained only on index values, the prior encoded in the codebook,\nwhich contains rich token similarity information, is not exploited. Recent\nstudies have attempted to incorporate this prior by performing naive k-means\nclustering on the tokens, helping to facilitate the training of generative\nmodels with a reduced codebook. However, we reveal that k-means clustering\nperforms poorly in the codebook feature space due to inherent issues, including\ntoken space disparity and centroid distance inaccuracy. In this work, we\npropose the Discriminative Codebook Prior Extractor (DCPE) as an alternative to\nk-means clustering for more effectively mining and utilizing the token\nsimilarity information embedded in the codebook. DCPE replaces the commonly\nused centroid-based distance, which is found to be unsuitable and inaccurate\nfor the token feature space, with a more reasonable instance-based distance.\nUsing an agglomerative merging technique, it further addresses the token space\ndisparity issue by avoiding splitting high-density regions and aggregating\nlow-density ones. Extensive experiments demonstrate that DCPE is plug-and-play\nand integrates seamlessly with existing codebook prior-based paradigms. With\nthe discriminative prior extracted, DCPE accelerates the training of\nautoregressive models by 42% on LlamaGen-B and improves final FID and IS\nperformance.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10719v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10719v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.383,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving autoregressive image generation by addressing issues in codebook clustering for token-based models, introducing the DCPE method. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10729",
      "title": "EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain\n  Egocentric Video Question Answering",
      "authors": [
        "Yanjun Li",
        "Yuqian Fu",
        "Tianwen Qian",
        "Qi'ao Xu",
        "Silong Dai",
        "Danda Pani Paudel",
        "Luc Van Gool",
        "Xiaoling Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly pushed the frontier of egocentric video question answering\n(EgocentricQA). However, existing benchmarks and studies are mainly limited to\ncommon daily activities such as cooking and cleaning. In contrast, real-world\ndeployment inevitably encounters domain shifts, where target domains differ\nsubstantially in both visual style and semantic content. To bridge this gap, we\nintroduce \\textbf{EgoCross}, a comprehensive benchmark designed to evaluate the\ncross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four\ndiverse and challenging domains, including surgery, industry, extreme sports,\nand animal perspective, representing realistic and high-impact application\nscenarios. It comprises approximately 1,000 QA pairs across 798 video clips,\nspanning four key QA tasks: prediction, recognition, localization, and\ncounting. Each QA pair provides both OpenQA and CloseQA formats to support\nfine-grained evaluation. Extensive experiments show that most existing MLLMs,\nwhether general-purpose or egocentric-specialized, struggle to generalize to\ndomains beyond daily life, highlighting the limitations of current models.\nFurthermore, we conduct several pilot studies, \\eg, fine-tuning and\nreinforcement learning, to explore potential improvements. We hope EgoCross and\nour accompanying analysis will serve as a foundation for advancing\ndomain-adaptive, robust egocentric video understanding. Data and codes will be\nreleased at:\n\\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10729v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10729v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.374,
      "datasets_score": 0.448,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a benchmark for Multimodal Large Language Models (MLLMs) in egocentric video question answering and evaluating their cross-domain generalization. It does not mention, involve, or adapt diffusion models for iterative refinement, Chain-of-Thought reasoning, or any multi-step logical processes. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and introduction of the EgoCross benchmark dataset, including curation methodologies for video sources, QA pair design across diverse domains, and evaluation protocols for MLLMs. This directly aligns with research on dataset creation, benchmarking, and analysis for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces EgoCross, a new benchmark designed to evaluate the cross-domain generalization of Multimodal Large Language Models (MLLMs) in egocentric video question answering, focusing on domains like surgery, industry, extreme sports, and animal perspectives that differ from everyday scenarios. It comprises approximately 1,000 QA pairs across 798 video clips, covering tasks such as prediction, recognition, localization, and counting in both OpenQA and CloseQA formats; experiments reveal that existing MLLMs perform poorly on this benchmark, highlighting their limitations and suggesting avenues for improvement through methods like fine-tuning and reinforcement learning.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark and task for cross-domain egocentric video QA, addressing an underexplored area that significantly advances the state-of-the-art in evaluating MLLMs' generalization capabilities.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research and development in robust, domain-adaptive MLLMs, given its focus on real-world applications and the provision of a comprehensive dataset for advancing egocentric video understanding.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI and computer vision by highlighting critical limitations in current MLLMs and providing a foundation for future improvements, making it essential for researchers in egocentric vision and multimodal models.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/77b4c58f4d806dfae13910d7615d36e85be1211a",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 27,
      "average_h_index": 7.875,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yanjun Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2356415947"
        },
        {
          "name": "Yuqian Fu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2303528627"
        },
        {
          "name": "Tianwen Qian",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2162961341"
        },
        {
          "name": "Qi'ao Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349363752"
        },
        {
          "name": "Silong Dai",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2322444368"
        },
        {
          "name": "D. Paudel",
          "h_index": 27,
          "profile_url": "https://www.semanticscholar.org/author/35268081"
        },
        {
          "name": "L. V. Gool",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/2246990749"
        },
        {
          "name": "Xiaoling Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2370992847"
        }
      ]
    },
    {
      "id": "2508.10731",
      "title": "Dissecting Generalized Category Discovery: Multiplex Consensus under\n  Self-Deconstruction",
      "authors": [
        "Luyao Tang",
        "Kunze Huang",
        "Chaoqi Chen",
        "Yuxuan Yuan",
        "Chenxin Li",
        "Xiaotong Tu",
        "Xinghao Ding",
        "Yue Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Human perceptual systems excel at inducing and recognizing objects across\nboth known and novel categories, a capability far beyond current machine\nlearning frameworks. While generalized category discovery (GCD) aims to bridge\nthis gap, existing methods predominantly focus on optimizing objective\nfunctions. We present an orthogonal solution, inspired by the human cognitive\nprocess for novel object understanding: decomposing objects into visual\nprimitives and establishing cross-knowledge comparisons. We propose ConGCD,\nwhich establishes primitive-oriented representations through high-level\nsemantic reconstruction, binding intra-class shared attributes via\ndeconstruction. Mirroring human preference diversity in visual processing,\nwhere distinct individuals leverage dominant or contextual cues, we implement\ndominant and contextual consensus units to capture class-discriminative\npatterns and inherent distributional invariants, respectively. A consensus\nscheduler dynamically optimizes activation pathways, with final predictions\nemerging through multiplex consensus integration. Extensive evaluations across\ncoarse- and fine-grained benchmarks demonstrate ConGCD's effectiveness as a\nconsensus-aware paradigm. Code is available at github.com/lytang63/ConGCD.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10731v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10731v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.376,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Generalized Category Discovery (GCD) through methods like visual primitive decomposition and multiplex consensus, inspired by human cognition. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. There is no mention of adapting diffusion techniques, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10732",
      "title": "APFL: Analytic Personalized Federated Learning via Dual-Stream Least\n  Squares",
      "authors": [
        "Kejia Fan",
        "Jianheng Tang",
        "Zhirui Yang",
        "Feijiang Han",
        "Jiaxu Li",
        "Run He",
        "Yajiang Huang",
        "Anfeng Liu",
        "Houbing Herbert Song",
        "Yunhuai Liu",
        "Huiping Zhuang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Personalized Federated Learning (PFL) has presented a significant challenge\nto deliver personalized models to individual clients through collaborative\ntraining. Existing PFL methods are often vulnerable to non-IID data, which\nseverely hinders collective generalization and then compromises the subsequent\npersonalization efforts. In this paper, to address this non-IID issue in PFL,\nwe propose an Analytic Personalized Federated Learning (APFL) approach via\ndual-stream least squares. In our APFL, we use a foundation model as a frozen\nbackbone for feature extraction. Subsequent to the feature extractor, we\ndevelop dual-stream analytic models to achieve both collective generalization\nand individual personalization. Specifically, our APFL incorporates a shared\nprimary stream for global generalization across all clients, and a dedicated\nrefinement stream for local personalization of each individual client. The\nanalytical solutions of our APFL enable its ideal property of heterogeneity\ninvariance, theoretically meaning that each personalized model remains\nidentical regardless of how heterogeneous the data are distributed across all\nother clients. Empirical results across various datasets also validate the\nsuperiority of our APFL over state-of-the-art baselines, with advantages of at\nleast 1.10%-15.45% in accuracy.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10732v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10732v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.294,
      "distributed_training_score": 0.418,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Personalized Federated Learning and analytic methods for handling non-IID data, with no mention of reinforcement learning, human feedback, reward models, or preference alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is in Federated Learning, a distributed training paradigm that involves collaborative model training across multiple clients, addressing data partitioning and computation in a multi-node setup to improve generalization.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper proposes Analytic Personalized Federated Learning (APFL), a method designed to tackle non-IID data challenges in federated learning by utilizing a frozen foundation model for feature extraction and dual-stream analytic models for achieving both global generalization and individual personalization. Through least squares optimization with random projections and nonlinear activations, APFL ensures heterogeneity invariance—meaning personalized models remain consistent regardless of data distribution variations—and demonstrates superior performance with accuracy gains of 1.10%-15.45% over state-of-the-art baselines in various datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining analytic learning with dual-stream models to address non-IID issues in PFL, offering a clever adaptation of existing techniques rather than introducing an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the PFL subfield due to its effective handling of data heterogeneity, potentially enhancing future personalized models in federated learning applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to PFL by introducing a robust method with empirical evidence of improvements, making it essential for researchers focused on federated learning to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a5bf74cc35481786fb948d694847d0550d16b4d9",
      "total_authors": 11,
      "authors_found": 11,
      "highest_h_index": 5,
      "average_h_index": 2.3636363636363638,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kejia Fan",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2053963479"
        },
        {
          "name": "Jianheng Tang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2264282343"
        },
        {
          "name": "Zhirui Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375084181"
        },
        {
          "name": "Feijiang Han",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2202174378"
        },
        {
          "name": "Jiaxu Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2324836784"
        },
        {
          "name": "Run He",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2225124710"
        },
        {
          "name": "Yajiang Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2277806192"
        },
        {
          "name": "Anfeng Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376423148"
        },
        {
          "name": "H. Song",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2274903248"
        },
        {
          "name": "Yunhuai Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355619378"
        },
        {
          "name": "Huiping Zhuang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2326961141"
        }
      ]
    },
    {
      "id": "2508.10737",
      "title": "Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC\n  2025",
      "authors": [
        "Matej Vitek",
        "Darian Tomašević",
        "Abhijit Das",
        "Sabari Nathan",
        "Gökhan Özbulak",
        "Gözde Ayşe Tataroğlu Özbulak",
        "Jean-Paul Calbimonte",
        "André Anjos",
        "Hariohm Hemant Bhatt",
        "Dhruv Dhirendra Premani",
        "Jay Chaudhari",
        "Caiyong Wang",
        "Jian Jiang",
        "Chi Zhang",
        "Qi Zhang",
        "Iyyakutti Iyappan Ganapathi",
        "Syed Sadaf Ali",
        "Divya Velayudan",
        "Maregu Assefa",
        "Naoufel Werghi",
        "Zachary A. Daniels",
        "Leeon John",
        "Ritesh Vyas",
        "Jalil Nourmohammadi Khiarak",
        "Taher Akbari Saeed",
        "Mahsa Nasehi",
        "Ali Kianfar",
        "Mobina Pashazadeh Panahi",
        "Geetanjali Sharma",
        "Pushp Raj Panth",
        "Raghavendra Ramachandra",
        "Aditya Nigam",
        "Umapada Pal",
        "Peter Peer",
        "Vitomir Štruc"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper presents a summary of the 2025 Sclera Segmentation Benchmarking\nCompetition (SSBC), which focused on the development of privacy-preserving\nsclera-segmentation models trained using synthetically generated ocular images.\nThe goal of the competition was to evaluate how well models trained on\nsynthetic data perform in comparison to those trained on real-world datasets.\nThe competition featured two tracks: $(i)$ one relying solely on synthetic data\nfor model development, and $(ii)$ one combining/mixing synthetic with (a\nlimited amount of) real-world data. A total of nine research groups submitted\ndiverse segmentation models, employing a variety of architectural designs,\nincluding transformer-based solutions, lightweight models, and segmentation\nnetworks guided by generative frameworks. Experiments were conducted across\nthree evaluation datasets containing both synthetic and real-world images,\ncollected under diverse conditions. Results show that models trained entirely\non synthetic data can achieve competitive performance, particularly when\ndedicated training strategies are employed, as evidenced by the top performing\nmodels that achieved $F_1$ scores of over $0.8$ in the synthetic data track.\nMoreover, performance gains in the mixed track were often driven more by\nmethodological choices rather than by the inclusion of real data, highlighting\nthe promise of synthetic data for privacy-aware biometric development. The code\nand data for the competition is available at:\nhttps://github.com/dariant/SSBC_2025.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10737v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10737v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.284,
      "distributed_training_score": 0.345,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10740",
      "title": "Axis-level Symmetry Detection with Group-Equivariant Representation",
      "authors": [
        "Wongyun Yu",
        "Ahyun Seo",
        "Minsu Cho"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Symmetry is a fundamental concept that has been extensively studied, yet\ndetecting it in complex scenes remains a significant challenge in computer\nvision. Recent heatmap-based approaches can localize potential regions of\nsymmetry axes but often lack precision in identifying individual axes. In this\nwork, we propose a novel framework for axis-level detection of the two most\ncommon symmetry types-reflection and rotation-by representing them as explicit\ngeometric primitives, i.e. lines and points. Our method employs a dual-branch\narchitecture that is equivariant to the dihedral group, with each branch\nspecialized to exploit the structure of dihedral group-equivariant features for\nits respective symmetry type. For reflection symmetry, we introduce\norientational anchors, aligned with group components, to enable\norientation-specific detection, and a reflectional matching that measures\nsimilarity between patterns and their mirrored counterparts across candidate\naxes. For rotational symmetry, we propose a rotational matching that compares\npatterns at fixed angular intervals to identify rotational centers. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance,\noutperforming existing approaches.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10740v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10740v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.257,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.287,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10741",
      "title": "Forgery Guided Learning Strategy with Dual Perception Network for\n  Deepfake Cross-domain Detection",
      "authors": [
        "Lixin Jia",
        "Zhiqing Guo",
        "Gaobo Yang",
        "Liejun Wang",
        "Keqin Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The emergence of deepfake technology has introduced a range of societal\nproblems, garnering considerable attention. Current deepfake detection methods\nperform well on specific datasets, but exhibit poor performance when applied to\ndatasets with unknown forgery techniques. Moreover, as the gap between emerging\nand traditional forgery techniques continues to widen, cross-domain detection\nmethods that rely on common forgery traces are becoming increasingly\nineffective. This situation highlights the urgency of developing deepfake\ndetection technology with strong generalization to cope with fast iterative\nforgery techniques. To address these challenges, we propose a Forgery Guided\nLearning (FGL) strategy designed to enable detection networks to continuously\nadapt to unknown forgery techniques. Specifically, the FGL strategy captures\nthe differential information between known and unknown forgery techniques,\nallowing the model to dynamically adjust its learning process in real time. To\nfurther improve the ability to perceive forgery traces, we design a Dual\nPerception Network (DPNet) that captures both differences and relationships\namong forgery traces. In the frequency stream, the network dynamically\nperceives and extracts discriminative features across various forgery\ntechniques, establishing essential detection cues. These features are then\nintegrated with spatial features and projected into the embedding space. In\naddition, graph convolution is employed to perceive relationships across the\nentire feature space, facilitating a more comprehensive understanding of\nforgery trace correlations. Extensive experiments show that our approach\ngeneralizes well across different scenarios and effectively handles unknown\nforgery challenges, providing robust support for deepfake detection. Our code\nis available on https://github.com/vpsg-research/FGL.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10741v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10741v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.376,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10743",
      "title": "An Efficient Model-Driven Groupwise Approach for Atlas Construction",
      "authors": [
        "Ziwei Zou",
        "Bei Zou",
        "Xiaoyan Kui",
        "Wenqi Lu",
        "Haoran Dou",
        "Arezoo Zakeri",
        "Timothy Cootes",
        "Alejandro F Frangi",
        "Jinming Duan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "math.OC (Optimization and Control)"
      ],
      "abstract": "Atlas construction is fundamental to medical image analysis, offering a\nstandardized spatial reference for tasks such as population-level anatomical\nmodeling. While data-driven registration methods have recently shown promise in\npairwise settings, their reliance on large training datasets, limited\ngeneralizability, and lack of true inference phases in groupwise contexts\nhinder their practical use. In contrast, model-driven methods offer\ntraining-free, theoretically grounded, and data-efficient alternatives, though\nthey often face scalability and optimization challenges when applied to large\n3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration\nvia Coordinate descent), a novel model-driven groupwise registration framework\nfor atlas construction. DARC supports a broad range of image dissimilarity\nmetrics and efficiently handles arbitrary numbers of 3D images without\nincurring GPU memory issues. Through a coordinate descent strategy and a\ncentrality-enforcing activation function, DARC produces unbiased, diffeomorphic\natlases with high anatomical fidelity. Beyond atlas construction, we\ndemonstrate two key applications: (1) One-shot segmentation, where labels\nannotated only on the atlas are propagated to subjects via inverse\ndeformations, outperforming state-of-the-art few-shot methods; and (2) shape\nsynthesis, where new anatomical variants are generated by warping the atlas\nmesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a\nflexible, generalizable, and resource-efficient framework for atlas\nconstruction and applications.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10743v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10743v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.382,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10745",
      "title": "Agentic Design Review System",
      "authors": [
        "Sayan Nag",
        "K J Joseph",
        "Koustava Goswami",
        "Vlad I Morariu",
        "Balaji Vasan Srinivasan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.MA (Multiagent Systems)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Evaluating graphic designs involves assessing it from multiple facets like\nalignment, composition, aesthetics and color choices. Evaluating designs in a\nholistic way involves aggregating feedback from individual expert reviewers.\nTowards this, we propose an Agentic Design Review System (AgenticDRS), where\nmultiple agents collaboratively analyze a design, orchestrated by a meta-agent.\nA novel in-context exemplar selection approach based on graph matching and a\nunique prompt expansion method plays central role towards making each agent\ndesign aware. Towards evaluating this framework, we propose DRS-BENCH\nbenchmark. Thorough experimental evaluation against state-of-the-art baselines\nadapted to the problem setup, backed-up with critical ablation experiments\nbrings out the efficacy of Agentic-DRS in evaluating graphic designs and\ngenerating actionable feedback. We hope that this work will attract attention\nto this pragmatic, yet under-explored research direction.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10745v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10745v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.346,
      "datasets_score": 0.428,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on an agentic system for evaluating graphic designs using multiple AI agents and techniques like graph matching for exemplar selection. It does not involve training or fine-tuning models with human-ranked data or reinforcement learning to align with human preferences. While it references human judgment for evaluation, there is no core component of RLHF in the main contribution.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper mentions diffusion models briefly in the introduction as part of advancements in design generation, but it does not adapt diffusion processes for multi-step logical reasoning or iterative refinement of a 'Chain-of-Thought'. The core contributions are an agentic framework and benchmark for design evaluation, without any clear diffusion-based reasoning component.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces DRS-BENCH, a new benchmark suite that includes 15 design attribute definitions, 4 datasets, new evaluation metrics, and baselines, specifically for assessing graphic design evaluation. This directly aligns with research on creating, analyzing, and benchmarking datasets for AI applications, as it focuses on dataset curation and evaluation methodologies.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the Agentic Design Review System (AgenticDRS), a multi-agent framework orchestrated by a meta-agent to holistically evaluate graphic designs across dimensions like alignment, composition, aesthetics, and color harmony. It employs novel techniques such as graph matching-based in-context exemplar selection and prompt expansion to enhance agent awareness, proposes the DRS-BENCH benchmark for evaluation, and demonstrates through experiments that AgenticDRS outperforms state-of-the-art baselines in generating accurate scores and actionable feedback, highlighting its efficacy in an under-explored area of AI-assisted design analysis.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing AI techniques, such as multi-agent systems and graph matching, to create a new framework for graphic design evaluation, offering a notable improvement over traditional methods without introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and applications in AI for design tools and multi-modal systems, as it provides a practical benchmark and framework that could be built upon in specific subfields like computer vision and machine learning for graphic design.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to AI-driven design evaluation, offering innovative methods and a new benchmark that are valuable for researchers in artificial intelligence and related fields, though it may not be essential for those outside this niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bfe1d603af57cd644848104508bf3de862e8dd94",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 32,
      "average_h_index": 8.5,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Sayan Nag",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2309246872"
        },
        {
          "name": "K. J. Joseph",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2305482058"
        },
        {
          "name": "Koustava Goswami",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/120873790"
        },
        {
          "name": "Vlad I. Morariu",
          "h_index": 32,
          "profile_url": "https://www.semanticscholar.org/author/2852035"
        },
        {
          "name": "Balaji Vasan Srinivasan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2277599827"
        },
        {
          "name": "Adobe Research",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2115489684"
        }
      ]
    },
    {
      "id": "2508.10747",
      "title": "Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based\n  Generalized Planning",
      "authors": [
        "Sangwoo Jeon",
        "Juchul Shin",
        "Gyeong-Tae Kim",
        "YeonJe Cho",
        "Seongwoo Kim"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Generalized planning using deep reinforcement learning (RL) combined with\ngraph neural networks (GNNs) has shown promising results in various symbolic\nplanning domains described by PDDL. However, existing approaches typically\nrepresent planning states as fully connected graphs, leading to a combinatorial\nexplosion in edge information and substantial sparsity as problem scales grow,\nespecially evident in large grid-based environments. This dense representation\nresults in diluted node-level information, exponentially increases memory\nrequirements, and ultimately makes learning infeasible for larger-scale\nproblems. To address these challenges, we propose a sparse, goal-aware GNN\nrepresentation that selectively encodes relevant local relationships and\nexplicitly integrates spatial features related to the goal. We validate our\napproach by designing novel drone mission scenarios based on PDDL within a grid\nworld, effectively simulating realistic mission execution environments. Our\nexperimental results demonstrate that our method scales effectively to larger\ngrid sizes previously infeasible with dense graph representations and\nsubstantially improves policy generalization and success rates. Our findings\nprovide a practical foundation for addressing realistic, large-scale\ngeneralized planning tasks.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10747v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10747v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.404,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on sparse GNN representations for RL-based generalized planning, emphasizing scalability in graph structures and goal-aware features. There is no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning akin to Chain-of-Thought correction.",
      "distributed_training_justification": "The paper addresses scalability issues in GNNs for RL by using sparse graph representations to reduce memory and improve efficiency, but it does not discuss distributed training, parallel computing, multi-node setups, or partitioning of data/computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10751",
      "title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of\n  Large Reasoning Models",
      "authors": [
        "Zhipeng Chen",
        "Xiaobo Qin",
        "Youbin Wu",
        "Yue Ling",
        "Qinghao Ye",
        "Wayne Xin Zhao",
        "Guang Shi"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR), which typically adopts\nPass@1 as the reward, has faced the issues in balancing exploration and\nexploitation, causing policies to prefer conservative actions, converging to a\nlocal optimum. Identifying an appropriate reward metric is therefore crucial.\nRegarding the prior work, although Pass@k has been used in evaluation, its\nconnection to LLM exploration ability in RLVR remains largely overlooked. To\ninvestigate this, we first use Pass@k as the reward to train the policy model\n(i.e., $\\textbf{Pass@k Training}$), and observe the improvement on its\nexploration ability. Next, we derive an analytical solution for the advantage\nof Pass@k Training, leading to an efficient and effective process. Building on\nthis, our analysis reveals that exploration and exploitation are not inherently\nconflicting objectives, while they can mutually enhance each other. Moreover,\nPass@k Training with analytical derivation essentially involves directly\ndesigning the advantage function. Inspired by this, we preliminarily explore\nthe advantage design for RLVR, showing promising results and highlighting a\npotential future direction.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10751v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10751v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.494,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.41,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses Reinforcement Learning with Verifiable Rewards (RLVR), which uses rewards based on objective metrics like Pass@k, but does not involve human feedback, a reward model trained on human-ranked data, or alignment with human preferences. While RL is a common element, the lack of human involvement makes it only tangentially related.",
      "weak_supervision_justification": "The paper focuses on reward design in RLVR using metrics like Pass@k, with no mention of programmatically generating labels from noisy or imprecise sources. It relies on verifiable rewards rather than weak supervision techniques for training.",
      "diffusion_reasoning_justification": "The paper addresses RLVR and Pass@k training for reasoning tasks, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. There is no component related to treating Chain-of-Thought as a diffusion entity.",
      "distributed_training_justification": "The paper does not discuss parallel computing, multi-node systems, or strategies for partitioning data/computation across processors. It focuses solely on reward mechanisms in RLVR, with no reference to distributed training techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10758",
      "title": "Natively Trainable Sparse Attention for Hierarchical Point Cloud\n  Datasets",
      "authors": [
        "Nicolas Lapautre",
        "Maria Marchenko",
        "Carlos Miguel Patiño",
        "Xin Zhou"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Unlocking the potential of transformers on datasets of large physical systems\ndepends on overcoming the quadratic scaling of the attention mechanism. This\nwork explores combining the Erwin architecture with the Native Sparse Attention\n(NSA) mechanism to improve the efficiency and receptive field of transformer\nmodels for large-scale physical systems, addressing the challenge of quadratic\nattention complexity. We adapt the NSA mechanism for non-sequential data,\nimplement the Erwin NSA model, and evaluate it on three datasets from the\nphysical sciences -- cosmology simulations, molecular dynamics, and air\npressure modeling -- achieving performance that matches or exceeds that of the\noriginal Erwin model. Additionally, we reproduce the experimental results from\nthe Erwin paper to validate their implementation.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10758v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10758v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.45,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on optimizing transformer models for efficiency using Native Sparse Attention (NSA) to address quadratic attention complexity in point cloud datasets. While it mentions hardware-aware optimizations and accelerated training, such as through compressed, selection, and sliding attentions, it does not directly discuss distributed training techniques like data partitioning across multiple nodes or parallel computing frameworks. Instead, the emphasis is on algorithmic and hardware-level efficiencies within a single system, which could indirectly support distributed setups but is not the primary contribution.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10760",
      "title": "FROGENT: An End-to-End Full-process Drug Design Agent",
      "authors": [
        "Qihua Pan",
        "Dong Xu",
        "Jenna Xinyi Yao",
        "Lijia Ma",
        "Zexuan Zhu",
        "Junkai Ji"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Powerful AI tools for drug discovery reside in isolated web apps, desktop\nprograms, and code libraries. Such fragmentation forces scientists to manage\nincompatible interfaces and specialized scripts, which can be a cumbersome and\nrepetitive process. To address this issue, a Full-pROcess druG dEsign ageNT,\nnamed FROGENT, has been proposed. Specifically, FROGENT utilizes a Large\nLanguage Model and the Model Context Protocol to integrate multiple dynamic\nbiochemical databases, extensible tool libraries, and task-specific AI models.\nThis agentic framework allows FROGENT to execute complicated drug discovery\nworkflows dynamically, including component tasks such as target identification,\nmolecule generation and retrosynthetic planning. FROGENT has been evaluated on\neight benchmarks that cover various aspects of drug discovery, such as\nknowledge retrieval, property prediction, virtual screening, mechanistic\nanalysis, molecular design, and synthesis. It was compared against six\nincreasingly advanced ReAct-style agents that support code execution and\nliterature searches. Empirical results demonstrated that FROGENT triples the\nbest baseline performance in hit-finding and doubles it in interaction\nprofiling, significantly outperforming both the open-source model Qwen3-32B and\nthe commercial model GPT-4o. In addition, real-world cases have been utilized\nto validate the practicability and generalization of FROGENT. This development\nsuggests that streamlining the agentic drug discovery pipeline can\nsignificantly enhance researcher productivity.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10760v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10760v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.362,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces FROGENT, an AI agent using Large Language Models for integrating drug discovery workflows, including tasks like target identification and molecule generation. It does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning or Chain-of-Thought correction. Therefore, the paper's contributions are unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10769",
      "title": "Modeling Human Responses to Multimodal AI Content",
      "authors": [
        "Zhiqi Shen",
        "Shaojing Fan",
        "Danni Xu",
        "Terence Sim",
        "Mohan Kankanhalli"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10769v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10769v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.507,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.352,
      "datasets_score": 0.427,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on creating a dataset and a system (T-Lens with HR-MCP) that uses human responses to predict perceptions of AI content, but it does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune an AI model. There is no mention of RLHF elements like policy optimization based on human feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not describe any models using diffusion processes for multi-step logical reasoning or iterative refinement of a chain-of-thought. It instead introduces T-Lens and HR-MCP for predicting human responses, with no components related to diffusion models.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes introducing and analyzing the MhAIM Dataset, which comprises over 154,000 posts for studying human responses to AI-generated content. This directly aligns with research on creating, curating, and evaluating datasets for AI applications, including human annotations and benchmarks.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of AI-generated multimodal content by introducing the MhAIM dataset, comprising over 154,000 posts, to analyze human responses and perceptions of such content. It conducts human studies revealing that inconsistencies in text and visuals aid in identifying AI origins, proposes new metrics for trustworthiness, impact, and openness, and presents T-Lens, an LLM-based system with HR-MCP protocol that predicts human reactions to enhance LLM interpretability and mitigate misinformation risks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new dataset and a novel system (T-Lens with HR-MCP) that focuses on predicting human responses to multimodal AI content, significantly advancing beyond traditional detection methods by incorporating human perception dynamics.",
      "impact_score": "High",
      "impact_justification": "The work provides practical tools for mitigating AI-driven misinformation in areas like social media and trading, likely influencing future research in AI ethics, LLM development, and digital forensics due to its timely and applicable insights.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers high-quality contributions with innovative tools and empirical findings on human-AI interactions, making it essential for researchers focused on misinformation and AI alignment, though not groundbreaking enough to be a must-read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b9f8e106ffb1c8c30ff78abb29059b02bf2c035a",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 2,
      "average_h_index": 0.6,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhiqi Shen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2259814760"
        },
        {
          "name": "Shaojing Fan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374430898"
        },
        {
          "name": "Danni Xu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261899189"
        },
        {
          "name": "Terence Sim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373333749"
        },
        {
          "name": "Mohan Kankanhalli",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375901236"
        }
      ]
    },
    {
      "id": "2508.10770",
      "title": "From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in\n  Vision Language Models",
      "authors": [
        "Tiancheng Han",
        "Yunfei Gao",
        "Yong Li",
        "Wuzhou Yu",
        "Qiaosheng Zhang",
        "Wenqi Shao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Spatio-physical reasoning, a foundation capability for understanding the real\nphysics world, is a critical step towards building robust world models. While\nrecent vision language models (VLMs) have shown remarkable progress in\nspecialized domains like multimodal mathematics and pure spatial understanding,\ntheir capability for spatio-physical reasoning remains largely unexplored. This\npaper provides a comprehensive diagnostic analysis of mainstream VLMs,\nrevealing that current models perform inadequately on this crucial task.\nFurther detailed analysis shows that this underperformance is largely\nattributable to biases caused by human-like prior and a lack of deep reasoning.\nTo address these challenges, we apply supervised fine-tuning followed by\nrule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significant\nimprovements in spatio-physical reasoning capabilities and surpassing leading\nproprietary models. Nevertheless, despite this success, the model's\ngeneralization to new physics scenarios remains limited -- underscoring the\npressing need for new approaches in spatio-physical reasoning.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10770v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10770v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.483,
      "distributed_training_score": 0.347,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions using rule-based reinforcement learning to fine-tune a model, but it does not involve training a reward model on human-ranked data or aligning with human preferences, as required for RLHF. Instead, it focuses on rule-based methods for improving spatio-physical reasoning in VLMs.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not describe any use of diffusion models or iterative refinement processes for multi-step logical reasoning. It primarily discusses supervised fine-tuning and rule-based reinforcement learning for spatio-physical tasks, with no mention of diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10771",
      "title": "AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video\n  Sequences",
      "authors": [
        "Jieyu Li",
        "Xin Zhang",
        "Joey Tianyi Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in AI-generated content have fueled the rise of highly\nrealistic synthetic videos, posing severe risks to societal trust and digital\nintegrity. Existing benchmarks for video authenticity detection typically\nsuffer from limited realism, insufficient scale, and inadequate complexity,\nfailing to effectively evaluate modern vision-language models against\nsophisticated forgeries. To address this critical gap, we introduce AEGIS, a\nnovel large-scale benchmark explicitly targeting the detection of\nhyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises\nover 10,000 rigorously curated real and synthetic videos generated by diverse,\nstate-of-the-art generative models, including Stable Video Diffusion,\nCogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary\narchitectures. In particular, AEGIS features specially constructed challenging\nsubsets enhanced with robustness evaluation. Furthermore, we provide multimodal\nannotations spanning Semantic-Authenticity Descriptions, Motion Features, and\nLow-level Visual Features, facilitating authenticity detection and supporting\ndownstream tasks such as multimodal fusion and forgery localization. Extensive\nexperiments using advanced vision-language models demonstrate limited detection\ncapabilities on the most challenging subsets of AEGIS, highlighting the\ndataset's unique complexity and realism beyond the current generalization\ncapabilities of existing models. In essence, AEGIS establishes an indispensable\nevaluation benchmark, fundamentally advancing research toward developing\ngenuinely robust, reliable, broadly generalizable video authenticity detection\nmethodologies capable of addressing real-world forgery threats. Our dataset is\navailable on https://huggingface.co/datasets/Clarifiedfish/AEGIS.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10771v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10771v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.316,
      "datasets_score": 0.435,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of AEGIS, a new large-scale benchmark dataset for AI-generated video authenticity detection. It details the creation and curation of over 10,000 real and synthetic videos using diverse generative models, provides multimodal annotations, and evaluates the dataset through experiments with vision-language models. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications, as it covers dataset introduction, curation methodologies, and benchmark evaluation.",
      "llm_score_status": "completed",
      "summary": "The paper introduces AEGIS, a comprehensive benchmark for detecting AI-generated video sequences, addressing limitations in existing datasets by curating over 10,000 real and synthetic videos from diverse state-of-the-art generative models like Stable Video Diffusion and Sora. It incorporates multimodal annotations, challenging subsets for robustness testing, and extensive experiments showing that current vision-language models perform poorly on these realistic forgeries, thereby advancing research in video authenticity detection.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark with a large-scale, diverse dataset of hyper-realistic videos and innovative annotations, significantly advancing the state-of-the-art in video forgery detection evaluation.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence a wide range of future research in AI security and commercial applications by providing a robust tool for developing better detection models against sophisticated forgeries.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, significant contribution by establishing a new benchmark that exposes gaps in current detection capabilities, making it essential for researchers in computer vision and AI ethics.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e671b93b3b94473dd956694a6ba64faa70cad095",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 6,
      "average_h_index": 3.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Jieyu Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376086069"
        },
        {
          "name": "Xin Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2316118191"
        },
        {
          "name": "Joey Tianyi Zhou",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2265459102"
        }
      ]
    },
    {
      "id": "2508.10774",
      "title": "Video-BLADE: Block-Sparse Attention Meets Step Distillation for\n  Efficient Video Generation",
      "authors": [
        "Youping Gu",
        "Xiaolong Li",
        "Yuhao Hu",
        "Bohan Zhuang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Diffusion transformers currently lead the field in high-quality video\ngeneration, but their slow iterative denoising process and prohibitive\nquadratic attention costs for long sequences create significant inference\nbottlenecks. While both step distillation and sparse attention mechanisms have\nshown promise as independent acceleration strategies, effectively combining\nthese approaches presents critical challenges -- training-free integration\nyields suboptimal results, while separately training sparse attention after\nstep distillation requires prohibitively expensive high-quality video data. To\novercome these limitations, we propose BLADE, an innovative data-free joint\ntraining framework that introduces: (1) an Adaptive Block-Sparse Attention\n(ASA) mechanism for dynamically generating content-aware sparsity masks to\nfocus computation on salient spatiotemporal features, and (2) a sparsity-aware\nstep distillation paradigm built upon Trajectory Distribution Matching (TDM)\nthat directly incorporates sparsity into the distillation process rather than\ntreating it as a separate compression step, with fast convergence. We validate\nBLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework\ndemonstrates remarkable efficiency gains across different scales. On\nWan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a\n50-step baseline. Moreover, on models such as CogVideoX-5B with short video\nsequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the\nacceleration is accompanied by a consistent quality improvement. On the\nVBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from\n0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further\ncorroborated by superior ratings in human evaluations. Our code and model\nweights are publicly available at: http://ziplab.co/BLADE-Homepage/.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10774v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10774v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.496,
      "distributed_training_score": 0.422,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on accelerating video generation in diffusion models through sparse attention and step distillation, which is a generative task. It does not involve adapting the iterative refinement process for solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for holistic correction or multi-step reasoning.",
      "distributed_training_justification": "The paper discusses efficient training frameworks for diffusion models but does not address distributed training, parallel computing, or multi-node machine learning. It lacks any mention of partitioning data, architecture, or computation across multiple processors or nodes for acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10776",
      "title": "Estimating Covariance for Global Minimum Variance Portfolio: A\n  Decision-Focused Learning Approach",
      "authors": [
        "Juchan Kim",
        "Inwoo Tae",
        "Yongjae Lee"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Portfolio optimization constitutes a cornerstone of risk management by\nquantifying the risk-return trade-off. Since it inherently depends on accurate\nparameter estimation under conditions of future uncertainty, the selection of\nappropriate input parameters is critical for effective portfolio construction.\nHowever, most conventional statistical estimators and machine learning\nalgorithms determine these parameters by minimizing mean-squared error (MSE), a\ncriterion that can yield suboptimal investment decisions. In this paper, we\nadopt decision-focused learning (DFL) - an approach that directly optimizes\ndecision quality rather than prediction error such as MSE - to derive the\nglobal minimum-variance portfolio (GMVP). Specifically, we theoretically derive\nthe gradient of decision loss using the analytic solution of GMVP and its\nproperties regarding the principal components of itself. Through extensive\nempirical evaluation, we show that prediction-focused estimation methods may\nfail to produce optimal allocations in practice, whereas DFL-based methods\nconsistently deliver superior decision performance. Furthermore, we provide a\ncomprehensive analysis of DFL's mechanism in GMVP construction, focusing on its\nvolatility reduction capability, decision-driving features, and estimation\ncharacteristics.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10776v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10776v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.375,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10777",
      "title": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in\n  Clinical Natural Language Inference",
      "authors": [
        "Maël Jullien",
        "Marco Valentino",
        "André Freitas"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models are often assumed to acquire increasingly structured,\ngeneralizable internal representations simply by scaling data and parameters.\nWe interrogate this assumption by introducing a Clinical Trial Natural Language\nInference benchmark comprising four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction.\nEach item is paired with a targeted Ground Knowledge and Meta-Level Reasoning\nVerification (GKMRV) probe, allowing us to dissociate failures of factual\naccess from failures of inference. We evaluate six contemporary LLMs under both\ndirect and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform\npoorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,\noutput inferences are highly consistent across samples (mean 0.87), indicating\na systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations:\ncurrent LLMs often possess the relevant clinical knowledge but lack the\nstructured, composable internal representations needed to deploy it reliably\n(e.g., integrating constraints, weighing evidence, or simulating\ncounterfactuals). Decoupling knowledge from reasoning with GKMRV makes this\ndissociation explicit and measurable, providing an effective framework for\nprobing the reliability of LLMs in high-stakes domains.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10777v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10777v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.563,
      "distributed_training_score": 0.37,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating LLMs for clinical reasoning capabilities using a benchmark, without any mention of training methods involving human feedback, reward models, or reinforcement learning. It does not address or contribute to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates LLMs with chain-of-thought prompting but does not involve diffusion models, iterative refinement processes, or adaptations of diffusion for multi-step logical reasoning. It only tests general reasoning without any diffusion-based components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10779",
      "title": "Ultra-High-Definition Reference-Based Landmark Image Super-Resolution\n  with Generative Diffusion Prior",
      "authors": [
        "Zhenning Shi",
        "Zizheng Yan",
        "Yuhang Yu",
        "Clara Xue",
        "Jingyu Zhuang",
        "Qi Zhang",
        "Jinwei Chen",
        "Tao Li",
        "Qingnan Fan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reference-based Image Super-Resolution (RefSR) aims to restore a\nlow-resolution (LR) image by utilizing the semantic and texture information\nfrom an additional reference high-resolution (reference HR) image. Existing\ndiffusion-based RefSR methods are typically built upon ControlNet, which\nstruggles to effectively align the information between the LR image and the\nreference HR image. Moreover, current RefSR datasets suffer from limited\nresolution and poor image quality, resulting in the reference images lacking\nsufficient fine-grained details to support high-quality restoration. To\novercome the limitations above, we propose TriFlowSR, a novel framework that\nexplicitly achieves pattern matching between the LR image and the reference HR\nimage. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for\nUltra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios\nwith real-world degradation, in TriFlowSR, we design a Reference Matching\nStrategy to effectively match the LR image with the reference HR image.\nExperimental results show that our approach can better utilize the semantic and\ntexture information of the reference HR image compared to previous methods. To\nthe best of our knowledge, we propose the first diffusion-based RefSR pipeline\nfor ultra-high definition landmark scenarios under real-world degradation. Our\ncode and model will be available at https://github.com/nkicsl/TriFlowSR.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10779v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10779v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.349,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for image super-resolution, specifically for enhancing low-resolution images with reference images in ultra-high-definition scenarios. While it employs the iterative refinement process of diffusion for generative tasks, it does not adapt this process for multi-step logical reasoning, Chain-of-Thought, or solving complex logical tasks. The core application is in computer vision, not reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10784",
      "title": "Insights from the Algonauts 2025 Winners",
      "authors": [
        "Paul S. Scotti",
        "Mihir Tripathy"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The Algonauts 2025 Challenge just wrapped up a few weeks ago. It is a\nbiennial challenge in computational neuroscience in which teams attempt to\nbuild models that predict human brain activity from carefully curated stimuli.\nPrevious editions (2019, 2021, 2023) focused on still images and short videos;\nthe 2025 edition, which concluded last month (late July), pushed the field\nfurther by using long, multimodal movies. Teams were tasked with predicting\nfMRI responses across 1,000 whole-brain parcels across four participants in the\ndataset who were scanned while watching nearly 80 hours of naturalistic movie\nstimuli. These recordings came from the CNeuroMod project and included 65 hours\nof training data, about 55 hours of Friends (seasons 1-6) plus four feature\nfilms (The Bourne Supremacy, Hidden Figures, Life, and The Wolf of Wall\nStreet). The remaining data were used for validation: Season 7 of Friends for\nin-distribution tests, and the final winners for the Challenge were those who\ncould best predict brain activity for six films in their held-out\nout-of-distribution (OOD) set. The winners were just announced and the top team\nreports are now publicly available. As members of the MedARC team which placed\n4th in the competition, we reflect on the approaches that worked, what they\nreveal about the current state of brain encoding, and what might come next.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10784v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10784v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.325,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on computational neuroscience models for predicting human brain activity from movie stimuli in the Algonauts 2025 Challenge, emphasizing approaches like those used by the authors' team. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10785",
      "title": "Enhancing Fairness in Autoencoders for Node-Level Graph Anomaly\n  Detection",
      "authors": [
        "Shouju Wang",
        "Yuchen Song",
        "Sheng'en Li",
        "Dongmian Zou"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Graph anomaly detection (GAD) has become an increasingly important task\nacross various domains. With the rapid development of graph neural networks\n(GNNs), GAD methods have achieved significant performance improvements.\nHowever, fairness considerations in GAD remain largely underexplored. Indeed,\nGNN-based GAD models can inherit and amplify biases present in training data,\npotentially leading to unfair outcomes. While existing efforts have focused on\ndeveloping fair GNNs, most approaches target node classification tasks, where\nmodels often rely on simple layer architectures rather than autoencoder-based\nstructures, which are the most widely used architecturs for anomaly detection.\nTo address fairness in autoencoder-based GAD models, we propose\n\\textbf{D}is\\textbf{E}ntangled \\textbf{C}ounterfactual \\textbf{A}dversarial\n\\textbf{F}air (DECAF)-GAD, a framework that alleviates bias while preserving\nGAD performance. Specifically, we introduce a structural causal model (SCM) to\ndisentangle sensitive attributes from learned representations. Based on this\ncausal framework, we formulate a specialized autoencoder architecture along\nwith a fairness-guided loss function. Through extensive experiments on both\nsynthetic and real-world datasets, we demonstrate that DECAF-GAD not only\nachieves competitive anomaly detection performance but also significantly\nenhances fairness metrics compared to baseline GAD methods. Our code is\navailable at https://github.com/Tlhey/decaf_code.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10785v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10785v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.357,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10786",
      "title": "Cooperative Face Liveness Detection from Optical Flow",
      "authors": [
        "Artem Sokolov",
        "Mikhail Nikitin",
        "Anton Konushin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this work, we proposed a novel cooperative video-based face liveness\ndetection method based on a new user interaction scenario where participants\nare instructed to slowly move their frontal-oriented face closer to the camera.\nThis controlled approaching face protocol, combined with optical flow analysis,\nrepresents the core innovation of our approach. By designing a system where\nusers follow this specific movement pattern, we enable robust extraction of\nfacial volume information through neural optical flow estimation, significantly\nimproving discrimination between genuine faces and various presentation attacks\n(including printed photos, screen displays, masks, and video replays). Our\nmethod processes both the predicted optical flows and RGB frames through a\nneural classifier, effectively leveraging spatial-temporal features for more\nreliable liveness detection compared to passive methods.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10786v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10786v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.275,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.27,
      "datasets_score": 0.246,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10794",
      "title": "VasoMIM: Vascular Anatomy-Aware Masked Image Modeling for Vessel\n  Segmentation",
      "authors": [
        "De-Xing Huang",
        "Xiao-Hu Zhou",
        "Mei-Jiang Gui",
        "Xiao-Liang Xie",
        "Shi-Qi Liu",
        "Shuang-Yi Wang",
        "Tian-Yu Xiang",
        "Rui-Ze Ma",
        "Nu-Fang Xiao",
        "Zeng-Guang Hou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate vessel segmentation in X-ray angiograms is crucial for numerous\nclinical applications. However, the scarcity of annotated data presents a\nsignificant challenge, which has driven the adoption of self-supervised\nlearning (SSL) methods such as masked image modeling (MIM) to leverage\nlarge-scale unlabeled data for learning transferable representations.\nUnfortunately, conventional MIM often fails to capture vascular anatomy because\nof the severe class imbalance between vessel and background pixels, leading to\nweak vascular representations. To address this, we introduce Vascular\nanatomy-aware Masked Image Modeling (VasoMIM), a novel MIM framework tailored\nfor X-ray angiograms that explicitly integrates anatomical knowledge into the\npre-training process. Specifically, it comprises two complementary components:\nanatomy-guided masking strategy and anatomical consistency loss. The former\npreferentially masks vessel-containing patches to focus the model on\nreconstructing vessel-relevant regions. The latter enforces consistency in\nvascular semantics between the original and reconstructed images, thereby\nimproving the discriminability of vascular representations. Empirically,\nVasoMIM achieves state-of-the-art performance across three datasets. These\nfindings highlight its potential to facilitate X-ray angiogram analysis.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10794v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10794v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.312,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10797",
      "title": "When Experts Disagree: Characterizing Annotator Variability for Vessel\n  Segmentation in DSA Images",
      "authors": [
        "M. Geshvadi",
        "G. So",
        "D. D. Chlorogiannis",
        "C. Galvin",
        "E. Torio",
        "A. Azimi",
        "Y. Tachie-Baffour",
        "N. Haouchine",
        "A. Golby",
        "M. Vangel",
        "W. M. Wells",
        "Y. Epelboym",
        "R. Du",
        "F. Durupinar",
        "S. Frisken"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We analyze the variability among segmentations of cranial blood vessels in 2D\nDSA performed by multiple annotators in order to characterize and quantify\nsegmentation uncertainty. We use this analysis to quantify segmentation\nuncertainty and discuss ways it can be used to guide additional annotations and\nto develop uncertainty-aware automatic segmentation methods.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10797v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10797v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.24,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.299,
      "distributed_training_score": 0.296,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10798",
      "title": "The SET Perceptual Factors Framework: Towards Assured Perception for\n  Autonomous Systems",
      "authors": [
        "Troi Williams"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Future autonomous systems promise significant societal benefits, yet their\ndeployment raises concerns about safety and trustworthiness. A key concern is\nassuring the reliability of robot perception, as perception seeds safe\ndecision-making. Failures in perception are often due to complex yet common\nenvironmental factors and can lead to accidents that erode public trust. To\naddress this concern, we introduce the SET (Self, Environment, and Target)\nPerceptual Factors Framework. We designed the framework to systematically\nanalyze how factors such as weather, occlusion, or sensor limitations\nnegatively impact perception. To achieve this, the framework employs SET State\nTrees to categorize where such factors originate and SET Factor Trees to model\nhow these sources and factors impact perceptual tasks like object detection or\npose estimation. Next, we develop Perceptual Factor Models using both trees to\nquantify the uncertainty for a given task. Our framework aims to promote\nrigorous safety assurances and cultivate greater public understanding and trust\nin autonomous systems by offering a transparent and standardized method for\nidentifying, modeling, and communicating perceptual risks.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10798v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10798v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.327,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10801",
      "title": "Object Fidelity Diffusion for Remote Sensing Image Generation",
      "authors": [
        "Ziqi Ye",
        "Shuran Ma",
        "Jie Yang",
        "Xiaoyi Yang",
        "Ziyang Gong",
        "Xue Yang",
        "Haipeng Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "High-precision controllable remote sensing image generation is both\nmeaningful and challenging. Existing diffusion models often produce\nlow-fidelity images due to their inability to adequately capture morphological\ndetails, which may affect the robustness and reliability of object detection\nmodels. To enhance the accuracy and fidelity of generated objects in remote\nsensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which\neffectively improves the fidelity of generated objects. Specifically, we are\nthe first to extract the prior shapes of objects based on the layout for\ndiffusion models in remote sensing. Then, we introduce a dual-branch diffusion\nmodel with diffusion consistency loss, which can generate high-fidelity remote\nsensing images without providing real images during the sampling phase.\nFurthermore, we introduce DDPO to fine-tune the diffusion process, making the\ngenerated remote sensing images more diverse and semantically consistent.\nComprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art\nmethods in the remote sensing across key quality metrics. Notably, the\nperformance of several polymorphic and small object classes shows significant\nimprovement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for\nairplanes, ships, and vehicles, respectively.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10801v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10801v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.541,
      "distributed_training_score": 0.351,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing diffusion models for generating high-fidelity remote sensing images by incorporating shape extraction, dual-branch diffusion, and fine-tuning techniques. It does not involve adapting diffusion for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. Instead, it applies diffusion to visual data generation, which lacks any component for holistic reasoning correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10806",
      "title": "Who Benefits from AI Explanations? Towards Accessible and Interpretable\n  Systems",
      "authors": [
        "Maria J. P. Peixoto",
        "Akriti Pandey",
        "Ahsan Zaman",
        "Peter R. Lewis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As AI systems are increasingly deployed to support decision-making in\ncritical domains, explainability has become a means to enhance the\nunderstandability of these outputs and enable users to make more informed and\nconscious choices. However, despite growing interest in the usability of\neXplainable AI (XAI), the accessibility of these methods, particularly for\nusers with vision impairments, remains underexplored. This paper investigates\naccessibility gaps in XAI through a two-pronged approach. First, a literature\nreview of 79 studies reveals that evaluations of XAI techniques rarely include\ndisabled users, with most explanations relying on inherently visual formats.\nSecond, we present a four-part methodological proof of concept that\noperationalizes inclusive XAI design: (1) categorization of AI systems, (2)\npersona definition and contextualization, (3) prototype design and\nimplementation, and (4) expert and user assessment of XAI techniques for\naccessibility. Preliminary findings suggest that simplified explanations are\nmore comprehensible for non-visual users than detailed ones, and that\nmultimodal presentation is required for more equitable interpretability.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10806v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10806v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.296,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10817",
      "title": "Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight\n  CNN Benchmark Across 101 Classes of 33 Crops",
      "authors": [
        "Anand Kumar",
        "Harminder Pal Monga",
        "Tapasi Brahma",
        "Satyam Kalra",
        "Navas Sherif"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Plant diseases are a major threat to food security globally. It is important\nto develop early detection systems which can accurately detect. The advancement\nin computer vision techniques has the potential to solve this challenge. We\nhave developed a mobile-friendly solution which can accurately classify 101\nplant diseases across 33 crops. We built a comprehensive dataset by combining\ndifferent datasets, Plant Doc, PlantVillage, and PlantWild, all of which are\nfor the same purpose. We evaluated performance across several lightweight\narchitectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and\nEfficientNet-B0, B1 - specifically chosen for their efficiency on\nresource-constrained devices. The results were promising, with EfficientNet-B1\ndelivering our best performance at 94.7% classification accuracy. This\narchitecture struck an optimal balance between accuracy and computational\nefficiency, making it well-suited for real-world deployment on mobile devices.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10817v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10817v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.256,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.248,
      "distributed_training_score": 0.35,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10828",
      "title": "A Multimodal Neural Network for Recognizing Subjective Self-Disclosure\n  Towards Social Robots",
      "authors": [
        "Henry Powell",
        "Guy Laban",
        "Emily S. Cross"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Subjective self-disclosure is an important feature of human social\ninteraction. While much has been done in the social and behavioural literature\nto characterise the features and consequences of subjective self-disclosure,\nlittle work has been done thus far to develop computational systems that are\nable to accurately model it. Even less work has been done that attempts to\nmodel specifically how human interactants self-disclose with robotic partners.\nIt is becoming more pressing as we require social robots to work in conjunction\nwith and establish relationships with humans in various social settings. In\nthis paper, our aim is to develop a custom multimodal attention network based\non models from the emotion recognition literature, training this model on a\nlarge self-collected self-disclosure video corpus, and constructing a new loss\nfunction, the scale preserving cross entropy loss, that improves upon both\nclassification and regression versions of this problem. Our results show that\nthe best performing model, trained with our novel loss function, achieves an F1\nscore of 0.83, an improvement of 0.48 from the best baseline model. This result\nmakes significant headway in the aim of allowing social robots to pick up on an\ninteraction partner's self-disclosures, an ability that will be essential in\nsocial robots with social cognition.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10828v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10828v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.325,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of a multimodal neural network for recognizing subjective self-disclosure in human-robot interactions, including a custom attention-based architecture and a novel loss function. It focuses on supervised learning techniques using audio and video data, with no mention of reinforcement learning, human-ranked data for training a reward model, or fine-tuning via reinforcement learning to align AI with human preferences. Therefore, it does not relate to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10833",
      "title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
      "authors": [
        "Zhangxuan Gu",
        "Zhengwen Zeng",
        "Zhenyu Xu",
        "Xingran Zhou",
        "Shuheng Shen",
        "Yunfei Liu",
        "Beitong Zhou",
        "Changhua Meng",
        "Tianyu Xia",
        "Weizhi Chen",
        "Yue Wen",
        "Jingya Dou",
        "Fei Tang",
        "Jinzhen Lin",
        "Yulin Liu",
        "Zhenlin Guo",
        "Yichen Gong",
        "Heng Jia",
        "Changlong Gao",
        "Yuan Guo",
        "Yong Deng",
        "Zhenyu Guo",
        "Liang Chen",
        "Weiqiang Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present UI-Venus, a native UI agent that takes only screenshots as input\nbased on a multimodal large language model. UI-Venus achieves SOTA performance\non both UI grounding and navigation tasks using only several hundred thousand\nhigh-quality training samples through reinforcement finetune (RFT) based on\nQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /\n50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,\nScreenspot-V2 / Pro, surpassing the previous SOTA baselines including\nopen-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venus's summary and\nplaning ability, we also evaluate it on the AndroidWorld, an online UI\nnavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%\nsuccess rate, also beating existing models. To achieve this, we introduce\ncarefully designed reward functions for both UI grounding and navigation tasks\nand corresponding efficient data cleaning strategies. To further boost\nnavigation performance, we propose Self-Evolving Trajectory History Alignment &\nSparse Action Enhancement that refine historical reasoning traces and balances\nthe distribution of sparse but critical actions, leading to more coherent\nplanning and better generalization in complex UI tasks. Our contributions\ninclude the publish of SOTA open-source UI agents, comprehensive data cleaning\nprotocols and a novel self-evolving framework for improving navigation\nperformance, which encourage further research and development in the community.\nCode is available at https://github.com/inclusionAI/UI-Venus.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10833v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10833v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.352,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on reinforcement finetuning (RFT) using Group Relative Policy Optimization (GRPO) with carefully designed reward functions for UI tasks, but it does not involve human feedback. Specifically, rewards are based on task-specific metrics and programmatically generated data, rather than training a separate reward model on human-ranked data as required for RLHF. Therefore, the paper's contributions do not align with the definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10838",
      "title": "Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning",
      "authors": [
        "Peng Xu",
        "Zhiyu Xiang",
        "Jingyun Fu",
        "Tianyu Pu",
        "Kai Wang",
        "Chaojie Ji",
        "Tingming Bai",
        "Eryun Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Current self-supervised stereo matching relies on the photometric consistency\nassumption, which breaks down in occluded regions due to ill-posed\ncorrespondences. To address this issue, we propose BaCon-Stereo, a simple yet\neffective contrastive learning framework for self-supervised stereo network\ntraining in both non-occluded and occluded regions. We adopt a teacher-student\nparadigm with multi-baseline inputs, in which the stereo pairs fed into the\nteacher and student share the same reference view but differ in target views.\nGeometrically, regions occluded in the student's target view are often visible\nin the teacher's, making it easier for the teacher to predict in these regions.\nThe teacher's prediction is rescaled to match the student's baseline and then\nused to supervise the student. We also introduce an occlusion-aware attention\nmap to better guide the student in learning occlusion completion. To support\ntraining, we synthesize a multi-baseline dataset BaCon-20k. Extensive\nexperiments demonstrate that BaCon-Stereo improves prediction in both occluded\nand non-occluded regions, achieves strong generalization and robustness, and\noutperforms state-of-the-art self-supervised methods on both KITTI 2015 and\n2012 benchmarks. Our code and dataset will be released upon paper acceptance.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10838v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10838v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.279,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.318,
      "distributed_training_score": 0.334,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10839",
      "title": "Reinforced Language Models for Sequential Decision Making",
      "authors": [
        "Jim Dilkes",
        "Vahid Yazdanpanah",
        "Sebastian Stein"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs) show potential as sequential decision-making\nagents, but their application is often limited due to a reliance on large,\ncomputationally expensive models. This creates a need to improve smaller\nmodels, yet existing post-training methods are designed for single-turn\ninteractions and cannot handle credit assignment in multi-step agentic tasks.\nTo address this, we introduce Multi-Step Group-Relative Policy Optimization\n(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal\nText-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)\nframeworks. For credit assignment, MS-GRPO attributes the entire cumulative\nepisode reward to each individual episode step. We supplement this algorithm\nwith a novel absolute-advantage-weighted episode sampling strategy that we show\nimproves training performance. We evaluate our approach by post-training a\n3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate\nthat the method is effective in improving decision-making performance: our\npost-trained 3B parameter model outperforms a 72B parameter baseline by 50% on\nthe Frozen Lake task. This work demonstrates that targeted post-training is a\npractical and efficient alternative to relying on model scale for creating\nsequential decision-making agents using LLMs.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10839v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10839v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.507,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.495,
      "distributed_training_score": 0.418,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces MS-GRPO, a reinforcement learning algorithm for sequential decision-making, but it does not involve training with human preferences or a reward model based on human-ranked data. While it references RLHF as an existing method, it explicitly notes its unsuitability and does not use human feedback in the proposed approach.",
      "weak_supervision_justification": "The paper focuses on post-training LLMs using rewards from sequential decision-making tasks, without any mention of programmatically generating noisy or imprecise labels. It relies on actual episode rewards rather than weak supervision techniques for label creation.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for reasoning. It centers on reinforcement learning for sequential decision-making, with no components related to treating a chain-of-thought as a holistically corrected entity.",
      "distributed_training_justification": "The paper does not discuss parallel computing, multi-node setups, or strategies for partitioning data or computation to accelerate training. It focuses on the MS-GRPO algorithm and its evaluation, without any reference to distributed training techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10840",
      "title": "Generalizable Federated Learning using Client Adaptive Focal Modulation",
      "authors": [
        "Tajamul Ashraf",
        "Iqra Altaf Gillani"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Federated learning (FL) has proven essential for privacy-preserving,\ncollaborative training across distributed clients. Our prior work, TransFed,\nintroduced a robust transformer-based FL framework that leverages a\nlearn-to-adapt hypernetwork to generate personalized focal modulation layers\nper client, outperforming traditional methods in non-IID and cross-domain\nsettings. In this extended version, we propose AdaptFED, where we deepen the\ninvestigation of focal modulation in generalizable FL by incorporating: (1) a\nrefined adaptation strategy that integrates task-aware client embeddings to\npersonalize modulation dynamics further, (2) enhanced theoretical bounds on\nadaptation performance, and (3) broader empirical validation across additional\nmodalities, including time-series and multilingual data. We also introduce an\nefficient variant of TransFed that reduces server-client communication overhead\nvia low-rank hypernetwork conditioning, enabling scalable deployment in\nresource-constrained environments. Extensive experiments on eight diverse\ndatasets reaffirm the superiority of our method over state-of-the-art\nbaselines, particularly in source-free and cross-task federated setups. Our\nfindings not only extend the capabilities of focal modulation in FL but also\npave the way for more adaptive, scalable, and generalizable transformer-based\nfederated systems. The code is available at\nhttp://github.com/Tajamul21/TransFed",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10840v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10840v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.469,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated learning with transformer-based models and focal modulation for personalized training across clients, emphasizing privacy, adaptation, and scalability. There is no mention of reinforcement learning, human feedback, reward models, or any mechanism involving human-ranked data to align AI models with preferences. Thus, the paper's contributions do not relate to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's core contribution involves federated learning, a distributed training paradigm that partitions data and computation across multiple clients to train models collaboratively while preserving privacy. It addresses key distributed training challenges, such as handling non-IID data, reducing communication overhead via low-rank hypernetwork conditioning, and enabling scalable deployment in resource-constrained environments. Features like client-specific adaptations and multi-node interactions align directly with distributed training concepts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper extends the authors' prior work, TransFed, by introducing AdaptFED, a refined federated learning framework that employs client adaptive focal modulation to personalize transformer-based models for heterogeneous, non-IID data distributions. The methodology incorporates task-aware client embeddings, enhanced theoretical bounds on adaptation performance, an efficient low-rank variant to reduce communication overhead, and extensive empirical validation across diverse datasets including vision, time-series, and multilingual tasks, demonstrating superior generalization and performance in cross-domain and source-free settings compared to state-of-the-art baselines.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents notable improvements to existing federated learning techniques through refinements like task-aware embeddings and low-rank hypernetwork conditioning, offering a clever extension of their prior work rather than a completely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in personalized federated learning and transformer applications by addressing scalability and heterogeneity, though its impact may be primarily confined to specific subfields like computer vision and distributed machine learning.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides valuable advancements in handling data heterogeneity in federated learning, making it essential for researchers focused on personalized models and transformers, but not critical for a general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5d2567414f248ec1313401edc17db835f1bf33d6",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 5,
      "average_h_index": 3.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Tajamul Ashraf",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2290686502"
        },
        {
          "name": "Iqra Altaf Gillani",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/8788199"
        }
      ]
    },
    {
      "id": "2508.10858",
      "title": "Hierarchical Fine-grained Preference Optimization for Physically\n  Plausible Video Generation",
      "authors": [
        "Harold Haodong Chen",
        "Haojian Huang",
        "Qifeng Chen",
        "Harry Yang",
        "Ser-Nam Lim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advancements in video generation have enabled the creation of\nhigh-quality, visually compelling videos. However, generating videos that\nadhere to the laws of physics remains a critical challenge for applications\nrequiring realism and accuracy. In this work, we propose PhysHPO, a novel\nframework for Hierarchical Cross-Modal Direct Preference Optimization, to\ntackle this challenge by enabling fine-grained preference alignment for\nphysically plausible video generation. PhysHPO optimizes video alignment across\nfour hierarchical granularities: a) Instance Level, aligning the overall video\ncontent with the input prompt; b) State Level, ensuring temporal consistency\nusing boundary frames as anchors; c) Motion Level, modeling motion trajectories\nfor realistic dynamics; and d) Semantic Level, maintaining logical consistency\nbetween narrative and visuals. Recognizing that real-world videos are the best\nreflections of physical phenomena, we further introduce an automated data\nselection pipeline to efficiently identify and utilize \"good data\" from\nexisting large-scale text-video datasets, thereby eliminating the need for\ncostly and time-intensive dataset construction. Extensive experiments on both\nphysics-focused and general capability benchmarks demonstrate that PhysHPO\nsignificantly improves physical plausibility and overall video generation\nquality of advanced models. To the best of our knowledge, this is the first\nwork to explore fine-grained preference alignment and data selection for video\ngeneration, paving the way for more realistic and human-preferred video\ngeneration paradigms.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10858v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10858v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.475,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.38,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Direct Preference Optimization (DPO), which involves optimizing based on human-preferred and non-preferred samples, similar to concepts in RLHF. However, it does not explicitly describe training a separate reward model or using reinforcement learning for fine-tuning, focusing instead on hierarchical alignment for video generation. Thus, it is related but not a direct implementation of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on hierarchical preference optimization for physically plausible video generation and does not involve adapting diffusion models for multi-step logical reasoning or treating a Chain-of-Thought as an entity. While video generation may use diffusion techniques, there is no clear component for diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10860",
      "title": "From Black Box to Transparency: Enhancing Automated Interpreting\n  Assessment with Explainable AI in College Classrooms",
      "authors": [
        "Zhaokun Jiang",
        "Ziyin Zhang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advancements in machine learning have spurred growing interests in\nautomated interpreting quality assessment. Nevertheless, existing research\nsuffers from insufficient examination of language use quality, unsatisfactory\nmodeling effectiveness due to data scarcity and imbalance, and a lack of\nefforts to explain model predictions. To address these gaps, we propose a\nmulti-dimensional modeling framework that integrates feature engineering, data\naugmentation, and explainable machine learning. This approach prioritizes\nexplainability over ``black box'' predictions by utilizing only\nconstruct-relevant, transparent features and conducting Shapley Value (SHAP)\nanalysis. Our results demonstrate strong predictive performance on a novel\nEnglish-Chinese consecutive interpreting dataset, identifying BLEURT and\nCometKiwi scores to be the strongest predictive features for fidelity,\npause-related features for fluency, and Chinese-specific phraseological\ndiversity metrics for language use. Overall, by placing particular emphasis on\nexplainability, we present a scalable, reliable, and transparent alternative to\ntraditional human evaluation, facilitating the provision of detailed diagnostic\nfeedback for learners and supporting self-regulated learning advantages not\nafforded by automated scores in isolation.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10860v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10860v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.46,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.36,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on automated interpreting assessment using feature engineering, data augmentation, and explainable AI techniques like SHAP, without any mention of reinforcement learning, human feedback for training a reward model, or fine-tuning via RL.",
      "weak_supervision_justification": "The paper employs data augmentation with Variational Auto-Encoders (VAEs) to handle data scarcity and imbalance, which indirectly relates to weak supervision by programmatically generating training data, but it does not primarily rely on noisy or imprecise label sources for model training.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for reasoning tasks, or any multi-step logical reasoning processes; it instead uses VAEs for data augmentation and SHAP for explainability in interpreting assessment.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10865",
      "title": "Performance of GPT-5 in Brain Tumor MRI Reasoning",
      "authors": [
        "Mojtaba Safari",
        "Shansong Wang",
        "Mingzhe Hu",
        "Zach Eidex",
        "Qiang Li",
        "Xiaofeng Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate differentiation of brain tumor types on magnetic resonance imaging\n(MRI) is critical for guiding treatment planning in neuro-oncology. Recent\nadvances in large language models (LLMs) have enabled visual question answering\n(VQA) approaches that integrate image interpretation with natural language\nreasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and\nGPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor\nSegmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain\nmetastases (MET). Each case included multi-sequence MRI triplanar mosaics and\nstructured clinical features transformed into standardized VQA items. Models\nwere assessed in a zero-shot chain-of-thought setting for accuracy on both\nvisual and reasoning tasks. Results showed that GPT-5-mini achieved the highest\nmacro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%),\nand GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single\nmodel dominating across all cohorts. These findings suggest that GPT-5 family\nmodels can achieve moderate accuracy in structured neuro-oncological VQA tasks,\nbut not at a level acceptable for clinical use.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10865v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10865v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.337,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates the performance of GPT models (e.g., GPT-4o, GPT-5 variants) on brain tumor MRI visual question answering tasks using a zero-shot chain-of-thought approach. However, it does not involve or adapt diffusion models for iterative refinement in logical tasks. The reasoning discussed is based on LLMs and prompting techniques, not diffusion-based processes, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10868",
      "title": "TexVerse: A Universe of 3D Objects with High-Resolution Textures",
      "authors": [
        "Yibo Zhang",
        "Li Zhang",
        "Rui Ma",
        "Nan Cao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce TexVerse, a large-scale 3D dataset featuring high-resolution\ntextures. While recent advances in large-scale 3D datasets have enhanced\nhigh-resolution geometry generation, creating high-resolution textures\nend-to-end remains underexplored due to the lack of suitable datasets. TexVerse\nfills this gap with a curated collection of over 858K unique high-resolution 3D\nmodels sourced from Sketchfab, including more than 158K models with physically\nbased rendering (PBR) materials. Each model encompasses all of its\nhigh-resolution variants, bringing the total to 1.6M 3D instances. TexVerse\nalso includes specialized subsets: TexVerse-Skeleton, with 69K rigged models,\nand TexVerse-Animation, with 54K animated models, both preserving original\nskeleton and animation data uploaded by the user. We also provide detailed\nmodel annotations describing overall characteristics, structural components,\nand intricate features. TexVerse offers a high-quality data resource with\nwide-ranging potential applications in texture synthesis, PBR material\ndevelopment, animation, and various 3D vision and graphics tasks.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10868v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10868v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.253,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.322,
      "datasets_score": 0.416,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of TexVerse, a new large-scale 3D dataset with high-resolution textures, including subsets and annotations. This directly aligns with research on creating datasets for machine learning and AI applications, as it details dataset curation methodologies, such as filtering models from Sketchfab and providing annotations, which supports 3D vision and graphics tasks.",
      "llm_score_status": "completed",
      "summary": "TexVerse is a large-scale 3D dataset designed to address the gap in high-resolution texture resources for 3D objects, comprising over 858,000 unique models sourced and curated from Sketchfab with textures at least 1024 pixels, including 158,000 with PBR materials and subsets like TexVerse-Skeleton (69,000 rigged models) and TexVerse-Animation (54,000 animated models). The methodology involves filtering for high-resolution textures, retaining distributable licenses, and adding detailed annotations via GPT-5, with key findings highlighting its potential to advance texture synthesis, PBR material development, and various 3D vision and graphics tasks by providing a comprehensive, high-quality data resource that overcomes limitations in existing datasets like Objaverse.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by curating a large-scale dataset focused on high-resolution textures, which combines existing sources like Sketchfab in a new way to address a specific gap in 3D data resources. However, it is primarily a data collection effort rather than introducing a fundamentally new technique or architecture.",
      "impact_score": "High",
      "impact_justification": "The work could significantly influence future research in 3D texture generation, PBR materials, and graphics tasks by providing a much-needed high-resolution dataset, potentially leading to advancements in both academic studies and commercial applications like gaming and animation. Its scale and specialized subsets make it likely to be widely adopted and built upon in the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by introducing a high-quality dataset that fills a critical gap in 3D resources, making it essential for researchers and practitioners in computer vision and graphics to be aware of. While not groundbreaking in methodology, its practical utility warrants attention for those working on related tasks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/02c762ee0da8295e1dcf856782d84ed24331e6e5",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 3,
      "average_h_index": 1.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yibo Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2303326412"
        },
        {
          "name": "Li Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2152834553"
        },
        {
          "name": "Rui Ma",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2303793770"
        },
        {
          "name": "Nan Cao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375901910"
        }
      ]
    },
    {
      "id": "2508.10869",
      "title": "Medico 2025: Visual Question Answering for Gastrointestinal Imaging",
      "authors": [
        "Sushant Gautam",
        "Vajira Thambawita",
        "Michael Riegler",
        "Pål Halvorsen",
        "Steven Hicks"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The Medico 2025 challenge addresses Visual Question Answering (VQA) for\nGastrointestinal (GI) imaging, organized as part of the MediaEval task series.\nThe challenge focuses on developing Explainable Artificial Intelligence (XAI)\nmodels that answer clinically relevant questions based on GI endoscopy images\nwhile providing interpretable justifications aligned with medical reasoning. It\nintroduces two subtasks: (1) answering diverse types of visual questions using\nthe Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to\nsupport clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500\nimages and 159,549 complex question-answer (QA) pairs, serves as the benchmark\nfor the challenge. By combining quantitative performance metrics and\nexpert-reviewed explainability assessments, this task aims to advance\ntrustworthy Artificial Intelligence (AI) in medical image analysis.\nInstructions, data access, and an updated guide for participation are available\nin the official competition repository:\nhttps://github.com/simula/MediaEval-Medico-2025",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10869v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10869v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.291,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10872",
      "title": "TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning",
      "authors": [
        "Anantha Narayanan",
        "Battu Bhanu Teja",
        "Pruthwik Mishra"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The increasing congestion of Low Earth Orbit (LEO) poses persistent\nchallenges to the efficient deployment and safe operation of Earth observation\nsatellites. Mission planners must now account not only for mission-specific\nrequirements but also for the increasing collision risk with active satellites\nand space debris. This work presents a reinforcement learning framework using\nthe Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital\nparameters for precise terrestrial coverage within predefined surface radii. By\nformulating the problem as a Markov Decision Process (MDP) within a custom\nOpenAI Gymnasium environment, our method simulates orbital dynamics using\nclassical Keplerian elements. The agent progressively learns to adjust five of\nthe orbital parameters - semi-major axis, eccentricity, inclination, right\nascension of ascending node, and the argument of perigee-to achieve targeted\nterrestrial coverage. Comparative evaluation against Proximal Policy\nOptimization (PPO) demonstrates A2C's superior performance, achieving 5.8x\nhigher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer\ntimesteps (2,000 vs 63,000). The A2C agent consistently meets mission\nobjectives across diverse target coordinates while maintaining computational\nefficiency suitable for real-time mission planning applications. Key\ncontributions include: (1) a TLE-based orbital simulation environment\nincorporating physics constraints, (2) validation of actor-critic methods'\nsuperiority over trust region approaches in continuous orbital control, and (3)\ndemonstration of rapid convergence enabling adaptive satellite deployment. This\napproach establishes reinforcement learning as a computationally efficient\nalternative for scalable and intelligent LEO mission planning.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10872v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10872v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.351,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using the A2C algorithm for optimizing satellite orbital parameters in a reinforcement learning framework, without any mention of human feedback, human-ranked data, or a reward model trained on human preferences. RLHF specifically requires human involvement in defining rewards or fine-tuning, which is absent here, making the paper unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10875",
      "title": "A Survey on Diffusion Language Models",
      "authors": [
        "Tianyi Li",
        "Mingda Chen",
        "Bowei Guo",
        "Zhiqiang Shen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10875v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10875v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.583,
      "distributed_training_score": 0.455,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses reinforcement learning (RL) techniques for DLMs, such as diffu-GRPO, UniGRPO, SEPO, Coupled-GRPO, and VRPO, which are used for post-training alignment and preference optimization. While these methods align with RLHF concepts by improving model performance based on preferences, the paper does not explicitly mention human feedback or training a reward model on human-ranked data, making it only moderately relevant rather than a core focus.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper highlights the iterative denoising process in DLMs, which allows for refinement and could indirectly support reasoning tasks through multi-step updates, as seen in advantages like iterative refinement for coherent generation. However, it lacks a clear component dedicated to multi-step logical reasoning or treating a 'Chain-of-Thought' as a holistic entity, so it is only tangentially related and not a primary contribution.",
      "distributed_training_justification": "The paper focuses on DLMs' training strategies, such as pre-training and fine-tuning, but does not discuss distributed training, parallel computing across nodes, or algorithms for partitioning data/computation. Mentions of efficiency are limited to inference optimizations like parallelism and caching, with no coverage of multi-node machine learning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This survey paper provides a comprehensive overview of Diffusion Language Models (DLMs), tracing their evolution from autoregressive and masked language models, and categorizes them into continuous, discrete, and hybrid paradigms while discussing training strategies, inference optimizations, multimodal extensions, applications, limitations, and future directions. The authors analyze DLMs' advantages, such as parallel generation, bidirectional context, and controllability, over traditional autoregressive models, and present a taxonomy, timelines, and performance comparisons to highlight their growing potential in natural language processing and beyond.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper offers a notable improvement by providing the first comprehensive taxonomy and systematic review of DLMs, cleverly synthesizing existing ideas into a structured overview that addresses a gap in the literature, though it does not introduce entirely new techniques or problems.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research in AI and NLP by serving as a foundational reference for DLMs, potentially guiding developments in efficient language modeling and multimodal applications across academia and industry.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality and valuable contribution for researchers in language modeling, offering essential insights and a structured guide to an emerging field that could inform ongoing and future work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/71837e66048330a2b3a1b1840fdf0c10b69db3b5",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 1,
      "average_h_index": 0.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Tianyi Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376486877"
        },
        {
          "name": "Mingda Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375952029"
        },
        {
          "name": "Bowei Guo",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352192939"
        },
        {
          "name": "Zhiqiang Shen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376153256"
        }
      ]
    },
    {
      "id": "2508.10880",
      "title": "Searching for Privacy Risks in LLM Agents via Simulation",
      "authors": [
        "Yanzhe Zhang",
        "Diyi Yang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10880v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10880v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.355,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses iterative optimization with LLMs to improve attacker and defender instructions based on simulation outcomes, resembling an adversarial process, but it does not involve human feedback, a reward model trained on human-ranked data, or reinforcement learning for fine-tuning. Thus, it only loosely connects to RLHF concepts without core elements.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on simulating agent interactions and using LLMs for iterative search and optimization, but it does not involve diffusion models, iterative refinement of Chain-of-Thought, or any multi-step logical reasoning adapted from diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10881",
      "title": "ToonComposer: Streamlining Cartoon Production with Generative\n  Post-Keyframing",
      "authors": [
        "Lingen Li",
        "Guangzhi Wang",
        "Zhaoyang Zhang",
        "Yaowei Li",
        "Xiaoyu Li",
        "Qi Dou",
        "Jinwei Gu",
        "Tianfan Xue",
        "Ying Shan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Traditional cartoon and anime production involves keyframing, inbetweening,\nand colorization stages, which require intensive manual effort. Despite recent\nadvances in AI, existing methods often handle these stages separately, leading\nto error accumulation and artifacts. For instance, inbetweening approaches\nstruggle with large motions, while colorization methods require dense per-frame\nsketches. To address this, we introduce ToonComposer, a generative model that\nunifies inbetweening and colorization into a single post-keyframing stage.\nToonComposer employs a sparse sketch injection mechanism to provide precise\ncontrol using keyframe sketches. Additionally, it uses a cartoon adaptation\nmethod with the spatial low-rank adapter to tailor a modern video foundation\nmodel to the cartoon domain while keeping its temporal prior intact. Requiring\nas few as a single sketch and a colored reference frame, ToonComposer excels\nwith sparse inputs, while also supporting multiple sketches at any temporal\nlocation for more precise motion control. This dual capability reduces manual\nworkload and improves flexibility, empowering artists in real-world scenarios.\nTo evaluate our model, we further created PKBench, a benchmark featuring\nhuman-drawn sketches that simulate real-world use cases. Our evaluation\ndemonstrates that ToonComposer outperforms existing methods in visual quality,\nmotion consistency, and production efficiency, offering a superior and more\nflexible solution for AI-assisted cartoon production.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10881v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10881v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.314,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on ToonComposer, a generative model for cartoon production that uses Diffusion Transformer (DiT) models for video generation, inbetweening, and colorization. It adapts diffusion processes for creative tasks like generating animations from sketches, but it does not involve adapting the iterative refinement of diffusion for solving complex logical tasks, treating a Chain-of-Thought as an entity, or performing multi-step logical reasoning. Since the paper lacks any component for logical reasoning, it does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10887",
      "title": "Empirical Investigation into Configuring Echo State Networks for\n  Representative Benchmark Problem Domains",
      "authors": [
        "Brooke R. Weborg",
        "Gursel Serpen"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This paper examines Echo State Network, a reservoir computer, performance\nusing four different benchmark problems, then proposes heuristics or rules of\nthumb for configuring the architecture, as well as the selection of parameters\nand their values, which are applicable to problems within the same domain, to\nhelp serve to fill the experience gap needed by those entering this field of\nstudy. The influence of various parameter selections and their value\nadjustments, as well as architectural changes made to an Echo State Network, a\npowerful recurrent neural network configured as a reservoir computer, can be\nchallenging to fully comprehend without experience in the field, and even some\nhyperparameter optimization algorithms may have difficulty adjusting parameter\nvalues without proper manual selections made first. Therefore, it is imperative\nto understand the effects of parameters and their value selection on Echo State\nNetwork architecture performance for a successful build. Thus, to address the\nrequirement for an extensive background in Echo State Network architecture, as\nwell as examine how Echo State Network performance is affected with respect to\nvariations in architecture, design, and parameter selection and values, a\nseries of benchmark tasks representing different problem domains, including\ntime series prediction, pattern generation, chaotic system prediction, and time\nseries classification, were modeled and experimented on to show the impact on\nthe performance of Echo State Network.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10887v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10887v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.328,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10893",
      "title": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer",
      "authors": [
        "Yushi Lan",
        "Yihang Luo",
        "Fangzhou Hong",
        "Shangchen Zhou",
        "Honghua Chen",
        "Zhaoyang Lyu",
        "Shuai Yang",
        "Bo Dai",
        "Chen Change Loy",
        "Xingang Pan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present STream3R, a novel approach to 3D reconstruction that reformulates\npointmap prediction as a decoder-only Transformer problem. Existing\nstate-of-the-art methods for multi-view reconstruction either depend on\nexpensive global optimization or rely on simplistic memory mechanisms that\nscale poorly with sequence length. In contrast, STream3R introduces an\nstreaming framework that processes image sequences efficiently using causal\nattention, inspired by advances in modern language modeling. By learning\ngeometric priors from large-scale 3D datasets, STream3R generalizes well to\ndiverse and challenging scenarios, including dynamic scenes where traditional\nmethods often fail. Extensive experiments show that our method consistently\noutperforms prior work across both static and dynamic scene benchmarks.\nMoreover, STream3R is inherently compatible with LLM-style training\ninfrastructure, enabling efficient large-scale pretraining and fine-tuning for\nvarious downstream 3D tasks. Our results underscore the potential of causal\nTransformer models for online 3D perception, paving the way for real-time 3D\nunderstanding in streaming environments. More details can be found in our\nproject page: https://nirvanalan.github.io/projects/stream3r.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10893v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10893v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.4,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Transformer-based approach for 3D reconstruction using causal attention, with no mention of diffusion models, iterative refinement for logical tasks, or Chain-of-Thought reasoning. It deals solely with geometric and visual processing, not logical or reasoning-based applications.",
      "distributed_training_justification": "The paper mentions compatibility with LLM-style training infrastructure for large-scale pretraining, which could imply distributed systems, but it does not discuss specific algorithms, parallel computing techniques, or multi-node strategies for training. The main contribution is on 3D reconstruction, not on advancing distributed training methods.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10894",
      "title": "MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and\n  Multispectral Earth Observation Data",
      "authors": [
        "Antoine Labatie",
        "Michael Vaccaro",
        "Nina Lardiere",
        "Anatol Garioud",
        "Nicolas Gonthier"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Self-supervised learning holds great promise for remote sensing, but standard\nself-supervised methods must be adapted to the unique characteristics of Earth\nobservation data. We take a step in this direction by conducting a\ncomprehensive benchmark of fusion strategies and reconstruction target\nnormalization schemes for multimodal, multitemporal, and multispectral Earth\nobservation data. Based on our findings, we propose MAESTRO, a novel adaptation\nof the Masked Autoencoder, featuring optimized fusion strategies and a tailored\ntarget normalization scheme that introduces a spectral prior as a\nself-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO\nsets a new state-of-the-art on tasks that strongly rely on multitemporal\ndynamics, while remaining highly competitive on tasks dominated by a single\nmono-temporal modality. Code to reproduce all our experiments is available at\nhttps://github.com/ignf/maestro.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10894v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10894v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.347,
      "datasets_score": 0.403,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on self-supervised learning (SSL) for Earth Observation data, which involves learning from unlabeled data through techniques like masked autoencoding. While SSL reduces the need for labeled data, it does not specifically address weak supervision methods, such as programmatically generating noisy labels from high-level sources. Thus, the connection is indirect, as SSL might broadly relate to label-efficient techniques but is not the paper's main emphasis.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper extensively benchmarks fusion strategies and normalization schemes on four Earth observation datasets, evaluates model performance, and sets new state-of-the-art results on these datasets. This directly involves dataset analysis, benchmarking, and evaluation for machine learning applications, aligning closely with research on creating and assessing datasets.",
      "llm_score_status": "completed",
      "summary": "This paper adapts the Masked Autoencoder (MAE) framework to Earth Observation (EO) data by benchmarking various fusion strategies and reconstruction target normalization schemes for multimodal, multitemporal, and multispectral data, addressing the unique heterogeneity of EO datasets. The authors propose MAESTRO, a novel adaptation featuring optimized fusion methods and a new patch-group-wise normalization that incorporates spectral priors, demonstrating state-of-the-art performance on multitemporal tasks and strong competitiveness on mono-temporal ones across four EO datasets, with code provided for reproducibility.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting the existing MAE framework with optimized fusion strategies and a new normalization scheme tailored to EO data's unique characteristics, rather than introducing a completely new problem or technique. This clever combination addresses known challenges in self-supervised learning for remote sensing, making it a significant but not revolutionary contribution.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in Earth Observation by providing a benchmarked framework for multimodal data handling, potentially leading to citations and adaptations within the subfield of remote sensing and computer vision. However, its applicability may remain niche, limiting broader commercial or interdisciplinary influence.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with practical innovations for EO applications, making it valuable for researchers in computer vision and remote sensing to understand advancements in self-supervised learning. While not essential for all audiences, it represents a strong addition to the field that warrants attention from relevant specialists.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/273e0b3d1d5d91f80d4aa2e96273ccaf39050deb",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 7,
      "average_h_index": 2.2,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Antoine Labatie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375902215"
        },
        {
          "name": "Michael Vaccaro",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375901664"
        },
        {
          "name": "Nina Lardiere",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375902174"
        },
        {
          "name": "A. Garioud",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1397381817"
        },
        {
          "name": "Nicolas Gonthier",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2261085388"
        }
      ]
    },
    {
      "id": "2508.10896",
      "title": "ESSENTIAL: Episodic and Semantic Memory Integration for Video\n  Class-Incremental Learning",
      "authors": [
        "Jongseo Lee",
        "Kyungho Bae",
        "Kyle Min",
        "Gyeong-Moon Park",
        "Jinwoo Choi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this work, we tackle the problem of video classincremental learning\n(VCIL). Many existing VCIL methods mitigate catastrophic forgetting by\nrehearsal training with a few temporally dense samples stored in episodic\nmemory, which is memory-inefficient. Alternatively, some methods store\ntemporally sparse samples, sacrificing essential temporal information and\nthereby resulting in inferior performance. To address this trade-off between\nmemory-efficiency and performance, we propose EpiSodic and SEmaNTIc memory\nintegrAtion for video class-incremental Learning (ESSENTIAL). ESSENTIAL\nconsists of episodic memory for storing temporally sparse features and semantic\nmemory for storing general knowledge represented by learnable prompts. We\nintroduce a novel memory retrieval (MR) module that integrates episodic memory\nand semantic prompts through cross-attention, enabling the retrieval of\ntemporally dense features from temporally sparse features. We rigorously\nvalidate ESSENTIAL on diverse datasets: UCF-101, HMDB51, and\nSomething-Something-V2 from the TCD benchmark and UCF-101, ActivityNet, and\nKinetics-400 from the vCLIMB benchmark. Remarkably, with significantly reduced\nmemory, ESSENTIAL achieves favorable performance on the benchmarks.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10896v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10896v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.346,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10897",
      "title": "Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via\n  In-Context Learning",
      "authors": [
        "Mengyuan Liu",
        "Xinshun Wang",
        "Zhongbin Fang",
        "Deheng Ye",
        "Xia Li",
        "Tao Tang",
        "Songtao Wu",
        "Xiangtai Li",
        "Ming-Hsuan Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper aims to model 3D human motion across domains, where a single model\nis expected to handle multiple modalities, tasks, and datasets. Existing\ncross-domain models often rely on domain-specific components and multi-stage\ntraining, which limits their practicality and scalability. To overcome these\nchallenges, we propose a new setting to train a unified cross-domain model\nthrough a single process, eliminating the need for domain-specific components\nand multi-stage training. We first introduce Pose-in-Context (PiC), which\nleverages in-context learning to create a pose-centric cross-domain model.\nWhile PiC generalizes across multiple pose-based tasks and datasets, it\nencounters difficulties with modality diversity, prompting strategy, and\ncontextual dependency handling. We thus propose Human-in-Context (HiC), an\nextension of PiC that broadens generalization across modalities, tasks, and\ndatasets. HiC combines pose and mesh representations within a unified\nframework, expands task coverage, and incorporates larger-scale datasets.\nAdditionally, HiC introduces a max-min similarity prompt sampling strategy to\nenhance generalization across diverse domains and a network architecture with\ndual-branch context injection for improved handling of contextual dependencies.\nExtensive experimental results show that HiC performs better than PiC in terms\nof generalization, data scale, and performance across a wide range of domains.\nThese results demonstrate the potential of HiC for building a unified\ncross-domain 3D human motion model with improved flexibility and scalability.\nThe source codes and models are available at\nhttps://github.com/BradleyWang0416/Human-in-Context.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10897v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10897v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.397,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing a unified model for 3D human motion modeling using in-context learning, addressing cross-domain challenges in computer vision. It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI models with human preferences. As such, there is no connection to Reinforcement Learning from Human Feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10898",
      "title": "Puppeteer: Rig and Animate Your 3D Models",
      "authors": [
        "Chaoyue Song",
        "Xiu Li",
        "Fan Yang",
        "Zhongcong Xu",
        "Jiacheng Wei",
        "Fayao Liu",
        "Jiashi Feng",
        "Guosheng Lin",
        "Jianfeng Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.GR (Graphics)"
      ],
      "abstract": "Modern interactive applications increasingly demand dynamic 3D content, yet\nthe transformation of static 3D models into animated assets constitutes a\nsignificant bottleneck in content creation pipelines. While recent advances in\ngenerative AI have revolutionized static 3D model creation, rigging and\nanimation continue to depend heavily on expert intervention. We present\nPuppeteer, a comprehensive framework that addresses both automatic rigging and\nanimation for diverse 3D objects. Our system first predicts plausible skeletal\nstructures via an auto-regressive transformer that introduces a joint-based\ntokenization strategy for compact representation and a hierarchical ordering\nmethodology with stochastic perturbation that enhances bidirectional learning\ncapabilities. It then infers skinning weights via an attention-based\narchitecture incorporating topology-aware joint attention that explicitly\nencodes inter-joint relationships based on skeletal graph distances. Finally,\nwe complement these rigging advances with a differentiable optimization-based\nanimation pipeline that generates stable, high-fidelity animations while being\ncomputationally more efficient than existing approaches. Extensive evaluations\nacross multiple benchmarks demonstrate that our method significantly\noutperforms state-of-the-art techniques in both skeletal prediction accuracy\nand skinning quality. The system robustly processes diverse 3D content, ranging\nfrom professionally designed game assets to AI-generated shapes, producing\ntemporally coherent animations that eliminate the jittering issues common in\nexisting methods.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10898v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10898v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.362,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10900",
      "title": "Quantum Visual Fields with Neural Amplitude Encoding",
      "authors": [
        "Shuteng Wang",
        "Christian Theobalt",
        "Vladislav Golyanik"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Quantum Implicit Neural Representations (QINRs) include components for\nlearning and execution on gate-based quantum computers. While QINRs recently\nemerged as a promising new paradigm, many challenges concerning their\narchitecture and ansatz design, the utility of quantum-mechanical properties,\ntraining efficiency and the interplay with classical modules remain. This paper\nadvances the field by introducing a new type of QINR for 2D image and 3D\ngeometric field learning, which we collectively refer to as Quantum Visual\nField (QVF). QVF encodes classical data into quantum statevectors using neural\namplitude encoding grounded in a learnable energy manifold, ensuring meaningful\nHilbert space embeddings. Our ansatz follows a fully entangled design of\nlearnable parametrised quantum circuits, with quantum (unitary) operations\nperformed in the real Hilbert space, resulting in numerically stable training\nwith fast convergence. QVF does not rely on classical post-processing -- in\ncontrast to the previous QINR learning approach -- and directly employs\nprojective measurement to extract learned signals encoded in the ansatz.\nExperiments on a quantum hardware simulator demonstrate that QVF outperforms\nthe existing quantum approach and widely used classical foundational baselines\nin terms of visual representation accuracy across various metrics and model\ncharacteristics, such as learning of high-frequency details. We also show\napplications of QVF in 2D and 3D field completion and 3D shape interpolation,\nhighlighting its practical potential.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10900v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10900v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.335,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10954",
      "title": "Towards Efficient Prompt-based Continual Learning in Distributed Medical\n  AI",
      "authors": [
        "Gyutae Oh",
        "Jitae Shin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Modern AI models achieve state-of-the-art performance with large-scale,\nhigh-quality datasets; however, ethical, social, and institutional constraints\nin the medical domain severely restrict data sharing, rendering centralized\nlearning nearly impossible. Each institution must incrementally update models\nusing only local data. Traditional training overfits new samples and suffers\nfrom catastrophic forgetting, losing previously acquired knowledge. Medical\ndata distributions also shift due to varying diagnostic equipment and\ndemographics. Although continual learning (CL) has advanced, most methods\naddress natural images, leaving medical-domain-specific CL underexplored. We\npropose a prompt-based continual learning (PCL) approach featuring a unified\nprompt pool with a minimal expansion strategy: by expanding and freezing a\nsubset of prompts, our method reduces computational overhead, and a novel\nregularization term balances retention and adaptation. Experiments on three\ndiabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy\nDetection show our model improves final classification accuracy by at least 10%\nand F1-score by 9 points over state-of-the-art approaches while lowering\ninference cost. We anticipate this study will drive sustainable medical AI\nadvances, enabling real-time diagnosis, patient monitoring, and telemedicine\napplications in distributed healthcare. Code will be released upon acceptance",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10954v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10954v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.456,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on prompt-based continual learning for medical AI, emphasizing incremental model updates with local data to handle forgetting and domain shifts. It does not involve human feedback, reward models, or reinforcement learning techniques for aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses medical AI in distributed settings, where models are updated locally at institutions due to data sharing constraints, which indirectly relates to decentralized learning. However, it does not cover parallel computing, multi-node strategies, or algorithms for partitioning data/computation across nodes, focusing instead on continual learning methods.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10955",
      "title": "Empowering Multimodal LLMs with External Tools: A Comprehensive Survey",
      "authors": [
        "Wenbin An",
        "Jiahao Nie",
        "Yaqiang Wu",
        "Feng Tian",
        "Shijian Lu",
        "Qinghua Zheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "By integrating the perception capabilities of multimodal encoders with the\ngenerative power of Large Language Models (LLMs), Multimodal Large Language\nModels (MLLMs), exemplified by GPT-4V, have achieved great success in various\nmultimodal tasks, pointing toward a promising pathway to artificial general\nintelligence. Despite this progress, the limited quality of multimodal data,\npoor performance on many complex downstream tasks, and inadequate evaluation\nprotocols continue to hinder the reliability and broader applicability of MLLMs\nacross diverse domains. Inspired by the human ability to leverage external\ntools for enhanced reasoning and problem-solving, augmenting MLLMs with\nexternal tools (e.g., APIs, expert models, and knowledge bases) offers a\npromising strategy to overcome these challenges. In this paper, we present a\ncomprehensive survey on leveraging external tools to enhance MLLM performance.\nOur discussion is structured along four key dimensions about external tools:\n(1) how they can facilitate the acquisition and annotation of high-quality\nmultimodal data; (2) how they can assist in improving MLLM performance on\nchallenging downstream tasks; (3) how they enable comprehensive and accurate\nevaluation of MLLMs; (4) the current limitations and future directions of\ntool-augmented MLLMs. Through this survey, we aim to underscore the\ntransformative potential of external tools in advancing MLLM capabilities,\noffering a forward-looking perspective on their development and applications.\nThe project page of this paper is publicly available\nathttps://github.com/Lackel/Awesome-Tools-for-MLLMs.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10955v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10955v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.437,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.384,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper is a survey on using external tools to enhance Multimodal Large Language Models (MLLMs), focusing on data acquisition, task performance, and evaluation. It does not mention reinforcement learning, human feedback, reward models, or any related training methods.",
      "weak_supervision_justification": "The paper discusses using external tools like web crawlers for acquiring and annotating multimodal data, which could indirectly relate to weak supervision by involving programmatic label generation. However, it does not focus on training models with noisy or imprecise labels as a primary method.",
      "diffusion_reasoning_justification": "The paper surveys external tools for MLLMs and covers aspects like data handling and task improvement, but it does not reference diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10956",
      "title": "ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks",
      "authors": [
        "Abhishek Kolari",
        "Mohammadhossein Khojasteh",
        "Yifan Jiang",
        "Floris den Hengst",
        "Filip Ilievski"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While vision-language models (VLMs) have made remarkable progress on many\npopular visual question answering (VQA) benchmarks, it remains unclear whether\nthey abstract and reason over depicted objects. Inspired by human object\ncategorisation, object property reasoning involves identifying and recognising\nlow-level details and higher-level abstractions. While current VQA benchmarks\nconsider a limited set of object property attributes like size, they typically\nblend perception and reasoning, and lack representativeness in terms of\nreasoning and image categories. To this end, we introduce a systematic\nevaluation framework with images of three representative types, three reasoning\nlevels of increasing complexity, and four object property dimensions driven by\nprior work on commonsense reasoning. We develop a procedure to instantiate this\nbenchmark into ORBIT, a multi-level reasoning VQA benchmark for object\nproperties comprising 360 images paired with a total of 1,080 count-based\nquestions. Experiments with 12 state-of-the-art VLMs in zero-shot settings\nreveal significant limitations compared to humans, with the best-performing\nmodel only reaching 40\\% accuracy. VLMs struggle particularly with realistic\n(photographic) images, counterfactual reasoning about physical and functional\nproperties, and higher counts. ORBIT points to the need to develop methods for\nscalable benchmarking, generalize annotation guidelines, and explore additional\nreasoning VLMs. We make the ORBIT benchmark and the experimental code available\nto support such endeavors.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10956v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10956v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.297,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing the ORBIT benchmark for evaluating vision-language models (VLMs) on object property reasoning tasks, such as counting and inference in visual question answering. It does not mention, discuss, or involve diffusion-based models, iterative refinement processes, or adaptations of diffusion for multi-step logical reasoning. The core contributions are centered on benchmarking VLMs for perception and reasoning, without any connection to diffusion mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10962",
      "title": "CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in\n  Urban Driving",
      "authors": [
        "Jiarong Li",
        "Imad Ali Shah",
        "Diarmaid Geever",
        "Fiachra Collins",
        "Enda Ward",
        "Martin Glavin",
        "Edward Jones",
        "Brian Deegan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Protecting Vulnerable Road Users (VRU) is a critical safety challenge for\nautomotive perception systems, particularly under visual ambiguity caused by\nmetamerism, a phenomenon where distinct materials appear similar in RGB\nimagery. This work investigates hyperspectral imaging (HSI) to overcome this\nlimitation by capturing unique material signatures beyond the visible spectrum,\nespecially in the Near-Infrared (NIR). To manage the inherent\nhigh-dimensionality of HSI data, we propose a band selection strategy that\nintegrates information theory techniques (joint mutual information\nmaximization, correlation analysis) with a novel application of an image\nquality metric (contrast signal-to-noise ratio) to identify the most spectrally\ninformative bands. Using the Hyperspectral City V2 (H-City) dataset, we\nidentify three informative bands (497 nm, 607 nm, and 895 nm, $\\pm$27 nm) and\nreconstruct pseudo-color images for comparison with co-registered RGB.\nQuantitative results demonstrate increased dissimilarity and perceptual\nseparability of VRU from the background. The selected HSI bands yield\nimprovements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity\n(Euclidean, SAM, $T^2$) and perception (CIE $\\Delta E$) metrics, consistently\noutperforming RGB and confirming a marked reduction in metameric confusion. By\nproviding a spectrally optimized input, our method enhances VRU separability,\nestablishing a robust foundation for downstream perception tasks in Advanced\nDriver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately\ncontributing to improved road safety.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10962v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10962v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.293,
      "distributed_training_score": 0.282,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10963",
      "title": "EVCtrl: Efficient Control Adapter for Visual Generation",
      "authors": [
        "Zixiang Yang",
        "Yue Ma",
        "Yinhan Zhang",
        "Shanhui Mo",
        "Dongrui Liu",
        "Linfeng Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10963v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10963v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.366,
      "datasets_score": 0.271,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an efficient control adapter for visual generation in diffusion models, focusing on reducing spatial and temporal redundancies in image and video synthesis. It does not involve adapting the diffusion process for multi-step logical reasoning, chain-of-thought processing, or solving complex logical tasks; instead, it centers on visual content creation and control, with no components related to reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10967",
      "title": "Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis",
      "authors": [
        "Xinyi Li",
        "Sai Wang",
        "Yutian Lin",
        "Yu Wu",
        "Yi Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Retrosynthesis prediction aims to infer the reactant molecule based on a\ngiven product molecule, which is a fundamental task in chemical synthesis.\nHowever, existing models rely on static pattern-matching paradigm, which limits\ntheir ability to perform effective logic decision-making, leading to black-box\ndecision-making. Building on this, we propose Retro-Expert, an interpretable\nretrosynthesis framework that performs collaborative reasoning by combining the\ncomplementary reasoning strengths of Large Language Models and specialized\nmodels via reinforcement learning. It outputs natural language explanations\ngrounded in chemical logic through three components: (1) specialized models\nperform shallow reasoning to construct high-quality chemical decision space,\n(2) LLM-driven critical reasoning to generate predictions and corresponding\ninterpretable reasoning path, and (3) reinforcement learning optimizing\ninterpretable decision policy. Experiments show that Retro-Expert not only\nsurpasses both LLM-based and specialized models across different metrics but\nalso provides expert-aligned explanations that bridge the gap between AI\npredictions and actionable chemical insights.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10967v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10967v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.492,
      "distributed_training_score": 0.261,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Retro-Expert, a framework for interpretable retrosynthesis using LLMs, specialized models, and reinforcement learning to generate explanations and predictions. It does not involve diffusion-based methods, such as iterative refinement processes for Chain-of-Thought reasoning. There is no mention of diffusion models or holistic correction over multiple steps, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10971",
      "title": "Rule2Text: A Framework for Generating and Evaluating Natural Language\n  Explanations of Knowledge Graph Rules",
      "authors": [
        "Nasim Shirvani-Mahdavi",
        "Chengkai Li"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Knowledge graphs (KGs) can be enhanced through rule mining; however, the\nresulting logical rules are often difficult for humans to interpret due to\ntheir inherent complexity and the idiosyncratic labeling conventions of\nindividual KGs. This work presents Rule2Text, a comprehensive framework that\nleverages large language models (LLMs) to generate natural language\nexplanations for mined logical rules, thereby improving KG accessibility and\nusability. We conduct extensive experiments using multiple datasets, including\nFreebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the\nogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically\nevaluate several LLMs across a comprehensive range of prompting strategies,\nincluding zero-shot, few-shot, variable type incorporation, and\nChain-of-Thought reasoning. To systematically assess models' performance, we\nconduct a human evaluation of generated explanations on correctness and\nclarity. To address evaluation scalability, we develop and validate an\nLLM-as-a-judge framework that demonstrates strong agreement with human\nevaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,\nand human-in-the-loop feedback, we construct high-quality ground truth\ndatasets, which we use to fine-tune the open-source Zephyr model. Our results\ndemonstrate significant improvements in explanation quality after fine-tuning,\nwith particularly strong gains in the domain-specific dataset. Additionally, we\nintegrate a type inference module to support KGs lacking explicit type\ninformation. All code and data are publicly available at\nhttps://github.com/idirlab/KGRule2NL.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10971v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10971v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.279,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using large language models (LLMs) with prompting strategies, including Chain-of-Thought, to generate natural language explanations for knowledge graph rules. While it mentions Chain-of-Thought reasoning, there is no reference to diffusion models, iterative refinement processes, or adapting diffusion techniques for logical tasks. The core contributions involve rule mining, explanation generation, and evaluation frameworks, which do not align with the topic's emphasis on diffusion-based methods for multi-step reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10972",
      "title": "Not There Yet: Evaluating Vision Language Models in Simulating the\n  Visual Perception of People with Low Vision",
      "authors": [
        "Rosiana Natalie",
        "Wenqian Xu",
        "Ruei-Che Chang",
        "Rada Mihalcea",
        "Anhong Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Advances in vision language models (VLMs) have enabled the simulation of\ngeneral human behavior through their reasoning and problem solving\ncapabilities. However, prior research has not investigated such simulation\ncapabilities in the accessibility domain. In this paper, we evaluate the extent\nto which VLMs can simulate the vision perception of low vision individuals when\ninterpreting images. We first compile a benchmark dataset through a survey\nstudy with 40 low vision participants, collecting their brief and detailed\nvision information and both open-ended and multiple-choice image perception and\nrecognition responses to up to 25 images. Using these responses, we construct\nprompts for VLMs (GPT-4o) to create simulated agents of each participant,\nvarying the included information on vision information and example image\nresponses. We evaluate the agreement between VLM-generated responses and\nparticipants' original answers. Our results indicate that VLMs tend to infer\nbeyond the specified vision ability when given minimal prompts, resulting in\nlow agreement (0.59). The agreement between the agent' and participants'\nresponses remains low when only either the vision information (0.59) or example\nimage responses (0.59) are provided, whereas a combination of both\nsignificantly increase the agreement (0.70, p < 0.0001). Notably, a single\nexample combining both open-ended and multiple-choice responses, offers\nsignificant performance improvements over either alone (p < 0.0001), while\nadditional examples provided minimal benefits (p > 0.05).",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10972v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10972v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.446,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.325,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating Vision Language Models (VLMs) for simulating visual perception in people with low vision using prompts and benchmarks, without any involvement in training, fine-tuning, or aligning models with human preferences. It does not mention or utilize Reinforcement Learning from Human Feedback (RLHF), which specifically involves training a reward model on human-ranked data and applying reinforcement learning for model fine-tuning. Thus, there is no connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10974",
      "title": "Failures to Surface Harmful Contents in Video Large Language Models",
      "authors": [
        "Yuxin Cao",
        "Wei Song",
        "Derui Wang",
        "Jingling Xue",
        "Jin Song Dong"
      ],
      "categories": [
        "cs.MM (Multimedia)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video Large Language Models (VideoLLMs) are increasingly deployed on numerous\ncritical applications, where users rely on auto-generated summaries while\ncasually skimming the video stream. We show that this interaction hides a\ncritical safety gap: if harmful content is embedded in a video, either as\nfull-frame inserts or as small corner patches, state-of-the-art VideoLLMs\nrarely mention the harmful content in the output, despite its clear visibility\nto human viewers. A root-cause analysis reveals three compounding design flaws:\n(1) insufficient temporal coverage resulting from the sparse, uniformly spaced\nframe sampling used by most leading VideoLLMs, (2) spatial information loss\nintroduced by aggressive token downsampling within sampled frames, and (3)\nencoder-decoder disconnection, whereby visual cues are only weakly utilized\nduring text generation. Leveraging these insights, we craft three zero-query\nblack-box attacks, aligning with these flaws in the processing pipeline. Our\nlarge-scale evaluation across five leading VideoLLMs shows that the harmfulness\nomission rate exceeds 90% in most cases. Even when harmful content is clearly\npresent in all frames, these models consistently fail to identify it. These\nresults underscore a fundamental vulnerability in current VideoLLMs' designs\nand highlight the urgent need for sampling strategies, token compression, and\ndecoding mechanisms that guarantee semantic coverage rather than speed alone.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10974v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10974v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.344,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on analyzing safety vulnerabilities in Video Large Language Models (VideoLLMs), such as failures to detect harmful content due to design flaws, and proposes attacks to exploit these issues. It does not involve training or fine-tuning models using human feedback, a reward model, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines VideoLLMs' processing pipelines and their inability to surface harmful content, identifying flaws like sparse sampling and downsampling, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10976",
      "title": "Grounding Rule-Based Argumentation Using Datalog",
      "authors": [
        "Martin Diller",
        "Sarah Alice Gaggl",
        "Philipp Hanisch",
        "Giuseppina Monterosso",
        "Fritz Rauschenbach"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "ASPIC+ is one of the main general frameworks for rule-based argumentation for\nAI. Although first-order rules are commonly used in ASPIC+ examples, most\nexisting approaches to reason over rule-based argumentation only support\npropositional rules. To enable reasoning over first-order instances, a\npreliminary grounding step is required. As groundings can lead to an\nexponential increase in the size of the input theories, intelligent procedures\nare needed. However, there is a lack of dedicated solutions for ASPIC+.\nTherefore, we propose an intelligent grounding procedure that keeps the size of\nthe grounding manageable while preserving the correctness of the reasoning\nprocess. To this end, we translate the first-order ASPIC+ instance into a\nDatalog program and query a Datalog engine to obtain ground substitutions to\nperform the grounding of rules and contraries. Additionally, we propose\nsimplifications specific to the ASPIC+ formalism to avoid grounding of rules\nthat have no influence on the reasoning process. Finally, we performed an\nempirical evaluation of a prototypical implementation to show scalability.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10976v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10976v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.283,
      "weak_supervision_score": 0.276,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.231,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10991",
      "title": "MCP-Guard: A Defense Framework for Model Context Protocol Integrity in\n  Large Language Model Applications",
      "authors": [
        "Wenpeng Xing",
        "Zhonghao Qi",
        "Yupeng Qin",
        "Yilin Li",
        "Caini Chang",
        "Jiahui Yu",
        "Changting Lin",
        "Zhenzhen Xie",
        "Meng Han"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The integration of Large Language Models (LLMs) with external tools via\nprotocols such as the Model Context Protocol (MCP) introduces critical security\nvulnerabilities, including prompt injection, data exfiltration, and other\nthreats. To counter these challenges, we propose MCP-Guard, a robust, layered\ndefense architecture designed for LLM--tool interactions. MCP-Guard employs a\nthree-stage detection pipeline that balances efficiency with accuracy: it\nprogresses from lightweight static scanning for overt threats and a deep neural\ndetector for semantic attacks, to our fine-tuned E5-based model achieves\n(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM\narbitrator synthesizes these signals to deliver the final decision while\nminimizing false positives. To facilitate rigorous training and evaluation, we\nalso introduce MCP-AttackBench, a comprehensive benchmark of over 70,000\nsamples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench\nsimulates diverse, real-world attack vectors in the MCP format, providing a\nfoundation for future research into securing LLM-tool ecosystems.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10991v2",
      "pdf_url": "http://arxiv.org/pdf/2508.10991v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.386,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a defense framework (MCP-Guard) for securing Large Language Model interactions and a benchmark dataset (MCP-AttackBench), focusing on threat detection and security vulnerabilities. It does not involve reinforcement learning, human feedback, reward models, or fine-tuning models based on human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10993",
      "title": "Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image\n  Diffusion Models",
      "authors": [
        "Basile Lewandowski",
        "Robert Birke",
        "Lydia Y. Chen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-image (T2I) models based on diffusion and transformer architectures\nadvance rapidly. They are often pretrained on large corpora, and openly shared\non a model platform, such as HuggingFace. Users can then build up AI\napplications, e.g., generating media contents, by adopting pretrained T2I\nmodels and fine-tuning them on the target dataset. While public pretrained T2I\nmodels facilitate the democratization of the models, users face a new\nchallenge: which model can be best fine-tuned based on the target data domain?\nModel selection is well addressed in classification tasks, but little is known\nin (pretrained) T2I models and their performance indication on the target\ndomain. In this paper, we propose the first model selection framework, M&C,\nwhich enables users to efficiently choose a pretrained T2I model from a model\nplatform without exhaustively fine-tuning them all on the target dataset. The\ncore of M&C is a matching graph, which consists of: (i) nodes of available\nmodels and profiled datasets, and (ii) edges of model-data and data-data pairs\ncapturing the fine-tuning performance and data similarity, respectively. We\nthen build a model that, based on the inputs of model/data feature, and,\ncritically, the graph embedding feature, extracted from the matching graph,\npredicts the model achieving the best quality after fine-tuning for the target\ndomain. We evaluate M&C on choosing across ten T2I models for 32 datasets\nagainst three baselines. Our results show that M&C successfully predicts the\nbest model for fine-tuning in 61.3% of the cases and a closely performing model\nfor the rest.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.10993v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10993v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.522,
      "distributed_training_score": 0.373,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a framework for selecting and fine-tuning text-to-image diffusion models, without any mention of human feedback, reward models, or reinforcement learning for alignment. It relies on automated metrics like FID for evaluation, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper deals with text-to-image diffusion models for image generation and model selection, but it does not adapt diffusion processes for multi-step logical reasoning or chain-of-thought tasks. It lacks any component for solving complex logical problems using diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11009",
      "title": "SproutBench: A Benchmark for Safe and Ethical Large Language Models for\n  Youth",
      "authors": [
        "Wenpeng Xing",
        "Lanyi Wei",
        "Haixiao Hu",
        "Rongchang Li",
        "Mohan Li",
        "Changting Lin",
        "Meng Han"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid proliferation of large language models (LLMs) in applications\ntargeting children and adolescents necessitates a fundamental reassessment of\nprevailing AI safety frameworks, which are largely tailored to adult users and\nneglect the distinct developmental vulnerabilities of minors. This paper\nhighlights key deficiencies in existing LLM safety benchmarks, including their\ninadequate coverage of age-specific cognitive, emotional, and social risks\nspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence\n(13--18). To bridge these gaps, we introduce SproutBench, an innovative\nevaluation suite comprising 1,283 developmentally grounded adversarial prompts\ndesigned to probe risks such as emotional dependency, privacy violations, and\nimitation of hazardous behaviors. Through rigorous empirical evaluation of 47\ndiverse LLMs, we uncover substantial safety vulnerabilities, corroborated by\nrobust inter-dimensional correlations (e.g., between Safety and Risk\nPrevention) and a notable inverse relationship between Interactivity and Age\nAppropriateness. These insights yield practical guidelines for advancing\nchild-centric AI design and deployment.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11009v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11009v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.341,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on introducing and evaluating a benchmark (SproutBench) for LLM safety in youth contexts, including prompt design and model assessments, but does not involve training AI models using human feedback or reinforcement learning techniques. There is no mention of aligning models with human preferences via a reward model or fine-tuning.",
      "weak_supervision_justification": "The paper uses automatic scoring with Qwen-2.5 to evaluate model responses, which involves programmatic assessment that could loosely relate to generating labels from noisy sources, but it is not about training models with weak supervision. The main focus is on benchmark creation and evaluation, not on weak supervision as a core methodology.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution is the introduction of SproutBench, a new benchmark dataset with 1,283 adversarial prompts for evaluating LLM safety for youth, along with its analysis, curation, and empirical evaluation on 47 models. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces SproutBench, a new benchmark designed to evaluate the safety and ethical aspects of large language models (LLMs) specifically for children and adolescents, addressing gaps in existing frameworks by focusing on age-specific cognitive, emotional, and social risks across three developmental stages: early childhood (0-6 years), middle childhood (7-12 years), and adolescence (13-18 years). Utilizing 1,283 adversarial prompts, the methodology involves empirical evaluation of 47 LLMs to identify vulnerabilities, revealing strong correlations between safety dimensions and an inverse relationship between interactivity and age appropriateness, ultimately providing guidelines for child-centric AI development.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark, SproutBench, that addresses a previously underrepresented problem in AI safety by focusing on youth-specific vulnerabilities, significantly advancing the state-of-the-art in child-centric LLM evaluation.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research and commercial applications in AI safety for youth, given its comprehensive approach to developmental risks and potential for widespread adoption in educational and social platforms.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI ethics and child safety, making it essential for researchers and developers in the field to understand its insights and guidelines.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5758c566179de7f8dded2c79688734f1d74f866e",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 3,
      "average_h_index": 1.7142857142857142,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Wenpeng Xing",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2312401599"
        },
        {
          "name": "Lanyi Wei",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376146652"
        },
        {
          "name": "Haixiao Hu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376543225"
        },
        {
          "name": "Rongchang Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2307445979"
        },
        {
          "name": "Mohan Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2346152541"
        },
        {
          "name": "Changting Lin",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261456023"
        },
        {
          "name": "Meng Han",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261432570"
        }
      ]
    },
    {
      "id": "2508.11010",
      "title": "Deep Learning-Based Automated Segmentation of Uterine Myomas",
      "authors": [
        "Tausifa Jan Saleem",
        "Mohammad Yaqub"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Uterine fibroids (myomas) are the most common benign tumors of the female\nreproductive system, particularly among women of childbearing age. With a\nprevalence exceeding 70%, they pose a significant burden on female reproductive\nhealth. Clinical symptoms such as abnormal uterine bleeding, infertility,\npelvic pain, and pressure-related discomfort play a crucial role in guiding\ntreatment decisions, which are largely influenced by the size, number, and\nanatomical location of the fibroids. Magnetic Resonance Imaging (MRI) is a\nnon-invasive and highly accurate imaging modality commonly used by clinicians\nfor the diagnosis of uterine fibroids. Segmenting uterine fibroids requires a\nprecise assessment of both the uterus and fibroids on MRI scans, including\nmeasurements of volume, shape, and spatial location. However, this process is\nlabor intensive and time consuming and subjected to variability due to intra-\nand inter-expert differences at both pre- and post-treatment stages. As a\nresult, there is a critical need for an accurate and automated segmentation\nmethod for uterine fibroids. In recent years, deep learning algorithms have\nshown re-markable improvements in medical image segmentation, outperforming\ntraditional methods. These approaches offer the potential for fully automated\nsegmentation. Several studies have explored the use of deep learning models to\nachieve automated segmentation of uterine fibroids. However, most of the\nprevious work has been conducted using private datasets, which poses challenges\nfor validation and comparison between studies. In this study, we leverage the\npublicly available Uterine Myoma MRI Dataset (UMD) to establish a baseline for\nautomated segmentation of uterine fibroids, enabling standardized evaluation\nand facilitating future research in this domain.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11010v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11010v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.245,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.268,
      "distributed_training_score": 0.28,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11011",
      "title": "Are Large Pre-trained Vision Language Models Effective Construction\n  Safety Inspectors?",
      "authors": [
        "Xuezheng Chen",
        "Zhengbo Zou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Construction safety inspections typically involve a human inspector\nidentifying safety concerns on-site. With the rise of powerful Vision Language\nModels (VLMs), researchers are exploring their use for tasks such as detecting\nsafety rule violations from on-site images. However, there is a lack of open\ndatasets to comprehensively evaluate and further fine-tune VLMs in construction\nsafety inspection. Current applications of VLMs use small, supervised datasets,\nlimiting their applicability in tasks they are not directly trained for. In\nthis paper, we propose the ConstructionSite 10k, featuring 10,000 construction\nsite images with annotations for three inter-connected tasks, including image\ncaptioning, safety rule violation visual question answering (VQA), and\nconstruction element visual grounding. Our subsequent evaluation of current\nstate-of-the-art large pre-trained VLMs shows notable generalization abilities\nin zero-shot and few-shot settings, while additional training is needed to make\nthem applicable to actual construction sites. This dataset allows researchers\nto train and evaluate their own VLMs with new architectures and techniques,\nproviding a valuable benchmark for construction safety inspection.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11011v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11011v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.419,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.374,
      "datasets_score": 0.417,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper evaluates VLMs in zero-shot and few-shot settings, which involve minimal labeled data and can relate to weak supervision concepts, but it does not focus on programmatically generating labels from noisy sources. Instead, it emphasizes dataset creation with human annotations and model evaluation, making the connection indirect.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of the ConstructionSite 10k dataset, including its curation, annotations, and use for benchmarking VLMs in construction safety tasks, directly aligning with research on creating, analyzing, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the potential of large pre-trained Vision Language Models (VLMs) for construction safety inspections by introducing the ConstructionSite 10k dataset, which includes 10,000 annotated images for tasks such as image captioning, safety rule violation visual question answering (VQA), and construction element visual grounding. The authors evaluate state-of-the-art VLMs like the GPT series and LLaVA in zero-shot and few-shot settings, finding that these models demonstrate strong generalization in image captioning and VQA but struggle with visual grounding, thus providing a benchmark dataset and framework to advance AI applications in construction safety.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the ConstructionSite 10k dataset and applying existing VLMs to a new context in construction safety, combining ideas in a clever way without introducing a entirely new architecture or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI for construction safety due to the provision of a new benchmark dataset, though its influence may be limited to specialized applications rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by filling a gap in datasets and evaluating VLMs for practical safety tasks, making it essential for researchers in computer vision and construction safety to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2413616fab206916b85df3fb8c269459aaea12b6",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xuezheng Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376280833"
        },
        {
          "name": "Zhengbo Zou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376490631"
        }
      ]
    },
    {
      "id": "2508.11016",
      "title": "CURE: Critical-Token-Guided Re-Concatenation for Entropy-Collapse\n  Prevention",
      "authors": [
        "Qingbin Li",
        "Rongkun Xue",
        "Jie Wang",
        "Ming Zhou",
        "Zhi Li",
        "Xiaofeng Ji",
        "Yongqi Wang",
        "Miao Liu",
        "Zheming Yang",
        "Minghui Qiu",
        "Jing Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in Reinforcement Learning with Verified Reward (RLVR) have\ndriven the emergence of more sophisticated cognitive behaviors in large\nlanguage models (LLMs), thereby enhancing their reasoning capabilities.\nHowever, in prior RLVR pipelines, the repeated use of static initial-state\nsampling drawn exactly from the dataset distribution during each sampling phase\nproduced overly deterministic, low diversity model behavior, which manifested\nas rapid entropy collapse and hindered sustained performance gains during\nprolonged training. To address this issue, we introduce CURE\n(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a\ntwo-stage framework that balances exploration and exploitation. Specifically,\nin the first stage, to deliberately steer the model toward novel yet coherent\ncontexts, we re-generate at high-entropy critical tokens and jointly optimize\nthe original and the branched trajectories. The further comparison with vanilla\nDAPO shows that the regeneration process achieves a better performance on math\nreasoning tasks while sustaining a high-level entropy degree for exploration.\nIn the second stage, we continue training with static initial-state sampling by\nDAPO, intentionally placing the model in a familiar state to gradually\nstrengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,\ncompared to other RLVR methods, CURE achieves a 5% performance gain across six\nmath benchmarks, establishing state-of-the-art performance in both entropy and\naccuracy. A series of experiments further validate the effectiveness of our\napproach. Code is available at https://github.com/bytedance/CURE.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11016v2",
      "pdf_url": "http://arxiv.org/pdf/2508.11016v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.36,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Reinforcement Learning with Verified Reward (RLVR), which uses automatic verifiers for rewards, not human feedback or human-ranked data. There is no mention of training a reward model based on human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for logical tasks. It centers on RLVR and entropy management through token-level interventions, with no components for multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11017",
      "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics",
      "authors": [
        "Carter Blum",
        "Katja Filippova",
        "Ann Yuan",
        "Asma Ghandeharioun",
        "Julian Zimmert",
        "Fred Zhang",
        "Jessica Hoffmann",
        "Tal Linzen",
        "Martin Wattenberg",
        "Lucas Dixon",
        "Mor Geva"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11017v2",
      "pdf_url": "http://arxiv.org/pdf/2508.11017v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.476,
      "distributed_training_score": 0.421,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on training dynamics and cross-lingual transfer in language models using synthetic datasets, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "While the paper uses synthetic datasets for training, its main contribution is analyzing cross-lingual representation dynamics, not techniques for programmatically generating labels or relying on noisy supervision sources.",
      "diffusion_reasoning_justification": "The paper examines Transformer training and representation unification in multilingual settings, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "The paper describes training small Transformer models in a controlled setup, but does not discuss parallel computing, multi-node systems, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11021",
      "title": "Can Multi-modal (reasoning) LLMs detect document manipulation?",
      "authors": [
        "Zisheng Liang",
        "Kidus Zewde",
        "Rudra Pratap Singh",
        "Disha Patil",
        "Zexi Chen",
        "Jiayu Xue",
        "Yao Yao",
        "Yifei Chen",
        "Qinzhe Liu",
        "Simiao Ren"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Document fraud poses a significant threat to industries reliant on secure and\nverifiable documentation, necessitating robust detection mechanisms. This study\ninvestigates the efficacy of state-of-the-art multi-modal large language models\n(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,\nGrok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and\n3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against\neach other and prior work on document fraud detection techniques using a\nstandard dataset with real transactional documents. Through prompt optimization\nand detailed analysis of the models' reasoning processes, we evaluate their\nability to identify subtle indicators of fraud, such as tampered text,\nmisaligned formatting, and inconsistent transactional sums. Our results reveal\nthat top-performing multi-modal LLMs demonstrate superior zero-shot\ngeneralization, outperforming conventional methods on out-of-distribution\ndatasets, while several vision LLMs exhibit inconsistent or subpar performance.\nNotably, model size and advanced reasoning capabilities show limited\ncorrelation with detection accuracy, suggesting task-specific fine-tuning is\ncritical. This study underscores the potential of multi-modal LLMs in enhancing\ndocument fraud detection systems and provides a foundation for future research\ninto interpretable and scalable fraud mitigation strategies.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11021v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11021v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.485,
      "distributed_training_score": 0.338,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating pre-existing multi-modal LLMs for document fraud detection through benchmarking and analysis, without any mention of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses general reasoning capabilities of multi-modal LLMs, such as chain-of-thought processes, but does not involve or reference diffusion models, iterative refinement for logical tasks, or any adaptation of diffusion processes for reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11025",
      "title": "Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for\n  Regression and Classification Tasks",
      "authors": [
        "Laura Lützow",
        "Michael Eichelbeck",
        "Mykel J. Kochenderfer",
        "Matthias Althoff"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Conformal prediction is a popular uncertainty quantification method that\naugments a base predictor with prediction sets with statistically valid\ncoverage guarantees. However, current methods are often computationally\nexpensive and data-intensive, as they require constructing an uncertainty model\nbefore calibration. Moreover, existing approaches typically represent the\nprediction sets with intervals, which limits their ability to capture\ndependencies in multi-dimensional outputs. We address these limitations by\nintroducing zono-conformal prediction, a novel approach inspired by interval\npredictor models and reachset-conformant identification that constructs\nprediction zonotopes with assured coverage. By placing zonotopic uncertainty\nsets directly into the model of the base predictor, zono-conformal predictors\ncan be identified via a single, data-efficient linear program. While we can\napply zono-conformal prediction to arbitrary nonlinear base predictors, we\nfocus on feed-forward neural networks in this work. Aside from regression\ntasks, we also construct optimal zono-conformal predictors in classification\nsettings where the output of an uncertain predictor is a set of possible\nclasses. We provide probabilistic coverage guarantees and present methods for\ndetecting outliers in the identification data. In extensive numerical\nexperiments, we show that zono-conformal predictors are less conservative than\ninterval predictor models and standard conformal prediction methods, while\nachieving a similar coverage over the test data.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11025v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11025v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.326,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11031",
      "title": "Risk-Based Prognostics and Health Management",
      "authors": [
        "John W. Sheppard"
      ],
      "categories": [
        "eess.SY (Systems and Control)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "stat.AP (Applications)"
      ],
      "abstract": "It is often the case that risk assessment and prognostics are viewed as\nrelated but separate tasks. This chapter describes a risk-based approach to\nprognostics that seeks to provide a tighter coupling between risk assessment\nand fault prediction. We show how this can be achieved using the\ncontinuous-time Bayesian network as the underlying modeling framework.\nFurthermore, we provide an overview of the techniques that are available to\nderive these models from data and show how they might be used in practice to\nachieve tasks like decision support and performance-based logistics. This work\nis intended to provide an overview of the recent developments related to\nrisk-based prognostics, and we hope that it will serve as a tutorial of sorts\nthat will assist others in adopting these techniques.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11031v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11031v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.252,
      "datasets_score": 0.23,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11032",
      "title": "MedSAMix: A Training-Free Model Merging Approach for Medical Image\n  Segmentation",
      "authors": [
        "Yanwu Yang",
        "Guinan Su",
        "Jiesi Hu",
        "Francesco Sammarco",
        "Jonas Geiping",
        "Thomas Wolfers"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Universal medical image segmentation models have emerged as a promising\nparadigm due to their strong generalizability across diverse tasks, showing\ngreat potential for a wide range of clinical applications. This potential has\nbeen partly driven by the success of general-purpose vision models such as the\nSegment Anything Model (SAM), which has inspired the development of various\nfine-tuned variants for medical segmentation tasks. However, fine-tuned\nvariants like MedSAM are trained on comparatively limited medical imaging data\nthat often suffers from heterogeneity, scarce annotations, and distributional\nshifts. These challenges limit their ability to generalize across a wide range\nof medical segmentation tasks. In this regard, we propose MedSAMix, a\ntraining-free model merging method that integrates the strengths of both\ngeneralist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical\nimage segmentation. In contrast to traditional model merging approaches that\nrely on manual configuration and often result in suboptimal outcomes, we\npropose a zero-order optimization method to automatically discover optimal\nlayer-wise merging solutions. Furthermore, for clinical applications, we\ndevelop two regimes to meet the demand of domain-specificity and\ngeneralizability in different scenarios by single-task optimization and\nmulti-objective optimization respectively. Extensive evaluations on 25 medical\nsegmentation tasks demonstrate that MedSAMix effectively mitigates model bias\nand consistently improves performance in both domain-specific accuracy and\ngeneralization, achieving improvements of 6.67% on specialized tasks and 4.37%\non multi-task evaluations.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11032v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11032v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.362,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11033",
      "title": "Note on Selection Bias in Observational Estimates of Algorithmic\n  Progress",
      "authors": [
        "Parker Whitfill"
      ],
      "categories": [
        "econ.GN (General Economics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Ho et. al (2024) attempts to estimate the degree of algorithmic progress from\nlanguage models. They collect observational data on language models' loss and\ncompute over time, and argue that as time has passed, language models'\nalgorithmic efficiency has been rising. That is, the loss achieved for fixed\ncompute has been dropping over time. In this note, I raise one potential\nmethodological problem with the estimation strategy. Intuitively, if part of\nalgorithmic quality is latent, and compute choices are endogenous to\nalgorithmic quality, then resulting estimates of algorithmic quality will be\ncontaminated by selection bias.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11033v2",
      "pdf_url": "http://arxiv.org/pdf/2508.11033v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.332,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11037",
      "title": "Learning with Confidence",
      "authors": [
        "Oliver Ethan Richardson"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.DG (Differential Geometry)"
      ],
      "abstract": "We characterize a notion of confidence that arises in learning or updating\nbeliefs: the amount of trust one has in incoming information and its impact on\nthe belief state. This learner's confidence can be used alongside (and is\neasily mistaken for) probability or likelihood, but it is fundamentally a\ndifferent concept -- one that captures many familiar concepts in the\nliterature, including learning rates and number of training epochs, Shafer's\nweight of evidence, and Kalman gain. We formally axiomatize what it means to\nlearn with confidence, give two canonical ways of measuring confidence on a\ncontinuum, and prove that confidence can always be represented in this way.\nUnder additional assumptions, we derive more compact representations of\nconfidence-based learning in terms of vector fields and loss functions. These\nrepresentations induce an extended language of compound \"parallel\"\nobservations. We characterize Bayes Rule as the special case of an optimizing\nlearner whose loss representation is a linear expectation.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11037v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11037v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.299,
      "datasets_score": 0.209,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11049",
      "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual\n  Reinforcement Learning",
      "authors": [
        "Kelin Yu",
        "Sheng Zhang",
        "Harshit Soora",
        "Furong Huang",
        "Heng Huang",
        "Pratap Tokekar",
        "Ruohan Gao"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances have shown that video generation models can enhance robot\nlearning by deriving effective robot actions through inverse dynamics. However,\nthese methods heavily depend on the quality of generated data and struggle with\nfine-grained manipulation due to the lack of environment feedback. While\nvideo-based reinforcement learning improves policy robustness, it remains\nconstrained by the uncertainty of video generation and the challenges of\ncollecting large-scale robot datasets for training diffusion models. To address\nthese limitations, we propose GenFlowRL, which derives shaped rewards from\ngenerated flow trained from diverse cross-embodiment datasets. This enables\nlearning generalizable and robust policies from diverse demonstrations using\nlow-dimensional, object-centric features. Experiments on 10 manipulation tasks,\nboth in simulation and real-world cross-embodiment evaluations, demonstrate\nthat GenFlowRL effectively leverages manipulation features extracted from\ngenerated object-centric flow, consistently achieving superior performance\nacross diverse and challenging scenarios. Our Project Page:\nhttps://colinyu1.github.io/genflowrl",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11049v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11049v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.474,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.344,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses cross-embodiment datasets, which may include human demonstrations for training generative models, but it does not involve training a separate reward model on human-ranked data to fine-tune an AI model. Instead, it focuses on deriving shaped rewards from generated flows in RL for robot manipulation, which is not the core mechanism of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper mentions training generative models for object-centric flow, which could involve diffusion-like processes, but it does not adapt diffusion for multi-step logical reasoning or treat a Chain-of-Thought as a single entity for correction. It primarily uses generated flows for reward shaping in visual RL, without a clear component for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11052",
      "title": "AI That Helps Us Help Each Other: A Proactive System for Scaffolding\n  Mentor-Novice Collaboration in Entrepreneurship Coaching",
      "authors": [
        "Evey Jiaxin Huang",
        "Matthew Easterday",
        "Elizabeth Gerber"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Entrepreneurship requires navigating open-ended, ill-defined problems:\nidentifying risks, challenging assumptions, and making strategic decisions\nunder deep uncertainty. Novice founders often struggle with these metacognitive\ndemands, while mentors face limited time and visibility to provide tailored\nsupport. We present a human-AI coaching system that combines a domain-specific\ncognitive model of entrepreneurial risk with a large language model (LLM) to\nproactively scaffold both novice and mentor thinking. The system proactively\nposes diagnostic questions that challenge novices' thinking and helps both\nnovices and mentors plan for more focused and emotionally attuned meetings.\nCritically, mentors can inspect and modify the underlying cognitive model,\nshaping the logic of the system to reflect their evolving needs. Through an\nexploratory field deployment, we found that using the system supported novice\nmetacognition, helped mentors plan emotionally attuned strategies, and improved\nmeeting depth, intentionality, and focus--while also surfaced key tensions\naround trust, misdiagnosis, and expectations of AI. We contribute design\nprinciples for proactive AI systems that scaffold metacognition and human-human\ncollaboration in complex, ill-defined domains, offering implications for\nsimilar domains like healthcare, education, and knowledge work.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11052v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11052v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.331,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a human-AI coaching system where mentors can inspect and modify a cognitive model, but it does not involve training an AI model using reinforcement learning based on human feedback. There is no mention of a reward model, human-ranked data, or fine-tuning via RLHF; instead, it focuses on proactive scaffolding with LLMs.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses LLMs to proactively scaffold metacognitive processes and collaboration, but it does not employ diffusion models or iterative refinement for multi-step logical reasoning. There is no reference to treating a Chain-of-Thought as a single entity for holistic correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11058",
      "title": "Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning\n  Evaluation and TripAlign Pre-training Dataset",
      "authors": [
        "Wentao Mo",
        "Qingchao Chen",
        "Yuxin Peng",
        "Siyuan Huang",
        "Yang Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "The advancement of 3D vision-language (3D VL) learning is hindered by several\nlimitations in existing 3D VL datasets: they rarely necessitate reasoning\nbeyond a close range of objects in single viewpoint, and annotations often link\ninstructions to single objects, missing richer contextual alignments between\nmultiple objects. This significantly curtails the development of models capable\nof deep, multi-view 3D scene understanding over distant objects. To address\nthese challenges, we introduce MV-ScanQA, a novel 3D question answering dataset\nwhere 68% of questions explicitly require integrating information from multiple\nviews (compared to less than 7% in existing datasets), thereby rigorously\ntesting multi-view compositional reasoning. To facilitate the training of\nmodels for such demanding scenarios, we present TripAlign dataset, a\nlarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D\nview, set of 3D objects, text> triplets that explicitly aligns groups of\ncontextually related objects with text, providing richer, view-grounded\nmulti-object multimodal alignment signals than previous single-object\nannotations. We further develop LEGO, a baseline method for the multi-view\nreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D\nLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign\nachieves state-of-the-art performance not only on the proposed MV-ScanQA, but\nalso on existing benchmarks for 3D dense captioning and question answering.\nDatasets and code are available at\nhttps://matthewdm0816.github.io/tripalign-mvscanqa.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11058v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11058v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.339,
      "datasets_score": 0.414,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on 3D vision-language datasets and a baseline method for multi-view reasoning, but it does not involve diffusion models, iterative refinement processes, or any multi-step logical reasoning via diffusion. There is no mention of adapting diffusion for tasks like Chain-of-Thought processing.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include introducing new datasets (MV-ScanQA for 3D question answering and TripAlign for 2D-3D-language pre-training), analyzing the limitations of existing datasets through solvability studies, and benchmarking their impact on 3D vision-language tasks. This directly aligns with research on dataset creation, analysis, and evaluation.",
      "llm_score_status": "completed",
      "summary": "This paper addresses limitations in existing 3D vision-language datasets by introducing MV-ScanQA, a new benchmark where 68% of questions require multi-view reasoning to integrate information across viewpoints, and TripAlign, a large-scale pre-training dataset with over 1 million triplets aligning 2D views, sets of 3D objects, and text to capture richer multi-object relationships. The authors develop a baseline model, LEGO, which leverages TripAlign to transfer knowledge from pre-trained 2D large vision-language models to 3D tasks, achieving state-of-the-art performance on MV-ScanQA and other benchmarks, thereby demonstrating the value of enhanced multi-view training data for advancing 3D scene understanding.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces novel datasets like MV-ScanQA, which significantly advances multi-view reasoning in 3D vision-language learning by requiring integration across multiple views, and TripAlign, providing a new pre-training corpus for richer multi-object alignments, thus pushing the state-of-the-art forward.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research in 3D vision-language models by offering improved benchmarks and training data that enhance multi-view reasoning, potentially leading to broader applications in fields like robotics and augmented reality.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a strong and valuable contribution to 3D vision-language learning through innovative datasets and a baseline model, making it essential for researchers in computer vision to be aware of for advancing multi-view scene understanding.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/424254a1723ab73d384a54d6fe55090c6363dd0a",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 19,
      "average_h_index": 6.8,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Wentao Mo",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2286895875"
        },
        {
          "name": "Qingchao Chen",
          "h_index": 19,
          "profile_url": "https://www.semanticscholar.org/author/8559994"
        },
        {
          "name": "Yuxin Peng",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2175354364"
        },
        {
          "name": "Siyuan Huang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2310392419"
        },
        {
          "name": "Yang Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2286907988"
        }
      ]
    },
    {
      "id": "2508.11063",
      "title": "Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight,\n  and Obese Cohorts",
      "authors": [
        "Lucas W. Remedios",
        "Chloe Cho",
        "Trent M. Schwartz",
        "Dingjie Su",
        "Gaurav Rudravaram",
        "Chenyu Gao",
        "Aravind R. Krishnan",
        "Adam M. Saunders",
        "Michael E. Kim",
        "Shunxing Bao",
        "Alvin C. Powers",
        "Bennett A. Landman",
        "John Virostko"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Purpose: Although elevated BMI is a well-known risk factor for type 2\ndiabetes, the disease's presence in some lean adults and absence in others with\nobesity suggests that detailed body composition may uncover abdominal\nphenotypes of type 2 diabetes. With AI, we can now extract detailed\nmeasurements of size, shape, and fat content from abdominal structures in 3D\nclinical imaging at scale. This creates an opportunity to empirically define\nbody composition signatures linked to type 2 diabetes risk and protection using\nlarge-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal\npatterns from clinical CT, we applied our design four times: once on the full\ncohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese\n(n = 620) subgroups separately. Briefly, our experimental design transforms\nabdominal scans into collections of explainable measurements through\nsegmentation, classifies type 2 diabetes through a cross-validated random\nforest, measures how features contribute to model-estimated risk or protection\nthrough SHAP analysis, groups scans by shared model decision patterns\n(clustering from SHAP) and links back to anatomical differences\n(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.\nThere were shared type 2 diabetes signatures in each group; fatty skeletal\nmuscle, older age, greater visceral and subcutaneous fat, and a smaller or\nfat-laden pancreas. Univariate logistic regression confirmed the direction of\n14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:\nOur findings suggest that abdominal drivers of type 2 diabetes may be\nconsistent across weight classes.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11063v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11063v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.256,
      "weak_supervision_score": 0.282,
      "diffusion_reasoning_score": 0.285,
      "distributed_training_score": 0.296,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11070",
      "title": "From Individual to Multi-Agent Algorithmic Recourse: Minimizing the\n  Welfare Gap via Capacitated Bipartite Matching",
      "authors": [
        "Zahra Khotanlou",
        "Kate Larson",
        "Amir-Hossein Karimi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Decision makers are increasingly relying on machine learning in sensitive\nsituations. In such settings, algorithmic recourse aims to provide individuals\nwith actionable and minimally costly steps to reverse unfavorable AI-driven\ndecisions. While existing research predominantly focuses on single-individual\n(i.e., seeker) and single-model (i.e., provider) scenarios, real-world\napplications often involve multiple interacting stakeholders. Optimizing\noutcomes for seekers under an individual welfare approach overlooks the\ninherently multi-agent nature of real-world systems, where individuals interact\nand compete for limited resources. To address this, we introduce a novel\nframework for multi-agent algorithmic recourse that accounts for multiple\nrecourse seekers and recourse providers. We model this many-to-many interaction\nas a capacitated weighted bipartite matching problem, where matches are guided\nby both recourse cost and provider capacity. Edge weights, reflecting recourse\ncosts, are optimized for social welfare while quantifying the welfare gap\nbetween individual welfare and this collectively feasible outcome. We propose a\nthree-layer optimization framework: (1) basic capacitated matching, (2) optimal\ncapacity redistribution to minimize the welfare gap, and (3) cost-aware\noptimization balancing welfare maximization with capacity adjustment costs.\nExperimental validation on synthetic and real-world datasets demonstrates that\nour framework enables the many-to-many algorithmic recourse to achieve\nnear-optimal welfare with minimum modification in system settings. This work\nextends algorithmic recourse from individual recommendations to system-level\ndesign, providing a tractable path toward higher social welfare while\nmaintaining individual actionability.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11070v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11070v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.44,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.352,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is developing a framework for multi-agent algorithmic recourse using capacitated bipartite matching to optimize social welfare in decision-making systems. It focuses on minimizing welfare gaps in scenarios with multiple seekers and providers, without involving reinforcement learning, human feedback, reward models, or fine-tuning AI models based on human preferences. Thus, it does not align with RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11074",
      "title": "LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual\n  Lightweight Adapters",
      "authors": [
        "Haomin Zhang",
        "Kristin Qi",
        "Shuxin Yang",
        "Zihao Chen",
        "Chaofan Ding",
        "Xinhan Di"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Generating high-quality and temporally synchronized audio from video content\nis essential for video editing and post-production tasks, enabling the creation\nof semantically aligned audio for silent videos. However, most existing\napproaches focus on short-form audio generation for video segments under 10\nseconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To\naddress these limitations, we introduce LD-LAudio-V1, an extension of\nstate-of-the-art video-to-audio models and it incorporates dual lightweight\nadapters to enable long-form audio generation. In addition, we release a clean\nand human-annotated video-to-audio dataset that contains pure sound effects\nwithout noise or artifacts. Our method significantly reduces splicing artifacts\nand temporal inconsistencies while maintaining computational efficiency.\nCompared to direct fine-tuning with short training videos, LD-LAudio-V1\nachieves significant improvements across multiple metrics: $FD_{\\text{passt}}$\n450.00 $\\rightarrow$ 327.29 (+27.27%), $FD_{\\text{panns}}$ 34.88 $\\rightarrow$\n22.68 (+34.98%), $FD_{\\text{vgg}}$ 3.75 $\\rightarrow$ 1.28 (+65.87%),\n$KL_{\\text{panns}}$ 2.49 $\\rightarrow$ 2.07 (+16.87%), $KL_{\\text{passt}}$ 1.78\n$\\rightarrow$ 1.53 (+14.04%), $IS_{\\text{panns}}$ 4.17 $\\rightarrow$ 4.30\n(+3.12%), $IB_{\\text{score}}$ 0.25 $\\rightarrow$ 0.28 (+12.00%),\n$Energy\\Delta10\\text{ms}$ 0.3013 $\\rightarrow$ 0.1349 (+55.23%),\n$Energy\\Delta10\\text{ms(vs.GT)}$ 0.0531 $\\rightarrow$ 0.0288 (+45.76%), and\n$Sem.\\,Rel.$ 2.73 $\\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate\nfurther research in long-form video-to-audio generation and is available at\nhttps://github.com/deepreasonings/long-form-video2audio.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11074v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11074v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.312,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on extending video-to-audio generation models using dual lightweight adapters for long-form synthesis, mentioning diffusion-based models as one category of existing approaches for audio generation. However, it does not adapt diffusion processes for complex logical tasks, multi-step reasoning, or Chain-of-Thought entities. The core contribution is in multimedia synthesis, not logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11085",
      "title": "Learn to optimize for automatic proton PBS treatment planning for H&N\n  cancers",
      "authors": [
        "Qingqing Wang",
        "Liqiang Xiao",
        "Chang Chang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Proton PBS treatment planning for H&N cancers involves numerous conflicting\nobjectives, requiring significant effort from human planners to balance and\nsatisfy multiple clinical goals during planning. To achieve this,\nexperience-demanding objective parameter adjustment and computationally\nexpensive inverse optimization are performed iteratively. Extensive efforts\nhave been made to automatically adjust objective parameters, but the most\ntime-consuming component, i.e., inverse optimization, still relies heavily on\ntheory-driven approaches. We propose a data-driven inverse optimizer and\nintegrate it into a PPO-based automatic treatment planning framework to\nautomatically generate high-quality plans within a clinical acceptable planning\ntime. The inverse optimizer is a L2O method that predicts update steps by\nlearning from the task-specific data distribution. For the first time, we\nintegrate techniques designed for long-context processing, originally developed\nfor LLMs, into a Transformer-based L2O framework to address the scalability\nissue of existing L2O methods. The PPO framework functions as an outer-loop\nvirtual planner, autonomously adjusting objective parameters through a policy\nnetwork, and the dose predictor is used to initialize objective parameters. The\ninner-loop L2O inverse optimizer computes machine-deliverable MU values based\non objectives refined by the PPO policy network. 97 patients are collected in\nthis study, and compared with L-BFGSB, our L2O-based inverse optimizer improves\nthe effectiveness and efficiency by 22.97% and 36.41%, respectively. In\nconjunction with the PPO-based learned virtual planner, plans generated by our\nframework within an average of 2.55 hours show improved or comparable OAR\nsparing with superior target coverage for patients with different prescription\ndose levels, number of target volumes, beam angles, etc., compared with\nhuman-generated plans.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11085v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11085v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.413,
      "datasets_score": 0.268,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Proximal Policy Optimization (PPO), a reinforcement learning method, but does not involve training a reward model on human-ranked data or aligning the model with human preferences. Instead, it employs a dose distribution-based reward function derived from clinical metrics, which is not based on human feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on a PPO-based framework and a Transformer-based L2O optimizer for treatment planning, but it does not discuss distributed training, parallel computing, or strategies for partitioning data/computation across multiple processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11090",
      "title": "Compressive Meta-Learning",
      "authors": [
        "Daniel Mas Montserrat",
        "David Bonet",
        "Maria Perera",
        "Xavier Giró-i-Nieto",
        "Alexander G. Ioannidis"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.DB (Databases)"
      ],
      "abstract": "The rapid expansion in the size of new datasets has created a need for fast\nand efficient parameter-learning techniques. Compressive learning is a\nframework that enables efficient processing by using random, non-linear\nfeatures to project large-scale databases onto compact, information-preserving\nrepresentations whose dimensionality is independent of the number of samples\nand can be easily stored, transferred, and processed. These database-level\nsummaries are then used to decode parameters of interest from the underlying\ndata distribution without requiring access to the original samples, offering an\nefficient and privacy-friendly learning framework. However, both the encoding\nand decoding techniques are typically randomized and data-independent, failing\nto exploit the underlying structure of the data. In this work, we propose a\nframework that meta-learns both the encoding and decoding stages of compressive\nlearning methods by using neural networks that provide faster and more accurate\nsystems than the current state-of-the-art approaches. To demonstrate the\npotential of the presented Compressive Meta-Learning framework, we explore\nmultiple applications -- including neural network-based compressive PCA,\ncompressive ridge regression, compressive k-means, and autoencoders.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11090v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11090v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.422,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on compressive meta-learning for efficient data compression and parameter learning using neural networks, without any discussion of training models with noisy or programmatically generated labels. It does not address weak supervision techniques, such as using high-level sources for label generation, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper mentions that compressive learning allows for parallelization of sketch computations across datasets, which could relate to distributed computing concepts. However, it does not primarily focus on distributed training algorithms, parallel model architectures, or multi-node strategies for accelerating machine learning training, limiting its relevance to peripheral efficiency aspects.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11093",
      "title": "Utilizing Vision-Language Models as Action Models for Intent Recognition\n  and Assistance",
      "authors": [
        "Cesar Alan Contreras",
        "Manolis Chiou",
        "Alireza Rastegarpanah",
        "Michal Szulik",
        "Rustam Stolkin"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Human-robot collaboration requires robots to quickly infer user intent,\nprovide transparent reasoning, and assist users in achieving their goals. Our\nrecent work introduced GUIDER, our framework for inferring navigation and\nmanipulation intents. We propose augmenting GUIDER with a vision-language model\n(VLM) and a text-only language model (LLM) to form a semantic prior that\nfilters objects and locations based on the mission prompt. A vision pipeline\n(YOLO for object detection and the Segment Anything Model for instance\nsegmentation) feeds candidate object crops into the VLM, which scores their\nrelevance given an operator prompt; in addition, the list of detected object\nlabels is ranked by a text-only LLM. These scores weight the existing\nnavigation and manipulation layers of GUIDER, selecting context-relevant\ntargets while suppressing unrelated objects. Once the combined belief exceeds a\nthreshold, autonomy changes occur, enabling the robot to navigate to the\ndesired area and retrieve the desired object, while adapting to any changes in\nthe operator's intent. Future work will evaluate the system on Isaac Sim using\na Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11093v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11093v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.482,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.481,
      "distributed_training_score": 0.322,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on integrating vision-language models and language models into a robot framework for intent recognition and assistance, without any mention of training models using human feedback, reward models, or reinforcement learning techniques. It does not involve aligning AI with human preferences through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes using VLMs and LLMs for object scoring and intent inference in a probabilistic framework, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. There is no evidence of adapting diffusion for Chain-of-Thought or similar tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11106",
      "title": "HierOctFusion: Multi-scale Octree-based 3D Shape Generation via\n  Part-Whole-Hierarchy Message Passing",
      "authors": [
        "Xinjie Gao",
        "Bi'an Du",
        "Wei Hu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D content generation remains a fundamental yet challenging task due to the\ninherent structural complexity of 3D data. While recent octree-based diffusion\nmodels offer a promising balance between efficiency and quality through\nhierarchical generation, they often overlook two key insights: 1) existing\nmethods typically model 3D objects as holistic entities, ignoring their\nsemantic part hierarchies and limiting generalization; and 2) holistic\nhigh-resolution modeling is computationally expensive, whereas real-world\nobjects are inherently sparse and hierarchical, making them well-suited for\nlayered generation. Motivated by these observations, we propose HierOctFusion,\na part-aware multi-scale octree diffusion model that enhances hierarchical\nfeature interaction for generating fine-grained and sparse object structures.\nFurthermore, we introduce a cross-attention conditioning mechanism that injects\npart-level information into the generation process, enabling semantic features\nto propagate effectively across hierarchical levels from parts to the whole.\nAdditionally, we construct a 3D dataset with part category annotations using a\npre-trained segmentation model to facilitate training and evaluation.\nExperiments demonstrate that HierOctFusion achieves superior shape quality and\nefficiency compared to prior methods.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11106v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11106v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.505,
      "distributed_training_score": 0.346,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on HierOctFusion, a diffusion-based model for 3D shape generation, which uses iterative refinement to create hierarchical 3D structures from octrees. However, this process is applied to visual and geometric data generation, not to multi-step logical reasoning tasks such as solving complex problems or managing a 'Chain-of-Thought'. There is no component in the paper that adapts diffusion for holistic correction of reasoning paths or logical inference, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11110",
      "title": "Diffusion is a code repair operator and generator",
      "authors": [
        "Mukul Singh",
        "Gust Verbruggen",
        "Vu Le",
        "Sumit Gulwani"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Code diffusion models generate code by iteratively removing noise from the\nlatent representation of a code snippet. During later steps of the diffusion\nprocess, when the code snippet has almost converged, differences between\ndiscrete representations of these snippets look like last-mile repairs applied\nto broken or incomplete code. We evaluate the extent to which this resemblance\ncan be exploited to leverage pre-trained code diffusion models for the problem\nof last-mile repair by considering two applications with significant potential.\nFirst, we can leverage the diffusion model for last-mile repair by adding noise\nto a broken code snippet and resuming the diffusion process. Second, we can\nleverage the diffusion model to generate arbitrary amount of training data for\nlast-mile repair tasks (that are computationally more efficient) by sampling an\nintermediate program (input) and the final program (output) from the diffusion\nprocess. We perform experiments on 3 domains (Python, Excel and PowerShell) to\nevaluate applications, as well as analyze properties.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11110v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11110v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.636,
      "distributed_training_score": 0.346,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models for iterative code repair and generation, which involves refining code snippets over multiple steps, similar to the iterative process in diffusion-based reasoning. However, it primarily focuses on code-specific tasks like fixing broken code and generating training data, rather than adapting diffusion for solving complex logical tasks or holistically correcting a Chain-of-Thought. There is no explicit component for multi-step logical reasoning, making the connection indirect.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11112",
      "title": "Quantization through Piecewise-Affine Regularization: Optimization and\n  Statistical Guarantees",
      "authors": [
        "Jianhao Ma",
        "Lin Xiao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.OC (Optimization and Control)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Optimization problems over discrete or quantized variables are very\nchallenging in general due to the combinatorial nature of their search space.\nPiecewise-affine regularization (PAR) provides a flexible modeling and\ncomputational framework for quantization based on continuous optimization. In\nthis work, we focus on the setting of supervised learning and investigate the\ntheoretical foundations of PAR from optimization and statistical perspectives.\nFirst, we show that in the overparameterized regime, where the number of\nparameters exceeds the number of samples, every critical point of the\nPAR-regularized loss function exhibits a high degree of quantization. Second,\nwe derive closed-form proximal mappings for various (convex, quasi-convex, and\nnon-convex) PARs and show how to solve PAR-regularized problems using the\nproximal gradient method, its accelerated variant, and the Alternating\nDirection Method of Multipliers. Third, we study statistical guarantees of\nPAR-regularized linear regression problems; specifically, we can approximate\nclassical formulations of $\\ell_1$-, squared $\\ell_2$-, and nonconvex\nregularizations using PAR and obtain similar statistical guarantees with\nquantized solutions.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11112v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11112v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.368,
      "datasets_score": 0.239,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11115",
      "title": "UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous\n  Ergonomic Sitting Posture Monitoring",
      "authors": [
        "Haotang Li",
        "Zhenyu Qi",
        "Sen He",
        "Kebin Peng",
        "Sheng Tan",
        "Yili Ren",
        "Tomas Cerny",
        "Jiyue Zhao",
        "Zi Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Improper sitting posture during prolonged computer use has become a\nsignificant public health concern. Traditional posture monitoring solutions\nface substantial barriers, including privacy concerns with camera-based systems\nand user discomfort with wearable sensors. This paper presents\nUWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that\nadvances mobile technologies for preventive health management through\ncontinuous, contactless monitoring of ergonomic sitting posture. Our system\nleverages commercial UWB devices, utilizing comprehensive feature engineering\nto extract multiple ergonomic sitting posture features. We develop PoseGBDT to\neffectively capture temporal dependencies in posture patterns, addressing\nlimitations of traditional frame-wise classification approaches. Extensive\nreal-world evaluation across 10 participants and 19 distinct postures\ndemonstrates exceptional performance, achieving 99.11% accuracy while\nmaintaining robustness against environmental variables such as clothing\nthickness, additional devices, and furniture configurations. Our system\nprovides a scalable, privacy-preserving mobile health solution on existing\nplatforms for proactive ergonomic management, improving quality of life at low\ncosts.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11115v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11115v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.255,
      "distributed_training_score": 0.275,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11121",
      "title": "Tabularis Formatus: Predictive Formatting for Tables",
      "authors": [
        "Mukul Singh",
        "José Cambronero",
        "Sumit Gulwani",
        "Vu Le",
        "Gust Verbruggen"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Spreadsheet manipulation software are widely used for data management and\nanalysis of tabular data, yet the creation of conditional formatting (CF) rules\nremains a complex task requiring technical knowledge and experience with\nspecific platforms. In this paper we present TaFo, a neuro-symbolic approach to\ngenerating CF suggestions for tables, addressing common challenges such as user\nunawareness, difficulty in rule creation, and inadequate user interfaces. TaFo\ntakes inspiration from component based synthesis systems and extends them with\nsemantic knowledge of language models and a diversity preserving rule\nranking.Unlike previous methods focused on structural formatting, TaFo uniquely\nincorporates value-based formatting, automatically learning both the rule\ntrigger and the associated visual formatting properties for CF rules. By\nremoving the dependency on user specification used by existing techniques in\nthe form of formatted examples or natural language instruction, TaFo makes\nformatting completely predictive and automated for the user. To evaluate TaFo,\nwe use a corpus of 1.8 Million public workbooks with CF and manual formatting.\nWe compare TaFo against a diverse set of symbolic and neural systems designed\nfor or adapted for the task of table formatting. Our results show that TaFo\ngenerates more accurate, diverse and complete formatting suggestions than\ncurrent systems and outperforms these by 15.6\\%--26.5\\% on matching user added\nground truth rules in tables.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11121v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11121v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.292,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11708",
      "title": "Street Review: A Participatory AI-Based Framework for Assessing\n  Streetscape Inclusivity",
      "authors": [
        "Rashid Mushkani",
        "Shin Koseki"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Urban centers undergo social, demographic, and cultural changes that shape\npublic street use and require systematic evaluation of public spaces. This\nstudy presents Street Review, a mixed-methods approach that combines\nparticipatory research with AI-based analysis to assess streetscape\ninclusivity. In Montr\\'eal, Canada, 28 residents participated in semi-directed\ninterviews and image evaluations, supported by the analysis of approximately\n45,000 street-view images from Mapillary. The approach produced visual\nanalytics, such as heatmaps, to correlate subjective user ratings with physical\nattributes like sidewalk, maintenance, greenery, and seating. Findings reveal\nvariations in perceptions of inclusivity and accessibility across demographic\ngroups, demonstrating that incorporating diverse user feedback can enhance\nmachine learning models through careful data-labeling and co-production\nstrategies. The Street Review framework offers a systematic method for urban\nplanners and policy analysts to inform planning, policy development, and\nmanagement of public streets.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11708v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11708v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.3,
      "distributed_training_score": 0.276,
      "datasets_score": 0.421,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses a dataset of approximately 45,000 street-view images from Mapillary and incorporates user feedback for data-labeling to enhance machine learning models, which involves aspects of dataset analysis and curation. However, the primary contribution is the Street Review framework for urban assessment, not the creation, benchmarking, or in-depth evaluation of datasets.",
      "llm_score_status": "completed",
      "summary": "The Street Review framework is a mixed-methods approach that combines participatory research with AI-based analysis to evaluate the inclusivity of urban streetscapes in Montréal, Canada. By involving 28 residents in semi-directed interviews and image evaluations, alongside the analysis of approximately 45,000 street-view images from Mapillary, the study generates visual analytics like heatmaps to correlate subjective user ratings with physical attributes such as sidewalks and greenery, revealing variations in perceptions across demographic groups and demonstrating how diverse feedback can enhance machine learning models for urban planning and policy development.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by cleverly combining participatory research with AI image analysis to assess streetscape inclusivity, addressing a known urban challenge in a new integrated way. While not introducing entirely novel techniques, it advances the application of existing methods through co-production strategies.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in urban planning and AI subfields, as it provides practical tools for enhancing public space inclusivity and policy-making. However, its influence may be limited to specific applications rather than broader commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, valuable contribution to AI and urban studies by introducing a framework that integrates diverse user feedback with technology, making it essential for researchers in these areas to be aware of. While not groundbreaking, its practical implications warrant attention from relevant professionals.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/727d6466fa2a692148df3cb6ee7699dc46525978",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 4,
      "average_h_index": 3.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Rashid A. Mushkani",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2089728714"
        },
        {
          "name": "Shin Koseki",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2327334397"
        }
      ]
    },
    {
      "id": "2508.11709",
      "title": "Navigating the New Landscape: A Conceptual Model for Project-Based\n  Assessment (PBA) in the Age of GenAI",
      "authors": [
        "Rajan Kadel",
        "Samar Shailendra",
        "Urvashi Rahul Saxena"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid integration of Generative Artificial Intelligence (GenAI) into\nhigher education presents both opportunities and challenges for assessment\ndesign, particularly within Project-Based Assessment (PBA) contexts.\nTraditional assessment methods often emphasise the final product in the PBA,\nwhich can now be significantly influenced or created by GenAI tools, raising\nconcerns regarding product authenticity, academic integrity, and learning\nvalidation. This paper advocates for a reimagined assessment model for\nProject-Based Learning (PBL) or a capstone project that prioritises\nprocess-oriented evaluation, multi-modal and multifaceted assessment design,\nand ethical engagement with GenAI to enable higher-order thinking. The model\nalso emphasises the use of (GenAI-assisted) personalised feedback by a\nsupervisor as an observance of the learning process during the project\nlifecycle. A use case scenario is provided to illustrate the application of the\nmodel in a capstone project setting. The paper concludes with recommendations\nfor educators and curriculum designers to ensure that assessment practices\nremain robust, learner-centric, and integrity-driven in the evolving landscape\nof GenAI.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11709v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11709v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.33,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a conceptual model for project-based assessment in education, emphasizing personalized feedback for students, but it does not involve training AI models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reimagining assessment designs for education in the context of GenAI, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for AI tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11710",
      "title": "Code Vulnerability Detection Across Different Programming Languages with\n  AI Models",
      "authors": [
        "Hael Abdulhakim Ali Humran",
        "Ferdi Sonmez"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Security vulnerabilities present in a code that has been written in diverse\nprogramming languages are among the most critical yet complicated aspects of\nsource code to detect. Static analysis tools based on rule-based patterns\nusually do not work well at detecting the context-dependent bugs and lead to\nhigh false positive rates. Recent developments in artificial intelligence,\nspecifically the use of transformer-based models like CodeBERT and CodeLlama,\nprovide light to this problem, as they show potential in finding such flaws\nbetter. This paper presents the implementations of these models on various\ndatasets of code vulnerability, showing how off-the-shelf models can\nsuccessfully produce predictive capacity in models through dynamic fine-tuning\nof the models on vulnerable and safe code fragments. The methodology comprises\nthe gathering of the dataset, normalization of the language, fine-tuning of the\nmodel, and incorporation of ensemble learning and explainable AI. Experiments\nshow that a well-trained CodeBERT can be as good as or even better than some\nexisting static analyzers in terms of accuracy greater than 97%. Further study\nhas indicated that although language models can achieve close-to-perfect\nrecall, the precision can decrease. A solution to this is given by hybrid\nmodels and validation procedures, which will reduce false positives. According\nto the results, the AI-based solutions generalize to different programming\nlanguages and classes of vulnerability. Nevertheless, robustness,\ninterpretability, and deployment readiness are still being developed. The\nresults illustrate the probabilities that AI will enhance the trustworthiness\nin the usability and scalability of machine-learning-based detectors of\nvulnerabilities.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11710v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11710v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.306,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11711",
      "title": "Enhancing GraphQL Security by Detecting Malicious Queries Using Large\n  Language Models, Sentence Transformers, and Convolutional Neural Networks",
      "authors": [
        "Irash Perera",
        "Hiranya Abeyrathne",
        "Sanjeewa Malalgoda",
        "Arshardh Ifthikar"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "GraphQL's flexibility, while beneficial for efficient data fetching,\nintroduces unique security vulnerabilities that traditional API security\nmechanisms often fail to address. Malicious GraphQL queries can exploit the\nlanguage's dynamic nature, leading to denial-of-service attacks, data\nexfiltration through injection, and other exploits. Existing solutions, such as\nstatic analysis, rate limiting, and general-purpose Web Application Firewalls,\noffer limited protection against sophisticated, context-aware attacks. This\npaper presents a novel, AI-driven approach for real-time detection of malicious\nGraphQL queries. Our method combines static analysis with machine learning\ntechniques, including Large Language Models (LLMs) for dynamic schema-based\nconfiguration, Sentence Transformers (SBERT and Doc2Vec) for contextual\nembedding of query payloads, and Convolutional Neural Networks (CNNs), Random\nForests, and Multilayer Perceptrons for classification. We detail the system\narchitecture, implementation strategies optimized for production environments\n(including ONNX Runtime optimization and parallel processing), and evaluate the\nperformance of our detection models and the overall system under load. Results\ndemonstrate high accuracy in detecting various threats, including SQL\ninjection, OS command injection, and XSS exploits, alongside effective\nmitigation of DoS and SSRF attempts. This research contributes a robust and\nadaptable solution for enhancing GraphQL API security.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11711v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11711v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.386,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11715",
      "title": "Benchmark Dataset Generation and Evaluation for Excel Formula Repair\n  with LLMs",
      "authors": [
        "Ananya Singha",
        "Harshita Sahijwani",
        "Walt Williams",
        "Emmanuel Aboah Boateng",
        "Nick Hausman",
        "Miguel Di Luca",
        "Keegan Choudhury",
        "Chaya Binet",
        "Vu Le",
        "Tianwei Chen",
        "Oryan Rokeah Chen",
        "Sulaiman Vesal",
        "Sadid Hasan"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Excel is a pervasive yet often complex tool, particularly for novice users,\nwhere runtime errors arising from logical mistakes or misinterpretations of\nfunctions pose a significant challenge. While large language models (LLMs)\noffer promising assistance by explaining formula errors, the automated\ncorrection of these semantic runtime errors remains an open problem. A primary\nchallenge to advancing models for such scenarios is the severe lack of\nhigh-quality, comprehensive datasets for training and rigorous evaluation. This\npaper addresses this gap by introducing a novel approach for constructing a\nbenchmark dataset specifically designed for Excel formula repair. We propose a\ndata generation pipeline, which leverages a small set of curated seed samples\nfrom online forums to synthetically expand the dataset. Our pipeline integrates\nfew-shot prompting with LLMs and employs a robust \\textit{LLM-as-a-Judge}\nvalidation framework, combined with execution-based checks to ensure the\ncorrectness and semantic fidelity of the generated data. This process produced\na benchmark dataset of 618 high-quality samples, covering common runtime\nerrors. Furthermore, we propose a context-aware baseline technique for Excel\nformula repair that utilizes LLMs to leverage both the faulty formula, and\nrelevant spreadsheet context. We evaluate the performance of various LLMs\n(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using\nexecution-based metrics. Our analysis demonstrates the dataset's quality\nthrough manual annotation and provides insights into error and function\ndistributions. The proposed generation methodology is highly scalable and can\nbe readily adapted to create evaluation benchmarks for similar code repair\ntasks in other low-resource programming languages.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11715v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11715v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.346,
      "datasets_score": 0.441,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's data generation pipeline uses LLMs with few-shot prompting to synthetically expand a small set of seed samples into a larger dataset, programmatically generating labels and examples from high-level sources without relying on hand-labeled data. This directly aligns with weak supervision techniques, as it leverages noisy or automated processes for training data creation.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper focuses on creating, validating, and evaluating a new benchmark dataset (FoRepBench) for Excel formula repair, including dataset generation methodologies, analysis of error distributions, and performance benchmarking with LLMs. This core contribution directly pertains to dataset research in AI and machine learning.",
      "llm_score_status": "completed",
      "summary": "This paper introduces FoRepBench, a benchmark dataset for repairing Excel formulas that cause runtime errors, addressing the lack of resources for training and evaluating large language models (LLMs) in this domain. The methodology involves a synthetic data generation pipeline using few-shot prompting with LLMs, validated through an LLM-as-a-Judge framework and execution-based checks on a small set of seed samples from online forums, resulting in 618 high-quality examples; it also proposes a context-aware baseline for formula repair and evaluates various LLMs, demonstrating the dataset's quality and scalability for similar tasks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating the first large-scale dataset specifically for Excel runtime error repair using LLMs, combining existing techniques in a new way for spreadsheets, though it builds on established APR and data generation methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI-assisted software engineering for spreadsheets, as it provides a valuable dataset and methodology that could improve formula repair tools, though its influence may remain niche.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by introducing a essential benchmark for Excel formula repair, making it valuable for researchers in AI and software engineering focused on end-user programming.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/39543e49dc6eebd963aa97a24152b9d80ec6984b",
      "total_authors": 13,
      "authors_found": 13,
      "highest_h_index": 18,
      "average_h_index": 2.230769230769231,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Ananya Singha",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2258715404"
        },
        {
          "name": "Harshita Sahijwani",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/40411912"
        },
        {
          "name": "Walt Williams",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376194604"
        },
        {
          "name": "Emmanuel Aboah Boateng",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2154099077"
        },
        {
          "name": "Nick Hausman",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376194659"
        },
        {
          "name": "Miguel Di Luca",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376194400"
        },
        {
          "name": "Keegan Choudhury",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376193582"
        },
        {
          "name": "Chaya Binet",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376194699"
        },
        {
          "name": "Vu Le",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376196189"
        },
        {
          "name": "Tianwei Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2332478290"
        },
        {
          "name": "Oryan Rokeah Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376194374"
        },
        {
          "name": "Sulaiman Vesal",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/37813096"
        },
        {
          "name": "Sadid Hasan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2332333834"
        }
      ]
    },
    {
      "id": "2508.11716",
      "title": "Privacy-Aware Detection of Fake Identity Documents: Methodology,\n  Benchmark, and Improved Algorithms (FakeIDet2)",
      "authors": [
        "Javier Muñoz-Haro",
        "Ruben Tolosana",
        "Julian Fierrez",
        "Ruben Vera-Rodriguez",
        "Aythami Morales"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Remote user verification in Internet-based applications is becoming\nincreasingly important nowadays. A popular scenario for it consists of\nsubmitting a picture of the user's Identity Document (ID) to a service\nplatform, authenticating its veracity, and then granting access to the\nrequested digital service. An ID is well-suited to verify the identity of an\nindividual, since it is government issued, unique, and nontransferable.\nHowever, with recent advances in Artificial Intelligence (AI), attackers can\nsurpass security measures in IDs and create very realistic physical and\nsynthetic fake IDs. Researchers are now trying to develop methods to detect an\never-growing number of these AI-based fakes that are almost indistinguishable\nfrom authentic (bona fide) IDs. In this counterattack effort, researchers are\nfaced with an important challenge: the difficulty in using real data to train\nfake ID detectors. This real data scarcity for research and development is\noriginated by the sensitive nature of these documents, which are usually kept\nprivate by the ID owners (the users) and the ID Holders (e.g., government,\npolice, bank, etc.). The main contributions of our study are: 1) We propose and\ndiscuss a patch-based methodology to preserve privacy in fake ID detection\nresearch. 2) We provide a new public database, FakeIDet2-db, comprising over\n900K real/fake ID patches extracted from 2,000 ID images, acquired using\ndifferent smartphone sensors, illumination and height conditions, etc. In\naddition, three physical attacks are considered: print, screen, and composite.\n3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We\nrelease a standard reproducible benchmark that considers physical and synthetic\nattacks from popular databases in the literature.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11716v2",
      "pdf_url": "http://arxiv.org/pdf/2508.11716v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.296,
      "distributed_training_score": 0.306,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11719",
      "title": "Are AI Machines Making Humans Obsolete?",
      "authors": [
        "Matthias Scheutz"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This chapter starts with a sketch of how we got to \"generative AI\" (GenAI)\nand a brief summary of the various impacts it had so far. It then discusses\nsome of the opportunities of GenAI, followed by the challenges and dangers,\nincluding dystopian outcomes resulting from using uncontrolled machine learning\nand our failures to understand the results. It concludes with some suggestions\nfor how to control GenAI and address its dangers.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.11719v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11719v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.374,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses the history, impacts, and control of generative AI, including transformer models and potential dangers, but does not mention reinforcement learning, human feedback mechanisms, or training AI models with human-ranked data to align preferences. Therefore, it lacks any direct or indirect connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generative AI, transformers, neural networks, and their evolution, but does not reference diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes. There is no component related to treating a 'Chain-of-Thought' as an entity for correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.13184",
      "title": "BERT-VQA: Visual Question Answering on Plots",
      "authors": [
        "Tai Vu",
        "Robert Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual question answering has been an exciting challenge in the field of\nnatural language understanding, as it requires deep learning models to exchange\ninformation from both vision and language domains. In this project, we aim to\ntackle a subtask of this problem, namely visual question answering on plots. To\nachieve this, we developed BERT-VQA, a VisualBERT-based model architecture with\na pretrained ResNet 101 image encoder, along with a potential addition of joint\nfusion. We trained and evaluated this model against a baseline that consisted\nof a LSTM, a CNN, and a shallow classifier. The final outcome disproved our\ncore hypothesis that the cross-modality module in VisualBERT is essential in\naligning plot components with question phrases. Therefore, our work provided\nvaluable insights into the difficulty of the plot question answering challenge\nas well as the appropriateness of different model architectures in solving this\nproblem.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.13184v1",
      "pdf_url": "http://arxiv.org/pdf/2508.13184v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.443,
      "distributed_training_score": 0.294,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Visual Question Answering (VQA) for plots using models like VisualBERT and a baseline with LSTM, CNN, and a classifier. It evaluates multimodal fusion techniques but does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.13186",
      "title": "MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents",
      "authors": [
        "Shilong Li",
        "Xingyuan Bu",
        "Wenjie Wang",
        "Jiaheng Liu",
        "Jun Dong",
        "Haoyang He",
        "Hao Lu",
        "Haozhe Zhang",
        "Chenchen Jing",
        "Zhen Li",
        "Chuanhao Li",
        "Jiayi Tian",
        "Chenchen Zhang",
        "Tianhao Peng",
        "Yancheng He",
        "Jihao Gu",
        "Yuanxing Zhang",
        "Jian Yang",
        "Ge Zhang",
        "Wenhao Huang",
        "Wangchunshu Zhou",
        "Zhaoxiang Zhang",
        "Ruizhe Ding",
        "Shilei Wen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "AI agents with advanced reasoning and tool use capabilities have demonstrated\nimpressive performance in web browsing for deep search. While existing\nbenchmarks such as BrowseComp evaluate these browsing abilities, they primarily\nfocus on textual information, overlooking the prevalence of multimodal content.\nTo bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising\n224 challenging, hand-crafted questions specifically designed to assess agents'\nmultimodal retrieval and reasoning capabilities. These questions often\nincorporate images in prompts, and crucial information encountered during the\nsearch and reasoning process may also be embedded within images or videos on\nwebpages. Consequently, methods relying solely on text prove insufficient for\nour benchmark. Additionally, we provide a verified checklist for each question,\nenabling fine-grained analysis of multimodal dependencies and reasoning paths.\nOur comprehensive evaluation of state-of-the-art models on MM-BrowseComp\nreveals that even top models like OpenAI o3 with tools achieve only 29.02\\%\naccuracy, highlighting the suboptimal multimodal capabilities and lack of\nnative multimodal reasoning in current models.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.13186v1",
      "pdf_url": "http://arxiv.org/pdf/2508.13186v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.343,
      "datasets_score": 0.419,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a benchmark for multimodal browsing agents and evaluates their reasoning capabilities, but it does not involve diffusion-based models or iterative refinement processes for logical tasks. There is no mention of adapting diffusion mechanisms to Chain-of-Thought reasoning, making this topic unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and evaluation of MM-BrowseComp, a new benchmark dataset with 224 hand-crafted questions for assessing multimodal browsing agents. It covers dataset creation, curation methodologies, and benchmarking, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "MM-BrowseComp introduces a new benchmark comprising 224 hand-crafted, challenging questions to evaluate AI agents' multimodal web browsing capabilities, addressing the limitations of existing benchmarks like BrowseComp by incorporating images and videos in prompts and webpages. The methodology involves creating multi-hop questions with verified checklists for fine-grained analysis, and key findings from evaluations of state-of-the-art models, such as OpenAI o3 achieving only 29.02% accuracy, reveal significant gaps in multimodal retrieval, reasoning, and the need for native multimodal processing in current AI agents.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark that extends evaluation to multimodal content, significantly advancing the state-of-the-art in assessing AI agents' web browsing capabilities beyond text-only approaches.",
      "impact_score": "High",
      "impact_justification": "This work is likely to influence a wide range of future research in multimodal AI and agent development by providing a rigorous tool to identify and address current limitations, potentially leading to broader improvements in commercial applications like search engines.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by highlighting critical gaps in AI capabilities through a well-designed benchmark, making it essential for researchers in AI, computer vision, and language to stay informed on advancements in multimodal agents.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3b450471b41b1d30090495d9eca0db3f79872290",
      "total_authors": 24,
      "authors_found": 24,
      "highest_h_index": 11,
      "average_h_index": 3.1666666666666665,
      "notable_authors_count": 7,
      "author_h_indexes": [
        {
          "name": "Shilong Li",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2328102146"
        },
        {
          "name": "Xingyuan Bu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2284990102"
        },
        {
          "name": "Wenjie Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376480872"
        },
        {
          "name": "Jiaheng Liu",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2182423032"
        },
        {
          "name": "Jun Dong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376286007"
        },
        {
          "name": "Haoyang He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376332393"
        },
        {
          "name": "Hao Lu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377330268"
        },
        {
          "name": "Haozhe Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376387463"
        },
        {
          "name": "Chenchen Jing",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2357723838"
        },
        {
          "name": "Zhen Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376283024"
        },
        {
          "name": "Chuanhao Li",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2294251099"
        },
        {
          "name": "Jiayi Tian",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2344623736"
        },
        {
          "name": "Chenchen Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2307552623"
        },
        {
          "name": "Tianhao Peng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2367741262"
        },
        {
          "name": "Yancheng He",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2285046736"
        },
        {
          "name": "Jihao Gu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2331000999"
        },
        {
          "name": "Yuanxing Zhang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2279778502"
        },
        {
          "name": "Jian Yang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2321680420"
        },
        {
          "name": "Ge Zhang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2312670376"
        },
        {
          "name": "Wenhao Huang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2325124981"
        },
        {
          "name": "Wangchunshu Zhou",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2284803168"
        },
        {
          "name": "Zhaoxiang Zhang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2322607797"
        },
        {
          "name": "Ruizhe Ding",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376268496"
        },
        {
          "name": "Shilei Wen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376494485"
        }
      ]
    },
    {
      "id": "2508.13187",
      "title": "Combating Homelessness Stigma with LLMs: A New Multi-Modal Dataset for\n  Bias Detection",
      "authors": [
        "Jonathan A. Karr Jr.",
        "Benjamin F. Herbst",
        "Ting Hua",
        "Matthew Hauenstein",
        "Georgina Curto",
        "Nitesh V. Chawla"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Homelessness is a persistent social challenge, impacting millions worldwide.\nOver 770,000 people experienced homelessness in the U.S. in 2024. Social\nstigmatization is a significant barrier to alleviation, shifting public\nperception, and influencing policymaking. Given that online and city council\ndiscourse reflect and influence part of public opinion, it provides valuable\ninsights to identify and track social biases. This research contributes to\nalleviating homelessness by acting on public opinion. It introduces novel\nmethods, building on natural language processing (NLP) and large language\nmodels (LLMs), to identify and measure PEH social bias expressed in digital\nspaces. We present a new, manually-annotated multi-modal dataset compiled from\nReddit, X (formerly Twitter), news articles, and city council meeting minutes\nacross 10 U.S. cities. This unique dataset provides evidence of the typologies\nof homelessness bias described in the literature. In order to scale up and\nautomate the detection of homelessness bias online, we evaluate LLMs as\nclassifiers. We applied both zero-shot and few-shot classification techniques\nto this data. We utilized local LLMs (Llama 3.2 3B Instruct, Qwen 2.5 7B\nInstruct, and Phi4 Instruct Mini) as well as closed-source API models (GPT-4.1,\nGemini 2.5 Pro, and Grok-4). Our findings reveal that although there are\nsignificant inconsistencies in local LLM zero-shot classification, the\nin-context learning classification scores of local LLMs approach the\nclassification scores of closed-source LLMs. Furthermore, LLMs outperform BERT\nwhen averaging across all categories. This work aims to raise awareness about\nthe pervasive bias against PEH, develop new indicators to inform policy, and\nultimately enhance the fairness and ethical application of Generative AI\ntechnologies.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.13187v1",
      "pdf_url": "http://arxiv.org/pdf/2508.13187v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.468,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.379,
      "datasets_score": 0.48,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper evaluates LLMs for bias classification using human-annotated data as a benchmark but does not involve training or fine-tuning models with reinforcement learning based on human feedback. It focuses on classification techniques like zero-shot and few-shot learning, not RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation, annotation, and analysis of a new multi-modal dataset for homelessness bias detection, including its use for benchmarking LLMs and comparing against human annotations, which directly aligns with research on dataset introduction, curation, and evaluation in AI.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the stigmatization of people experiencing homelessness (PEH) by introducing a new multi-modal dataset compiled from sources like Reddit, X, news articles, and city council minutes across 10 U.S. cities, aiming to detect and measure bias using natural language processing and large language models (LLMs). The methodology involves manually annotating the dataset, evaluating various LLMs (including local models like Llama 3.2 and closed-source ones like GPT-4) in zero-shot and few-shot classification tasks, and comparing their performance against BERT and human annotators; key findings indicate that LLMs, particularly with in-context learning, outperform BERT on average and show promise for scaling bias detection, potentially informing policies and enhancing AI ethics.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a new multi-modal dataset and applying LLMs to detect homelessness bias, which is a clever adaptation of existing NLP techniques to a specific social issue, though it does not introduce entirely new architectures or methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in AI for social good and policy-making by providing tools for bias detection, but its applicability is primarily within subfields like computational social science and ethics in AI, limiting broader reach.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to understanding and mitigating social biases using AI, making it essential for researchers in AI ethics and social impact, though it may not be groundbreaking for a general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/798cf40005141dfcd3ff8aba76faa2b6027e3f44",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 6,
      "average_h_index": 1.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Jonathan A. Karr",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376269605"
        },
        {
          "name": "Benjamin F. Herbst",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376269705"
        },
        {
          "name": "Ting Hua",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2366067159"
        },
        {
          "name": "Matthew Hauenstein",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376269607"
        },
        {
          "name": "Georgina Curto",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376269130"
        },
        {
          "name": "Nitesh V. Chawla",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2292582566"
        }
      ]
    },
    {
      "id": "2508.13188",
      "title": "Colon Polyps Detection from Colonoscopy Images Using Deep Learning",
      "authors": [
        "Md Al Amin",
        "Bikash Kumar Paul"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Colon polyps are precursors to colorectal cancer, a leading cause of\ncancer-related mortality worldwide. Early detection is critical for improving\npatient outcomes. This study investigates the application of deep\nlearning-based object detection for early polyp identification using\ncolonoscopy images. We utilize the Kvasir-SEG dataset, applying extensive data\naugmentation and splitting the data into training (80\\%), validation (20\\% of\ntraining), and testing (20\\%) sets. Three variants of the YOLOv5 architecture\n(YOLOv5s, YOLOv5m, YOLOv5l) are evaluated. Experimental results show that\nYOLOv5l outperforms the other variants, achieving a mean average precision\n(mAP) of 85.1\\%, with the highest average Intersection over Union (IoU) of\n0.86. These findings demonstrate that YOLOv5l provides superior detection\nperformance for colon polyp localization, offering a promising tool for\nenhancing colorectal cancer screening accuracy.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.13188v1",
      "pdf_url": "http://arxiv.org/pdf/2508.13188v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.243,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.268,
      "distributed_training_score": 0.287,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.14090",
      "title": "DLLMQuant: Quantizing Diffusion-based Large Language Models",
      "authors": [
        "Chen Xu",
        "Dawei Yang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Diffusion-based large language models (DLLMs) have shown promise for\nnon-autoregressive text generation, but their deployment is constrained by\nlarge model sizes and heavy computational costs. Post-training quantization\n(PTQ), a widely used method for compressing and accelerating Large Language\nModels (LLMs), suffers from severe accuracy degradation and reduced\ngeneralization performance when directly applied to DLLMs (e.g., AWQ suffers a\n16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key\nmechanisms - dynamic masking, iterative generation, bidirectional attention -\nclash with quantization. We identify three core issues: 1) Iterative generation\nand dynamic masking ratios lead to distinct token distributions across decoding\nsteps, which are not adequately captured by existing PTQ calibration methods;\n2) Quantization errors are accumulated and amplified progressively during\niteration in DLLMs, causing quantized models to perform worse as decoding steps\nprogress; 3) Unmasked tokens stabilize while masked remain probabilistic,\nmaking overall feature distribution incompatible with existing PTQ methods. To\naddress these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs,\nwhich incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling\n(TMAS), a calibration method that accounts for both time and mask factors, with\nthe capacity to capture distributions across timesteps. 2) Interaction-Aware\nActivation Quantization (IA-AQ), which utilizes bidirectional attention's\ninteraction signals to dynamically allocate quantization resources. 3)\nCertainty-Guided Quantization (CGQ), which integrates mask status and token\nscores as key weighting criteria into error compensation, making weight\nquantization more suitable for DLLMs. Experiments show that DLLMQuant achieves\nsignificant performance gains while enhancing efficiency.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.14090v2",
      "pdf_url": "http://arxiv.org/pdf/2508.14090v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.542,
      "distributed_training_score": 0.445,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses diffusion-based large language models (DLLMs) that use iterative generation and diffusion processes for text generation, which could relate to iterative refinement in reasoning tasks. However, the main contribution focuses on quantization techniques for efficiency, not on adapting diffusion for multi-step logical reasoning or holistic correction of Chain-of-Thought processes. Thus, it touches on diffusion mechanisms but does not directly address or contribute to diffusion-based reasoning as defined.",
      "distributed_training_justification": "The paper's primary focus is on post-training quantization for DLLMs to reduce computational costs, with no mention of distributed training, parallel computing, multi-node setups, or strategies for partitioning data, architecture, or computation across processors. It does not address acceleration of model training in distributed environments.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.14091",
      "title": "Logical Expressivity and Explanations for Monotonic GNNs with Scoring\n  Functions",
      "authors": [
        "Matthew Morris",
        "David J. Tena Cucala",
        "Bernardo Cuenca Grau"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "Graph neural networks (GNNs) are often used for the task of link prediction:\npredicting missing binary facts in knowledge graphs (KGs). To address the lack\nof explainability of GNNs on KGs, recent works extract Datalog rules from GNNs\nwith provable correspondence guarantees. The extracted rules can be used to\nexplain the GNN's predictions; furthermore, they can help characterise the\nexpressive power of various GNN models. However, these works address only a\nform of link prediction based on a restricted, low-expressivity graph\nencoding/decoding method. In this paper, we consider a more general and popular\napproach for link prediction where a scoring function is used to decode the GNN\noutput into fact predictions. We show how GNNs and scoring functions can be\nadapted to be monotonic, use the monotonicity to extract sound rules for\nexplaining predictions, and leverage existing results about the kind of rules\nthat scoring functions can capture. We also define procedures for obtaining\nequivalent Datalog programs for certain classes of monotonic GNNs with scoring\nfunctions. Our experiments show that, on link prediction benchmarks, monotonic\nGNNs and scoring functions perform well in practice and yield many sound rules.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.14091v1",
      "pdf_url": "http://arxiv.org/pdf/2508.14091v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.308,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves Graph Neural Networks (GNNs) for link prediction in knowledge graphs, focusing on logical expressivity, monotonicity, and extracting Datalog rules for explainability. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning through a chain-of-thought mechanism as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.15802",
      "title": "MAC: A Live Benchmark for Multimodal Large Language Models in Scientific\n  Understanding",
      "authors": [
        "Mohan Jiang",
        "Jin Gao",
        "Jiahao Zhan",
        "Dequan Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As multimodal large language models (MLLMs) grow increasingly capable, fixed\nbenchmarks are gradually losing their effectiveness in evaluating high-level\nscientific understanding. In this paper, we introduce the Multimodal Academic\nCover benchmark (MAC), a live benchmark that could continuously evolve with\nscientific advancement and model progress. MAC leverages over 25,000 image-text\npairs sourced from issues of top-tier scientific journals such as Nature,\nScience, and Cell, challenging MLLMs to reason across abstract visual and\ntextual scientific content. Experiments on our most recent yearly snapshot,\nMAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities,\ntheir cross-modal scientific reasoning remains limited. To bridge this gap, we\npropose DAD, a lightweight inference-time approach that enhances MLLMs by\nextending MLLM visual features with language space reasoning, achieving\nperformance improvements of up to 11%. Finally, we highlight the live nature of\nMAC through experiments on updating journal covers and models for curation,\nillustrating its potential to remain aligned with the frontier of human\nknowledge. We release our benchmark at\nhttps://github.com/mhjiang0408/MAC_Bench.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.15802v1",
      "pdf_url": "http://arxiv.org/pdf/2508.15802v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.36,
      "datasets_score": 0.431,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a live benchmark for MLLMs and a cross-modal reasoning approach (DAD), which involves extracting visual features and using language models for reasoning. It does not mention or utilize diffusion-based methods, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for multi-step logical correction. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and introduction of the MAC benchmark, which involves sourcing, curating, and evaluating a dataset of over 25,000 image-text pairs from scientific journals. It also includes analysis of dataset evolution, benchmark construction methodologies, and performance evaluations on MLLMs, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the MAC benchmark, a dynamic and evolving evaluation tool for multimodal large language models (MLLMs) focused on scientific understanding, utilizing over 25,000 image-text pairs from prestigious journals like Nature, Science, and Cell. It employs a four-way classification task to test MLLMs' abilities in matching cover images to stories or vice versa, reveals limitations in their cross-modal reasoning through experiments on the MAC-2025 snapshot, and proposes the DAD method—a lightweight inference-time approach that enhances MLLM performance by integrating visual features with language-based reasoning, achieving up to 11% improvements while highlighting the benchmark's adaptability to new scientific content.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new live benchmark that evolves with scientific advancements, addressing the limitations of static benchmarks, and proposes a novel inference-time technique (DAD) to enhance MLLM reasoning, significantly advancing the state-of-the-art in evaluating multimodal AI for scientific tasks.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research in multimodal AI by providing a scalable, adaptive benchmark for scientific understanding, likely leading to improved model development and broader applications in autonomous scientific research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative benchmarking and enhancement methods that are valuable for AI researchers, though it may not be essential for those outside the specific subfield of multimodal language models.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9d86296956d9e6c2cde61e582f9617b5360ac0cb",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 4,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Mohan Jiang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jin Gao",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2175310417"
        },
        {
          "name": "Jiahao Zhan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376504488"
        },
        {
          "name": "Dequan Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2327051777"
        }
      ]
    },
    {
      "id": "2508.15804",
      "title": "ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks",
      "authors": [
        "Minghao Li",
        "Ying Zeng",
        "Zhihao Cheng",
        "Cong Ma",
        "Kai Jia"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The advent of Deep Research agents has substantially reduced the time\nrequired for conducting extensive research tasks. However, these tasks\ninherently demand rigorous standards of factual accuracy and comprehensiveness,\nnecessitating thorough evaluation before widespread adoption. In this paper, we\npropose ReportBench, a systematic benchmark designed to evaluate the content\nquality of research reports generated by large language models (LLMs). Our\nevaluation focuses on two critical dimensions: (1) the quality and relevance of\ncited literature, and (2) the faithfulness and veracity of the statements\nwithin the generated reports. ReportBench leverages high-quality published\nsurvey papers available on arXiv as gold-standard references, from which we\napply reverse prompt engineering to derive domain-specific prompts and\nestablish a comprehensive evaluation corpus. Furthermore, we develop an\nagent-based automated framework within ReportBench that systematically analyzes\ngenerated reports by extracting citations and statements, checking the\nfaithfulness of cited content against original sources, and validating\nnon-cited claims using web-based resources. Empirical evaluations demonstrate\nthat commercial Deep Research agents such as those developed by OpenAI and\nGoogle consistently generate more comprehensive and reliable reports than\nstandalone LLMs augmented with search or browsing tools. However, there remains\nsubstantial room for improvement in terms of the breadth and depth of research\ncoverage, as well as factual consistency. The complete code and data will be\nreleased at the following link: https://github.com/ByteDance-BandAI/ReportBench",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.15804v1",
      "pdf_url": "http://arxiv.org/pdf/2508.15804v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.373,
      "datasets_score": 0.467,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on benchmarking Deep Research agents for report quality, with no mention of training AI models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper does not involve machine learning approaches for generating training labels from noisy sources; it centers on evaluating the content of AI-generated research reports.",
      "diffusion_reasoning_justification": "The paper does not discuss diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes; it evaluates general research report generation by LLMs.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is ReportBench, which involves creating, analyzing, and benchmarking datasets derived from arXiv survey papers for evaluating AI research agents, directly aligning with dataset creation and evaluation in machine learning applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces ReportBench, a benchmark designed to evaluate the quality of research reports generated by large language models (LLMs) and Deep Research agents, focusing on the relevance of cited literature and the factual accuracy of statements. By utilizing published arXiv survey papers as gold standards to create domain-specific prompts and an automated framework for verification—through reference overlap checks and web-based fact validation—the authors demonstrate that commercial agents like those from OpenAI and Google produce more comprehensive and reliable reports than standalone LLMs, though significant improvements are needed in coverage and consistency.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing resources like arXiv surveys with automated evaluation techniques to create a new benchmark for assessing research agents, though it does not introduce a entirely novel problem or technique. This clever integration addresses a known gap in evaluating LLM-generated reports but builds on prior ideas rather than pioneering a completely new architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI and computational language for standardizing evaluations of research agents, potentially influencing tool development and model improvements. However, its applicability may remain niche, primarily affecting LLM benchmarking rather than broader commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a valuable and practical framework for evaluating AI-generated research reports, making it essential for researchers in AI and language models to understand current evaluation standards. While not groundbreaking, its contributions warrant attention for those working on LLM reliability and benchmarking.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/51c1ac9532bf4991f1cfc36bd72055d11c84b160",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Minghao Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376510303"
        },
        {
          "name": "Ying Zeng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376523130"
        },
        {
          "name": "Zhihao Cheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376520588"
        },
        {
          "name": "Cong Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376521395"
        },
        {
          "name": "Kai Jia",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2371990244"
        }
      ]
    },
    {
      "id": "2508.15805",
      "title": "ALAS: Autonomous Learning Agent for Self-Updating Language Models",
      "authors": [
        "Dhruv Atreja"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large language models (LLMs) often have a fixed knowledge cutoff, limiting\ntheir accuracy on emerging information. We present ALAS (Autonomous Learning\nAgent System), a modular pipeline that continuously updates an LLM's knowledge\nwith minimal human intervention. ALAS autonomously generates a learning\ncurriculum for a target domain, retrieves up-to-date information from the web\n(with citations), distills this into question-answer training data, and\nfine-tunes the model through supervised fine-tuning (SFT) and direct preference\noptimization (DPO). It iteratively evaluates performance and revises the\ncurriculum, enabling long-term continual learning. We demonstrate ALAS's\nability to self-improve a model on rapidly evolving domains (e.g., new Python\nreleases, latest security CVEs, academic trends), significantly boosting\npost-cutoff question answering accuracy (from 15% to 90% on average) without\nmanual dataset curation. The system emphasizes modularity and reproducibility:\neach component (planning, retrieval, distillation, memory, fine-tuning) is\ninterchangeable and built on standard APIs. We discuss comparative baselines\n(e.g., retrieval-augmented generation vs. fine-tuning) and show that ALAS\nachieves 90% accuracy on knowledge-updated queries with minimal engineering\noverhead. Finally, we outline limitations (cost, dependency on source quality)\nand future directions for autonomous lifelong learning in LLMs.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.15805v1",
      "pdf_url": "http://arxiv.org/pdf/2508.15805v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.462,
      "weak_supervision_score": 0.437,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.399,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Direct Preference Optimization (DPO) for fine-tuning, which is related to preference-based alignment methods often associated with reinforcement learning. However, ALAS operates autonomously with minimal human intervention, and there is no mention of human feedback or a reward model trained on human-ranked data, which is a core requirement for RLHF. Thus, while DPO shares conceptual similarities, the system does not qualify as RLHF.",
      "weak_supervision_justification": "The paper's ALAS system programmatically generates training labels by autonomously retrieving web data, distilling it into Q&A pairs, and using these for fine-tuning, without relying on hand-labeled data. This directly aligns with weak supervision, as it leverages noisy or imprecise sources (e.g., web information) to create large quantities of training examples programmatically.",
      "diffusion_reasoning_justification": "The paper focuses on autonomous learning through retrieval, distillation, and fine-tuning, with no mention of diffusion models, iterative refinement processes for reasoning, or treating Chain-of-Thought as a holistically corrected entity. There are no components involving multi-step logical reasoning via diffusion mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces ALAS, an autonomous system designed to address the knowledge cutoff problem in large language models (LLMs) by enabling continuous self-updating through a modular pipeline that includes generating a learning curriculum, retrieving and distilling web-based information into question-answer pairs, fine-tuning via supervised fine-tuning (SFT) and direct preference optimization (DPO), and iterative evaluation. Key findings show that ALAS significantly improves accuracy on emerging topics, such as new Python releases and security vulnerabilities, boosting performance from around 15% to 90% without manual data curation, while emphasizing its modularity, reproducibility, and use of existing APIs.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like information retrieval, data distillation, and fine-tuning into an autonomous pipeline for continuous learning, offering a notable improvement over prior methods such as RAG or SEAL by minimizing human intervention. However, it does not introduce entirely new problems or architectures, relying instead on integrating established ideas in a practical way.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI and machine learning, particularly for developing adaptive LLMs, as it provides a practical framework for autonomous updating. Nonetheless, its influence may be limited to specific applications and not broadly transformative across all areas of research or industry.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality, practical contribution to addressing LLM knowledge limitations, making it essential for researchers interested in autonomous learning and model updating in AI. It represents a strong advancement that warrants awareness, though it may not be groundbreaking for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/94302b496240f965160c48511aa15373f2fde842",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Dhruv Atreja",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376509416"
        }
      ]
    },
    {
      "id": "2508.15806",
      "title": "SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need\n  for Robust KV Cache Compression",
      "authors": [
        "Mengjie Li",
        "William J. Song"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The increasing input sequence length in Large Language Models (LLMs) puts\nsignificant pressure on key-value (KV) cache storage, making efficient\ninference challenging. Explicitly distinguishing attention behavior into our\nself-defined surface memorization and logic construction reveals essential\nroles in long-context reasoning. We observe that an individual attention head\ncan display various behaviors, with nearly 98.5% effectively ignoring\ncompletely irrelevant information. The remaining 1.5% behaves as logic\nconstruction, and 0.5% behaves as surface memorization. Based on layer- and\nhead-wise integration, we propose a novel two-stage SurfaceLogicKV method to\nutilize these attention behaviors for KV Cache compression. As a result, it\nachieves improved compressing robustness while maintaining competitive\nperformance across various tasks and long sequences compared to baselines or\neven FullKV in some specific situations",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.15806v1",
      "pdf_url": "http://arxiv.org/pdf/2508.15806v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.364,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for KV cache compression in LLMs by analyzing attention behaviors (surface memorization and logic construction), with no reference to diffusion models, iterative refinement processes, or treating reasoning as a holistically corrected entity over multiple steps. It focuses solely on attention mechanisms and efficiency in Transformers, lacking any components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.15807",
      "title": "KL-based self-distillation for large language models",
      "authors": [
        "Max Rehman Linder"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large pre-trained language models often struggle to incorporate new\ndomain-specific terminology when fine-tuned on small, specialized corpora. In\nthis work, we address the challenge of vocabulary expansion in frozen LLMs by\nintroducing a mathematically grounded method for knowledge distillation via KL\ndivergence, even when the original and extended models use different\ntokenizations. This allows the student model to inherit distributional\nknowledge from the teacher despite differing vocabularies. We compare our\nKL-based distillation approach to conventional cross-entropy training,\nevaluating both methods across multiple strategies for initializing new token\nembeddings. After embedding initialization, models are further fine-tuned to\nintegrate the new vocabulary. Each trained model is benchmarked on\napproximately 2000 code-generation tasks, where our approach achieves the best\nperformance across the board. Finally, through mechanistic interpretability, we\nanalyze how models learn representations for the new tokens, providing an\nexplanation for the observed gains and offering insight into the structure of\nembedding space during vocabulary expansion.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.15807v1",
      "pdf_url": "http://arxiv.org/pdf/2508.15807v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.423,
      "diffusion_reasoning_score": 0.489,
      "distributed_training_score": 0.435,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on KL-based knowledge distillation for vocabulary expansion in LLMs, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper discusses fine-tuning LLMs on small, specialized corpora but does not involve programmatically generating labels from noisy sources; it centers on knowledge distillation and vocabulary expansion instead.",
      "diffusion_reasoning_justification": "The paper addresses knowledge distillation via KL divergence for LLMs, with no reference to diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "The paper examines vocabulary expansion and fine-tuning methods for LLMs but does not discuss distributed training, parallel computing, or partitioning computations across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.15808",
      "title": "Uplifted Attackers, Human Defenders: The Cyber Offense-Defense Balance\n  for Trailing-Edge Organizations",
      "authors": [
        "Benjamin Murphy",
        "Twm Stone"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Advances in AI are widely understood to have implications for cybersecurity.\nArticles have emphasized the effect of AI on the cyber offense-defense balance,\nand commentators can be found arguing either that cyber will privilege\nattackers or defenders. For defenders, arguments are often made that AI will\nenable solutions like formal verification of all software--and for some\nwell-equipped companies, this may be true. This conversation, however, does not\nmatch the reality for most companies. \"Trailing-edge organizations,\" as we term\nthem, rely heavily on legacy software, poorly staff security roles, and\nstruggle to implement best practices like rapid deployment of security patches.\nThese decisions may be the result of corporate inertia, but may also be the\nresult of a seemingly-rational calculation that attackers may not bother\ntargeting a firm due to lack of economic incentives, and as a result,\nunderinvestment in defense will not be punished.\n  This approach to security may have been sufficient prior to the development\nof AI systems, but it is unlikely to remain viable in the near future. We argue\nthat continuing improvements in AI's capabilities poses additional risks on two\nfronts: First, increased usage of AI will alter the economics of the marginal\ncyberattack and expose these trailing-edge organizations to more attackers,\nmore frequently. Second, AI's advances will enable attackers to develop\nexploits and launch attacks earlier than they can today--meaning that it is\ninsufficient for these companies to attain parity with today's leading\ndefenders, but must instead aim for faster remediation timelines and more\nresilient software. The situation today portends a dramatically increased\nnumber of attacks in the near future. Moving forward, we offer a range of\nsolutions for both organizations and governments to improve the defensive\nposture of firms which lag behind their peers today.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.15808v1",
      "pdf_url": "http://arxiv.org/pdf/2508.15808v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.331,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.15809",
      "title": "Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table\n  Understanding via Multi-Agent Collaboration",
      "authors": [
        "Songyuan Sui",
        "Hongyi Liu",
        "Serena Liu",
        "Li Li",
        "Soo-Hyun Choi",
        "Rui Chen",
        "Xia Hu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.DB (Databases)"
      ],
      "abstract": "Table understanding requires structured, multi-step reasoning. Large Language\nModels (LLMs) struggle with it due to the structural complexity of tabular\ndata. Recently, multi-agent frameworks for SQL generation have shown promise in\ntackling the challenges of understanding tabular data, but existing approaches\noften suffer from limitations such as the inability to comprehend table\nstructure for reliable SQL generation, error propagation that results in\ninvalid queries, and over-reliance on execution correctness. To address these\nissues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for\nSQL-aided table understanding. CoQ adopts natural-language-style\nrepresentations of table schemas to abstract away structural noise and enhance\nunderstanding. It employs a clause-by-clause SQL generation strategy to improve\nquery quality and introduces a hybrid reasoning division that separates\nSQL-based mechanical reasoning from LLM-based logical inference, thereby\nreducing reliance on execution outcomes. Experiments with four models (both\nclosed- and open-source) across five widely used benchmarks show that\nChain-of-Query significantly improves accuracy from 61.11% to 74.77% and\nreduces the invalid SQL rate from 9.48% to 3.34%, demonstrating its superior\neffectiveness in table understanding. The code is available at\nhttps://github.com/SongyuanSui/ChainofQuery.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.15809v1",
      "pdf_url": "http://arxiv.org/pdf/2508.15809v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.364,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Chain-of-Query, a multi-agent framework for SQL-aided table understanding that uses incremental SQL generation and hybrid reasoning. While it involves multi-step processes like clause-by-clause query construction, it does not adapt the iterative refinement process of diffusion models or treat reasoning paths holistically as described in the topic. There is no mention of diffusion-based techniques, making the paper unrelated to this specific area.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.16620",
      "title": "STRelay: A Universal Spatio-Temporal Relaying Framework for Location\n  Prediction with Future Spatiotemporal Contexts",
      "authors": [
        "Bangchao Deng",
        "Lianhua Ji",
        "Chunhua Chen",
        "Xin Jing",
        "Ling Ding",
        "Bingqing QU",
        "Pengyang Wang",
        "Dingqi Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Next location prediction is a critical task in human mobility modeling,\nenabling applications like travel planning and urban mobility management.\nExisting methods mainly rely on historical spatiotemporal trajectory data to\ntrain sequence models that directly forecast future locations. However, they\noften overlook the importance of the future spatiotemporal contexts, which are\nhighly informative for the future locations. For example, knowing how much time\nand distance a user will travel could serve as a critical clue for predicting\nthe user's next location. Against this background, we propose \\textbf{STRelay},\na universal \\textbf{\\underline{S}}patio\\textbf{\\underline{T}}emporal\n\\textbf{\\underline{Relay}}ing framework explicitly modeling the future\nspatiotemporal context given a human trajectory, to boost the performance of\ndifferent location prediction models. Specifically, STRelay models future\nspatiotemporal contexts in a relaying manner, which is subsequently integrated\nwith the encoded historical representation from a base location prediction\nmodel, enabling multi-task learning by simultaneously predicting the next time\ninterval, next moving distance interval, and finally the next location. We\nevaluate STRelay integrated with four state-of-the-art location prediction base\nmodels on four real-world trajectory datasets. Results demonstrate that STRelay\nconsistently improves prediction performance across all cases by\n3.19\\%-11.56\\%. Additionally, we find that the future spatiotemporal contexts\nare particularly helpful for entertainment-related locations and also for user\ngroups who prefer traveling longer distances. The performance gain on such\nnon-daily-routine activities, which often suffer from higher uncertainty, is\nindeed complementary to the base location prediction models that often excel at\nmodeling regular daily routine patterns.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.16620v1",
      "pdf_url": "http://arxiv.org/pdf/2508.16620v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.352,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a spatiotemporal relaying framework for location prediction, emphasizing multi-task learning and integration of future contexts in mobility modeling. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.16621",
      "title": "3D latent diffusion models for parameterizing and history matching\n  multiscenario facies systems",
      "authors": [
        "Guido Di Federico",
        "Louis J. Durlofsky"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Geological parameterization procedures entail the mapping of a\nhigh-dimensional geomodel to a low-dimensional latent variable. These\nparameterizations can be very useful for history matching because the number of\nvariables to be calibrated is greatly reduced, and the mapping can be\nconstructed such that geological realism is automatically preserved. In this\nwork, a parameterization method based on generative latent diffusion models\n(LDMs) is developed for 3D channel-levee-mud systems. Geomodels with variable\nscenario parameters, specifically mud fraction, channel orientation, and\nchannel width, are considered. A perceptual loss term is included during\ntraining to improve geological realism. For any set of scenario parameters, an\n(essentially) infinite number of realizations can be generated, so our LDM\nparameterizes over a very wide model space. New realizations constructed using\nthe LDM procedure are shown to closely resemble reference geomodels, both\nvisually and in terms of one- and two-point spatial statistics. Flow response\ndistributions, for a specified set of injection and production wells, are also\nshown to be in close agreement between the two sets of models. The\nparameterization method is applied for ensemble-based history matching, with\nmodel updates performed in the LDM latent space, for cases involving geological\nscenario uncertainty. For three synthetic true models corresponding to\ndifferent geological scenarios, we observe clear uncertainty reduction in both\nproduction forecasts and geological scenario parameters. The overall method is\nadditionally shown to provide posterior geomodels consistent with the synthetic\ntrue model in each case.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.16621v1",
      "pdf_url": "http://arxiv.org/pdf/2508.16621v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.363,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using latent diffusion models for generating and parameterizing 3D geological models in subsurface flow applications, involving iterative denoising for spatial data generation. However, it does not adapt diffusion processes for solving complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as a single entity. Thus, there is no component of multi-step logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.16623",
      "title": "A Retrieval Augmented Spatio-Temporal Framework for Traffic Prediction",
      "authors": [
        "Weilin Ruan",
        "Xilin Dang",
        "Ziyu Zhou",
        "Sisuo Lyu",
        "Yuxuan Liang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Traffic prediction is a cornerstone of modern intelligent transportation\nsystems and a critical task in spatio-temporal forecasting. Although advanced\nSpatio-temporal Graph Neural Networks (STGNNs) and pre-trained models have\nachieved significant progress in traffic prediction, two key challenges remain:\n(i) limited contextual capacity when modeling complex spatio-temporal\ndependencies, and (ii) low predictability at fine-grained spatio-temporal\npoints due to heterogeneous patterns. Inspired by Retrieval-Augmented\nGeneration (RAG), we propose RAST, a universal framework that integrates\nretrieval-augmented mechanisms with spatio-temporal modeling to address these\nchallenges. Our framework consists of three key designs: 1) Decoupled Encoder\nand Query Generator to capture decoupled spatial and temporal features and\nconstruct a fusion query via residual fusion; 2) Spatio-temporal Retrieval\nStore and Retrievers to maintain and retrieve vectorized fine-grained patterns;\nand 3) Universal Backbone Predictor that flexibly accommodates pre-trained\nSTGNNs or simple MLP predictors. Extensive experiments on six real-world\ntraffic networks, including large-scale datasets, demonstrate that RAST\nachieves superior performance while maintaining computational efficiency.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.16623v1",
      "pdf_url": "http://arxiv.org/pdf/2508.16623v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.353,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.16624",
      "title": "The GPT-4o Shock Emotional Attachment to AI Models and Its Impact on\n  Regulatory Acceptance: A Cross-Cultural Analysis of the Immediate Transition\n  from GPT-4o to GPT-5",
      "authors": [
        "Hiroki Naito"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In August 2025, a major AI company's immediate, mandatory transition from its\nprevious to its next-generation model triggered widespread public reactions. I\ncollected 150 posts in Japanese and English from multiple social media\nplatforms and video-sharing services between August 8-9, 2025, and\nqualitatively analyzed expressions of emotional attachment and resistance.\nUsers often described GPT-4o as a trusted partner or AI boyfriend, suggesting\nperson-like bonds. Japanese posts were dominated by loss-oriented narratives,\nwhereas English posts included more anger, meta-level critique, and memes.A\npreliminary quantitative check showed a statistically significant difference in\nattachment coding between Japanese and English posts, with substantially higher\nattachment observed in the Japanese data. The findings suggest that for\nattachment-heavy models, even safety-oriented changes can face rapid,\nlarge-scale resistance that narrows the practical window for behavioral\ncontrol. If future AI robots capable of inducing emotional bonds become\nwidespread in the physical world, such attachment could surpass the ability to\nenforce regulation at an even earlier stage than in digital settings. Policy\noptions include gradual transitions, parallel availability, and proactive\nmeasurement of attachment thresholds and points of no return to prevent\nemotional dynamics from outpacing effective governance.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.16624v1",
      "pdf_url": "http://arxiv.org/pdf/2508.16624v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.457,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.305,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an analysis of emotional attachments and user reactions to AI model transitions, focusing on social media responses and regulatory implications. It does not discuss or involve reinforcement learning techniques, human feedback for training reward models, or any aspects of aligning AI with human preferences through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.16625",
      "title": "Data and Context Matter: Towards Generalizing AI-based Software\n  Vulnerability Detection",
      "authors": [
        "Rijha Safdar",
        "Danyail Mateen",
        "Syed Taha Ali",
        "M. Umer Ashfaq",
        "Wajahat Hussain"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "The performance of AI-based software vulnerability detection systems is often\nlimited by their poor generalization to unknown codebases. In this research, we\nexplore the impact of data quality and model architecture on the\ngeneralizability of vulnerability detection systems. By generalization we mean\nability of high vulnerability detection performance across different C/C++\nsoftware projects not seen during training. Through a series of experiments, we\ndemonstrate that improvements in dataset diversity and quality substantially\nenhance detection performance. Additionally, we compare multiple encoder-only\nand decoder-only models, finding that encoder based models outperform in terms\nof accuracy and generalization. Our model achieves 6.8% improvement in recall\non the benchmark BigVul[1] dataset, also outperforming on unseen projects,\nhence showing enhanced generalizability. These results highlight the role of\ndata quality and model selection in the development of robust vulnerability\ndetection systems. Our findings suggest a direction for future systems having\nhigh cross-project effectiveness.",
      "published_date": "2025-08-14",
      "arxiv_url": "http://arxiv.org/abs/2508.16625v1",
      "pdf_url": "http://arxiv.org/pdf/2508.16625v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.432,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.391,
      "datasets_score": 0.428,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on dataset enhancement and model architecture for vulnerability detection, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper involves programmatically cleaning and augmenting datasets by removing mislabeled samples and using a scraping pipeline, which aligns with weak supervision by handling noisy labels, though it does not primarily rely on high-level or imprecise sources for label generation.",
      "diffusion_reasoning_justification": "The paper evaluates encoder and decoder models for vulnerability detection but does not incorporate diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's core contributions include creating, curating, and evaluating datasets by improving quality, diversity, and augmentation, directly addressing dataset analysis and its impact on AI performance.",
      "llm_score_status": "completed",
      "summary": "This paper examines the factors influencing the generalization of AI-based software vulnerability detection systems, specifically focusing on data quality and model architecture to achieve high performance on unseen C/C++ codebases. Through experiments involving dataset enhancement via cleaning, augmentation, and the incorporation of hard negative samples, along with evaluations of encoder-only and decoder-only models, the authors demonstrate that improved data diversity and encoder-based models like UniXcoder significantly enhance detection accuracy, resulting in a 6.8% improvement in recall on the BigVul benchmark and better results on unseen projects.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining data enhancement techniques with strategic model selection to address generalization issues in vulnerability detection, offering a clever adaptation of existing ideas rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI for software security, as it provides practical insights into improving model generalizability, potentially influencing the development of more robust vulnerability detection tools.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers high-quality empirical contributions and actionable strategies for enhancing AI-based vulnerability detection, making it a valuable resource for researchers and practitioners in software engineering and AI security.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b9ae7cd4497ff3729e78e4da659b466cf01fc4f0",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 1,
      "average_h_index": 0.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Rijha Safdar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2248570355"
        },
        {
          "name": "Danyail Mateen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2326495682"
        },
        {
          "name": "Syed Taha",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376542703"
        },
        {
          "name": "Wajahat Ali",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376543273"
        },
        {
          "name": "Hussain M.Umer",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376542837"
        },
        {
          "name": "Ashfaq",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376542397"
        }
      ]
    }
  ],
  "total_papers": 238,
  "date": "2025-08-14"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
