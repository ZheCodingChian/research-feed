<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 11 September 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 11 September 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 11 September 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2509.09064",
      "title": "Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D\n  Multimodal Large Language Models",
      "authors": [
        "Qiuhui Chen",
        "Xuancheng Yao",
        "Huping Ye",
        "Yi Hong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Understanding 3D medical image volumes is critical in the medical field, yet\nexisting 3D medical convolution and transformer-based self-supervised learning\n(SSL) methods often lack deep semantic comprehension. Recent advancements in\nmultimodal large language models (MLLMs) provide a promising approach to\nenhance image understanding through text descriptions. To leverage these 2D\nMLLMs for improved 3D medical image understanding, we propose Med3DInsight, a\nnovel pretraining framework that integrates 3D image encoders with 2D MLLMs via\na specially designed plane-slice-aware transformer module. Additionally, our\nmodel employs a partial optimal transport based alignment, demonstrating\ngreater tolerance to noise introduced by potential noises in LLM-generated\ncontent. Med3DInsight introduces a new paradigm for scalable multimodal 3D\nmedical representation learning without requiring human annotations. Extensive\nexperiments demonstrate our state-of-the-art performance on two downstream\ntasks, i.e., segmentation and classification, across various public datasets\nwith CT and MRI modalities, outperforming current SSL methods. Med3DInsight can\nbe seamlessly integrated into existing 3D medical image understanding networks,\npotentially enhancing their performance. Our source code, generated datasets,\nand pre-trained models will be available at\nhttps://github.com/Qybc/Med3DInsight.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09064v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09064v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.388,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing 3D medical image understanding using pretraining with 2D multimodal large language models, transformers, and alignment techniques like Partial Optimal Transport. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. The main contributions are in medical image representation learning, not in adapting diffusion for reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09066",
      "title": "Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations\n  on Cold-Start Users",
      "authors": [
        "Haowei Yang",
        "Yushang Zhao",
        "Sitao Min",
        "Bo Su",
        "Chao Yao",
        "Wei Xu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The cold-start user issue further compromises the effectiveness of\nrecommender systems in limiting access to the historical behavioral\ninformation. It is an effective pipeline to optimize instructional prompts on a\nfew-shot large language model (LLM) used in recommender tasks. We introduce a\ncontext-conditioned prompt formulation method P(u,\\ Ds)\\ \\rightarrow\\\nR\\widehat, where u is a cold-start user profile, Ds is a curated support set,\nand R\\widehat is the predicted ranked list of items. Based on systematic\nexperimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,\nGPT-4), we provide empirical evidence that optimal exemplar injection and\ninstruction structuring can significantly improve the precision@k and NDCG\nscores of such models in low-data settings. The pipeline uses token-level\nalignments and embedding space regularization with a greater semantic fidelity.\nOur findings not only show that timely composition is not merely syntactic but\nalso functional as it is in direct control of attention scales and decoder\nconduct through inference. This paper shows that prompt-based adaptation may be\nconsidered one of the ways to address cold-start recommendation issues in\nLLM-based pipelines.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09066v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09066v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.457,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.316,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is on optimizing instructional prompts for few-shot LLMs in recommender systems for cold-start users, focusing on prompt formulation, exemplar injection, and performance metrics like precision@k and NDCG. It does not involve human feedback, training a reward model, or using reinforcement learning to fine-tune models, which are essential elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09067",
      "title": "Improvement of Human-Object Interaction Action Recognition Using Scene\n  Information and Multi-Task Learning Approach",
      "authors": [
        "Hesham M. Shehata",
        "Mohammad Abdolrahmani"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent graph convolutional neural networks (GCNs) have shown high performance\nin the field of human action recognition by using human skeleton poses.\nHowever, it fails to detect human-object interaction cases successfully due to\nthe lack of effective representation of the scene information and appropriate\nlearning architectures. In this context, we propose a methodology to utilize\nhuman action recognition performance by considering fixed object information in\nthe environment and following a multi-task learning approach. In order to\nevaluate the proposed method, we collected real data from public environments\nand prepared our data set, which includes interaction classes of hands-on fixed\nobjects (e.g., ATM ticketing machines, check-in/out machines, etc.) and\nnon-interaction classes of walking and standing. The multi-task learning\napproach, along with interaction area information, succeeds in recognizing the\nstudied interaction and non-interaction actions with an accuracy of 99.25%,\noutperforming the accuracy of the base model using only human skeleton poses by\n2.75%.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09067v3",
      "pdf_url": "http://arxiv.org/pdf/2509.09067v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.322,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09070",
      "title": "STRIDE: Subset-Free Functional Decomposition for XAI in Tabular Settings",
      "authors": [
        "Chaeyun Ko"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Most explainable AI (XAI) frameworks are limited in their expressiveness,\nsummarizing complex feature effects as single scalar values \\phi_i. This\napproach answers \"what\" features are important but fails to reveal \"how\" they\ninteract. Furthermore, methods that attempt to capture interactions, like those\nbased on Shapley values, often face an exponential computational cost. We\npresent STRIDE, a scalable framework that addresses both limitations by\nreframing explanation as a subset-enumeration-free, orthogonal \"functional\ndecomposition\" in a Reproducing Kernel Hilbert Space (RKHS). In the tabular\nsetups we study, STRIDE analytically computes functional components f_S(x_S)\nvia a recursive kernel-centering procedure. The approach is model-agnostic and\ntheoretically grounded with results on orthogonality and L^2 convergence. In\ntabular benchmarks (10 datasets, median over 10 seeds), STRIDE attains a 3.0\ntimes median speedup over TreeSHAP and a mean R^2=0.93 for reconstruction. We\nalso introduce \"component surgery\", a diagnostic that isolates a learned\ninteraction and quantifies its contribution; on California Housing, removing a\nsingle interaction reduces test R^2 from 0.019 to 0.027.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09070v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09070v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.345,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09071",
      "title": "Strategic Tradeoffs Between Humans and AI in Multi-Agent Bargaining",
      "authors": [
        "Crystal Qian",
        "Kehang Zhu",
        "John Horton",
        "Benjamin S. Manning",
        "Vivian Tsai",
        "James Wexler",
        "Nithum Thain"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.GT (Computer Science and Game Theory)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Coordination tasks traditionally performed by humans are increasingly being\ndelegated to autonomous agents. As this pattern progresses, it becomes critical\nto evaluate not only these agents' performance but also the processes through\nwhich they negotiate in dynamic, multi-agent environments. Furthermore,\ndifferent agents exhibit distinct advantages: traditional statistical agents,\nsuch as Bayesian models, may excel under well-specified conditions, whereas\nlarge language models (LLMs) can generalize across contexts. In this work, we\ncompare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in\na dynamic negotiation setting that enables direct, identical-condition\ncomparisons across populations, capturing both outcomes and behavioral\ndynamics. Bayesian agents extract the highest surplus through aggressive\noptimization, at the cost of frequent trade rejections. Humans and LLMs can\nachieve similar overall surplus, but through distinct behaviors: LLMs favor\nconservative, concessionary trades with few rejections, while humans employ\nmore strategic, risk-taking, and fairness-oriented behaviors. Thus, we find\nthat performance parity -- a common benchmark in agent evaluation -- can\nconceal fundamental differences in process and alignment, which are critical\nfor practical deployment in real-world coordination tasks.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09071v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09071v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.474,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.35,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on comparing humans, LLMs, and Bayesian agents in a bargaining game, emphasizing evaluation of behaviors and outcomes, but does not involve training or fine-tuning AI models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates LLMs and other agents in negotiation tasks without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09074",
      "title": "KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for\n  Motion Planning",
      "authors": [
        "Alice Kate Li",
        "Thales C Silva",
        "Victoria Edwards",
        "Vijay Kumar",
        "M. Ani Hsieh"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In this work, we propose a novel flow field-based motion planning method that\ndrives a robot from any initial state to a desired reference trajectory such\nthat it converges to the trajectory's end point. Despite demonstrated efficacy\nin using Koopman operator theory for modeling dynamical systems, Koopman does\nnot inherently enforce convergence to desired trajectories nor to specified\ngoals -- a requirement when learning from demonstrations (LfD). We present\nKoopMotion which represents motion flow fields as dynamical systems,\nparameterized by Koopman Operators to mimic desired trajectories, and leverages\nthe divergence properties of the learnt flow fields to obtain smooth motion\nfields that converge to a desired reference trajectory when a robot is placed\naway from the desired trajectory, and tracks the trajectory until the end\npoint. To demonstrate the effectiveness of our approach, we show evaluations of\nKoopMotion on the LASA human handwriting dataset and a 3D manipulator\nend-effector trajectory dataset, including spectral analysis. We also perform\nexperiments on a physical robot, verifying KoopMotion on a miniature autonomous\nsurface vehicle operating in a non-static fluid flow environment. Our approach\nis highly sample efficient in both space and time, requiring only 3\\% of the\nLASA dataset to generate dense motion plans. Additionally, KoopMotion provides\na significant improvement over baselines when comparing metrics that measure\nspatial and temporal dynamics modeling efficacy.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09074v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09074v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.329,
      "datasets_score": 0.268,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Koopman operator theory to learn flow fields for motion planning in robotics, emphasizing convergence to desired trajectories and dynamical systems modeling. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for tasks like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09085",
      "title": "IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for\n  Multispectral Object Detection",
      "authors": [
        "Jifeng Shen",
        "Haibo Zhan",
        "Xin Zuo",
        "Heng Fan",
        "Xiaohui Yuan",
        "Jun Li",
        "Wankou Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Current multispectral object detection methods often retain extraneous\nbackground or noise during feature fusion, limiting perceptual performance. To\naddress this, we propose an innovative feature fusion framework based on\ncross-modal feature contrastive and screening strategy, diverging from\nconventional approaches. The proposed method adaptively enhances salient\nstructures by fusing object-aware complementary cross-modal features while\nsuppressing shared background interference. Our solution centers on two novel,\nspecially designed modules: the Mutual Feature Refinement Module (MFRM) and the\nDifferential Feature Feedback Module (DFFM). The MFRM enhances intra- and\ninter-modal feature representations by modeling their relationships, thereby\nimproving cross-modal alignment and discriminative power. Inspired by feedback\ndifferential amplifiers, the DFFM dynamically computes inter-modal differential\nfeatures as guidance signals and feeds them back to the MFRM, enabling adaptive\nfusion of complementary information while suppressing common-mode noise across\nmodalities. To enable robust feature learning, the MFRM and DFFM are integrated\ninto a unified framework, which is formally formulated as an Iterative\nRelation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion.\nIRDFusion enables high-quality cross-modal fusion by progressively amplifying\nsalient relational signals through iterative feedback, while suppressing\nfeature noise, leading to significant performance gains. In extensive\nexperiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves\nstate-of-the-art performance and consistently outperforms existing methods\nacross diverse challenging scenarios, demonstrating its robustness and\neffectiveness. Code will be available at\nhttps://github.com/61s61min/IRDFusion.git.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09085v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09085v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.34,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes an iterative feature fusion framework for multispectral object detection, using modules like MFRM and DFFM with feedback mechanisms inspired by differential amplifiers. While it involves iteration for refinement, it does not adapt diffusion models for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for holistic correction. The core contributions are in object detection and feature fusion, not diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09090",
      "title": "SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for\n  High-Performance Vision-Language-Action Models",
      "authors": [
        "Hengyu Fang",
        "Yijiang Liu",
        "Yuan Du",
        "Li Du",
        "Huanrui Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vision-Language-Action (VLA) models exhibit unprecedented capabilities for\nembodied intelligence. However, their extensive computational and memory costs\nhinder their practical deployment. Existing VLA compression and acceleration\napproaches conduct quantization or token pruning in an ad-hoc manner but fail\nto enable both for a holistic efficiency improvement due to an observed\nincompatibility. This work introduces SQAP-VLA, the first structured,\ntraining-free VLA inference acceleration framework that simultaneously enables\nstate-of-the-art quantization and token pruning. We overcome the\nincompatibility by co-designing the quantization and token pruning pipeline,\nwhere we propose new quantization-aware token pruning criteria that work on an\naggressively quantized model while improving the quantizer design to enhance\npruning effectiveness. When applied to standard VLA models, SQAP-VLA yields\nsignificant gains in computational efficiency and inference speed while\nsuccessfully preserving core model performance, achieving a $\\times$1.93\nspeedup and up to a 4.5\\% average success rate enhancement compared to the\noriginal model.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09090v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09090v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.42,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a framework for quantization and token pruning to accelerate inference in Vision-Language-Action (VLA) models, focusing on model compression for deployment on resource-constrained devices. It does not address distributed training, parallel computing across nodes, or strategies for partitioning data/computation during model training, making it unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09091",
      "title": "Towards Confidential and Efficient LLM Inference with Dual Privacy\n  Protection",
      "authors": [
        "Honglan Yu",
        "Yibin Wang",
        "Feifei Dai",
        "Dong Liu",
        "Haihui Fan",
        "Xiaoyan Gu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "CPU-based trusted execution environments (TEEs) and differential privacy (DP)\nhave gained wide applications for private inference. Due to high inference\nlatency in TEEs, researchers use partition-based approaches that offload linear\nmodel components to GPUs. However, dense nonlinear layers of large language\nmodels (LLMs) result in significant communication overhead between TEEs and\nGPUs. DP-based approaches apply random noise to protect data privacy, but this\ncompromises LLM performance and semantic understanding. To overcome the above\ndrawbacks, this paper proposes CMIF, a Confidential and efficient Model\nInference Framework. CMIF confidentially deploys the embedding layer in the\nclient-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes\nthe Report-Noisy-Max mechanism to protect sensitive inputs with a slight\ndecrease in model performance. Extensive experiments on Llama-series models\ndemonstrate that CMIF reduces additional inference overhead in TEEs while\npreserving user data privacy.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09091v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09091v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.428,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on privacy-preserving inference for LLMs using TEEs and optimized privacy mechanisms, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses efficient and confidential LLM inference through model partitioning and privacy techniques, but it does not involve diffusion models, iterative refinement for logical tasks, or multi-step chain-of-thought reasoning.",
      "distributed_training_justification": "The paper discusses partitioning LLM components between TEEs and GPUs for efficient inference, which involves parallel computing across devices, but it pertains to inference rather than training, data partitioning, or multi-node machine learning algorithms.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09097",
      "title": "DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large\n  Language Models",
      "authors": [
        "Honghui Xu",
        "Shiva Shrestha",
        "Wei Chen",
        "Zhiyuan Li",
        "Zhipeng Cai"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As on-device large language model (LLM) systems become increasingly\nprevalent, federated fine-tuning enables advanced language understanding and\ngeneration directly on edge devices; however, it also involves processing\nsensitive, user-specific data, raising significant privacy concerns within the\nfederated learning framework. To address these challenges, we propose\nDP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates\nLoRA-based adaptation with differential privacy in a communication-efficient\nsetting. Each client locally clips and perturbs its LoRA matrices using\nGaussian noise to satisfy ($\\epsilon$, $\\delta$)-differential privacy. We\nfurther provide a theoretical analysis demonstrating the unbiased nature of the\nupdates and deriving bounds on the variance introduced by noise, offering\npractical guidance for privacy-budget calibration. Experimental results across\nmainstream benchmarks show that DP-FedLoRA delivers competitive performance\nwhile offering strong privacy guarantees, paving the way for scalable and\nprivacy-preserving LLM deployment in on-device environments.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09097v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09097v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.453,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.475,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated fine-tuning of large language models with differential privacy, emphasizing privacy protection in distributed settings. It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a federated fine-tuning framework for on-device LLMs, which involves distributed training across edge devices, sharing model updates, and communication-efficient mechanisms. This directly aligns with distributed training concepts like parallel computing and multi-node learning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces DP-FedLoRA, a novel framework for privacy-enhanced federated fine-tuning of on-device large language models (LLMs), which integrates Low-Rank Adaptation (LoRA) with differential privacy to protect user data during collaborative training. The methodology involves clients locally clipping and adding Gaussian noise to their LoRA matrices to achieve (ε, δ)-differential privacy, followed by secure aggregation at a central server, with theoretical analysis showing unbiased updates and bounded variance for practical privacy calibration. Experimental results demonstrate that DP-FedLoRA maintains competitive performance on benchmarks while providing strong privacy guarantees, facilitating scalable and secure LLM deployment on edge devices.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new integration of LoRA with differential privacy in federated learning specifically for on-device LLMs, addressing a significant privacy gap and providing original theoretical analysis on update variance, which advances the state-of-the-art in privacy-preserving AI.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like federated learning and AI security due to its practical framework for privacy in on-device LLMs, though its influence may remain confined to specialized applications rather than broadly transformative ones.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a valuable and innovative contribution to privacy in federated AI, with strong methodology and empirical validation, making it essential for researchers focused on AI security and on-device learning.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f621fb10ef1c9f66e0da3984676c99f8af1af6a4",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 11,
      "average_h_index": 4.2,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Honghui Xu",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2027660698"
        },
        {
          "name": "Shiva Shrestha",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376781873"
        },
        {
          "name": "Wei Chen",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2294845809"
        },
        {
          "name": "Zhiyuan Li",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2294674012"
        },
        {
          "name": "Zhipeng Cai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381254986"
        }
      ]
    },
    {
      "id": "2509.09110",
      "title": "S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR\n  Global Localization",
      "authors": [
        "Chenghao Zhang",
        "Lun Luo",
        "Si-Yuan Cao",
        "Xiaokai Bai",
        "Yuncheng Jin",
        "Zhu Yu",
        "Beinan Yu",
        "Yisen Wang",
        "Hui-Liang Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "LiDAR-based global localization is an essential component of simultaneous\nlocalization and mapping (SLAM), which helps loop closure and re-localization.\nCurrent approaches rely on ground-truth poses obtained from GPS or SLAM\nodometry to supervise network training. Despite the great success of these\nsupervised approaches, substantial cost and effort are required for\nhigh-precision ground-truth pose acquisition. In this work, we propose\nS-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for\nLiDAR global localization, which eliminates the need for ground-truth poses and\nis highly scalable. We construct training triplets from single BEV images by\nleveraging the known geographic distances between keypoint-centered BEV\npatches. Convolutional neural network (CNN) is used to extract local features,\nand NetVLAD is employed to aggregate global descriptors. Moreover, we introduce\nSoftCos loss to enhance learning from the generated triplets. Experimental\nresults on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves\nstate-of-the-art performance in place recognition, loop closure, and global\nlocalization tasks, while offering scalability that would require extra effort\nfor supervised approaches.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09110v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09110v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.346,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09111",
      "title": "FPI-Det: a face--phone Interaction Dataset for phone-use detection and\n  understanding",
      "authors": [
        "Jianqin Gao",
        "Tianqi Wang",
        "Yu Zhang",
        "Yishu Zhang",
        "Chenyuan Wang",
        "Allan Dong",
        "Zihao Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The widespread use of mobile devices has created new challenges for vision\nsystems in safety monitoring, workplace productivity assessment, and attention\nmanagement. Detecting whether a person is using a phone requires not only\nobject recognition but also an understanding of behavioral context, which\ninvolves reasoning about the relationship between faces, hands, and devices\nunder diverse conditions. Existing generic benchmarks do not fully capture such\nfine-grained human--device interactions. To address this gap, we introduce the\nFPI-Det, containing 22{,}879 images with synchronized annotations for faces and\nphones across workplace, education, transportation, and public scenarios. The\ndataset features extreme scale variation, frequent occlusions, and varied\ncapture conditions. We evaluate representative YOLO and DETR detectors,\nproviding baseline results and an analysis of performance across object sizes,\nocclusion levels, and environments. Source code and dataset is available at\nhttps://github.com/KvCgRv/FPI-Det.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09111v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09111v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.367,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new dataset, FPI-Det, which is specifically designed for machine learning and AI applications in object detection and human-device interaction. It covers dataset creation with 22,879 annotated images, analysis of challenges like scale variation and occlusions, benchmarking with YOLO and DETR detectors, and evaluation protocols, all of which directly align with research on creating, analyzing, benchmarking, and evaluating datasets.",
      "llm_score_status": "completed",
      "summary": "The paper introduces FPI-Det, a new dataset comprising 22,879 images with synchronized annotations for faces and phones across diverse scenarios like workplaces, education, transportation, and public spaces, aimed at advancing phone-use detection by addressing challenges such as scale variations, occlusions, and varied capture conditions. It evaluates baseline performance using representative detectors like YOLO and DETR, providing insights into their effectiveness in understanding fine-grained human-phone interactions, and highlights the dataset's role in bridging gaps in existing benchmarks for behavioral context analysis.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset specifically designed for fine-grained face-phone interaction detection, which significantly advances the state-of-the-art by addressing gaps in existing benchmarks for human-device interactions.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the computer vision subfield, particularly for applications in safety and behavioral monitoring, due to its provision of a specialized benchmark for phone-use detection.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by introducing a valuable dataset that fills a research gap in human-device interaction, making it essential for researchers in computer vision and pattern recognition to be aware of for advancing related studies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/05dffc8d2de18a157a77a54abba100574f214174",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jianqin Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380567163"
        },
        {
          "name": "Tianqi Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362387389"
        },
        {
          "name": "Yu Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380098737"
        },
        {
          "name": "Yishu Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380093675"
        },
        {
          "name": "Chenyuan Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2361891463"
        },
        {
          "name": "Allan Dong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380028363"
        },
        {
          "name": "Zihao Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380082593"
        }
      ]
    },
    {
      "id": "2509.09112",
      "title": "Character-Level Perturbations Disrupt LLM Watermarks",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "He Zhang",
        "Shirui Pan",
        "Bo Liu",
        "Asif Qumer Gill",
        "Leo Yu Zhang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Model (LLM) watermarking embeds detectable signals into\ngenerated text for copyright protection, misuse prevention, and content\ndetection. While prior studies evaluate robustness using watermark removal\nattacks, these methods are often suboptimal, creating the misconception that\neffective removal requires large perturbations or powerful adversaries.\n  To bridge the gap, we first formalize the system model for LLM watermark, and\ncharacterize two realistic threat models constrained on limited access to the\nwatermark detector. We then analyze how different types of perturbation vary in\ntheir attack range, i.e., the number of tokens they can affect with a single\nedit. We observe that character-level perturbations (e.g., typos, swaps,\ndeletions, homoglyphs) can influence multiple tokens simultaneously by\ndisrupting the tokenization process. We demonstrate that character-level\nperturbations are significantly more effective for watermark removal under the\nmost restrictive threat model. We further propose guided removal attacks based\non the Genetic Algorithm (GA) that uses a reference detector for optimization.\nUnder a practical threat model with limited black-box queries to the watermark\ndetector, our method demonstrates strong removal performance. Experiments\nconfirm the superiority of character-level perturbations and the effectiveness\nof the GA in removing watermarks under realistic constraints. Additionally, we\nargue there is an adversarial dilemma when considering potential defenses: any\nfixed defense can be bypassed by a suitable perturbation strategy. Motivated by\nthis principle, we propose an adaptive compound character-level attack.\nExperimental results show that this approach can effectively defeat the\ndefenses. Our findings highlight significant vulnerabilities in existing LLM\nwatermark schemes and underline the urgency for the development of new robust\nmechanisms.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09112v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09112v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.323,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09116",
      "title": "Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation\n  Models and Text-to-image Attention",
      "authors": [
        "Junhao Xing",
        "Ryohei Miyakawa",
        "Yang Yang",
        "Xinpeng Liu",
        "Risa Shinoda",
        "Hiroaki Santo",
        "Yosuke Toda",
        "Fumio Okura"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Foundation segmentation models achieve reasonable leaf instance extraction\nfrom top-view crop images without training (i.e., zero-shot). However,\nsegmenting entire plant individuals with each consisting of multiple\noverlapping leaves remains challenging. This problem is referred to as a\nhierarchical segmentation task, typically requiring annotated training\ndatasets, which are often species-specific and require notable human labor. To\naddress this, we introduce ZeroPlantSeg, a zero-shot segmentation for\nrosette-shaped plant individuals from top-view images. We integrate a\nfoundation segmentation model, extracting leaf instances, and a vision-language\nmodel, reasoning about plants' structures to extract plant individuals without\nadditional training. Evaluations on datasets with multiple plant species,\ngrowth stages, and shooting environments demonstrate that our method surpasses\nexisting zero-shot methods and achieves better cross-domain performance than\nsupervised methods. Implementations are available at\nhttps://github.com/JunhaoXing/ZeroPlantSeg.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09116v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09116v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.333,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09118",
      "title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust\n  Text-based Person Retrieval",
      "authors": [
        "Tianlu Zheng",
        "Yifan Zhang",
        "Xiang An",
        "Ziyong Feng",
        "Kaicheng Yang",
        "Qichuan Ding"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Although Contrastive Language-Image Pre-training (CLIP) exhibits strong\nperformance across diverse vision tasks, its application to person\nrepresentation learning faces two critical challenges: (i) the scarcity of\nlarge-scale annotated vision-language data focused on person-centric images,\nand (ii) the inherent limitations of global contrastive learning, which\nstruggles to maintain discriminative local features crucial for fine-grained\nmatching while remaining vulnerable to noisy text tokens. This work advances\nCLIP for person representation learning through synergistic improvements in\ndata curation and model architecture. First, we develop a noise-resistant data\nconstruction pipeline that leverages the in-context learning capabilities of\nMLLMs to automatically filter and caption web-sourced images. This yields\nWebPerson, a large-scale dataset of 5M high-quality person-centric image-text\npairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking\nSynergetic) framework, which improves cross-modal alignment by adaptively\nmasking noisy textual tokens based on the gradient-attention similarity score.\nAdditionally, we incorporate masked token prediction objectives that compel the\nmodel to predict informative text tokens, enhancing fine-grained semantic\nrepresentation learning. Extensive experiments show that GA-DMS achieves\nstate-of-the-art performance across multiple benchmarks.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09118v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09118v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.373,
      "datasets_score": 0.414,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the introduction of a new dataset, WebPerson, comprising 5 million high-quality person-centric image-text pairs. It details a novel data curation pipeline using MLLMs for filtering and annotating web-sourced images, which directly aligns with research on creating and curating datasets for AI applications. This focus on dataset construction and methodology makes it highly relevant to the topic.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of CLIP in text-based person retrieval by introducing WebPerson, a large-scale dataset of 5 million high-quality person-centric image-text pairs created through an automated pipeline using Multimodal Large Language Models for filtering and captioning. It proposes the GA-DMS framework, which enhances cross-modal alignment by adaptively masking noisy text tokens based on gradient-attention similarity and incorporates masked token prediction to improve fine-grained semantic representations, resulting in state-of-the-art performance across multiple benchmarks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing techniques like MLLMs for data curation with a new dual-masking framework to address specific challenges in person retrieval, though it builds on established concepts in vision-language models.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in text-based person retrieval by providing a new dataset and framework that could enhance applications in surveillance and biometric systems, but its applicability is primarily within this subfield.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a valuable contribution to computer vision, particularly for those focused on person retrieval, due to its innovative dataset and framework achieving state-of-the-art results, making it important for relevant researchers.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fae43e0a8ee6449ac10407a033280867085ab6a6",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 10,
      "average_h_index": 3.6666666666666665,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Tianlu Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363115102"
        },
        {
          "name": "Yifan Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380041408"
        },
        {
          "name": "Xiang An",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2054941340"
        },
        {
          "name": "Ziyong Feng",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2292273156"
        },
        {
          "name": "Kaicheng Yang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2297821508"
        },
        {
          "name": "Qichuan Ding",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380031613"
        }
      ]
    },
    {
      "id": "2509.09125",
      "title": "Automated Classification of Tutors' Dialogue Acts Using Generative AI: A\n  Case Study Using the CIMA Corpus",
      "authors": [
        "Liqun He",
        "Jiaqi Xu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study explores the use of generative AI for automating the\nclassification of tutors' Dialogue Acts (DAs), aiming to reduce the time and\neffort required by traditional manual coding. This case study uses the\nopen-source CIMA corpus, in which tutors' responses are pre-annotated into four\nDA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored\nprompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of\n0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and\nindicating substantial agreement with human annotations. These findings suggest\nthat generative AI has strong potential to provide an efficient and accessible\napproach to DA classification, with meaningful implications for educational\ndialogue analysis. The study also highlights the importance of task-specific\nlabel definitions and contextual information in enhancing the quality of\nautomated annotation. Finally, it underscores the ethical considerations\nassociated with the use of generative AI and the need for responsible and\ntransparent research practices. The script of this research is publicly\navailable at\nhttps://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09125v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09125v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.322,
      "datasets_score": 0.401,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on using pre-trained generative AI models (GPT-3.5-turbo and GPT-4) with tailored prompts for dialogue act classification, without any mention of training a reward model, fine-tuning via reinforcement learning, or incorporating human feedback. Thus, it does not involve RLHF techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs GPT models for classification tasks, which are based on transformer architectures, and does not involve diffusion models, iterative refinement of reasoning paths, or multi-step logical reasoning as described. There is no component for holistically correcting a chain-of-thought using diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses the open-source CIMA corpus for evaluating generative AI in dialogue act classification, including performance benchmarking (e.g., accuracy and F1-score), which aligns with dataset analysis and evaluation for AI applications. However, the primary focus is on AI classification methods rather than creating, curating, or deeply analyzing datasets.",
      "llm_score_status": "completed",
      "summary": "This paper investigates the application of generative AI models, specifically GPT-3.5-turbo and GPT-4, to automate the classification of tutors' Dialogue Acts (DAs) in the CIMA corpus, with the primary objective of reducing the manual effort required for annotation. Using tailored prompts, the study evaluates these models' performance, finding that GPT-4 achieves 80% accuracy, a weighted F1-score of 0.81, and a Cohen's Kappa of 0.74, which exceeds baseline results and demonstrates the potential for efficient, accessible DA classification in educational contexts while emphasizing the role of precise label definitions, contextual information, and ethical considerations.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying generative AI to automate dialogue act classification in educational settings, combining existing AI techniques in a practical new way for this domain. However, it does not introduce a entirely new problem or architecture, as AI-based classification methods are already established.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in AI for educational dialogue analysis by demonstrating an efficient annotation method, potentially leading to broader adoption in subfields like computational linguistics. Nonetheless, its impact may be limited to specific applications rather than widespread commercial or general AI advancements.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a strong, practical contribution with solid results and ethical insights that are valuable for researchers in AI and education. It is not essential for all readers but provides useful knowledge for those working in related areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e784ba58ebe3ab310f48fa1c96b549a4fe0fb40d",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Liqun He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380465694"
        },
        {
          "name": "Jiaqi Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380087699"
        }
      ]
    },
    {
      "id": "2509.09127",
      "title": "Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis\n  on Identifying High-risk Bank Clients with Supervised Learning",
      "authors": [
        "Khashayar Namdar",
        "Pin-Chien Wang",
        "Tushar Raju",
        "Steven Zheng",
        "Fiona Li",
        "Safwat Tahmin Khan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Anti-money laundering (AML) actions and measurements are among the priorities\nof financial institutions, for which machine learning (ML) has shown to have a\nhigh potential. In this paper, we propose a comprehensive and systematic\napproach for developing ML pipelines to identify high-risk bank clients in a\ndataset curated for Task 1 of the University of Toronto 2023-2024 Institute for\nManagement and Innovation (IMI) Big Data and Artificial Intelligence\nCompetition. The dataset included 195,789 customer IDs, and we employed a\n16-step design and statistical analysis to ensure the final pipeline was\nrobust. We also framed the data in a SQLite database, developed SQL-based\nfeature engineering algorithms, connected our pre-trained model to the\ndatabase, and made it inference-ready, and provided explainable artificial\nintelligence (XAI) modules to derive feature importance. Our pipeline achieved\na mean area under the receiver operating characteristic curve (AUROC) of 0.961\nwith a standard deviation (SD) of 0.005. The proposed pipeline achieved second\nplace in the competition.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09127v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09127v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.27,
      "distributed_training_score": 0.296,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09130",
      "title": "ALL-PET: A Low-resource and Low-shot PET Foundation Model in Projection\n  Domain",
      "authors": [
        "Bin Huang",
        "Kang Chen",
        "Bingxuan Li",
        "Huafeng Liu",
        "Qiegen Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Building large-scale foundation model for PET imaging is hindered by limited\naccess to labeled data and insufficient computational resources. To overcome\ndata scarcity and efficiency limitations, we propose ALL-PET, a low-resource,\nlow-shot PET foundation model operating directly in projection domain. ALL-PET\nleverages a latent diffusion model (LDM) with three key innovations. First, we\ndesign a Radon mask augmentation strategy (RMAS) that generates over 200,000\nstructurally diverse training samples by projecting randomized image-domain\nmasks into sinogram space, significantly improving generalization with minimal\ndata. This is extended by a dynamic multi-mask (DMM) mechanism that varies mask\nquantity and distribution, enhancing data diversity without added model\ncomplexity. Second, we implement positive/negative mask constraints to embed\nstrict geometric consistency, reducing parameter burden while preserving\ngeneration quality. Third, we introduce transparent medical attention (TMA), a\nparameter-free, geometry-driven mechanism that enhances lesion-related regions\nin raw projection data. Lesion-focused attention maps are derived from coarse\nsegmentation, covering both hypermetabolic and hypometabolic areas, and\nprojected into sinogram space for physically consistent guidance. The system\nsupports clinician-defined ROI adjustments, ensuring flexible, interpretable,\nand task-adaptive emphasis aligned with PET acquisition physics. Experimental\nresults show that ALL-PET achieves high-quality sinogram generation using only\n500 samples, with performance comparable to models trained on larger datasets.\nALL-PET generalizes across tasks including low-dose reconstruction, attenuation\ncorrection, delayed-frame prediction, and tracer separation, operating\nefficiently with memory use under 24GB.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09130v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09130v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.413,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.379,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using techniques like the Radon mask augmentation strategy (RMAS) to programmatically generate a large number of diverse training samples (over 200,000) from a minimal dataset (only 500 samples), which directly aligns with weak supervision. This approach relies on automated, synthetic data creation rather than extensive hand-labeled data, fitting the definition of training models with programmatically generated, potentially noisy or imprecise labels to overcome data scarcity.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces ALL-PET, a low-resource and low-shot foundation model for PET imaging that operates in the projection domain to address data scarcity and computational limitations. It utilizes a latent diffusion model enhanced with innovations such as Radon mask augmentation strategy for generating diverse training samples, dynamic multi-mask mechanisms for increased data variety, positive/negative mask constraints for geometric consistency, and transparent medical attention for lesion-focused guidance in sinogram space. Experimental results demonstrate that ALL-PET achieves high-quality sinogram generation with only 500 samples, performing comparably to models trained on larger datasets and generalizing effectively to tasks like low-dose reconstruction and attenuation correction while maintaining efficiency with under 24GB memory usage.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing diffusion models with novel techniques like Radon mask augmentation and transparent medical attention, offering a new approach to low-resource PET imaging that improves generalization without introducing entirely new architectures. While it advances the field by addressing data scarcity, it builds on established methods rather than introducing a truly groundbreaking problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in medical imaging by providing efficient solutions for low-resource scenarios, potentially leading to citations and applications in PET-related subfields. However, its impact may be limited to specialized areas of computer vision and not extend broadly to general AI or commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to low-resource medical imaging with practical innovations, making it essential for researchers in computer vision and PET applications to be aware of its methods and findings. While not universally groundbreaking, its efficiency and generalization capabilities add significant value in its niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6135bafe057cd27e4b62f7f6362214118629456b",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 4,
      "average_h_index": 1.4,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Bin Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2283516470"
        },
        {
          "name": "Kang Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2368778333"
        },
        {
          "name": "Bingxuan Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2254637034"
        },
        {
          "name": "Huafeng Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380204542"
        },
        {
          "name": "Qiegen Liu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2277685588"
        }
      ]
    },
    {
      "id": "2509.09131",
      "title": "ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for\n  Vietnamese Reranking",
      "authors": [
        "Phuong-Nam Dang",
        "Kieu-Linh Nguyen",
        "Thanh-Hieu Pham"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents ViRanker, a cross-encoder reranking model tailored to the\nVietnamese language. Built on the BGE-M3 encoder and enhanced with the\nBlockwise Parallel Transformer, ViRanker addresses the lack of competitive\nrerankers for Vietnamese, a low-resource language with complex syntax and\ndiacritics. The model was trained on an 8 GB curated corpus and fine-tuned with\nhybrid hard-negative sampling to strengthen robustness. Evaluated on the\nMMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing\nmultilingual baselines and competing closely with PhoRanker. By releasing the\nmodel openly on Hugging Face, we aim to support reproducibility and encourage\nwider adoption in real-world retrieval systems. Beyond Vietnamese, this study\nillustrates how careful architectural adaptation and data curation can advance\nreranking in other underrepresented languages.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09131v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09131v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.342,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09140",
      "title": "Noise-Robust Topology Estimation of 2D Image Data via Neural Networks\n  and Persistent Homology",
      "authors": [
        "Dylan Peek",
        "Matthew P. Skerritt",
        "Stephan Chalup"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer\ncontrasting approaches to inferring topological structure from data. In this\nstudy, we examine the noise robustness of a supervised neural network trained\nto predict Betti numbers in 2D binary images. We compare an ANN approach\nagainst a PH pipeline based on cubical complexes and the Signed Euclidean\nDistance Transform (SEDT), which is a widely adopted strategy for noise-robust\ntopological analysis. Using one synthetic and two real-world datasets, we show\nthat ANNs can outperform this PH approach under noise, likely due to their\ncapacity to learn contextual and geometric priors from training data. Though\nstill emerging, the use of ANNs for topology estimation offers a compelling\nalternative to PH under structural noise.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09140v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09140v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.283,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09143",
      "title": "Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene\n  Evaluation",
      "authors": [
        "Yuiko Uchida",
        "Ren Togo",
        "Keisuke Maeda",
        "Takahiro Ogawa",
        "Miki Haseyama"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.GR (Graphics)"
      ],
      "abstract": "This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric\nfor 3D scenes that explicitly focuses on \"objects,\" which are fundamental units\nof human visual perception. Existing metrics assess overall image quality,\nleading to discrepancies with human perception. Inspired by neuropsychological\ninsights, we hypothesize that human recognition of 3D scenes fundamentally\ninvolves attention to individual objects. OSIM enables object-centric\nevaluations by leveraging an object detection model and its feature\nrepresentations to quantify the \"objectness\" of each object in the scene. Our\nuser study demonstrates that OSIM aligns more closely with human perception\ncompared to existing metrics. We also analyze the characteristics of OSIM using\nvarious approaches. Moreover, we re-evaluate recent 3D reconstruction and\ngeneration models under a standardized experimental setup to clarify\nadvancements in this field. The code is available at\nhttps://github.com/Objectness-Similarity/OSIM.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09143v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09143v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.274,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09151",
      "title": "Video Understanding by Design: How Datasets Shape Architectures and\n  Insights",
      "authors": [
        "Lei Wang",
        "Piotr Koniusz",
        "Yongsheng Gao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Video understanding has advanced rapidly, fueled by increasingly complex\ndatasets and powerful architectures. Yet existing surveys largely classify\nmodels by task or family, overlooking the structural pressures through which\ndatasets guide architectural evolution. This survey is the first to adopt a\ndataset-driven perspective, showing how motion complexity, temporal span,\nhierarchical composition, and multimodal richness impose inductive biases that\nmodels should encode. We reinterpret milestones, from two-stream and 3D CNNs to\nsequential, transformer, and multimodal foundation models, as concrete\nresponses to these dataset-driven pressures. Building on this synthesis, we\noffer practical guidance for aligning model design with dataset invariances\nwhile balancing scalability and task demands. By unifying datasets, inductive\nbiases, and architectures into a coherent framework, this survey provides both\na comprehensive retrospective and a prescriptive roadmap for advancing\ngeneral-purpose video understanding.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09151v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09151v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.367,
      "datasets_score": 0.518,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is a dataset-driven survey that systematically analyzes how dataset properties, such as motion complexity, temporal span, hierarchical structure, and multimodal richness, shape architectural evolution in video understanding. It examines existing datasets like Kinetics and ActivityNet, linking their characteristics to model design and performance, which directly aligns with research on dataset analysis, benchmarking, and evaluation in AI applications. While it does not introduce new datasets or curation methods, its focus on interpreting dataset structures as drivers of innovation fits the topic's scope.",
      "llm_score_status": "completed",
      "summary": "This survey explores how dataset properties, such as motion complexity, temporal span, hierarchical structure, and multimodal richness, shape the evolution of video understanding architectures by imposing inductive biases that drive model design from early CNNs to modern transformers and foundation models. By synthesizing these elements into a unified framework, the paper reinterprets key milestones in the field and provides practical guidance for aligning future model development with dataset characteristics to advance general-purpose video understanding.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset-driven perspective on video understanding surveys, significantly advancing the state-of-the-art by linking dataset properties directly to architectural evolution, which has not been comprehensively addressed in prior works.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the computer vision and AI subfields by offering a cohesive framework for model design, though its influence may be limited to academic research rather than broad commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a high-quality synthesis of video understanding advancements with practical guidance, making it a valuable resource for researchers in the field, though it may not be essential for those outside specialized areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/47f433a58b4ff82a10940d012ba5ea2ad4bf1e18",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 37,
      "average_h_index": 17.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Lei Wang",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/46194997"
        },
        {
          "name": "Piotr Koniusz",
          "h_index": 37,
          "profile_url": "https://www.semanticscholar.org/author/2155775"
        },
        {
          "name": "Yongsheng Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380085786"
        }
      ]
    },
    {
      "id": "2509.09153",
      "title": "OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge",
      "authors": [
        "JaeWoong Shin",
        "Jeongun Ryu",
        "Aaron Valero Puche",
        "Jinhee Lee",
        "Biagio Brattoli",
        "Wonkyung Jung",
        "Soo Ick Cho",
        "Kyunghyun Paeng",
        "Chan-Young Ock",
        "Donggeun Yoo",
        "Zhaoyang Li",
        "Wangkai Li",
        "Huayu Mai",
        "Joshua Millward",
        "Zhen He",
        "Aiden Nibali",
        "Lydia Anette Schoenpflug",
        "Viktor Hendrik Koelzer",
        "Xu Shuoyu",
        "Ji Zheng",
        "Hu Bin",
        "Yu-Wen Lo",
        "Ching-Hui Yang",
        "Sérgio Pereira"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Pathologists routinely alternate between different magnifications when\nexamining Whole-Slide Images, allowing them to evaluate both broad tissue\nmorphology and intricate cellular details to form comprehensive diagnoses.\nHowever, existing deep learning-based cell detection models struggle to\nreplicate these behaviors and learn the interdependent semantics between\nstructures at different magnifications. A key barrier in the field is the lack\nof datasets with multi-scale overlapping cell and tissue annotations. The\nOCELOT 2023 challenge was initiated to gather insights from the community to\nvalidate the hypothesis that understanding cell and tissue (cell-tissue)\ninteractions is crucial for achieving human-level performance, and to\naccelerate the research in this field. The challenge dataset includes\noverlapping cell detection and tissue segmentation annotations from six organs,\ncomprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA)\nWhole-Slide Images with hematoxylin and eosin staining, divided into training,\nvalidation, and test subsets. Participants presented models that significantly\nenhanced the understanding of cell-tissue relationships. Top entries achieved\nup to a 7.99 increase in F1-score on the test set compared to the baseline\ncell-only model that did not incorporate cell-tissue relationships. This is a\nsubstantial improvement in performance over traditional cell-only detection\nmethods, demonstrating the need for incorporating multi-scale semantics into\nthe models. This paper provides a comparative analysis of the methods used by\nparticipants, highlighting innovative strategies implemented in the OCELOT 2023\nchallenge.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09153v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09153v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.263,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.315,
      "distributed_training_score": 0.315,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09154",
      "title": "Mind Meets Space: Rethinking Agentic Spatial Intelligence from a\n  Neuroscience-inspired Perspective",
      "authors": [
        "Bui Duc Manh",
        "Soumyaratna Debnath",
        "Zetong Zhang",
        "Shriram Damodaran",
        "Arvind Kumar",
        "Yueyi Zhang",
        "Lu Mi",
        "Erik Cambria",
        "Lin Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in agentic AI have led to systems capable of autonomous task\nexecution and language-based reasoning, yet their spatial reasoning abilities\nremain limited and underexplored, largely constrained to symbolic and\nsequential processing. In contrast, human spatial intelligence, rooted in\nintegrated multisensory perception, spatial memory, and cognitive maps, enables\nflexible, context-aware decision-making in unstructured environments.\nTherefore, bridging this gap is critical for advancing Agentic Spatial\nIntelligence toward better interaction with the physical 3D world. To this end,\nwe first start from scrutinizing the spatial neural models as studied in\ncomputational neuroscience, and accordingly introduce a novel computational\nframework grounded in neuroscience principles. This framework maps core\nbiological functions to six essential computation modules: bio-inspired\nmultimodal sensing, multi-sensory integration, egocentric-allocentric\nconversion, an artificial cognitive map, spatial memory, and spatial reasoning.\nTogether, these modules form a perspective landscape for agentic spatial\nreasoning capability across both virtual and physical environments. On top, we\nconduct a framework-guided analysis of recent methods, evaluating their\nrelevance to each module and identifying critical gaps that hinder the\ndevelopment of more neuroscience-grounded spatial reasoning modules. We further\nexamine emerging benchmarks and datasets and explore potential application\ndomains ranging from virtual to embodied systems, such as robotics. Finally, we\noutline potential research directions, emphasizing the promising roadmap that\ncan generalize spatial reasoning across dynamic or unstructured environments.\nWe hope this work will benefit the research community with a\nneuroscience-grounded perspective and a structured pathway. Our project page\ncan be found at Github.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09154v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09154v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.346,
      "datasets_score": 0.38,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a neuroscience-inspired framework for agentic spatial intelligence, focusing on modules like multimodal sensing and cognitive maps, without any mention of reinforcement learning, human feedback, or training models based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses spatial reasoning through neuroscience principles and cognitive processes, but it does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning paths as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09155",
      "title": "HISPASpoof: A New Dataset For Spanish Speech Forensics",
      "authors": [
        "Maria Risques",
        "Kratika Bhagtani",
        "Amit Kumar Singh Yadav",
        "Edward J. Delp"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced\nrapidly, enabling the generation of highly realistic synthetic speech and\nraising serious concerns about their misuse. While numerous detectors have been\ndeveloped for English and Chinese, Spanish-spoken by over 600 million people\nworldwide-remains underrepresented in speech forensics. To address this gap, we\nintroduce HISPASpoof, the first large-scale Spanish dataset designed for\nsynthetic speech detection and attribution. It includes real speech from public\ncorpora across six accents and synthetic speech generated with six zero-shot\nTTS systems. We evaluate five representative methods, showing that detectors\ntrained on English fail to generalize to Spanish, while training on HISPASpoof\nsubstantially improves detection. We also evaluate synthetic speech attribution\nperformance on HISPASpoof, i.e., identifying the generation method of synthetic\nspeech. HISPASpoof thus provides a critical benchmark for advancing reliable\nand inclusive speech forensics in Spanish.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09155v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09155v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.289,
      "distributed_training_score": 0.33,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09157",
      "title": "RT-DETR++ for UAV Object Detection",
      "authors": [
        "Yuan Shufang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Object detection in unmanned aerial vehicle (UAV) imagery presents\nsignificant challenges. Issues such as densely packed small objects, scale\nvariations, and occlusion are commonplace. This paper introduces RT-DETR++,\nwhich enhances the encoder component of the RT-DETR model. Our improvements\nfocus on two key aspects. First, we introduce a channel-gated attention-based\nupsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes\nerrors and preserves details during feature layer propagation. Second, we\nincorporate CSP-PAC during feature fusion. This technique employs parallel\nhollow convolutions to process local and contextual information within the same\nlayer, facilitating the integration of multi-scale features. Evaluation\ndemonstrates that our novel neck design achieves superior performance in\ndetecting small and densely packed objects. The model maintains sufficient\nspeed for real-time detection without increasing computational complexity. This\nstudy provides an effective approach for feature encoding design in real-time\ndetection systems.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09157v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09157v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.422,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing an object detection model (RT-DETR++) for UAV imagery using attention-based mechanisms and feature fusion, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. It is purely about visual feature processing for detection.",
      "distributed_training_justification": "The paper addresses improvements to the RT-DETR model for real-time object detection on UAVs, emphasizing efficiency in inference through architectural changes, but it does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation to accelerate training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09159",
      "title": "A Knowledge Noise Mitigation Framework for Knowledge-based Visual\n  Question Answering",
      "authors": [
        "Zhiyue Liu",
        "Sihang Liu",
        "Jinyuan Liu",
        "Xinru Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Knowledge-based visual question answering (KB-VQA) requires a model to\nunderstand images and utilize external knowledge to provide accurate answers.\nExisting approaches often directly augment models with retrieved information\nfrom knowledge sources while ignoring substantial knowledge redundancy, which\nintroduces noise into the answering process. To address this, we propose a\ntraining-free framework with knowledge focusing for KB-VQA, that mitigates the\nimpact of noise by enhancing knowledge relevance and reducing redundancy.\nFirst, for knowledge retrieval, our framework concludes essential parts from\nthe image-question pairs, creating low-noise queries that enhance the retrieval\nof highly relevant knowledge. Considering that redundancy still persists in the\nretrieved knowledge, we then prompt large models to identify and extract\nanswer-beneficial segments from knowledge. In addition, we introduce a\nselective knowledge integration strategy, allowing the model to incorporate\nknowledge only when it lacks confidence in answering the question, thereby\nmitigating the influence of redundant information. Our framework enables the\nacquisition of accurate and critical knowledge, and extensive experiments\ndemonstrate that it outperforms state-of-the-art methods.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09159v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09159v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.435,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.318,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper proposes a training-free framework for KB-VQA that focuses on knowledge retrieval and noise mitigation using existing models, without any involvement in generating or using weak labels for training. It does not rely on programmatically generated noisy labels or weak supervision techniques, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "The paper does not utilize diffusion models or iterative refinement processes for reasoning tasks. It employs VLMs and LLMs for knowledge focusing and selective integration, with no mention of adapting diffusion for multi-step logical reasoning or Chain-of-Thought refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09160",
      "title": "Target-oriented Multimodal Sentiment Classification with\n  Counterfactual-enhanced Debiasing",
      "authors": [
        "Zhiyue Liu",
        "Fanrong Ma",
        "Xin Ling"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Target-oriented multimodal sentiment classification seeks to predict\nsentiment polarity for specific targets from image-text pairs. While existing\nworks achieve competitive performance, they often over-rely on textual content\nand fail to consider dataset biases, in particular word-level contextual\nbiases. This leads to spurious correlations between text features and output\nlabels, impairing classification accuracy. In this paper, we introduce a novel\ncounterfactual-enhanced debiasing framework to reduce such spurious\ncorrelations. Our framework incorporates a counterfactual data augmentation\nstrategy that minimally alters sentiment-related causal features, generating\ndetail-matched image-text samples to guide the model's attention toward content\ntied to sentiment. Furthermore, for learning robust features from\ncounterfactual data and prompting model decisions, we introduce an adaptive\ndebiasing contrastive learning mechanism, which effectively mitigates the\ninfluence of biased words. Experimental results on several benchmark datasets\nshow that our proposed method outperforms state-of-the-art baselines.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09160v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09160v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.341,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution focuses on counterfactual data augmentation and debiasing for multimodal sentiment classification, aiming to reduce biases in image-text pairs. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09163",
      "title": "CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain\n  Convolution",
      "authors": [
        "Yulin Tong",
        "Fengzong Zhang",
        "Haiqin Cheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Hyperspectral remote sensing technology has significant application value in\nfields such as forestry ecology and precision agriculture, while also putting\nforward higher requirements for fine ground object classification. However,\nalthough hyperspectral images are rich in spectral information and can improve\nrecognition accuracy, they tend to cause prominent feature redundancy due to\ntheir numerous bands, high dimensionality, and spectral mixing characteristics.\nTo address this, this study used hyperspectral images from the ZY1F satellite\nas a data source and selected Yugan County, Shangrao City, Jiangxi Province as\nthe research area to perform ground object classification research. A\nclassification framework named CWSSNet was proposed, which integrates 3D\nspectral-spatial features and wavelet convolution. This framework integrates\nmultimodal information us-ing a multiscale convolutional attention module and\nbreaks through the classification performance bottleneck of traditional methods\nby introducing multi-band decomposition and convolution operations in the\nwavelet domain. The experiments showed that CWSSNet achieved 74.50\\%, 82.73\\%,\nand 84.94\\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and\nmean F1-score (mF1) respectively in Yugan County. It also obtained the highest\nIntersection over Union (IoU) in the classifica-tion of water bodies,\nvegetation, and bare land, demonstrating good robustness. Additionally, when\nthe training set proportion was 70\\%, the increase in training time was\nlimited, and the classification effect was close to the optimal level,\nindicating that the model maintains reliable performance under small-sample\ntraining conditions.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09163v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09163v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.259,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.284,
      "distributed_training_score": 0.315,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09168",
      "title": "Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in\n  Semantic Communication",
      "authors": [
        "Omar Erak",
        "Omar Alhussein",
        "Hatem Abou-Zeid",
        "Mehdi Bennis"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Large-scale transformer models have emerged as a powerful tool for semantic\ncommunication systems, enabling edge devices to extract rich representations\nfor robust inference across noisy wireless channels. However, their substantial\ncomputational demands remain a major barrier to practical deployment in\nresource-constrained 6G networks. In this paper, we present a training-free\nframework for adaptive token merging in pretrained vision transformers to\njointly reduce inference time and transmission resource usage. We formulate the\nselection of per-layer merging proportions as a multi-objective optimization\nproblem to balance accuracy and computational cost. We employ Gaussian\nprocess-based Bayesian optimization to construct a Pareto frontier of optimal\nconfigurations, enabling flexible runtime adaptation to dynamic application\nrequirements and channel conditions. Extensive experiments demonstrate that our\nmethod consistently outperforms other baselines and achieves significant\nreductions in floating-point operations while maintaining competitive accuracy\nacross a wide range of signal-to-noise ratio (SNR) conditions. Additional\nresults highlight the effectiveness of adaptive policies that adjust merging\naggressiveness in response to channel quality, providing a practical mechanism\nto trade off latency and semantic fidelity on demand. These findings establish\na scalable and efficient approach for deploying transformer-based semantic\ncommunication in future edge intelligence systems.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09168v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09168v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.486,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adaptive token merging in pretrained vision transformers for semantic communication in edge devices, emphasizing multi-objective optimization and Bayesian optimization to reduce computational costs. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper addresses inference optimization for pretrained transformer models in semantic communication, including techniques like token merging and Bayesian optimization for edge deployment. It does not discuss distributed training, parallel computing, multi-node setups, or strategies for accelerating model training across processors, as its focus is solely on inference efficiency rather than training processes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09172",
      "title": "Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking\n  AI-Generated Image Detection in Challenging Scenarios",
      "authors": [
        "Chunxiao Li",
        "Xiaoxiao Wang",
        "Meiling Li",
        "Boming Miao",
        "Peng Sun",
        "Yunjian Zhang",
        "Xiangyang Ji",
        "Yao Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With the rapid advancement of generative models, highly realistic image\nsynthesis has posed new challenges to digital security and media credibility.\nAlthough AI-generated image detection methods have partially addressed these\nconcerns, a substantial research gap remains in evaluating their performance\nunder complex real-world conditions. This paper introduces the Real-World\nRobustness Dataset (RRDataset) for comprehensive evaluation of detection models\nacross three dimensions: 1) Scenario Generalization: RRDataset encompasses\nhigh-quality images from seven major scenarios (War and Conflict, Disasters and\nAccidents, Political and Social Events, Medical and Public Health, Culture and\nReligion, Labor and Production, and everyday life), addressing existing dataset\ngaps from a content perspective. 2) Internet Transmission Robustness: examining\ndetector performance on images that have undergone multiple rounds of sharing\nacross various social media platforms. 3) Re-digitization Robustness: assessing\nmodel effectiveness on images altered through four distinct re-digitization\nmethods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on\nRRDataset and conducted a large-scale human study involving 192 participants to\ninvestigate human few-shot learning capabilities in detecting AI-generated\nimages. The benchmarking results reveal the limitations of current AI detection\nmethods under real-world conditions and underscore the importance of drawing on\nhuman adaptability to develop more robust detection algorithms.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09172v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09172v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.446,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.372,
      "datasets_score": 0.487,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on benchmarking AI-generated image detection and human studies, but it does not involve training models using human feedback to fine-tune via reinforcement learning. While human performance is analyzed for inspiration, no RLHF methodology is applied.",
      "weak_supervision_justification": "The paper introduces a dataset for evaluation and benchmarks detection models, but it does not discuss training models with programmatically generated or noisy labels, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "The paper addresses detection of AI-generated images, potentially from diffusion models, but it does not adapt diffusion processes for multi-step logical reasoning tasks. The focus is on benchmarking detectors, not on diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and benchmarking of the RRDataset, including its curation for real-world scenarios, which directly aligns with research on developing, analyzing, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of existing benchmarks for AI-generated image detection by introducing the Real-World Robustness Dataset (RRDataset), which includes images from seven diverse scenarios and simulates real-world conditions like internet transmission and re-digitization processes. The authors benchmark 17 detection methods and 10 vision-language models, along with a human study involving 192 participants, revealing significant performance declines in AI detectors under these conditions and highlighting humans' superior adaptability through few-shot learning, thereby emphasizing the need for more robust detection algorithms.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new dataset and benchmarking approach that addresses gaps in real-world evaluation of AI-generated image detection, though it builds on existing techniques rather than introducing entirely new ones.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in AI-generated content detection by providing a more comprehensive benchmark, potentially leading to improved models in computer vision and digital security subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers significant contributions through its new dataset and human benchmarks, making it valuable for researchers focused on AI ethics and robust detection methods, though it is not essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a4c5a4f22b8b32c68d0ba93f7c0e1bfedc495023",
      "total_authors": 8,
      "authors_found": 7,
      "highest_h_index": 2,
      "average_h_index": 0.8571428571428571,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Chunxiao Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2320734988"
        },
        {
          "name": "Xiaoxiao Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2332307114"
        },
        {
          "name": "Meiling Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380408802"
        },
        {
          "name": "Boming Miao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2320668036"
        },
        {
          "name": "Peng Sun",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2331217056"
        },
        {
          "name": "Yunjian Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2331018914"
        },
        {
          "name": "Xiangyang Ji",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yao Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2320759381"
        }
      ]
    },
    {
      "id": "2509.09174",
      "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for\n  Speech-to-Speech LLMs",
      "authors": [
        "Yuhao Zhang",
        "Yuhao Du",
        "Zhanchen Dai",
        "Xiangnan Ma",
        "Kaiqi Kou",
        "Benyou Wang",
        "Haizhou Li"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.SD (Sound)"
      ],
      "abstract": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09174v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09174v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.442,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.403,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on mitigating the acoustic-semantic gap in SLLMs through EchoX, which involves semantic representations and speech token generation, but does not mention human feedback, reward models, or reinforcement learning for alignment.",
      "weak_supervision_justification": "The paper describes training SLLMs on speech data with semantic representations, but it does not involve programmatically generating labels from noisy or imprecise sources; instead, it relies on standard training data without indications of weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper addresses speech-to-speech modeling and acoustic-semantic integration, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "The paper discusses training EchoX on a specific amount of data but does not cover distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09177",
      "title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level\n  RL",
      "authors": [
        "Hanyi Mao",
        "Quanjia Xiao",
        "Lei Pang",
        "Haixiao Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping on\nthe importance-sampling (IS) weight. We study RL methods with sequence-level IS\nand identify a mismatch when PPO/GRPO-style clipping is transplanted to\nsequences: a fixed clip range systematically reweights short vs.\\ long\nresponses, distorting the optimization direction. FSPO introduces a simple\nremedy: we clip the sequence log-IS ratio with a band that scales as\n$\\sqrt{L}$. Theoretically, we formalize length fairness via a Length\nReweighting Error (LRE) and prove that small LRE yields a cosine directional\nguarantee between the clipped and true updates. Empirically, FSPO flattens clip\nrates across length bins, stabilizes training, and outperforms all baselines\nacross multiple evaluation datasets on Qwen3-8B-Base model.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09177v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09177v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.48,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.366,
      "datasets_score": 0.271,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on FSPO, a method for improving sequence-level reinforcement learning in LLMs by enforcing length fairness in clipping mechanisms, particularly for tasks like mathematical reasoning with verifiable rewards. While it involves RL techniques that could potentially be applied in RLHF pipelines, it does not mention or rely on human feedback, such as training a reward model with human-ranked data. Thus, it is related to the broader RL aspects of LLMs but not directly aligned with the core elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09183",
      "title": "Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection",
      "authors": [
        "Jiasheng Guo",
        "Xin Gao",
        "Yuxiang Yan",
        "Guanghao Li",
        "Jian Pu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Low-light Object detection is crucial for many real-world applications but\nremains challenging due to degraded image quality. While recent studies have\nshown that RAW images offer superior potential over RGB images, existing\napproaches either use RAW-RGB images with information loss or employ complex\nframeworks. To address these, we propose a lightweight and self-adaptive Image\nSignal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW\nimages in dark environments, enabling seamless end-to-end training for object\ndetection. Our key innovations are: (1) We deconstruct conventional ISP\npipelines into sequential linear (sensor calibration) and nonlinear (tone\nmapping) sub-modules, recasting them as differentiable components optimized\nthrough task-driven losses. Each module is equipped with content-aware\nadaptability and physics-informed priors, enabling automatic RAW-to-RGB\nconversion aligned with detection objectives. (2) By exploiting the ISP\npipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that\nfacilitates cooperation between sub-modules. Through extensive experiments on\nthree RAW image datasets, we demonstrate that our method outperforms\nstate-of-the-art RGB- and RAW-based detection approaches, achieving superior\nresults with minimal parameters in challenging low-light environments.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09183v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09183v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.361,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09190",
      "title": "VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal\n  Models: Methods and Results",
      "authors": [
        "Hanwei Zhu",
        "Haoning Wu",
        "Zicheng Zhang",
        "Lingyu Zhu",
        "Yixuan Li",
        "Peilin Chen",
        "Shiqi Wang",
        "Chris Wei Zhou",
        "Linhan Cao",
        "Wei Sun",
        "Xiangyang Zhu",
        "Weixia Zhang",
        "Yucheng Zhu",
        "Jing Liu",
        "Dandan Zhu",
        "Guangtao Zhai",
        "Xiongkuo Min",
        "Zhichao Zhang",
        "Xinyue Li",
        "Shubo Xu",
        "Anh Dao",
        "Yifan Li",
        "Hongyuan Yu",
        "Jiaojiao Yi",
        "Yiding Tian",
        "Yupeng Wu",
        "Feiran Sun",
        "Lijuan Liao",
        "Song Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper presents a summary of the VQualA 2025 Challenge on Visual Quality\nComparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025\nWorkshop on Visual Quality Assessment. The challenge aims to evaluate and\nenhance the ability of state-of-the-art LMMs to perform open-ended and detailed\nreasoning about visual quality differences across multiple images. To this end,\nthe competition introduces a novel benchmark comprising thousands of\ncoarse-to-fine grained visual quality comparison tasks, spanning single images,\npairs, and multi-image groups. Each task requires models to provide accurate\nquality judgments. The competition emphasizes holistic evaluation protocols,\nincluding 2AFC-based binary preference and multi-choice questions (MCQs).\nAround 100 participants submitted entries, with five models demonstrating the\nemerging capabilities of instruction-tuned LMMs on quality assessment. This\nchallenge marks a significant step toward open-domain visual quality reasoning\nand comparison and serves as a catalyst for future research on interpretable\nand human-aligned quality evaluation systems.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09190v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09190v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.234,
      "weak_supervision_score": 0.243,
      "diffusion_reasoning_score": 0.23,
      "distributed_training_score": 0.219,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09192",
      "title": "Probing Pre-trained Language Models on Code Changes: Insights from\n  ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset",
      "authors": [
        "Doha Nam",
        "Taehyoun Kim",
        "Duksan Ryu",
        "Jongmoon Baik"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Just-in-Time software defect prediction (JIT-SDP) plays a critical role in\nprioritizing risky code changes during code review and continuous integration.\nHowever, existing datasets often suffer from noisy labels and low precision in\nidentifying bug-inducing commits. To address this, we present ReDef\n(Revert-based Defect dataset), a high-confidence benchmark of function-level\nmodifications curated from 22 large-scale C/C++ projects. Defective cases are\nanchored by revert commits, while clean cases are validated through post-hoc\nhistory checks. Ambiguous instances are conservatively filtered out via a\nGPT-assisted triage process involving multiple votes and audits. This pipeline\nyields 3,164 defective and 10,268 clean modifications, offering substantially\nmore reliable labels than prior existing resources. Beyond dataset\nconstruction, we provide the first systematic evaluation of how pre-trained\nlanguage models (PLMs) reason about code modifications -- specifically, which\ninput encodings most effectively expose change information, and whether models\ngenuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder\nunder five encoding strategies, and further probe their sensitivity through\ncounterfactual perturbations that swap added/deleted blocks, invert diff\npolarity, or inject spurious markers. Our results show that compact diff-style\nencodings consistently outperform whole-function formats across all PLMs, with\nstatistical tests confirming large, model-independent effects. However, under\ncounterfactual tests, performance degrades little or not at all -- revealing\nthat what appears to be robustness in fact reflects reliance on superficial\ncues rather than true semantic understanding. These findings indicate that,\nunlike in snapshot-based tasks, current PLMs remain limited in their ability to\ngenuinely comprehend code modifications.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09192v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09192v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.376,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on creating a high-confidence dataset for software defect prediction and evaluating pre-trained language models (PLMs) on code changes, including using GPT for dataset filtering. However, it does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune models based on human preferences, which are core to RLHF.",
      "weak_supervision_justification": "The paper uses programmatically generated labels from revert commits and GPT-assisted triage to create a dataset, which aligns with weak supervision by relying on high-level, potentially noisy sources rather than perfect hand-labeled data. While this is part of the dataset construction, the main contribution is evaluating PLMs, not primarily advancing weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper evaluates PLMs for code change understanding and defect prediction through fine-tuning and perturbations, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces ReDef, a high-confidence dataset for Just-in-Time software defect prediction (JIT-SDP) derived from revert commits in 22 large-scale C/C++ projects, addressing the limitations of noisy labels in existing datasets by curating 3,164 defective and 10,268 clean function-level modifications through a GPT-assisted filtering process. It evaluates the performance of pre-trained language models (PLMs) such as CodeBERT, CodeT5+, and UniXcoder on code changes using five input encoding strategies and counterfactual perturbations, finding that diff-style encodings yield superior results but that PLMs primarily rely on superficial cues rather than true semantic understanding, highlighting limitations in current models for comprehending code modifications.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel dataset construction method using revert commits and conducts the first systematic evaluation of PLMs on code changes with counterfactual tests, significantly advancing the state-of-the-art in JIT-SDP and AI-driven code analysis.",
      "impact_score": "Moderate",
      "impact_justification": "The work provides a reliable benchmark and insights into PLM limitations, likely to be cited and built upon in software engineering and AI subfields for improving defect prediction models. However, its influence may be confined to specific areas like code change analysis rather than broader applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution through its high-quality dataset and empirical insights into PLM behavior, making it essential for researchers in software engineering and AI to understand current limitations in code change prediction.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2a415acbd3957f6a34b761d6a61af90898be0053",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 9,
      "average_h_index": 3.75,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Doha Nam",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380028509"
        },
        {
          "name": "Taehyoun Kim",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2278684339"
        },
        {
          "name": "Duksan Ryu",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/1912898"
        },
        {
          "name": "Jong-Chan Baik",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/98798518"
        }
      ]
    },
    {
      "id": "2509.09194",
      "title": "On Integrating Large Language Models and Scenario-Based Programming for\n  Improving Software Reliability",
      "authors": [
        "Ayelet Berzack",
        "Guy Katz"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are fast becoming indispensable tools for\nsoftware developers, assisting or even partnering with them in crafting complex\nprograms. The advantages are evident -- LLMs can significantly reduce\ndevelopment time, generate well-organized and comprehensible code, and\noccasionally suggest innovative ideas that developers might not conceive on\ntheir own. However, despite their strengths, LLMs will often introduce\nsignificant errors and present incorrect code with persuasive confidence,\npotentially misleading developers into accepting flawed solutions.\n  In order to bring LLMs into the software development cycle in a more reliable\nmanner, we propose a methodology for combining them with ``traditional''\nsoftware engineering techniques in a structured way, with the goal of\nstreamlining the development process, reducing errors, and enabling users to\nverify crucial program properties with increased confidence. Specifically, we\nfocus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,\nscenario-based approach for software engineering -- to allow human developers\nto pour their expert knowledge into the LLM, as well as to inspect and verify\nits outputs.\n  To evaluate our methodology, we conducted a significant case study, and used\nit to design and implement the Connect4 game. By combining LLMs and SBP we were\nable to create a highly-capable agent, which could defeat various strong\nexisting agents. Further, in some cases, we were able to formally verify the\ncorrectness of our agent. Finally, our experience reveals interesting insights\nregarding the ease-of-use of our proposed approach. The full code of our\ncase-study will be made publicly available with the final version of this\npaper.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09194v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09194v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.337,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a methodology using human feedback in a development workflow to guide LLMs for software reliability, involving iterative refinement and human oversight. However, it does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune the LLM, which are core to RLHF. Instead, it focuses on human-in-the-loop processes for code generation without any model training or alignment via RL.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses iterative refinement in the context of LLMs and Scenario-Based Programming for software development, but it does not adapt diffusion models or processes for multi-step logical reasoning. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction using diffusion techniques; the methodology relies on structured prompts and human verification, not diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09195",
      "title": "Breaking the Statistical Similarity Trap in Extreme Convection Detection",
      "authors": [
        "Md Tanveer Hossain Munim"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Current evaluation metrics for deep learning weather models create a\n\"Statistical Similarity Trap\", rewarding blurry predictions while missing rare,\nhigh-impact events. We provide quantitative evidence of this trap, showing\nsophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous\nconvection detection. We introduce DART (Dual Architecture for Regression\nTasks), a framework addressing the challenge of transforming coarse atmospheric\nforecasts into high-resolution satellite brightness temperature fields\noptimized for extreme convection detection (below 220 K). DART employs\ndual-decoder architecture with explicit background/extreme decomposition,\nphysically motivated oversampling, and task-specific loss functions. We present\nfour key findings: (1) empirical validation of the Statistical Similarity Trap\nacross multiple sophisticated baselines; (2) the \"IVT Paradox\", removing\nIntegrated Water Vapor Transport, widely regarded as essential for atmospheric\nriver analysis, improves extreme convection detection by 270%; (3)\narchitectural necessity demonstrated through operational flexibility (DART\nachieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent\nCSI), and (4) real-world validation with the August 2023 Chittagong flooding\ndisaster as a case study. To our knowledge, this is the first work to\nsystematically address this hybrid conversion-segmentation-downscaling task,\nwith no direct prior benchmarks identified in existing literature. Our\nvalidation against diverse statistical and deep learning baselines sufficiently\ndemonstrates DART's specialized design. The framework enables precise\noperational calibration through beta-tuning, trains in under 10 minutes on\nstandard hardware, and integrates seamlessly with existing meteorological\nworkflows, demonstrating a pathway toward trustworthy AI for extreme weather\npreparedness.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09195v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09195v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.401,
      "datasets_score": 0.404,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a deep learning framework for extreme convection detection in weather prediction, introducing a dual-decoder architecture and addressing issues like the Statistical Similarity Trap. It does not involve diffusion models, iterative refinement for logical reasoning, or treating chains-of-thought as entities for correction. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "The paper discusses training the DART framework in under 10 minutes on standard hardware but does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors. The core contributions are on model architecture and weather prediction, not acceleration techniques.",
      "datasets_justification": "The paper uses existing datasets like ERA5 for training and validation, and mentions benchmarking against baselines for the new task, but it does not primarily focus on creating, analyzing, or evaluating datasets. Its main contribution is the DART framework for weather prediction, with datasets serving only as supporting elements.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09196",
      "title": "Efficient Trie-based Biasing using K-step Prediction for Rare Word\n  Recognition",
      "authors": [
        "Chin Yuen Kwok",
        "Jia Qi yip"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Contextual biasing improves rare word recognition of ASR models by\nprioritizing the output of rare words during decoding. A common approach is\nTrie-based biasing, which gives \"bonus scores\" to partial hypothesis (e.g.\n\"Bon\") that may lead to the generation of the rare word (e.g. \"Bonham\"). If the\nfull word (\"Bonham\") isn't ultimately recognized, the system revokes those\nearlier bonuses. This revocation is limited to beam search and is\ncomputationally expensive, particularly for models with large decoders. To\novercome these limitations, we propose adapting ASR models to look ahead and\npredict multiple steps at once. This avoids the revocation step entirely by\nbetter estimating whether a partial hypothesis will lead to the generation of\nthe full rare word. By fine-tuning Whisper with only 10 hours of synthetic\ndata, our method reduces the word error rate on the NSC Part 2 test set from\n30.86% to 12.19%.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09196v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09196v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.327,
      "datasets_score": 0.23,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09197",
      "title": "Improving Synthetic Data Training for Contextual Biasing Models with a\n  Keyword-Aware Cost Function",
      "authors": [
        "Chin Yuen Kwok",
        "Jia Qi Yip",
        "Eng Siong Chng"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Rare word recognition can be improved by adapting ASR models to synthetic\ndata that includes these words. Further improvements can be achieved through\ncontextual biasing, which trains and adds a biasing module into the model\narchitecture to prioritize rare words. While training the module on synthetic\nrare word data is more effective than using non-rare-word data, it can lead to\noverfitting due to artifacts in the synthetic audio. To address this, we\nenhance the TCPGen-based contextual biasing approach and propose a\nkeyword-aware loss function that additionally focuses on biased words when\ntraining biasing modules. This loss includes a masked cross-entropy term for\nbiased word prediction and a binary classification term for detecting biased\nword positions. These two terms complementarily support the decoding of biased\nwords during inference. By adapting Whisper to 10 hours of synthetic data, our\nmethod reduced the word error rate on the NSC Part 2 test set from 29.71% to\n11.81%.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09197v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09197v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.448,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.364,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using synthetic data generated via text-to-speech (TTS) systems as a substitute for rare-word training data, which aligns directly with weak supervision. This approach programmatically generates large quantities of training labels or data that are noisy due to artifacts like unnatural prosody, rather than relying on hand-labeled data. The authors address overfitting from this noisy data by proposing a keyword-aware cost function, making the paper's focus on handling imperfect, programmatically derived supervision highly pertinent to weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of recognizing rare words in automatic speech recognition (ASR) by enhancing training on synthetic data using a contextual biasing model, specifically by proposing a keyword-aware cost function for the TCPGen module integrated with the Whisper model. The methodology involves adding a masked cross-entropy loss for predicting biased words and a binary classification loss for detecting their positions, which mitigates overfitting and improves performance; key findings include a significant reduction in word error rate from 29.71% to 11.81% on the NSC Part 2 test set, along with better interpretability and effectiveness in rare word recognition.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques, such as TCPGen and loss functions, to create a keyword-aware cost function that effectively reduces overfitting in synthetic data training for ASR, offering a notable improvement on known problems rather than a completely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of ASR and contextual biasing, as it provides a practical method for improving rare word recognition with synthetic data, potentially influencing related applications in speech technology.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong and valuable contribution to ASR research by demonstrating effective techniques for handling rare words, making it essential for researchers in computation and language or artificial intelligence to be aware of its insights and methods.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/28dd0f7493e4dd055e782f9c2bd14dec2f99ffa9",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 21,
      "average_h_index": 10.333333333333334,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Chin Yuen Kwok",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2319375447"
        },
        {
          "name": "J. Yip",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2179885989"
        },
        {
          "name": "E. Chng",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/2457835"
        }
      ]
    },
    {
      "id": "2509.09200",
      "title": "MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with\n  Recursive Refinement Network",
      "authors": [
        "Ge Sun",
        "Jun Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate human trajectory prediction is crucial for robotics navigation and\nautonomous driving. Recent research has demonstrated that incorporating goal\nguidance significantly enhances prediction accuracy by reducing uncertainty and\nleveraging prior knowledge. Most goal-guided approaches decouple the prediction\ntask into two stages: goal prediction and subsequent trajectory completion\nbased on the predicted goal, which operate at extreme granularities:\ncoarse-grained goal prediction forecasts the overall intention, while\nfine-grained trajectory completion needs to generate the positions for all\nfuture timesteps. The potential utility of intermediate temporal granularity\nremains largely unexplored, which motivates multi-granularity trajectory\nmodeling. While prior work has shown that multi-granularity representations\ncapture diverse scales of human dynamics and motion patterns, effectively\nintegrating this concept into goal-guided frameworks remains challenging. In\nthis paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for\nhuman Trajectory prediction. MGTraj recursively encodes trajectory proposals\nfrom coarse to fine granularity levels. At each level, a transformer-based\nrecursive refinement network (RRN) captures features and predicts progressive\nrefinements. Features across different granularities are integrated using a\nweight-sharing strategy, and velocity prediction is employed as an auxiliary\ntask to further enhance performance. Comprehensive experimental results in\nEHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline\nmethods and achieves state-of-the-art performance among goal-guided methods.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09200v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09200v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.374,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a recursive refinement network for multi-granularity human trajectory prediction, which involves iterative refinement of trajectory proposals. However, it does not involve diffusion models, noise-based processes, or adaptations for multi-step logical reasoning tasks. The method uses a transformer-based approach for trajectory refinement, which is unrelated to the iterative denoising or holistic Chain-of-Thought correction described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09204",
      "title": "Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection\n  Systems",
      "authors": [
        "Chin Yuen Kwok",
        "Jia Qi Yip",
        "Zhen Qiu",
        "Chi Hung Chi",
        "Kwok Yan Lam"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Audio deepfake detection (ADD) models are commonly evaluated using datasets\nthat combine multiple synthesizers, with performance reported as a single Equal\nError Rate (EER). However, this approach disproportionately weights\nsynthesizers with more samples, underrepresenting others and reducing the\noverall reliability of EER. Additionally, most ADD datasets lack diversity in\nbona fide speech, often featuring a single environment and speech style (e.g.,\nclean read speech), limiting their ability to simulate real-world conditions.\nTo address these challenges, we propose bona fide cross-testing, a novel\nevaluation framework that incorporates diverse bona fide datasets and\naggregates EERs for more balanced assessments. Our approach improves robustness\nand interpretability compared to traditional evaluation methods. We benchmark\nover 150 synthesizers across nine bona fide speech types and release a new\ndataset to facilitate further research at\nhttps://github.com/cyaaronk/audio_deepfake_eval.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09204v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09204v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.345,
      "datasets_score": 0.397,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09208",
      "title": "Incentivizing Safer Actions in Policy Optimization for Constrained\n  Reinforcement Learning",
      "authors": [
        "Somnath Hazra",
        "Pallab Dasgupta",
        "Soumyajit Dey"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Constrained Reinforcement Learning (RL) aims to maximize the return while\nadhering to predefined constraint limits, which represent domain-specific\nsafety requirements. In continuous control settings, where learning agents\ngovern system actions, balancing the trade-off between reward maximization and\nconstraint satisfaction remains a significant challenge. Policy optimization\nmethods often exhibit instability near constraint boundaries, resulting in\nsuboptimal training performance. To address this issue, we introduce a novel\napproach that integrates an adaptive incentive mechanism in addition to the\nreward structure to stay within the constraint bound before approaching the\nconstraint boundary. Building on this insight, we propose Incrementally\nPenalized Proximal Policy Optimization (IP3O), a practical algorithm that\nenforces a progressively increasing penalty to stabilize training dynamics.\nThrough empirical evaluation on benchmark environments, we demonstrate the\nefficacy of IP3O compared to the performance of state-of-the-art Safe RL\nalgorithms. Furthermore, we provide theoretical guarantees by deriving a bound\non the worst-case error of the optimality achieved by our algorithm.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09208v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09208v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.325,
      "datasets_score": 0.248,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of a novel algorithm, IP3O, for constrained reinforcement learning to handle safety constraints in environments like robotics and autonomous driving. It focuses on adaptive penalties for policy optimization without any involvement of human feedback, such as training reward models on human-ranked data or aligning AI with human preferences. Therefore, the paper does not address or relate to RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09210",
      "title": "ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint\n  Multi-agent Motion Forecasting",
      "authors": [
        "Xing Gao",
        "Zherui Huang",
        "Weiyao Lin",
        "Xiao Sun"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Accurate motion prediction of surrounding agents is crucial for the safe\nplanning of autonomous vehicles. Recent advancements have extended prediction\ntechniques from individual agents to joint predictions of multiple interacting\nagents, with various strategies to address complex interactions within future\nmotions of agents. However, these methods overlook the evolving nature of these\ninteractions. To address this limitation, we propose a novel progressive\nmulti-scale decoding strategy, termed ProgD, with the help of dynamic\nheterogeneous graph-based scenario modeling. In particular, to explicitly and\ncomprehensively capture the evolving social interactions in future scenarios,\ngiven their inherent uncertainty, we design a progressive modeling of scenarios\nwith dynamic heterogeneous graphs. With the unfolding of such dynamic\nheterogeneous graphs, a factorized architecture is designed to process the\nspatio-temporal dependencies within future scenarios and progressively\neliminate uncertainty in future motions of multiple agents. Furthermore, a\nmulti-scale decoding procedure is incorporated to improve on the future\nscenario modeling and consistent prediction of agents' future motion. The\nproposed ProgD achieves state-of-the-art performance on the INTERACTION\nmulti-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2\nmulti-world forecasting benchmark.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09210v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09210v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.469,
      "distributed_training_score": 0.421,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on progressive multi-scale decoding with dynamic graphs for multi-agent motion forecasting, emphasizing iterative graph-based prediction to refine trajectories. However, it does not involve diffusion models, iterative refinement for logical reasoning tasks, or treating a Chain-of-Thought as a single entity for holistic correction. There is no mention of diffusion-based techniques, making this topic unrelated.",
      "distributed_training_justification": "The paper's main contribution is a novel architecture for motion prediction using dynamic graphs, with no discussion of distributed training methods, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors. It addresses prediction techniques, not training acceleration or distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09215",
      "title": "Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges,\n  and Solutions",
      "authors": [
        "Qinnan Hu",
        "Yuntao Wang",
        "Yuan Gao",
        "Zhou Su",
        "Linkang Du"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Large language models (LLMs)-empowered autonomous agents are transforming\nboth digital and physical environments by enabling adaptive, multi-agent\ncollaboration. While these agents offer significant opportunities across\ndomains such as finance, healthcare, and smart manufacturing, their\nunpredictable behaviors and heterogeneous capabilities pose substantial\ngovernance and accountability challenges. In this paper, we propose a\nblockchain-enabled layered architecture for regulatory agent collaboration,\ncomprising an agent layer, a blockchain data layer, and a regulatory\napplication layer. Within this framework, we design three key modules: (i) an\nagent behavior tracing and arbitration module for automated accountability,\n(ii) a dynamic reputation evaluation module for trust assessment in\ncollaborative scenarios, and (iii) a malicious behavior forecasting module for\nearly detection of adversarial activities. Our approach establishes a\nsystematic foundation for trustworthy, resilient, and scalable regulatory\nmechanisms in large-scale agent ecosystems. Finally, we discuss the future\nresearch directions for blockchain-enabled regulatory frameworks in multi-agent\nsystems.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09215v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09215v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.378,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a blockchain-enabled architecture for regulating multi-agent collaborations, focusing on aspects like agent behavior tracing, reputation evaluation, and malicious behavior forecasting. It discusses LLMs in the context of autonomous agents but does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI models with human preferences. Therefore, there is no connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09219",
      "title": "Vejde: A Framework for Inductive Deep Reinforcement Learning Based on\n  Factor Graph Color Refinement",
      "authors": [
        "Jakob Nyberg",
        "Pontus Johnson"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present and evaluate Vejde; a framework which combines data abstraction,\ngraph neural networks and reinforcement learning to produce inductive policy\nfunctions for decision problems with richly structured states, such as object\nclasses and relations. MDP states are represented as data bases of facts about\nentities, and Vejde converts each state to a bipartite graph, which is mapped\nto latent states through neural message passing. The factored representation of\nboth states and actions allows Vejde agents to handle problems of varying size\nand structure. We tested Vejde agents on eight problem domains defined in RDDL,\nwith ten problem instances each, where policies were trained using both\nsupervised and reinforcement learning. To test policy generalization, we\nseparate problem instances in two sets, one for training and the other solely\nfor testing. Test results on unseen instances for the Vejde agents were\ncompared to MLP agents trained on each problem instance, as well as the online\nplanning algorithm Prost. Our results show that Vejde policies in average\ngeneralize to the test instances without a significant loss in score.\nAdditionally, the inductive agents received scores on unseen test instances\nthat on average were close to the instance-specific MLP agents.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09219v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09219v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.349,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a framework combining graph neural networks and reinforcement learning for inductive policies in structured decision problems, using supervised learning from agent-generated data and PPO for RL. It does not involve human feedback, such as ranking data to train a reward model, which is central to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes graph neural networks with neural message passing for state representation and policy learning, but it does not incorporate diffusion models, iterative refinement processes for logical reasoning, or treat reasoning paths as entities for holistic correction, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09227",
      "title": "Dynamic Structural Recovery Parameters Enhance Prediction of Visual\n  Outcomes After Macular Hole Surgery",
      "authors": [
        "Yinzheng Zhao",
        "Zhihao Zhao",
        "Rundong Jiang",
        "Louisa Sackewitz",
        "Quanmin Liang",
        "Mathias Maier",
        "Daniel Zapp",
        "Peter Charbel Issa",
        "Mohammad Ali Nasseri"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Purpose: To introduce novel dynamic structural parameters and evaluate their\nintegration within a multimodal deep learning (DL) framework for predicting\npostoperative visual recovery in idiopathic full-thickness macular hole (iFTMH)\npatients. Methods: We utilized a publicly available longitudinal OCT dataset at\nfive stages (preoperative, 2 weeks, 3 months, 6 months, and 12 months). A stage\nspecific segmentation model delineated related structures, and an automated\npipeline extracted quantitative, composite, qualitative, and dynamic features.\nBinary logistic regression models, constructed with and without dynamic\nparameters, assessed their incremental predictive value for best-corrected\nvisual acuity (BCVA). A multimodal DL model combining clinical variables,\nOCT-derived features, and raw OCT images was developed and benchmarked against\nregression models. Results: The segmentation model achieved high accuracy\nacross all timepoints (mean Dice > 0.89). Univariate and multivariate analyses\nidentified base diameter, ellipsoid zone integrity, and macular hole area as\nsignificant BCVA predictors (P < 0.05). Incorporating dynamic recovery rates\nconsistently improved logistic regression AUC, especially at the 3-month\nfollow-up. The multimodal DL model outperformed logistic regression, yielding\nhigher AUCs and overall accuracy at each stage. The difference is as high as\n0.12, demonstrating the complementary value of raw image volume and dynamic\nparameters. Conclusions: Integrating dynamic parameters into the multimodal DL\nmodel significantly enhances the accuracy of predictions. This fully automated\nprocess therefore represents a promising clinical decision support tool for\npersonalized postoperative management in macular hole surgery.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09227v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09227v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.307,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09232",
      "title": "Medverse: A Universal Model for Full-Resolution 3D Medical Image\n  Segmentation, Transformation and Enhancement",
      "authors": [
        "Jiesi Hu",
        "Jianfeng Cao",
        "Yanwu Yang",
        "Chenfei Ye",
        "Yixuan Zhang",
        "Hanyang Peng",
        "Ting Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In-context learning (ICL) offers a promising paradigm for universal medical\nimage analysis, enabling models to perform diverse image processing tasks\nwithout retraining. However, current ICL models for medical imaging remain\nlimited in two critical aspects: they cannot simultaneously achieve\nhigh-fidelity predictions and global anatomical understanding, and there is no\nunified model trained across diverse medical imaging tasks (e.g., segmentation\nand enhancement) and anatomical regions. As a result, the full potential of ICL\nin medical imaging remains underexplored. Thus, we present \\textbf{Medverse}, a\nuniversal ICL model for 3D medical imaging, trained on 22 datasets covering\ndiverse tasks in universal image segmentation, transformation, and enhancement\nacross multiple organs, imaging modalities, and clinical centers. Medverse\nemploys a next-scale autoregressive in-context learning framework that\nprogressively refines predictions from coarse to fine, generating consistent,\nfull-resolution volumetric outputs and enabling multi-scale anatomical\nawareness. We further propose a blockwise cross-attention module that\nfacilitates long-range interactions between context and target inputs while\npreserving computational efficiency through spatial sparsity. Medverse is\nextensively evaluated on a broad collection of held-out datasets covering\npreviously unseen clinical centers, organs, species, and imaging modalities.\nResults demonstrate that Medverse substantially outperforms existing ICL\nbaselines and establishes a novel paradigm for in-context learning. Code and\nmodel weights will be made publicly available. Our model are publicly available\nat https://github.com/jiesihu/Medverse.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09232v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09232v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.384,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09235",
      "title": "Virtual staining for 3D X-ray histology of bone implants",
      "authors": [
        "Sarah C. Irvine",
        "Christian Lucas",
        "Diana Krüger",
        "Bianca Guedert",
        "Julian Moosmann",
        "Berit Zeller-Plumhoff"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Three-dimensional X-ray histology techniques offer a non-invasive alternative\nto conventional 2D histology, enabling volumetric imaging of biological tissues\nwithout the need for physical sectioning or chemical staining. However, the\ninherent greyscale image contrast of X-ray tomography limits its biochemical\nspecificity compared to traditional histological stains. Within digital\npathology, deep learning-based virtual staining has demonstrated utility in\nsimulating stained appearances from label-free optical images. In this study,\nwe extend virtual staining to the X-ray domain by applying cross-modality image\ntranslation to generate artificially stained slices from\nsynchrotron-radiation-based micro-CT scans. Using over 50 co-registered image\npairs of micro-CT and toluidine blue-stained histology from bone-implant\nsamples, we trained a modified CycleGAN network tailored for limited paired\ndata. Whole slide histology images were downsampled to match the voxel size of\nthe CT data, with on-the-fly data augmentation for patch-based training. The\nmodel incorporates pixelwise supervision and greyscale consistency terms,\nproducing histologically realistic colour outputs while preserving\nhigh-resolution structural detail. Our method outperformed Pix2Pix and standard\nCycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the\nmodel can be applied to full CT volumes to generate virtually stained 3D\ndatasets, enhancing interpretability without additional sample preparation.\nWhile features such as new bone formation were able to be reproduced, some\nvariability in the depiction of implant degradation layers highlights the need\nfor further training data and refinement. This work introduces virtual staining\nto 3D X-ray imaging and offers a scalable route for chemically informative,\nlabel-free tissue characterisation in biomedical research.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09235v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09235v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.313,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09242",
      "title": "CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for\n  Gastric Tissue Classification",
      "authors": [
        "Mustafa Yurdakul",
        "Sakir Tasdemir"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Background and objective Early diagnosis of gastric diseases is crucial to\nprevent fatal outcomes. Although histopathologic examination remains the\ndiagnostic gold standard, it is performed entirely manually, making evaluations\nlabor-intensive and prone to variability among pathologists. Critical findings\nmay be missed, and lack of standard procedures reduces consistency. These\nlimitations highlight the need for automated, reliable, and efficient methods\nfor gastric tissue analysis. Methods In this study, a novel hybrid model named\nCoAtNeXt was proposed for the classification of gastric tissue images. The\nmodel is built upon the CoAtNet architecture by replacing its MBConv layers\nwith enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block\nAttention Module (CBAM) is integrated to improve local feature extraction\nthrough channel and spatial attention mechanisms. The architecture was scaled\nto achieve a balance between computational efficiency and classification\nperformance. CoAtNeXt was evaluated on two publicly available datasets,\nHMU-GC-HE-30K for eight-class classification and GasHisSDB for binary\nclassification, and was compared against 10 Convolutional Neural Networks\n(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved\n96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%\nAUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%\nprecision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all\nCNN and ViT models tested and surpassed previous studies in the literature.\nConclusion Experimental results show that CoAtNeXt is a robust architecture for\nhistopathological classification of gastric tissue images, providing\nperformance on binary and multiclass. Its highlights its potential to assist\npathologists by enhancing diagnostic accuracy and reducing workload.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09242v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09242v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.239,
      "weak_supervision_score": 0.25,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.266,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09245",
      "title": "Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and\n  Inference-Time Value-Guided Search",
      "authors": [
        "Shuocheng Li",
        "Yihao Liu",
        "Silin Du",
        "Wenxuan Zeng",
        "Zhe Xu",
        "Mengyu Zhou",
        "Yeye He",
        "Haoyu Dong",
        "Shi Han",
        "Dongmei Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have shown great promise in automating data\nscience workflows, but existing models still struggle with multi-step reasoning\nand tool use, which limits their effectiveness on complex data analysis tasks.\nTo address this, we propose a scalable pipeline that extracts high-quality,\ntool-based data analysis tasks and their executable multi-step solutions from\nreal-world Jupyter notebooks and associated data files. Using this pipeline, we\nintroduce NbQA, a large-scale dataset of standardized task-solution pairs that\nreflect authentic tool-use patterns in practical data science scenarios. To\nfurther enhance multi-step reasoning, we present Jupiter, a framework that\nformulates data analysis as a search problem and applies Monte Carlo Tree\nSearch (MCTS) to generate diverse solution trajectories for value model\nlearning. During inference, Jupiter combines the value model and node visit\ncounts to efficiently collect executable multi-step plans with minimal search\nsteps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on\nNbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,\nrespectively-matching or surpassing GPT-4o and advanced agent frameworks.\nFurther evaluations demonstrate improved generalization and stronger tool-use\nreasoning across diverse multi-step reasoning tasks.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09245v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09245v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.484,
      "distributed_training_score": 0.388,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves programmatically generating and filtering training data (tasks and solutions) from Jupyter notebooks using LLMs like GPT-4o mini, which aligns with weak supervision by relying on noisy or imprecise automated sources rather than hand-labeled data. However, this is a supporting aspect for dataset creation and not the main focus of the paper's contributions on multi-step reasoning and search frameworks.",
      "diffusion_reasoning_justification": "The paper focuses on Monte Carlo Tree Search (MCTS) for multi-step reasoning in data analysis, which is a search-based method and does not involve diffusion models or iterative refinement processes as defined for diffusion-based reasoning. There is no mention of adapting diffusion techniques for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Jupiter, a framework designed to improve large language models' (LLMs) capabilities in multi-step data analysis by creating NbQA, a dataset extracted from real-world Jupyter notebooks, and employing Monte Carlo Tree Search (MCTS) for value-guided search. The methodology involves crawling and processing notebooks to generate task-solution pairs, fine-tuning models on NbQA, and using MCTS to enhance reasoning, with key findings showing that fine-tuned Qwen2.5-7B and 14B models achieve 77.82% and 86.38% success rates on InfiAgent-DABench, respectively, matching or exceeding GPT-4o and demonstrating improved generalization across benchmarks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel dataset (NbQA) derived from real Jupyter notebooks and a new framework (Jupiter) that applies MCTS for inference-time value-guided search, significantly advancing LLM capabilities in multi-step data analysis and tool use. This represents a true innovation by combining large-scale data extraction with search-based reasoning to address limitations in existing models.",
      "impact_score": "High",
      "impact_justification": "The work has high potential impact as it enhances open-source LLMs to match proprietary models like GPT-4o in data analysis tasks, likely influencing future research in automated data science workflows and tool integration. Its scalable pipeline and framework could lead to broader applications in AI-driven data processing and decision-making.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper is a high-quality contribution that offers valuable insights and methods for improving LLM-based data analysis, making it essential for researchers in AI and data science to be aware of its advancements. While not groundbreaking for all audiences, its practical implications and strong experimental results warrant attention from relevant fields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c8c3158df78811944732bbf3d5327178e1f93b1c",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 10,
      "average_h_index": 2.888888888888889,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shuocheng Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2364823958"
        },
        {
          "name": "Yihao Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2348795216"
        },
        {
          "name": "Silin Du",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381240758"
        },
        {
          "name": "Wenxuan Zeng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381146481"
        },
        {
          "name": "Zhe Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380293565"
        },
        {
          "name": "Mengyu Zhou",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/144203509"
        },
        {
          "name": "Yeye He",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2258672950"
        },
        {
          "name": "Haoyu Dong",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shi Han",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2285058007"
        },
        {
          "name": "Dongmei Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2258706371"
        }
      ]
    },
    {
      "id": "2509.09254",
      "title": "Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset\n  for Panoramic X-ray Analysis",
      "authors": [
        "Jing Hao",
        "Yuxuan Fan",
        "Yanpeng Sun",
        "Kaixin Guo",
        "Lizhuo Lin",
        "Jinrong Yang",
        "Qi Yong H. Ai",
        "Lun M. Wong",
        "Hao Tang",
        "Kuo Feng Hung"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Recent advances in large vision-language models (LVLMs) have demonstrated\nstrong performance on general-purpose medical tasks. However, their\neffectiveness in specialized domains such as dentistry remains underexplored.\nIn particular, panoramic X-rays, a widely used imaging modality in oral\nradiology, pose interpretative challenges due to dense anatomical structures\nand subtle pathological cues, which are not captured by existing medical\nbenchmarks or instruction datasets. To this end, we introduce MMOral, the first\nlarge-scale multimodal instruction dataset and benchmark tailored for panoramic\nX-ray interpretation. MMOral consists of 20,563 annotated images paired with\n1.3 million instruction-following instances across diverse task types,\nincluding attribute extraction, report generation, visual question answering,\nand image-grounded dialogue. In addition, we present MMOral-Bench, a\ncomprehensive evaluation suite covering five key diagnostic dimensions in\ndentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the\nbest-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing\nsignificant limitations of current models in this domain. To promote the\nprogress of this specific domain, we also propose OralGPT, which conducts\nsupervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated\nMMOral instruction dataset. Remarkably, a single epoch of SFT yields\nsubstantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a\n24.73% improvement. Both MMOral and OralGPT hold significant potential as a\ncritical foundation for intelligent dentistry and enable more clinically\nimpactful multimodal AI systems in the dental field. The dataset, model,\nbenchmark, and evaluation suite are available at\nhttps://github.com/isbrycee/OralGPT.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09254v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09254v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.347,
      "datasets_score": 0.425,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of MMOral, a new large-scale multimodal instruction dataset for panoramic X-ray analysis, along with its curation pipeline, annotation details, and associated benchmark (MMOral-Bench). It covers dataset creation, benchmarking, and evaluation of models on this dataset, directly aligning with research on creating, analyzing, and evaluating datasets for AI and machine learning applications in specialized domains.",
      "llm_score_status": "completed",
      "summary": "This paper introduces MMOral, a large-scale multimodal instruction dataset and benchmark comprising 20,563 annotated panoramic X-ray images with 1.3 million instances for tasks like attribute extraction, report generation, visual question answering, and dialogue, aimed at addressing the gap in AI for dental analysis. The authors evaluate 64 large vision-language models on MMOral-Bench, revealing significant limitations in current models, and propose OralGPT, a fine-tuned version of Qwen2.5-VL-7B that achieves a 24.73% performance improvement after one epoch of supervised fine-tuning, highlighting the dataset's potential to advance intelligent dentistry.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces the first large-scale dataset and benchmark specifically for panoramic X-ray analysis in dentistry, addressing a previously underexplored domain and significantly advancing the state-of-the-art in medical AI. This novel contribution provides new resources and methodologies that enable specialized model training and evaluation.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research in dental AI by providing a foundational dataset and benchmark, leading to improved models for clinical applications and broader adoption in oral radiology. Its open-source availability and demonstrated performance gains could drive advancements in multimodal AI systems for specialized medical fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers high-quality, domain-specific contributions that are valuable for researchers in medical AI and computer vision, providing essential tools and insights for advancing dentistry applications. While not universally essential, it is highly relevant for those working in vision-language models for healthcare.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8f80e3a52457f68e44c740a69056d7a14d8d6583",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 8,
      "average_h_index": 1.1,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Jing Hao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380968778"
        },
        {
          "name": "Yuxuan Fan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356186184"
        },
        {
          "name": "Yanpeng Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380625180"
        },
        {
          "name": "Kaixin Guo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380434984"
        },
        {
          "name": "Lizhuo Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380082585"
        },
        {
          "name": "Jinrong Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380077519"
        },
        {
          "name": "Qi Yong H. Ai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370477748"
        },
        {
          "name": "L. M. Wong",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/115698397"
        },
        {
          "name": "Hao Tang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380084306"
        },
        {
          "name": "Kuo Feng Hung",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2281745621"
        }
      ]
    },
    {
      "id": "2509.09262",
      "title": "Adaptive Knowledge Distillation using a Device-Aware Teacher for\n  Low-Complexity Acoustic Scene Classification",
      "authors": [
        "Seung Gyu Jeong",
        "Seong Eun Kim"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this technical report, we describe our submission for Task 1,\nLow-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025\nChallenge. Our work tackles the dual challenges of strict complexity\nconstraints and robust generalization to both seen and unseen devices, while\nalso leveraging the new rule allowing the use of device labels at test time.\nOur proposed system is based on a knowledge distillation framework where an\nefficient CP-MobileNet student learns from a compact, specialized two-teacher\nensemble. This ensemble combines a baseline PaSST teacher, trained with\nstandard cross-entropy, and a 'generalization expert' teacher. This expert is\ntrained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted\nfrom prior work, which explicitly structures the feature space for device\nrobustness. To capitalize on the availability of test-time device labels, the\ndistilled student model then undergoes a final device-specific fine-tuning\nstage. Our proposed system achieves a final accuracy of 57.93\\% on the\ndevelopment set, demonstrating a significant improvement over the official\nbaseline, particularly on unseen devices.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09262v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09262v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.406,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves knowledge distillation, feature alignment, and device-specific fine-tuning for acoustic scene classification, with no discussion of distributed training, parallel computing, multi-node machine learning, or strategies for partitioning data/computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09263",
      "title": "DATE: Dynamic Absolute Time Enhancement for Long Video Understanding",
      "authors": [
        "Chao Yuan",
        "Yang Yang",
        "Yehui Yang",
        "Zach Cheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Long video understanding remains a fundamental challenge for multimodal large\nlanguage models (MLLMs), particularly in tasks requiring precise temporal\nreasoning and event localization. Existing approaches typically adopt uniform\nframe sampling and rely on implicit position encodings to model temporal order.\nHowever, these methods struggle with long-range dependencies, leading to\ncritical information loss and degraded temporal comprehension. In this paper,\nwe propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal\nawareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a\nsemantically guided Temporal-Aware Similarity Sampling (TASS) strategy.\nSpecifically, we interleave video frame embeddings with textual timestamp\ntokens to construct a continuous temporal reference system. We further\nreformulate the video sampling problem as a vision-language retrieval task and\nintroduce a two-stage algorithm to ensure both semantic relevance and temporal\ncoverage: enriching each query into a descriptive caption to better align with\nthe vision feature, and sampling key event with a similarity-driven temporally\nregularized greedy strategy. Our method achieves remarkable improvements w.r.t.\nabsolute time understanding and key event localization, resulting in\nstate-of-the-art performance among 7B and 72B models on hour-long video\nbenchmarks. Particularly, our 7B model even exceeds many 72B models on some\nbenchmarks.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09263v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09263v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.343,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution focuses on enhancing temporal awareness in multimodal large language models for long video understanding through mechanisms like Timestamp Injection and semantic-guided sampling. It does not involve diffusion models, iterative refinement processes, or any multi-step logical reasoning based on diffusion techniques. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09267",
      "title": "Unified Start, Personalized End: Progressive Pruning for Efficient 3D\n  Medical Image Segmentation",
      "authors": [
        "Linhao Li",
        "Yiwen Ye",
        "Ziyang Chen",
        "Yong Xia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D medical image segmentation often faces heavy resource and time\nconsumption, limiting its scalability and rapid deployment in clinical\nenvironments. Existing efficient segmentation models are typically static and\nmanually designed prior to training, which restricts their adaptability across\ndiverse tasks and makes it difficult to balance performance with resource\nefficiency. In this paper, we propose PSP-Seg, a progressive pruning framework\nthat enables dynamic and efficient 3D segmentation. PSP-Seg begins with a\nredundant model and iteratively prunes redundant modules through a combination\nof block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on\nfive public datasets, benchmarking it against seven state-of-the-art models and\nsix efficient segmentation models. Results demonstrate that the lightweight\nvariant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU\nmemory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%\nacross all datasets. These findings underscore PSP-Seg's potential as a\ncost-effective yet high-performing alternative for widespread clinical\napplication.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09267v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09267v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.432,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a progressive pruning framework for efficient 3D medical image segmentation, which focuses on dynamically reducing model size and resource usage during training on a single setup. It does not involve distributed training techniques, such as partitioning data or computation across multiple processors or nodes, parallel computing strategies, or multi-node machine learning algorithms. Therefore, there is no connection to the topic of distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09272",
      "title": "Fusing Knowledge and Language: A Comparative Study of Knowledge\n  Graph-Based Question Answering with LLMs",
      "authors": [
        "Vaibhav Chaudhary",
        "Neha Soni",
        "Narotam Singh",
        "Amita Kapoor"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Knowledge graphs, a powerful tool for structuring information through\nrelational triplets, have recently become the new front-runner in enhancing\nquestion-answering systems. While traditional Retrieval Augmented Generation\n(RAG) approaches are proficient in fact-based and local context-based\nextraction from concise texts, they encounter limitations when addressing the\nthematic and holistic understanding of complex, extensive texts, requiring a\ndeeper analysis of both text and context. This paper presents a comprehensive\ntechnical comparative study of three different methodologies for constructing\nknowledge graph triplets and integrating them with Large Language Models (LLMs)\nfor question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all\nleveraging open source technologies. We evaluate the effectiveness,\nfeasibility, and adaptability of these methods by analyzing their capabilities,\nstate of development, and their impact on the performance of LLM-based question\nanswering. Experimental results indicate that while OpenIE provides the most\ncomprehensive coverage of triplets, GraphRAG demonstrates superior reasoning\nabilities among the three. We conclude with a discussion on the strengths and\nlimitations of each method and provide insights into future directions for\nimproving knowledge graph-based question answering.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09272v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09272v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.464,
      "distributed_training_score": 0.292,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines knowledge graph-based question answering with LLMs, comparing methods like spaCy, Stanford CoreNLP-OpenIE, and GraphRAG for triplet construction. It discusses reasoning abilities in the context of these methods but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion, as required by the topic definition.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09284",
      "title": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for\n  Multistep Reasoning",
      "authors": [
        "Bingning Huang",
        "Tu Nguyen",
        "Matthieu Zimmer"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advances in reasoning with large language models (LLMs) have shown the\neffectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality\nintermediate trajectories, particularly in math and symbolic domains. Inspired\nby this, we explore how MCTS-derived trajectories, traditionally used for\ntraining value or reward models, can be repurposed to improve policy\noptimization in preference-based reinforcement learning (RL). Specifically, we\nfocus on Group Relative Policy Optimization (GRPO), a recent algorithm that\nenables preference-consistent policy learning without value networks. We\npropose a staged GRPO training paradigm where completions are derived from\npartially revealed MCTS rollouts, introducing a novel tree-structured setting\nfor advantage estimation. This leads to a rich class of prefix-conditioned\nreward signals, which we analyze theoretically and empirically. Our initial\nresults indicate that while structured advantage estimation can stabilize\nupdates and better reflect compositional reasoning quality, challenges such as\nadvantage saturation and reward signal collapse remain. We propose heuristic\nand statistical solutions to mitigate these issues and discuss open challenges\nfor learning under staged or tree-like reward structures.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09284v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09284v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.496,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.494,
      "distributed_training_score": 0.367,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Monte Carlo Tree Search (MCTS) trajectories for preference-based reinforcement learning with Group Relative Policy Optimization (GRPO), without involving human feedback or a reward model trained on human-ranked data. Since RLHF specifically requires human preferences and ranked data, this paper does not align with the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses Monte Carlo Tree Search (MCTS) for multistep reasoning and policy optimization, which is unrelated to diffusion models or their iterative refinement processes for logical tasks. There is no mention of diffusion-based methods or treating Chain-of-Thought as a single entity for refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09286",
      "title": "Visual Programmability: A Guide for Code-as-Thought in Chart\n  Understanding",
      "authors": [
        "Bohao Tang",
        "Yan Ma",
        "Fei Zhang",
        "Jiadi Su",
        "Ethan Chern",
        "Zhulin Hu",
        "Zhixin Wang",
        "Pengfei Liu",
        "Ya Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Chart understanding presents a critical test to the reasoning capabilities of\nVision-Language Models (VLMs). Prior approaches face critical limitations: some\nrely on external tools, making them brittle and constrained by a predefined\ntoolkit, while others fine-tune specialist models that often adopt a single\nreasoning strategy, such as text-based chain-of-thought (CoT). The intermediate\nsteps of text-based reasoning are difficult to verify, which complicates the\nuse of reinforcement-learning signals that reward factual accuracy. To address\nthis, we propose a Code-as-Thought (CaT) approach to represent the visual\ninformation of a chart in a verifiable, symbolic format. Our key insight is\nthat this strategy must be adaptive: a fixed, code-only implementation\nconsistently fails on complex charts where symbolic representation is\nunsuitable. This finding leads us to introduce Visual Programmability: a\nlearnable property that determines if a chart-question pair is better solved\nwith code or direct visual analysis. We implement this concept in an adaptive\nframework where a VLM learns to choose between the CaT pathway and a direct\nvisual reasoning pathway. The selection policy of the model is trained with\nreinforcement learning using a novel dual-reward system. This system combines a\ndata-accuracy reward to ground the model in facts and prevent numerical\nhallucination, with a decision reward that teaches the model when to use each\nstrategy, preventing it from defaulting to a single reasoning mode. Experiments\ndemonstrate strong and robust performance across diverse chart-understanding\nbenchmarks. Our work shows that VLMs can be taught not only to reason but also\nhow to reason, dynamically selecting the optimal reasoning pathway for each\ntask.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09286v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09286v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.525,
      "distributed_training_score": 0.355,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with a dual-reward system (data-accuracy and decision rewards) to train a VLM for adaptive reasoning, but it does not involve human feedback, a reward model trained on human-ranked data, or any alignment with human preferences. Instead, rewards are derived algorithmically, making it distinct from RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adaptive reasoning frameworks using reinforcement learning for chart understanding, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity. There is no component for multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09290",
      "title": "Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in\n  Multimodal MRI with Sequences Unavailable During Training",
      "authors": [
        "Anthony P. Addison",
        "Felix Wagner",
        "Wentian Xu",
        "Natalie Voets",
        "Konstantinos Kamnitsas"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Segmentation models are important tools for the detection and analysis of\nlesions in brain MRI. Depending on the type of brain pathology that is imaged,\nMRI scanners can acquire multiple, different image modalities (contrasts). Most\nsegmentation models for multimodal brain MRI are restricted to fixed modalities\nand cannot effectively process new ones at inference. Some models generalize to\nunseen modalities but may lose discriminative modality-specific information.\nThis work aims to develop a model that can perform inference on data that\ncontain image modalities unseen during training, previously seen modalities,\nand heterogeneous combinations of both, thus allowing a user to utilize any\navailable imaging modalities. We demonstrate this is possible with a simple,\nthus practical alteration to the U-net architecture, by integrating a\nmodality-agnostic input channel or pathway, alongside modality-specific input\nchannels. To train this modality-agnostic component, we develop an image\naugmentation scheme that synthesizes artificial MRI modalities. Augmentations\ndifferentially alter the appearance of pathological and healthy brain tissue to\ncreate artificial contrasts between them while maintaining realistic anatomical\nintegrity. We evaluate the method using 8 MRI databases that include 5 types of\npathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and\nwhite matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,\nDWI, ADC and FLAIR). The results demonstrate that the approach preserves the\nability to effectively process MRI modalities encountered during training,\nwhile being able to process new, unseen modalities to improve its segmentation.\nProject code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09290v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09290v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.359,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a modified U-net architecture for MRI brain lesion segmentation, focusing on handling unseen modalities through modality-agnostic channels and augmentations. It does not involve diffusion models, iterative refinement processes, or any application to complex logical tasks or chain-of-thought reasoning. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09292",
      "title": "LightAgent: Production-level Open-source Agentic AI Framework",
      "authors": [
        "Weige Cai",
        "Tong Zhu",
        "Jinyi Niu",
        "Ruiqi Hu",
        "Lingyao Li",
        "Tenglong Wang",
        "Xiaowu Dai",
        "Weining Shen",
        "Liwen Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the rapid advancement of large language models (LLMs), Multi-agent\nSystems (MAS) have achieved significant progress in various application\nscenarios. However, substantial challenges remain in designing versatile,\nrobust, and efficient platforms for agent deployment. To address these\nlimitations, we propose \\textbf{LightAgent}, a lightweight yet powerful agentic\nframework, effectively resolving the trade-off between flexibility and\nsimplicity found in existing frameworks. LightAgent integrates core\nfunctionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while\nmaintaining an extremely lightweight structure. As a fully open-source\nsolution, it seamlessly integrates with mainstream chat platforms, enabling\ndevelopers to easily build self-learning agents. We have released LightAgent at\n\\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09292v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09292v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.393,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a lightweight framework for multi-agent systems using LLMs, emphasizing features like memory, tools, and Tree of Thought for agent deployment. It does not involve training models with human feedback, reward models, or reinforcement learning techniques to align AI with human preferences, as required for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a framework with Tree of Thought (ToT) for task decomposition and reasoning, but it does not adapt diffusion models or their iterative refinement processes for multi-step logical tasks. There is no mention of treating a Chain-of-Thought as a holistic entity for correction via diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09297",
      "title": "Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable\n  UAV Perception",
      "authors": [
        "Spyridon Loukovitis",
        "Anastasios Arsenos",
        "Vasileios Karampinis",
        "Athanasios Voulodimos"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Open-set detection is crucial for robust UAV autonomy in air-to-air object\ndetection under real-world conditions. Traditional closed-set detectors degrade\nsignificantly under domain shifts and flight data corruption, posing risks to\nsafety-critical applications. We propose a novel, model-agnostic open-set\ndetection framework designed specifically for embedding-based detectors. The\nmethod explicitly handles unknown object rejection while maintaining robustness\nagainst corrupted flight data. It estimates semantic uncertainty via entropy\nmodeling in the embedding space and incorporates spectral normalization and\ntemperature scaling to enhance open-set discrimination. We validate our\napproach on the challenging AOT aerial benchmark and through extensive\nreal-world flight tests. Comprehensive ablation studies demonstrate consistent\nimprovements over baseline methods, achieving up to a 10\\% relative AUROC gain\ncompared to standard YOLO-based detectors. Additionally, we show that\nbackground rejection further strengthens robustness without compromising\ndetection accuracy, making our solution particularly well-suited for reliable\nUAV perception in dynamic air-to-air environments.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09297v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09297v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.356,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09298",
      "title": "Learning Object-Centric Representations in SAR Images with Multi-Level\n  Feature Fusion",
      "authors": [
        "Oh-Tae Jang",
        "Min-Gon Cho",
        "Kyung-Tae Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Synthetic aperture radar (SAR) images contain not only targets of interest\nbut also complex background clutter, including terrain reflections and speckle\nnoise. In many cases, such clutter exhibits intensity and patterns that\nresemble targets, leading models to extract entangled or spurious features.\nSuch behavior undermines the ability to form clear target representations,\nregardless of the classifier. To address this challenge, we propose a novel\nobject-centric learning (OCL) framework, named SlotSAR, that disentangles\ntarget representations from background clutter in SAR images without mask\nannotations. SlotSAR first extracts high-level semantic features from SARATR-X\nand low-level scattering features from the wavelet scattering network in order\nto obtain complementary multi-level representations for robust target\ncharacterization. We further present a multi-level slot attention module that\nintegrates these low- and high-level features to enhance slot-wise\nrepresentation distinctiveness, enabling effective OCL. Experimental results\ndemonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery\nby preserving structural details compared to existing OCL methods.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09298v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09298v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.323,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09307",
      "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization",
      "authors": [
        "Zhengzhao Lai",
        "Youbin Zheng",
        "Zhenyang Cai",
        "Haonan Lyu",
        "Jinpu Yang",
        "Hongqing Liang",
        "Yan Hu",
        "Benyou Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09307v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09307v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.36,
      "datasets_score": 0.433,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on introducing and evaluating a benchmark for MLLMs in materials characterization, without any discussion of training models using programmatically generated labels or weak supervision techniques. It does not address machine learning approaches involving noisy or imprecise labels.",
      "diffusion_reasoning_justification": "The paper mentions chain-of-thought prompting as a strategy for model evaluation, but it does not involve diffusion models or adapt iterative refinement processes for multi-step logical reasoning. There is no component that treats reasoning paths as entities for holistic correction using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of MatCha, a new benchmark dataset for evaluating MLLMs on materials characterization tasks, including its creation, task design, and analysis through model evaluations. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces MatCha, a comprehensive benchmark designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to understand and interpret materials characterization images, comprising 1,500 expert-level questions across 21 tasks that mirror real-world scientific workflows. It assesses state-of-the-art MLLMs through zero-shot, few-shot, and chain-of-thought methods, revealing significant performance gaps compared to human experts, particularly in complex tasks, and underscores the limitations of current models in adapting to materials science scenarios, thereby advocating for future enhancements.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark specifically for materials characterization image understanding, addressing an underexplored area in MLLMs and significantly advancing evaluation methods in AI for materials science.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfields of AI and materials science, as it provides a new tool for model evaluation and highlights key limitations, potentially influencing future developments in scientific AI applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by establishing a new benchmark that reveals critical gaps in MLLMs for materials science, making it essential for researchers in AI and related fields to be aware of its insights and implications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6d70907c32bd18821460c0c660f9c147271569d2",
      "total_authors": 8,
      "authors_found": 7,
      "highest_h_index": 5,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhengzhao Lai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380028503"
        },
        {
          "name": "Youbin Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380082930"
        },
        {
          "name": "Zhenyang Cai",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2308610920"
        },
        {
          "name": "Haonan Lyu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380030828"
        },
        {
          "name": "Jinpu Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380079377"
        },
        {
          "name": "Hongqing Liang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yan Hu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380141367"
        },
        {
          "name": "Benyou Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2343777007"
        }
      ]
    },
    {
      "id": "2509.09310",
      "title": "You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative\n  Perception",
      "authors": [
        "Hao Si",
        "Ehsan Javanmardi",
        "Manabu Tsukada"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Collaborative perception enables vehicles to overcome individual perception\nlimitations by sharing information, allowing them to see further and through\nocclusions. In real-world scenarios, models on different vehicles are often\nheterogeneous due to manufacturer variations. Existing methods for\nheterogeneous collaborative perception address this challenge by fine-tuning\nadapters or the entire network to bridge the domain gap. However, these methods\nare impractical in real-world applications, as each new collaborator must\nundergo joint training with the ego vehicle on a dataset before inference, or\nthe ego vehicle stores models for all potential collaborators in advance.\nTherefore, we pose a new question: Can we tackle this challenge directly during\ninference, eliminating the need for joint training? To answer this, we\nintroduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel\nframework that formulates the problem as few-shot unsupervised domain\nadaptation. Unlike previous work, PHCP dynamically aligns features by\nself-training an adapter during inference, eliminating the need for labeled\ndata and joint training. Extensive experiments on the OPV2V dataset demonstrate\nthat PHCP achieves strong performance across diverse heterogeneous scenarios.\nNotably, PHCP achieves performance comparable to SOTA methods trained on the\nentire dataset while using only a small amount of unlabeled data.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09310v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09310v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.413,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on unsupervised domain adaptation and self-training for collaborative perception in vehicles, with no mention of human feedback, reward models, or reinforcement learning techniques. It relies solely on unlabeled data from other agents, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper involves vehicles sharing data and features in a collaborative setup, which could loosely relate to distributed systems, but it primarily addresses inference-time adaptation rather than distributed training methods like parallel computing or multi-node model training for acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09311",
      "title": "Image Recognition with Vision and Language Embeddings of VLMs",
      "authors": [
        "Illia Volkov",
        "Nikita Kisel",
        "Klara Janouskova",
        "Jiri Matas"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-language models (VLMs) have enabled strong zero-shot classification\nthrough image-text alignment. Yet, their purely visual inference capabilities\nremain under-explored. In this work, we conduct a comprehensive evaluation of\nboth language-guided and vision-only image classification with a diverse set of\ndual-encoder VLMs, including both well-established and recent models such as\nSigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the\nImageNet-1k validation set and its label-corrected variant. The key factors\naffecting accuracy are analysed, including prompt design, class diversity, the\nnumber of neighbours in k-NN, and reference set size. We show that language and\nvision offer complementary strengths, with some classes favouring textual\nprompts and others better handled by visual similarity. To exploit this\ncomplementarity, we introduce a simple, learning-free fusion method based on\nper-class precision that improves classification performance. The code is\navailable at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09311v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09311v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.319,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates vision-language models (VLMs) for image classification, focusing on zero-shot capabilities, vision-only methods, and fusion techniques. It does not involve diffusion models, iterative refinement processes, or adaptations for logical reasoning tasks. There is no mention of chain-of-thought or multi-step logical processes, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09312",
      "title": "Explaining Tournament Solutions with Minimal Supports",
      "authors": [
        "Clément Contet",
        "Umberto Grandi",
        "Jérôme Mengin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Tournaments are widely used models to represent pairwise dominance between\ncandidates, alternatives, or teams. We study the problem of providing certified\nexplanations for why a candidate appears among the winners under various\ntournament rules. To this end, we identify minimal supports, minimal\nsub-tournaments in which the candidate is guaranteed to win regardless of how\nthe rest of the tournament is completed (that is, the candidate is a necessary\nwinner of the sub-tournament). This notion corresponds to an abductive\nexplanation for the question,\"Why does the winner win the tournament\", a\ncentral concept in formal explainable AI. We focus on common tournament\nsolutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,\nthe maximin rule, and the weighted uncovered set. For each rule we determine\nthe size of the smallest minimal supports, and we present polynomial-time\nalgorithms to compute them for all but the weighted uncovered set, for which\nthe problem is NP-complete. Finally, we show how minimal supports can serve to\nproduce compact, certified, and intuitive explanations.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09312v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09312v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.268,
      "weak_supervision_score": 0.258,
      "diffusion_reasoning_score": 0.327,
      "distributed_training_score": 0.187,
      "datasets_score": 0.191,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09314",
      "title": "Measuring Implicit Spatial Coordination in Teams: Effects on Collective\n  Intelligence and Performance",
      "authors": [
        "Thuy Ngoc Nguyen",
        "Anita Williams Woolley",
        "Cleotilde Gonzalez"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Coordinated teamwork is essential in fast-paced decision-making environments\nthat require dynamic adaptation, often without an opportunity for explicit\ncommunication. Although implicit coordination has been extensively considered\nin the existing literature, the majority of work has focused on co-located,\nsynchronous teamwork (such as sports teams) or, in distributed teams, primarily\non coordination of knowledge work. However, many teams (firefighters, military,\nlaw enforcement, emergency response) must coordinate their movements in\nphysical space without the benefit of visual cues or extensive explicit\ncommunication. This paper investigates how three dimensions of spatial\ncoordination, namely exploration diversity, movement specialization, and\nadaptive spatial proximity, influence team performance in a collaborative\nonline search and rescue task where explicit communication is restricted and\nteam members rely on movement patterns to infer others' intentions and\ncoordinate actions. Our metrics capture the relational aspects of teamwork by\nmeasuring spatial proximity, distribution patterns, and alignment of movements\nwithin shared environments. We analyze data from 34 four-person teams (136\nparticipants) assigned to specialized roles in a search and rescue task.\nResults show that spatial specialization positively predicts performance, while\nadaptive spatial proximity exhibits a marginal inverted U-shaped relationship,\nsuggesting moderate levels of adaptation are optimal. Furthermore, the temporal\ndynamics of these metrics differentiate high- from low-performing teams over\ntime. These findings provide insights into implicit spatial coordination in\nrole-based teamwork and highlight the importance of balanced adaptive\nstrategies, with implications for training and AI-assisted team support\nsystems.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09314v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09314v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.313,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09321",
      "title": "Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain\n  Expansion, and Metric Optimization",
      "authors": [
        "Hangyi Jia",
        "Yuxi Qian",
        "Hanwen Tong",
        "Xinhui Wu",
        "Lin Chen",
        "Feng Wei"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled the emergence of\ngeneral-purpose agents for automating end-to-end machine learning (ML)\nworkflows, including data analysis, feature engineering, model training, and\ncompetition solving. However, existing benchmarks remain limited in task\ncoverage, domain diversity, difficulty modeling, and evaluation rigor, failing\nto capture the full capabilities of such agents in realistic settings. We\npresent TAM Bench, a diverse, realistic, and structured benchmark for\nevaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three\nkey innovations: (1) A browser automation and LLM-based task acquisition system\nthat automatically collects and structures ML challenges from platforms such as\nKaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities\n(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty\nmodeling mechanism that estimates task complexity using participant counts and\nscore dispersion, enabling scalable and objective task calibration; (3) A\nmulti-dimensional evaluation framework incorporating performance, format\ncompliance, constraint adherence, and task generalization. Based on 150 curated\nAutoML tasks, we construct three benchmark subsets of different sizes -- Lite,\nMedium, and Full -- designed for varying evaluation scenarios. The Lite\nversion, with 18 tasks and balanced coverage across modalities and difficulty\nlevels, serves as a practical testbed for daily benchmarking and comparative\nstudies.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09321v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09321v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.401,
      "datasets_score": 0.469,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on creating a benchmark for evaluating LLM-based agents on ML tasks, including automated task collection and evaluation frameworks. It does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper describes automated systems for collecting and structuring ML tasks from platforms, but it does not address training models using programmatically generated labels from noisy sources. Its focus is on benchmark creation, not weak supervision methodologies.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper is centered on benchmarking LLM-based agents for ML workflows, including task acquisition and evaluation, and does not discuss parallel computing, multi-node training, or strategies for distributing model training across processors.",
      "datasets_justification": "The paper's main contribution is the development of TAM Bench, which involves creating, curating, and evaluating a diverse set of ML tasks from various platforms, including standardization, difficulty modeling, and benchmarking for AI applications. This directly aligns with research on datasets, benchmarks, and evaluation methodologies.",
      "llm_score_status": "completed",
      "summary": "This paper introduces TAM Bench, a novel benchmark designed to evaluate large language model (LLM)-based agents on end-to-end machine learning (ML) tasks by addressing limitations in existing benchmarks, such as manual data collection, imbalanced task types, inadequate difficulty modeling, and simplistic metrics. It employs a web-agent-driven system for automated task acquisition from platforms like Kaggle, a leaderboard-based mechanism for objective difficulty assessment, and a multi-dimensional evaluation framework that includes performance, format compliance, constraint adherence, and generalization, resulting in three benchmark subsets (Lite, Medium, and Full) based on 150 curated tasks to facilitate scalable and realistic agent testing.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining automated web-agent collection and leaderboard-based difficulty modeling to enhance existing benchmarks, though it builds on prior ideas rather than introducing a completely new problem. This clever integration addresses key gaps in task diversity and evaluation rigor without pioneering an entirely novel architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in AI benchmarking by providing a more comprehensive framework for evaluating LLM agents, potentially leading to wider adoption in subfields focused on ML workflows. However, its impact may be confined to specific areas like benchmark development rather than broader commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to AI research by improving benchmark methodologies, making it essential for those working on LLM agents and evaluation standards. While not groundbreaking enough to be a must-read, it provides practical insights that could enhance ongoing studies in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4671b06cd53ee3f26e6ce68562ffc7540e810f6e",
      "total_authors": 6,
      "authors_found": 5,
      "highest_h_index": 1,
      "average_h_index": 0.2,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Hangyi Jia",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380461075"
        },
        {
          "name": "Yuxi Qian",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Hanwen Tong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380029379"
        },
        {
          "name": "Xinhui Wu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2325313586"
        },
        {
          "name": "Lin Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2346435415"
        },
        {
          "name": "Feng Wei",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380356394"
        }
      ]
    },
    {
      "id": "2509.09324",
      "title": "Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark\n  and dataset from LMM",
      "authors": [
        "Hui Li",
        "Yi You",
        "Qiqi Chen",
        "Bingfeng Zhang",
        "George Q. Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Generative AI evolves the execution of complex workflows in industry, where\nthe large multimodal model empowers fashion design in the garment industry.\nCurrent generation AI models magically transform brainstorming into fancy\ndesigns easily, but the fine-grained customization still suffers from text\nuncertainty without professional background knowledge from end-users. Thus, we\npropose the Better Understanding Generation (BUG) workflow with LMM to\nautomatically create and fine-grain customize the cloth designs from chat with\nimage-into-prompt. Our framework unleashes users' creative potential beyond\nwords and also lowers the barriers of clothing design/editing without further\nhuman involvement. To prove the effectiveness of our model, we propose a new\nFashionEdit dataset that simulates the real-world clothing design workflow,\nevaluated from generation similarity, user satisfaction, and quality. The code\nand dataset: https://github.com/detectiveli/FashionEdit.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09324v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09324v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.333,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes Stable Diffusion 3, a diffusion-based model, for initial image generation and iterative refinement in fashion design, which involves multi-step processes similar to diffusion's iterative nature. However, it does not adapt this process for complex logical tasks or treat a Chain-of-Thought as a holistic entity for reasoning; instead, it focuses on creative image editing and customization, lacking a clear component for multi-step logical reasoning using diffusion models.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09327",
      "title": "Exploring Pre-training Across Domains for Few-Shot Surgical Skill\n  Assessment",
      "authors": [
        "Dimitrios Anastasiou",
        "Razvan Caramalau",
        "Nazir Sirajudeen",
        "Matthew Boal",
        "Philip Edwards",
        "Justin Collins",
        "John Kelly",
        "Ashwin Sridhar",
        "Maxine Tran",
        "Faiz Mumtaz",
        "Nevil Pavithran",
        "Nader Francis",
        "Danail Stoyanov",
        "Evangelos B. Mazomenos"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Automated surgical skill assessment (SSA) is a central task in surgical\ncomputer vision. Developing robust SSA models is challenging due to the\nscarcity of skill annotations, which are time-consuming to produce and require\nexpert consensus. Few-shot learning (FSL) offers a scalable alternative\nenabling model development with minimal supervision, though its success\ncritically depends on effective pre-training. While widely studied for several\nsurgical downstream tasks, pre-training has remained largely unexplored in SSA.\nIn this work, we formulate SSA as a few-shot task and investigate how\nself-supervised pre-training strategies affect downstream few-shot SSA\nperformance. We annotate a publicly available robotic surgery dataset with\nObjective Structured Assessment of Technical Skill (OSATS) scores, and evaluate\nvarious pre-training sources across three few-shot settings. We quantify domain\nsimilarity and analyze how domain gap and the inclusion of procedure-specific\ndata into pre-training influence transferability. Our results show that small\nbut domain-relevant datasets can outperform large scale, less aligned ones,\nachieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot\nsettings, respectively. Moreover, incorporating procedure-specific data into\npre-training with a domain-relevant external dataset significantly boosts\ndownstream performance, with an average gain of +1.22% in accuracy and +2.28%\nin F1-score; however, applying the same strategy with less similar but\nlarge-scale sources can instead lead to performance degradation. Code and\nmodels are available at https://github.com/anastadimi/ssa-fsl.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09327v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09327v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.362,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on few-shot learning and self-supervised pre-training for surgical skill assessment, which involves learning from unlabeled data to reduce the need for expert annotations. This aligns with weak supervision, as self-supervised methods programmatically generate pseudo-labels from data itself, minimizing reliance on precise hand-labeled data. However, the paper's primary emphasis is on few-shot learning and domain-specific pre-training rather than explicitly exploring weak supervision techniques like noisy label generation or rule-based labeling, making it moderately relevant.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of automated surgical skill assessment (SSA) by formulating it as a few-shot learning (FSL) task, leveraging self-supervised pre-training to minimize the need for extensive annotations. The authors annotate a robotic surgery dataset with OSATS scores and evaluate various pre-training strategies across 1-, 2-, and 5-shot settings, finding that domain-relevant datasets outperform larger but less aligned ones, with accuracies up to 73.65% in the 5-shot setting, and that incorporating procedure-specific data enhances performance by an average of +1.22% in accuracy and +2.28% in F1-score, though mismatched large-scale sources can degrade results.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel application of few-shot learning to surgical skill assessment, which is underexplored, and systematically evaluates self-supervised pre-training strategies for this task, significantly advancing the state-of-the-art in automated SSA.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in surgical computer vision by demonstrating effective pre-training for few-shot SSA, potentially leading to more scalable models in this subfield.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to machine learning in surgery through its innovative approach and practical findings, making it essential for researchers in computer vision and surgical data science to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9ed2918e399caf89e5f06a6bc551f035f3e2fd09",
      "total_authors": 14,
      "authors_found": 14,
      "highest_h_index": 12,
      "average_h_index": 3.4285714285714284,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "D. Anastasiou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2265325690"
        },
        {
          "name": "Razvan Caramalau",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/9352327"
        },
        {
          "name": "N. Sirajudeen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2308035700"
        },
        {
          "name": "M. Boal",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2343081273"
        },
        {
          "name": "Philip Edwards",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380032700"
        },
        {
          "name": "J. Collins",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2125078685"
        },
        {
          "name": "J. Kelly",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2266234852"
        },
        {
          "name": "A. Sridhar",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2239520059"
        },
        {
          "name": "M. Tran",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/31955494"
        },
        {
          "name": "Faiz Mumtaz",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313051852"
        },
        {
          "name": "N. Pavithran",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/6838844"
        },
        {
          "name": "Nader Francis",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2304148248"
        },
        {
          "name": "D. Stoyanov",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2327028473"
        },
        {
          "name": "Evangelos Mazomenos",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375324967"
        }
      ]
    },
    {
      "id": "2509.09332",
      "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and\n  Embodiment-aware Reasoning",
      "authors": [
        "Yuecheng Liu",
        "Dafeng Chi",
        "Shiguang Wu",
        "Zhanguang Zhang",
        "Yuzheng Zhuang",
        "Bowen Yang",
        "He Zhu",
        "Lingfeng Zhang",
        "Pengwei Xie",
        "David Gamaliel Arcos Bravo",
        "Yingxue Zhang",
        "Jianye Hao",
        "Xingyue Quan"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have opened new\nopportunities for embodied intelligence, enabling multimodal understanding,\nreasoning, and interaction, as well as continuous spatial decision-making.\nNevertheless, current MLLM-based embodied systems face two critical\nlimitations. First, Geometric Adaptability Gap: models trained solely on 2D\ninputs or with hard-coded 3D geometry injection suffer from either insufficient\nspatial information or restricted 2D generalization, leading to poor\nadaptability across tasks with diverse spatial demands. Second, Embodiment\nConstraint Gap: prior work often neglects the physical constraints and\ncapacities of real robots, resulting in task plans that are theoretically valid\nbut practically infeasible. To address these gaps, we introduce OmniEVA -- an\nembodied versatile planner that enables advanced embodied reasoning and task\nplanning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding\nmechanism, which introduces a gated router to perform explicit selective\nregulation of 3D fusion based on contextual requirements, enabling\ncontext-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware\nReasoning framework that jointly incorporates task goals and embodiment\nconstraints into the reasoning loop, resulting in planning decisions that are\nboth goal-directed and executable. Extensive experimental results demonstrate\nthat OmniEVA not only achieves state-of-the-art general embodied reasoning\nperformance, but also exhibits a strong ability across a wide range of\ndownstream scenarios. Evaluations of a suite of proposed embodied benchmarks,\nincluding both primitive and composite tasks, confirm its robust and versatile\nplanning capabilities. Project page: https://omnieva.github.io",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09332v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09332v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.321,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on OmniEVA, a framework for embodied reasoning that emphasizes task-adaptive 3D grounding and embodiment-aware reasoning using multimodal large language models. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical tasks. The main contributions are centered on spatial and physical constraints in embodied AI, with no mention of treating reasoning paths as entities for holistic correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09337",
      "title": "MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph\n  Experts",
      "authors": [
        "Junda Ye",
        "Zhongbao Zhang",
        "Li Sun",
        "Siqiang Luo"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While graph neural networks (GNNs) have achieved great success in learning\nfrom graph-structured data, their reliance on local, pairwise message passing\nrestricts their ability to capture complex, high-order subgraph patterns.\nleading to insufficient structural expressiveness. Recent efforts have\nattempted to enhance structural expressiveness by integrating random walk\nkernels into GNNs. However, these methods are inherently designed for\ngraph-level tasks, which limits their applicability to other downstream tasks\nsuch as node classification. Moreover, their fixed kernel configurations hinder\nthe model's flexibility in capturing diverse subgraph structures. To address\nthese limitations, this paper proposes a novel Mixture of Subgraph Experts\n(MoSE) framework for flexible and expressive subgraph-based representation\nlearning across diverse graph tasks. Specifically, MoSE extracts informative\nsubgraphs via anonymous walks and dynamically routes them to specialized\nexperts based on structural semantics, enabling the model to capture diverse\nsubgraph patterns with improved flexibility and interpretability. We further\nprovide a theoretical analysis of MoSE's expressivity within the Subgraph\nWeisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.\nExtensive experiments, together with visualizations of learned subgraph\nexperts, demonstrate that MoSE not only outperforms competitive baselines but\nalso provides interpretable insights into structural patterns learned by the\nmodel.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09337v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09337v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.322,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09349",
      "title": "Classification of Driver Behaviour Using External Observation Techniques\n  for Autonomous Vehicles",
      "authors": [
        "Ian Nell",
        "Shane Gilroy"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)",
        "cs.RO (Robotics)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Road traffic accidents remain a significant global concern, with human error,\nparticularly distracted and impaired driving, among the leading causes. This\nstudy introduces a novel driver behavior classification system that uses\nexternal observation techniques to detect indicators of distraction and\nimpairment. The proposed framework employs advanced computer vision\nmethodologies, including real-time object tracking, lateral displacement\nanalysis, and lane position monitoring. The system identifies unsafe driving\nbehaviors such as excessive lateral movement and erratic trajectory patterns by\nimplementing the YOLO object detection model and custom lane estimation\nalgorithms. Unlike systems reliant on inter-vehicular communication, this\nvision-based approach enables behavioral analysis of non-connected vehicles.\nExperimental evaluations on diverse video datasets demonstrate the framework's\nreliability and adaptability across varying road and environmental conditions.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09349v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09349v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.326,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09352",
      "title": "Texture-aware Intrinsic Image Decomposition with Model- and\n  Learning-based Priors",
      "authors": [
        "Xiaodong Wang",
        "Zijun He",
        "Xin Yuan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper aims to recover the intrinsic reflectance layer and shading layer\ngiven a single image. Though this intrinsic image decomposition problem has\nbeen studied for decades, it remains a significant challenge in cases of\ncomplex scenes, i.e. spatially-varying lighting effect and rich textures. In\nthis paper, we propose a novel method for handling severe lighting and rich\ntextures in intrinsic image decomposition, which enables to produce\nhigh-quality intrinsic images for real-world images. Specifically, we observe\nthat previous learning-based methods tend to produce texture-less and\nover-smoothing intrinsic images, which can be used to infer the lighting and\ntexture information given a RGB image. In this way, we design a texture-guided\nregularization term and formulate the decomposition problem into an\noptimization framework, to separate the material textures and lighting effect.\nWe demonstrate that combining the novel texture-aware prior can produce\nsuperior results to existing approaches.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09352v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09352v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.282,
      "datasets_score": 0.261,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09356",
      "title": "Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement\n  Learning",
      "authors": [
        "Abdel Hakim Drid",
        "Vincenzo Suriani",
        "Daniele Nardi",
        "Abderrezzak Debilou"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Navigating and understanding complex and unknown environments autonomously\ndemands more than just basic perception and movement from embodied agents.\nTruly effective exploration requires agents to possess higher-level cognitive\nabilities, the ability to reason about their surroundings, and make more\ninformed decisions regarding exploration strategies. However, traditional RL\napproaches struggle to balance efficient exploration and semantic understanding\ndue to limited cognitive capabilities embedded in the small policies for the\nagents, leading often to human drivers when dealing with semantic exploration.\nIn this paper, we address this challenge by presenting a novel Deep\nReinforcement Learning (DRL) architecture that is specifically designed for\nresource efficient semantic exploration. A key methodological contribution is\nthe integration of a Vision-Language Model (VLM) common-sense through a layered\nreward function. The VLM query is modeled as a dedicated action, allowing the\nagent to strategically query the VLM only when deemed necessary for gaining\nexternal guidance, thereby conserving resources. This mechanism is combined\nwith a curriculum learning strategy designed to guide learning at different\nlevels of complexity to ensure robust and stable learning. Our experimental\nevaluation results convincingly demonstrate that our agent achieves\nsignificantly enhanced object discovery rates and develops a learned capability\nto effectively navigate towards semantically rich regions. Furthermore, it also\nshows a strategic mastery of when to prompt for external environmental\ninformation. By demonstrating a practical and scalable method for embedding\ncommon-sense semantic reasoning with autonomous agents, this research provides\na novel approach to pursuing a fully intelligent and self-guided exploration in\nrobotics.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09356v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09356v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.459,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.481,
      "distributed_training_score": 0.349,
      "datasets_score": 0.398,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a DRL architecture with a layered reward function and VLM integration for autonomous exploration, without any mention of human feedback, ranked data, or a separate reward model trained on human preferences. Thus, it does not align with RLHF concepts.",
      "weak_supervision_justification": "The paper describes a DRL approach with rewards derived from environmental interactions and VLM queries, but it does not involve programmatically generating training labels from noisy or imprecise sources. There is no evidence of weak supervision techniques, such as using heuristics for label creation, in the methodology.",
      "diffusion_reasoning_justification": "The paper employs DRL and VLM for semantic exploration, including strategic querying and curriculum learning, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. No components resemble a 'Chain-of-Thought' refinement via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09365",
      "title": "Plug-and-play Diffusion Models for Image Compressive Sensing with Data\n  Consistency Projection",
      "authors": [
        "Xiaodong Wang",
        "Ping Wang",
        "Zhangyuan Li",
        "Xin Yuan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We explore the connection between Plug-and-Play (PnP) methods and Denoising\nDiffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a\nfocus on single-pixel imaging. We begin by identifying key distinctions between\nPnP and diffusion models-particularly in their denoising mechanisms and\nsampling procedures. By decoupling the diffusion process into three\ninterpretable stages: denoising, data consistency enforcement, and sampling, we\nprovide a unified framework that integrates learned priors with physical\nforward models in a principled manner. Building upon this insight, we propose a\nhybrid data-consistency module that linearly combines multiple PnP-style\nfidelity terms. This hybrid correction is applied directly to the denoised\nestimate, improving measurement consistency without disrupting the diffusion\nsampling trajectory. Experimental results on single-pixel imaging tasks\ndemonstrate that our method achieves better reconstruction quality.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09365v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09365v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.533,
      "distributed_training_score": 0.328,
      "datasets_score": 0.258,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on integrating Plug-and-Play methods with Diffusion Models for image compressive sensing, specifically for tasks like single-pixel imaging reconstruction. It involves iterative denoising and data consistency enforcement for inverse problems in computational imaging, but does not adapt diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. There is no component related to holistic correction of reasoning paths, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09368",
      "title": "A Fully Automatic Framework for Intracranial Pressure Grading:\n  Integrating Keyframe Identification, ONSD Measurement and Clinical Data",
      "authors": [
        "Pengxu Wen",
        "Tingting Yu",
        "Ziwei Nie",
        "Cheng Jiang",
        "Zhenyu Yin",
        "Mingyang He",
        "Bo Liao",
        "Xiaoping Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Intracranial pressure (ICP) elevation poses severe threats to cerebral\nfunction, thus necessitating monitoring for timely intervention. While lumbar\npuncture is the gold standard for ICP measurement, its invasiveness and\nassociated risks drive the need for non-invasive alternatives. Optic nerve\nsheath diameter (ONSD) has emerged as a promising biomarker, as elevated ICP\ndirectly correlates with increased ONSD. However, current clinical practices\nfor ONSD measurement suffer from inconsistency in manual operation,\nsubjectivity in optimal view selection, and variability in thresholding,\nlimiting their reliability. To address these challenges, we introduce a fully\nautomatic two-stage framework for ICP grading, integrating keyframe\nidentification, ONSD measurement and clinical data. Specifically, the fundus\nultrasound video processing stage performs frame-level anatomical segmentation,\nrule-based keyframe identification guided by an international consensus\nstatement, and precise ONSD measurement. The intracranial pressure grading\nstage then fuses ONSD metrics with clinical features to enable the prediction\nof ICP grades, thereby demonstrating an innovative blend of interpretable\nultrasound analysis and multi-source data integration for objective clinical\nevaluation. Experimental results demonstrate that our method achieves a\nvalidation accuracy of $0.845 \\pm 0.071$ (with standard deviation from\nfive-fold cross-validation) and an independent test accuracy of 0.786,\nsignificantly outperforming conventional threshold-based method ($0.637 \\pm\n0.111$ validation accuracy, $0.429$ test accuracy). Through effectively\nreducing operator variability and integrating multi-source information, our\nframework establishes a reliable non-invasive approach for clinical ICP\nevaluation, holding promise for improving patient management in acute\nneurological conditions.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09368v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09368v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.28,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.297,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09375",
      "title": "Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic\n  Normality",
      "authors": [
        "Botong Zhao",
        "Qijun Shi",
        "Shujing Lyu",
        "Yue Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained\ndefects that depress yield and reliability. Most industrial defect segmentation\ncompares a test image against an external normal set, a strategy that is\nbrittle for IC imagery where layouts vary across products and accurate\nalignment is difficult. We observe that defects are predominantly local, while\neach image still contains rich, repeatable normal patterns. We therefore\npropose an unsupervised IC defect segmentation framework that requires no\nexternal normal support. A learnable normal-information extractor aggregates\nrepresentative normal features from the test image, and a coherence loss\nenforces their association with normal regions. Guided by these features, a\ndecoder reconstructs only normal content; the reconstruction residual then\nsegments defects. Pseudo-anomaly augmentation further stabilizes training.\nExperiments on datasets from three IC process stages show consistent\nimprovements over existing approaches and strong robustness to product\nvariability.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09375v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09375v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.328,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper proposes an unsupervised framework for IC defect segmentation that uses synthetic anomaly generation to train a normal-information extractor, which involves programmatically creating training data similar to weak supervision. However, the main focus is on eliminating external references and leveraging image-intrinsic normality, rather than emphasizing weak supervision as a core methodology. Thus, it is only tangentially related.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09380",
      "title": "Robust Non-Linear Correlations via Polynomial Regression",
      "authors": [
        "Luca Giuliani",
        "Michele Lombardi"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)",
        "math.NA (Numerical Analysis)"
      ],
      "abstract": "The Hirschfeld-Gebelein-R\\'enyi (HGR) correlation coefficient is an extension\nof Pearson's correlation that is not limited to linear correlations, with\npotential applications in algorithmic fairness, scientific analysis, and causal\ndiscovery. Recently, novel algorithms to estimate HGR in a differentiable\nmanner have been proposed to facilitate its use as a loss regularizer in\nconstrained machine learning applications. However, the inherent\nuncomputability of HGR requires a bias-variance trade-off, which can possibly\ncompromise the robustness of the proposed methods, hence raising technical\nconcerns if applied in real-world scenarios. We introduce a novel computational\napproach for HGR that relies on user-configurable polynomial kernels, offering\ngreater robustness compared to previous methods and featuring a faster yet\nalmost equally effective restriction. Our approach provides significant\nadvantages in terms of robustness and determinism, making it a more reliable\noption for real-world applications. Moreover, we present a brief experimental\nanalysis to validate the applicability of our approach within a constrained\nmachine learning framework, showing that its computation yields an insightful\nsubgradient that can serve as a loss regularizer.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09380v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09380v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.308,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09387",
      "title": "MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for\n  Hyper-parameters Optimization",
      "authors": [
        "Mohammed Tiouti",
        "Mohamed Bal-Ghaoui"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Effective model and hyperparameter selection remains a major challenge in\ndeep learning, often requiring extensive expertise and computation. While\nAutoML and large language models (LLMs) promise automation, current LLM-based\napproaches rely on trial and error and expensive APIs, which provide limited\ninterpretability and generalizability. We propose MetaLLMiX, a zero-shot\nhyperparameter optimization framework combining meta-learning, explainable AI,\nand efficient LLM reasoning. By leveraging historical experiment outcomes with\nSHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained\nmodels without additional trials. We further employ an LLM-as-judge evaluation\nto control output format, accuracy, and completeness. Experiments on eight\nmedical imaging datasets using nine open-source lightweight LLMs show that\nMetaLLMiX achieves competitive or superior performance to traditional HPO\nmethods while drastically reducing computational cost. Our local deployment\noutperforms prior API-based approaches, achieving optimal results on 5 of 8\ntasks, response time reductions of 99.6-99.9%, and the fastest training times\non 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of\nbest-performing baselines.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09387v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09387v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.461,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.427,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a zero-shot HPO framework using meta-learning, LLMs, and XAI, with an LLM-as-judge for evaluation. However, it does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune models based on human feedback. The LLM-as-judge is for output control, not RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs LLMs for reasoning in HPO but does not adapt diffusion models or their iterative refinement processes for multi-step logical reasoning. It uses a zero-shot approach with meta-learning and SHAP, without any mention of treating Chain-of-Thought as a holistically corrected entity via diffusion.",
      "distributed_training_justification": "The paper discusses reducing computational costs and achieving faster training times (e.g., 2.4-15.7x faster) through efficient HPO and local deployment of smaller LLMs, which indirectly relates to computational efficiency. However, it does not focus on algorithms for partitioning data or computation across multiple nodes, as in distributed training or parallel computing.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09396",
      "title": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of\n  Self-Generated Counterfactual Explanations",
      "authors": [
        "Harry Mayne",
        "Ryan Othniel Kearns",
        "Yushi Yang",
        "Andrew M. Bean",
        "Eoin Delaney",
        "Chris Russell",
        "Adam Mahdi"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "To collaborate effectively with humans, language models must be able to\nexplain their decisions in natural language. We study a specific type of\nself-explanation: self-generated counterfactual explanations (SCEs), where a\nmodel explains its prediction by modifying the input such that it would have\npredicted a different outcome. We evaluate whether LLMs can produce SCEs that\nare valid, achieving the intended outcome, and minimal, modifying the input no\nmore than necessary. When asked to generate counterfactuals, we find that LLMs\ntypically produce SCEs that are valid, but far from minimal, offering little\ninsight into their decision-making behaviour. Worryingly, when asked to\ngenerate minimal counterfactuals, LLMs typically make excessively small edits\nthat fail to change predictions. The observed validity-minimality trade-off is\nconsistent across several LLMs, datasets, and evaluation settings. Our findings\nsuggest that SCEs are, at best, an ineffective explainability tool and, at\nworst, can provide misleading insights into model behaviour. Proposals to\ndeploy LLMs in high-stakes settings must consider the impact of unreliable\nself-explanations on downstream decision-making. Our code is available at\nhttps://github.com/HarryMayne/SCEs.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09396v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09396v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.278,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating the reliability of self-generated counterfactual explanations in LLMs, specifically their validity and minimality in decision-making contexts. It does not involve training AI models using human feedback, reward models, or reinforcement learning techniques, which are core to RLHF. Therefore, there is no connection to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines LLMs' ability to generate counterfactual explanations through prompting, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches. It lacks components for holistically correcting reasoning paths, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09397",
      "title": "Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot\n  Adaptation under Shift",
      "authors": [
        "Umaima Rahman",
        "Raza Imam",
        "Mohammad Yaqub",
        "Dwarikanath Mahapatra"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical vision-language models (VLMs) offer promise for clinical decision\nsupport, yet their reliability under distribution shifts remains a major\nconcern for safe deployment. These models often learn task-agnostic\ncorrelations due to variability in imaging protocols and free-text reports,\nlimiting their generalizability and increasing the risk of failure in\nreal-world settings. We propose DRiFt, a structured feature decoupling\nframework that explicitly separates clinically relevant signals from\ntask-agnostic noise using parameter-efficient tuning (LoRA) and learnable\nprompt tokens. To enhance cross-modal alignment and reduce uncertainty, we\ncurate high-quality, clinically grounded image-text pairs by generating\ncaptions for a diverse medical dataset. Our approach improves in-distribution\nperformance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based\nmethods, while maintaining strong robustness across unseen datasets. Ablation\nstudies reveal that disentangling task-relevant features and careful alignment\nsignificantly enhance model generalization and reduce unpredictable behavior\nunder domain shift. These insights contribute toward building safer, more\ntrustworthy VLMs for clinical use. The code is available at\nhttps://github.com/rumaima/DRiFt.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09397v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09397v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.402,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on feature decoupling in vision-language models for medical imaging using techniques like LoRA and caption generation, with no mention of human feedback, reward models, or reinforcement learning for alignment.",
      "weak_supervision_justification": "The paper involves programmatically generating captions for medical datasets to create training labels, which aligns with weak supervision by using high-level or automated sources rather than hand-labeled data. However, the primary focus is on feature decoupling and model robustness, not on weak supervision as a core methodology.",
      "diffusion_reasoning_justification": "The paper deals with vision-language models and feature decoupling for medical applications, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "The paper discusses parameter-efficient tuning with LoRA to reduce computational demands, but it does not address distributed training, parallel computing across nodes, or partitioning data/computation for acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces DRiFt, a framework designed to enhance the reliability of medical vision-language models (VLMs) under distribution shifts by decoupling clinically relevant features from task-agnostic noise using parameter-efficient tuning with LoRA and learnable prompt tokens. By curating high-quality, clinically grounded image-text pairs from the MedIMeta dataset, the method improves in-distribution performance with gains of +11.4% in Top-1 accuracy and +3.3% in Macro-F1 over prior approaches, while demonstrating stronger robustness and generalization across unseen datasets through explicit feature disentanglement and cross-modal alignment.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework for feature decoupling in medical VLMs, addressing a significant gap in handling distribution shifts by combining LoRA tuning and prompt learning in a way that advances state-of-the-art reliability in clinical applications.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in medical imaging and AI by providing a method for more robust VLMs, though its impact may be confined to specific subfields dealing with domain shifts rather than broader applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to improving the trustworthiness of medical AI models, making it essential for researchers in computer vision and healthcare AI to be aware of its insights and methods.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5802b9b4e4fb1ed187c44b716fd32a5a64488fcd",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 2.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Umaima Rahman",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2310606804"
        },
        {
          "name": "Raza Imam",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2370053022"
        },
        {
          "name": "Mohammad Yaqub",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2325156605"
        },
        {
          "name": "Dwarikanath Mahapatra",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2291135952"
        }
      ]
    },
    {
      "id": "2509.09414",
      "title": "We're Still Doing It (All) Wrong: Recommender Systems, Fifteen Years\n  Later",
      "authors": [
        "Alan Said",
        "Maria Soledad Pera",
        "Michael D. Ekstrand"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In 2011, Xavier Amatriain sounded the alarm: recommender systems research was\n\"doing it all wrong\" [1]. His critique, rooted in statistical misinterpretation\nand methodological shortcuts, remains as relevant today as it was then. But\nrather than correcting course, we added new layers of sophistication on top of\nthe same broken foundations. This paper revisits Amatriain's diagnosis and\nargues that many of the conceptual, epistemological, and infrastructural\nfailures he identified still persist, in more subtle or systemic forms. Drawing\non recent work in reproducibility, evaluation methodology, environmental\nimpact, and participatory design, we showcase how the field's accelerating\ncomplexity has outpaced its introspection. We highlight ongoing community-led\ninitiatives that attempt to shift the paradigm, including workshops, evaluation\nframeworks, and calls for value-sensitive and participatory research. At the\nsame time, we contend that meaningful change will require not only new metrics\nor better tooling, but a fundamental reframing of what recommender systems\nresearch is for, who it serves, and how knowledge is produced and validated.\nOur call is not just for technical reform, but for a recommender systems\nresearch agenda grounded in epistemic humility, human impact, and sustainable\npractice.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09414v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09414v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.293,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09424",
      "title": "ENSI: Efficient Non-Interactive Secure Inference for Large Language\n  Models",
      "authors": [
        "Zhiyu He",
        "Maojiang Wang",
        "Xinwen Gao",
        "Yuchuan Luo",
        "Lin Liu",
        "Shaojing Fu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Secure inference enables privacy-preserving machine learning by leveraging\ncryptographic protocols that support computations on sensitive user data\nwithout exposing it. However, integrating cryptographic protocols with large\nlanguage models (LLMs) presents significant challenges, as the inherent\ncomplexity of these protocols, together with LLMs' massive parameter scale and\nsophisticated architectures, severely limits practical usability. In this work,\nwe propose ENSI, a novel non-interactive secure inference framework for LLMs,\nbased on the principle of co-designing the cryptographic protocols and LLM\narchitecture. ENSI employs an optimized encoding strategy that seamlessly\nintegrates CKKS scheme with a lightweight LLM variant, BitNet, significantly\nreducing the computational complexity of encrypted matrix multiplications. In\nresponse to the prohibitive computational demands of softmax under homomorphic\nencryption (HE), we pioneer the integration of the sigmoid attention mechanism\nwith HE as a seamless, retraining-free alternative. Furthermore, by embedding\nthe Bootstrapping operation within the RMSNorm process, we efficiently refresh\nciphertexts while markedly decreasing the frequency of costly bootstrapping\ninvocations. Experimental evaluations demonstrate that ENSI achieves\napproximately an 8x acceleration in matrix multiplications and a 2.6x speedup\nin softmax inference on CPU compared to state-of-the-art method, with the\nproportion of bootstrapping is reduced to just 1%.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09424v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09424v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.424,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on secure inference for large language models using homomorphic encryption, optimizing computations like matrix multiplications and activation functions. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought.",
      "distributed_training_justification": "The paper discusses efficient secure inference for LLMs, including optimizations for CPU and GPU implementations, but it does not address distributed training, parallel computing for model training, or strategies for partitioning data/computation across multiple nodes during the training process.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09427",
      "title": "FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal\n  image fusion and super-resolution",
      "authors": [
        "Yuchan Jie",
        "Yushen Xu",
        "Xiaosong Li",
        "Fuqiang Zhou",
        "Jianming Lv",
        "Huafeng Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "As an influential information fusion and low-level vision technique, image\nfusion integrates complementary information from source images to yield an\ninformative fused image. A few attempts have been made in recent years to\njointly realize image fusion and super-resolution. However, in real-world\napplications such as military reconnaissance and long-range detection missions,\nthe target and background structures in multimodal images are easily corrupted,\nwith low resolution and weak semantic information, which leads to suboptimal\nresults in current fusion techniques. In response, we propose FS-Diff, a\nsemantic guidance and clarity-aware joint image fusion and super-resolution\nmethod. FS-Diff unifies image fusion and super-resolution as a conditional\ngeneration problem. It leverages semantic guidance from the proposed clarity\nsensing mechanism for adaptive low-resolution perception and cross-modal\nfeature extraction. Specifically, we initialize the desired fused result as\npure Gaussian noise and introduce the bidirectional feature Mamba to extract\nthe global features of the multimodal images. Moreover, utilizing the source\nimages and semantics as conditions, we implement a random iterative denoising\nprocess via a modified U-Net network. This network istrained for denoising at\nmultiple noise levels to produce high-resolution fusion results with\ncross-modal features and abundant semantic information. We also construct a\npowerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images.\nExtensive joint image fusion and super-resolution experiments on six public and\nour AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art\nmethods at multiple magnifications and can recover richer details and semantics\nin the fused images. The code is available at\nhttps://github.com/XylonXu01/FS-Diff.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09427v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09427v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.472,
      "distributed_training_score": 0.362,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using a denoising diffusion probabilistic model (DDPM) for image fusion and super-resolution, involving iterative denoising to generate high-resolution fused images from multimodal inputs. While it employs an iterative refinement process, this is applied to visual tasks like feature extraction and image enhancement, not to multi-step logical reasoning or treating a 'Chain-of-Thought' as a holistic entity for solving complex logical tasks. The core contribution is in computer vision, lacking any components for logical problem-solving or reasoning paths, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09429",
      "title": "Semantic Concentration for Self-Supervised Dense Representations\n  Learning",
      "authors": [
        "Peisong Wen",
        "Qianqian Xu",
        "Siran Dai",
        "Runmin Cong",
        "Qingming Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advances in image-level self-supervised learning (SSL) have made\nsignificant progress, yet learning dense representations for patches remains\nchallenging. Mainstream methods encounter an over-dispersion phenomenon that\npatches from the same instance/category scatter, harming downstream performance\non dense tasks. This work reveals that image-level SSL avoids over-dispersion\nby involving implicit semantic concentration. Specifically, the non-strict\nspatial alignment ensures intra-instance consistency, while shared patterns,\ni.e., similar parts of within-class instances in the input space, ensure\ninter-image consistency. Unfortunately, these approaches are infeasible for\ndense SSL due to their spatial sensitivity and complicated scene-centric data.\nThese observations motivate us to explore explicit semantic concentration for\ndense SSL. First, to break the strict spatial alignment, we propose to distill\nthe patch correspondences. Facing noisy and imbalanced pseudo labels, we\npropose a noise-tolerant ranking loss. The core idea is extending the Average\nPrecision (AP) loss to continuous targets, such that its decision-agnostic and\nadaptive focusing properties prevent the student model from being misled.\nSecond, to discriminate the shared patterns from complicated scenes, we propose\nthe object-aware filter to map the output space to an object-based space.\nSpecifically, patches are represented by learnable prototypes of objects via\ncross-attention. Last but not least, empirical studies across various tasks\nsoundly support the effectiveness of our method. Code is available in\nhttps://github.com/KID-7391/CoTAP.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09429v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09429v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.393,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves self-supervised learning where noisy and imbalanced pseudo labels are generated programmatically for patch correspondences, aligning with weak supervision's use of imprecise labels. However, the primary focus is on improving dense representations in SSL, not explicitly on weak supervision techniques, making it moderately relevant rather than central.",
      "diffusion_reasoning_justification": "The paper focuses on self-supervised learning for dense visual representations and does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes, so it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the over-dispersion issue in self-supervised dense representation learning by introducing explicit semantic concentration techniques to improve patch alignment and consistency. It proposes distilling patch correspondences using a noise-tolerant ranking loss called Continuous-Target Average Precision (CoTAP) to handle noisy pseudo labels, and an Object-Aware Filter to map features into an object-based space for capturing shared patterns across images. Empirical evaluations on various downstream tasks, such as semantic segmentation and object detection, demonstrate significant performance improvements, validating the effectiveness of the proposed framework.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by extending the Average Precision loss to continuous targets and introducing the Object-Aware Filter, which cleverly adapts existing ideas for dense SSL to address over-dispersion. While it builds on self-distillation frameworks, it offers a new combination that enhances inter-image consistency without relying on strict spatial alignment.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of dense self-supervised learning, as it tackles a key challenge in tasks like semantic segmentation. However, its influence may be limited to specific applications in computer vision rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a high-quality contribution with innovative techniques that advance dense SSL, making it valuable for researchers in computer vision and machine learning. It is significant but not essential for those outside this niche area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0b934046a5afad5fdecea1b781238720e3376928",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 26,
      "average_h_index": 10.6,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Peisong Wen",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/1941733102"
        },
        {
          "name": "Qianqian Xu",
          "h_index": 26,
          "profile_url": "https://www.semanticscholar.org/author/34679664"
        },
        {
          "name": "Siran Dai",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2265491263"
        },
        {
          "name": "Runmin Cong",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2258230512"
        },
        {
          "name": "Qingming Huang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2279863252"
        }
      ]
    },
    {
      "id": "2509.09448",
      "title": "TORSO: Template-Oriented Reasoning Towards General Tasks",
      "authors": [
        "Minhyuk Kim",
        "Seungyoon Lee",
        "Heuiseok Lim"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The approaches that guide Large Language Models (LLMs) to emulate human\nreasoning during response generation have emerged as an effective method for\nenabling them to solve complex problems in a step-by-step manner, thereby\nachieving superior performance. However, most existing approaches using\nfew-shot prompts to generate responses heavily depend on the provided examples,\nlimiting the utilization of the model's inherent reasoning capabilities.\nMoreover, constructing task-specific few-shot prompts is often costly and may\nlead to inconsistencies across different tasks. In this work, we introduce\nTemplate-Oriented Reasoning (TORSO), which elicits the model to utilize\ninternal reasoning abilities to generate proper responses across various tasks\nwithout the need for manually crafted few-shot examples. Our experimental\nresults demonstrate that TORSO achieves strong performance on diverse LLMs\nbenchmarks with reasonable rationales.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09448v3",
      "pdf_url": "http://arxiv.org/pdf/2509.09448v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.522,
      "distributed_training_score": 0.309,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces TORSO, a method using logit processing and token injection to guide LLMs for reasoning without few-shot prompts. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity over multiple steps, as defined for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09456",
      "title": "FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion\n  based on diffusion model",
      "authors": [
        "Yushen Xu",
        "Xiaosong Li",
        "Yuchun Wang",
        "Xiaoqi Cheng",
        "Huafeng Li",
        "Haishu Tan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Different modalities of medical images provide unique physiological and\nanatomical information for diseases. Multi-modal medical image fusion\nintegrates useful information from different complementary medical images with\ndifferent modalities, producing a fused image that comprehensively and\nobjectively reflects lesion characteristics to assist doctors in clinical\ndiagnosis. However, existing fusion methods can only handle a fixed number of\nmodality inputs, such as accepting only two-modal or tri-modal inputs, and\ncannot directly process varying input quantities, which hinders their\napplication in clinical settings. To tackle this issue, we introduce\nFlexiD-Fuse, a diffusion-based image fusion network designed to accommodate\nflexible quantities of input modalities. It can end-to-end process two-modal\nand tri-modal medical image fusion under the same weight. FlexiD-Fuse\ntransforms the diffusion fusion problem, which supports only fixed-condition\ninputs, into a maximum likelihood estimation problem based on the diffusion\nprocess and hierarchical Bayesian modeling. By incorporating the\nExpectation-Maximization algorithm into the diffusion sampling iteration\nprocess, FlexiD-Fuse can generate high-quality fused images with cross-modal\ninformation from source images, independently of the number of input images. We\ncompared the latest two and tri-modal medical image fusion methods, tested them\non Harvard datasets, and evaluated them using nine popular metrics. The\nexperimental results show that our method achieves the best performance in\nmedical image fusion with varying inputs. Meanwhile, we conducted extensive\nextension experiments on infrared-visible, multi-exposure, and multi-focus\nimage fusion tasks with arbitrary numbers, and compared them with the\nperspective SOTA methods. The results of the extension experiments consistently\ndemonstrate the effectiveness and superiority of our method.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09456v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09456v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.498,
      "distributed_training_score": 0.325,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for multi-modal medical image fusion, specifically adapting diffusion processes for image generation and integration. While it involves iterative refinement in the diffusion sampling process, this is applied to image fusion tasks, not to solving complex logical tasks or treating a 'Chain-of-Thought' as an entity for holistic correction and improvement. There is no component for multi-step logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09467",
      "title": "Inteligencia Artificial jurídica y el desafío de la veracidad:\n  análisis de alucinaciones, optimización de RAG y principios para una\n  integración responsable",
      "authors": [
        "Alex Dantart"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This technical report analyzes the challenge of \"hallucinations\" (false\ninformation) in LLMs applied to law. It examines their causes, manifestations,\nand the effectiveness of the RAG mitigation strategy, highlighting its\nlimitations and proposing holistic optimizations. The paper explores the\nethical and regulatory implications, emphasizing human oversight as an\nirreplaceable role. It concludes that the solution lies not in incrementally\nimproving generative models, but in adopting a \"consultative\" AI paradigm that\nprioritizes veracity and traceability, acting as a tool to amplify, not\nreplace, professional judgment.\n  --\n  Este informe t\\'ecnico analiza el desaf\\'io de las \"alucinaciones\"\n(informaci\\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,\nmanifestaciones y la efectividad de la estrategia de mitigaci\\'on RAG,\nexponiendo sus limitaciones y proponiendo optimizaciones hol\\'isticas. Se\nexploran las implicaciones \\'eticas y regulatorias, enfatizando la\nsupervisi\\'on humana como un rol insustituible. El documento concluye que la\nsoluci\\'on no reside en mejorar incrementalmente los modelos generativos, sino\nen adoptar un paradigma de IA \"consultiva\" que priorice la veracidad y la\ntrazabilidad, actuando como una herramienta para amplificar, y no sustituir, el\njuicio profesional.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09467v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09467v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.289,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09469",
      "title": "Resource-Efficient Glioma Segmentation on Sub-Saharan MRI",
      "authors": [
        "Freedmore Sidume",
        "Oumayma Soula",
        "Joseph Muthui Wacira",
        "YunFei Zhu",
        "Abbas Rabiu Muhammad",
        "Abderrazek Zeraii",
        "Oluwaseun Kalejaye",
        "Hajer Ibrahim",
        "Olfa Gaddour",
        "Brain Halubanza",
        "Dong Zhang",
        "Udunna C Anazodo",
        "Confidence Raymond"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Gliomas are the most prevalent type of primary brain tumors, and their\naccurate segmentation from MRI is critical for diagnosis, treatment planning,\nand longitudinal monitoring. However, the scarcity of high-quality annotated\nimaging data in Sub-Saharan Africa (SSA) poses a significant challenge for\ndeploying advanced segmentation models in clinical workflows. This study\nintroduces a robust and computationally efficient deep learning framework\ntailored for resource-constrained settings. We leveraged a 3D Attention UNet\narchitecture augmented with residual blocks and enhanced through transfer\nlearning from pre-trained weights on the BraTS 2021 dataset. Our model was\nevaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma\nsegmentation in SSA MRI data. Despite the limited data quality and quantity,\nour approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80\nfor Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding\nNon-Functional Hemisphere (SNFH). These results demonstrate the\ngeneralizability of the proposed model and its potential to support clinical\ndecision making in low-resource settings. The compact architecture,\napproximately 90 MB, and sub-minute per-volume inference time on consumer-grade\nhardware further underscore its practicality for deployment in SSA health\nsystems. This work contributes toward closing the gap in equitable AI for\nglobal health by empowering underserved regions with high-performing and\naccessible medical imaging solutions.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09469v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09469v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.36,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09470",
      "title": "AEGIS: An Agent for Extraction and Geographic Identification in\n  Scholarly Proceedings",
      "authors": [
        "Om Vishesh",
        "Harshad Khadilkar",
        "Deepak Akkil"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Keeping pace with the rapid growth of academia literature presents a\nsignificant challenge for researchers, funding bodies, and academic societies.\nTo address the time-consuming manual effort required for scholarly discovery,\nwe present a novel, fully automated system that transitions from data discovery\nto direct action. Our pipeline demonstrates how a specialized AI agent,\n'Agent-E', can be tasked with identifying papers from specific geographic\nregions within conference proceedings and then executing a Robotic Process\nAutomation (RPA) to complete a predefined action, such as submitting a\nnomination form. We validated our system on 586 papers from five different\nconferences, where it successfully identified every target paper with a recall\nof 100% and a near perfect accuracy of 99.4%. This demonstration highlights the\npotential of task-oriented AI agents to not only filter information but also to\nactively participate in and accelerate the workflows of the academic community.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09470v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09470v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.31,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper describes an AI agent for extracting and analyzing geographic information from scholarly proceedings, which involves processing data from papers. While this indirectly relates to handling datasets (e.g., collections of papers as data sources), it does not focus on creating, analyzing, benchmarking, or evaluating datasets for ML/AI applications. The main contribution is automation of extraction and actions, not dataset-related research.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09488",
      "title": "Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts",
      "authors": [
        "Felix Mächtle",
        "Ashwath Shetty",
        "Jonas Sander",
        "Nils Loose",
        "Sören Pirk",
        "Thomas Eisenbarth"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Diffusion models have significantly advanced text-to-image generation,\nenabling the creation of highly realistic images conditioned on textual prompts\nand seeds. Given the considerable intellectual and economic value embedded in\nsuch prompts, prompt theft poses a critical security and privacy concern. In\nthis paper, we investigate prompt-stealing attacks targeting diffusion models.\nWe reveal that numerical optimization-based prompt recovery methods are\nfundamentally limited as they do not account for the initial random noise used\nduring image generation. We identify and exploit a noise-generation\nvulnerability (CWE-339), prevalent in major image-generation frameworks,\noriginating from PyTorch's restriction of seed values to a range of $2^{32}$\nwhen generating the initial random noise on CPUs. Through a large-scale\nempirical analysis conducted on images shared via the popular platform CivitAI,\nwe demonstrate that approximately 95% of these images' seed values can be\neffectively brute-forced in 140 minutes per seed using our seed-recovery tool,\nSeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic\nalgorithm-based optimization method explicitly designed for prompt stealing.\nPromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and\nCLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.\nFurthermore, we introduce straightforward and effective countermeasures that\nrender seed stealing, and thus optimization-based prompt stealing, ineffective.\nWe have disclosed our findings responsibly and initiated coordinated mitigation\nefforts with the developers to address this critical vulnerability.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09488v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09488v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.337,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on prompt-stealing attacks and seed recovery vulnerabilities in diffusion models for text-to-image generation, focusing on security aspects like brute-force attacks and genetic algorithms for optimization. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, which are the core elements of the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09494",
      "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
      "authors": [
        "Zhuoyuan Li",
        "Jiacheng Li",
        "Yao Li",
        "Jialin Li",
        "Li Li",
        "Dong Liu",
        "Feng Wu"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09494v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09494v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.351,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09495",
      "title": "OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake\n  Detection",
      "authors": [
        "Victor Livernoche",
        "Akshatha Arodi",
        "Andreea Musulan",
        "Zachary Yang",
        "Adam Salvail",
        "Gaétan Marceau Caron",
        "Jean-François Godbout",
        "Reihaneh Rabbany"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Deepfakes, synthetic media created using advanced AI techniques, have\nintensified the spread of misinformation, particularly in politically sensitive\ncontexts. Existing deepfake detection datasets are often limited, relying on\noutdated generation methods, low realism, or single-face imagery, restricting\nthe effectiveness for general synthetic image detection. By analyzing social\nmedia posts, we identify multiple modalities through which deepfakes propagate\nmisinformation. Furthermore, our human perception study demonstrates that\nrecently developed proprietary models produce synthetic images increasingly\nindistinguishable from real ones, complicating accurate identification by the\ngeneral public. Consequently, we present a comprehensive, politically-focused\ndataset specifically crafted for benchmarking detection against modern\ngenerative models. This dataset contains three million real images paired with\ndescriptive captions, which are used for generating 963k corresponding\nhigh-quality synthetic images from a mix of proprietary and open-source models.\nRecognizing the continual evolution of generative techniques, we introduce an\ninnovative crowdsourced adversarial platform, where participants are\nincentivized to generate and submit challenging synthetic images. This ongoing\ncommunity-driven initiative ensures that deepfake detection methods remain\nrobust and adaptive, proactively safeguarding public discourse from\nsophisticated misinformation threats.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09495v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09495v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.413,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.395,
      "datasets_score": 0.492,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper mentions using a vision-language model (e.g., Qwen2.5-VL) to extract thematic captions or prompts from images, which involves programmatically generating labels from noisy or imprecise sources, aligning somewhat with weak supervision concepts. However, this is not the main focus; the paper primarily deals with dataset creation and deepfake detection, rather than emphasizing weak supervision as a core machine learning approach for training models.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of the OpenFake dataset, including its creation, curation from social media sources, generation of synthetic images, benchmarking against existing detectors, and analysis of its effectiveness for deepfake detection. This directly aligns with research on creating, analyzing, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces OpenFake, a comprehensive dataset and platform designed to advance deepfake detection, particularly in politically sensitive contexts, by addressing limitations in existing datasets that rely on outdated methods. It compiles 3 million real images from social media paired with 963k high-quality synthetic images generated using modern AI models, conducts a human perception study revealing the difficulty in distinguishing real from fake images, and proposes OpenFake Arena, a crowdsourced adversarial platform to continually update the dataset and improve detection robustness against evolving generative techniques.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset tailored for modern deepfake detection and an innovative crowdsourced adversarial platform, significantly advancing the state-of-the-art by addressing gaps in existing benchmarks and ensuring adaptability to emerging AI techniques.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in AI-driven misinformation detection and commercial applications for safeguarding digital media, given its large-scale, politically relevant dataset and ongoing community-driven platform.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, significant contribution to deepfake detection research with practical tools and insights, making it essential for researchers in AI, computer vision, and misinformation to be aware of, though not universally groundbreaking.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f9f764e931c027129380e2eff61947ca6f8969f3",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 14,
      "average_h_index": 4.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Victor Livernoche",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2219226719"
        },
        {
          "name": "Akshatha Arodi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2140495277"
        },
        {
          "name": "Andreea Musulan",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/116225563"
        },
        {
          "name": "Zachary Yang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2155115994"
        },
        {
          "name": "Adam Salvail",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/1417159390"
        },
        {
          "name": "Ga'etan Marceau Caron",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2323512378"
        },
        {
          "name": "J. Godbout",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/2028704969"
        },
        {
          "name": "Reihaneh Rabbany",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2306673886"
        }
      ]
    },
    {
      "id": "2509.09496",
      "title": "Improving Human Motion Plausibility with Body Momentum",
      "authors": [
        "Ha Linh Nguyen",
        "Tze Ho Elden Tse",
        "Angela Yao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Many studies decompose human motion into local motion in a frame attached to\nthe root joint and global motion of the root joint in the world frame, treating\nthem separately. However, these two components are not independent. Global\nmovement arises from interactions with the environment, which are, in turn,\ndriven by changes in the body configuration. Motion models often fail to\nprecisely capture this physical coupling between local and global dynamics,\nwhile deriving global trajectories from joint torques and external forces is\ncomputationally expensive and complex. To address these challenges, we propose\nusing whole-body linear and angular momentum as a constraint to link local\nmotion with global movement. Since momentum reflects the aggregate effect of\njoint-level dynamics on the body's movement through space, it provides a\nphysically grounded way to relate local joint behavior to global displacement.\nBuilding on this insight, we introduce a new loss term that enforces\nconsistency between the generated momentum profiles and those observed in\nground-truth data. Incorporating our loss reduces foot sliding and jitter,\nimproves balance, and preserves the accuracy of the recovered motion. Code and\ndata are available at the project page https://hlinhn.github.io/momentum_bmvc.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09496v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09496v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.274,
      "datasets_score": 0.228,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09498",
      "title": "SEDM: Scalable Self-Evolving Distributed Memory for Agents",
      "authors": [
        "Haoran Xu",
        "Jiacong Hu",
        "Ke Zhang",
        "Lei Yu",
        "Yuxin Tang",
        "Xinyuan Song",
        "Yiqun Duan",
        "Lynn Ai",
        "Bill Shi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Long-term multi-agent systems inevitably generate vast amounts of\ntrajectories and historical interactions, which makes efficient memory\nmanagement essential for both performance and scalability. Existing methods\ntypically depend on vector retrieval and hierarchical storage, yet they are\nprone to noise accumulation, uncontrolled memory expansion, and limited\ngeneralization across domains. To address these challenges, we present SEDM,\nSelf-Evolving Distributed Memory, a verifiable and adaptive framework that\ntransforms memory from a passive repository into an active, self-optimizing\ncomponent. SEDM integrates verifiable write admission based on reproducible\nreplay, a self-scheduling memory controller that dynamically ranks and\nconsolidates entries according to empirical utility, and cross-domain knowledge\ndiffusion that abstracts reusable insights to support transfer across\nheterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM\nimproves reasoning accuracy while reducing token overhead compared with strong\nmemory baselines, and further enables knowledge distilled from fact\nverification to enhance multi-hop reasoning. The results highlight SEDM as a\nscalable and sustainable memory mechanism for open-ended multi-agent\ncollaboration. The code will be released in the later stage of this project.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09498v3",
      "pdf_url": "http://arxiv.org/pdf/2509.09498v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.406,
      "datasets_score": 0.398,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on SEDM, a framework for efficient memory management in multi-agent systems, emphasizing verifiable admission, self-scheduling, and knowledge transfer. It does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction, as required for diffusion-based reasoning. The term \"diffusion\" in the paper refers to knowledge transfer across domains, not the technical diffusion model concept.",
      "distributed_training_justification": "The paper discusses distributed memory in multi-agent systems, which involves scalability and management across agents, potentially implying distributed computing elements. However, it primarily addresses memory optimization for runtime operations, not core aspects of distributed training like partitioning data or models for parallel training across nodes. Thus, while there is a loose connection to distributed systems, it does not focus on accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09501",
      "title": "Region-Wise Correspondence Prediction between Manga Line Art Images",
      "authors": [
        "Yingxuan Li",
        "Jiafeng Mao",
        "Qianru Qiu",
        "Yusuke Matsui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Understanding region-wise correspondence between manga line art images is a\nfundamental task in manga processing, enabling downstream applications such as\nautomatic line art colorization and in-between frame generation. However, this\ntask remains largely unexplored, especially in realistic scenarios without\npre-existing segmentation or annotations. In this paper, we introduce a novel\nand practical task: predicting region-wise correspondence between raw manga\nline art images without any pre-existing labels or masks. To tackle this\nproblem, we divide each line art image into a set of patches and propose a\nTransformer-based framework that learns patch-level similarities within and\nacross images. We then apply edge-aware clustering and a region matching\nalgorithm to convert patch-level predictions into coherent region-level\ncorrespondences. To support training and evaluation, we develop an automatic\nannotation pipeline and manually refine a subset of the data to construct\nbenchmark datasets. Experiments on multiple datasets demonstrate that our\nmethod achieves high patch-level accuracy (e.g., 96.34%) and generates\nconsistent region-level correspondences, highlighting its potential for\nreal-world manga applications.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09501v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09501v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.298,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09508",
      "title": "Incorporating AI Incident Reporting into Telecommunications Law and\n  Policy: Insights from India",
      "authors": [
        "Avinash Agarwal",
        "Manisha J. Nene"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "The integration of artificial intelligence (AI) into telecommunications\ninfrastructure introduces novel risks, such as algorithmic bias and\nunpredictable system behavior, that fall outside the scope of traditional\ncybersecurity and data protection frameworks. This paper introduces a precise\ndefinition and a detailed typology of telecommunications AI incidents,\nestablishing them as a distinct category of risk that extends beyond\nconventional cybersecurity and data protection breaches. It argues for their\nrecognition as a distinct regulatory concern. Using India as a case study for\njurisdictions that lack a horizontal AI law, the paper analyzes the country's\nkey digital regulations. The analysis reveals that India's existing legal\ninstruments, including the Telecommunications Act, 2023, the CERT-In Rules, and\nthe Digital Personal Data Protection Act, 2023, focus on cybersecurity and data\nbreaches, creating a significant regulatory gap for AI-specific operational\nincidents, such as performance degradation and algorithmic bias. The paper also\nexamines structural barriers to disclosure and the limitations of existing AI\nincident repositories. Based on these findings, the paper proposes targeted\npolicy recommendations centered on integrating AI incident reporting into\nIndia's existing telecom governance. Key proposals include mandating reporting\nfor high-risk AI failures, designating an existing government body as a nodal\nagency to manage incident data, and developing standardized reporting\nframeworks. These recommendations aim to enhance regulatory clarity and\nstrengthen long-term resilience, offering a pragmatic and replicable blueprint\nfor other nations seeking to govern AI risks within their existing sectoral\nframeworks.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09508v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09508v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.284,
      "distributed_training_score": 0.265,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09513",
      "title": "Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided\n  Protocol on the Connectome 2.0 scanner",
      "authors": [
        "Quentin Uhl",
        "Tommaso Pavan",
        "Julianna Gerold",
        "Kwok-Shing Chan",
        "Yohan Jun",
        "Shohei Fujita",
        "Aneri Bhatt",
        "Yixin Ma",
        "Qiaochu Wang",
        "Hong-Hsi Lee",
        "Susie Y. Huang",
        "Berkin Bilgic",
        "Ileana Jelescu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "The diffusion MRI Neurite Exchange Imaging model offers a promising framework\nfor probing gray matter microstructure by estimating parameters such as\ncompartment sizes, diffusivities, and inter-compartmental water exchange time.\nHowever, existing protocols require long scan times. This study proposes a\nreduced acquisition scheme for the Connectome 2.0 scanner that preserves model\naccuracy while substantially shortening scan duration. We developed a\ndata-driven framework using explainable artificial intelligence with a guided\nrecursive feature elimination strategy to identify an optimal 8-feature subset\nfrom a 15-feature protocol. The performance of this optimized protocol was\nvalidated in vivo and benchmarked against the full acquisition and alternative\nreduction strategies. Parameter accuracy, preservation of anatomical contrast,\nand test-retest reproducibility were assessed. The reduced protocol yielded\nparameter estimates and cortical maps comparable to the full protocol, with low\nestimation errors in synthetic data and minimal impact on test-retest\nvariability. Compared to theory-driven and heuristic reduction schemes, the\noptimized protocol demonstrated superior robustness, reducing the deviation in\nwater exchange time estimates by over two-fold. In conclusion, this hybrid\noptimization framework enables viable imaging of neurite exchange in 14 minutes\nwithout loss of parameter fidelity. This approach supports the broader\napplication of exchange-sensitive diffusion magnetic resonance imaging in\nneuroscience and clinical research, and offers a generalizable method for\ndesigning efficient acquisition protocols in biophysical parameter mapping.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09513v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09513v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.456,
      "distributed_training_score": 0.354,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of an optimized acquisition protocol for diffusion MRI using explainable AI techniques like XGBoost and SHAP to accelerate microstructure imaging. It focuses on medical imaging applications, specifically estimating parameters in brain tissue, and does not involve adapting diffusion models for iterative refinement in multi-step logical reasoning or Chain-of-Thought processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09522",
      "title": "Towards Explainable Job Title Matching: Leveraging Semantic Textual\n  Relatedness and Knowledge Graphs",
      "authors": [
        "Vadim Zadykian",
        "Bruno Andrade",
        "Haithem Afli"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Semantic Textual Relatedness (STR) captures nuanced relationships between\ntexts that extend beyond superficial lexical similarity. In this study, we\ninvestigate STR in the context of job title matching - a key challenge in\nresume recommendation systems, where overlapping terms are often limited or\nmisleading. We introduce a self-supervised hybrid architecture that combines\ndense sentence embeddings with domain-specific Knowledge Graphs (KGs) to\nimprove both semantic alignment and explainability. Unlike previous work that\nevaluated models on aggregate performance, our approach emphasizes data\nstratification by partitioning the STR score continuum into distinct regions:\nlow, medium, and high semantic relatedness. This stratified evaluation enables\na fine-grained analysis of model performance across semantically meaningful\nsubspaces. We evaluate several embedding models, both with and without KG\nintegration via graph neural networks. The results show that fine-tuned SBERT\nmodels augmented with KGs produce consistent improvements in the high-STR\nregion, where the RMSE is reduced by 25% over strong baselines. Our findings\nhighlight not only the benefits of combining KGs with text embeddings, but also\nthe importance of regional performance analysis in understanding model\nbehavior. This granular approach reveals strengths and weaknesses hidden by\nglobal metrics, and supports more targeted model selection for use in Human\nResources (HR) systems and applications where fairness, explainability, and\ncontextual matching are essential.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09522v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09522v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.336,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a self-supervised hybrid architecture using SBERT embeddings and Knowledge Graphs for semantic textual relatedness in job title matching. It does not involve human feedback, reward models, or reinforcement learning techniques for model fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses semantic alignment and explainability using embeddings and Knowledge Graphs, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought generation.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09527",
      "title": "Generative Diffusion Contrastive Network for Multi-View Clustering",
      "authors": [
        "Jian Zhu",
        "Xin Zou",
        "Xi Wang",
        "Ning Zhang",
        "Bian Wu",
        "Yao Yang",
        "Ying Zhou",
        "Lingfang Zeng",
        "Chang Tang",
        "Cheng Luo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In recent years, Multi-View Clustering (MVC) has been significantly advanced\nunder the influence of deep learning. By integrating heterogeneous data from\nmultiple views, MVC enhances clustering analysis, making multi-view fusion\ncritical to clustering performance. However, there is a problem of low-quality\ndata in multi-view fusion. This problem primarily arises from two reasons: 1)\nCertain views are contaminated by noisy data. 2) Some views suffer from missing\ndata. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF)\nmethod to address this problem. SGDF leverages a multiple generative mechanism\nfor the multi-view feature of each sample. It is robust to low-quality data.\nBuilding on SGDF, we further present the Generative Diffusion Contrastive\nNetwork (GDCN). Extensive experiments show that GDCN achieves the\nstate-of-the-art results in deep MVC tasks. The source code is publicly\navailable at https://github.com/HackerHyper/GDCN.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09527v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09527v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.272,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.376,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a Stochastic Generative Diffusion Fusion (SGDF) method inspired by diffusion models for multi-view clustering, focusing on robust data fusion by generating and averaging multiple features. While it draws from diffusion concepts like iterative generation, it applies them to data processing and clustering tasks, not to multi-step logical reasoning or chain-of-thought correction as specified in the topic. Thus, there is a minor connection through the use of diffusion mechanisms, but no direct relevance to solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09529",
      "title": "A modified RIME algorithm with covariance learning and diversity\n  enhancement for numerical optimization",
      "authors": [
        "Shangqing Shi",
        "Luoxiao Zhang",
        "Yuchen Yin",
        "Xiong Yang",
        "Hoileong Lee"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)"
      ],
      "abstract": "Metaheuristics are widely applied for their ability to provide more efficient\nsolutions. The RIME algorithm is a recently proposed physical-based\nmetaheuristic algorithm with certain advantages. However, it suffers from rapid\nloss of population diversity during optimization and is prone to fall into\nlocal optima, leading to unbalanced exploitation and exploration. To address\nthe shortcomings of RIME, this paper proposes a modified RIME with covariance\nlearning and diversity enhancement (MRIME-CD). The algorithm applies three\nstrategies to improve the optimization capability. First, a covariance learning\nstrategy is introduced in the soft-rime search stage to increase the population\ndiversity and balance the over-exploitation ability of RIME through the\nbootstrapping effect of dominant populations. Second, in order to moderate the\ntendency of RIME population to approach the optimal individual in the early\nsearch stage, an average bootstrapping strategy is introduced into the\nhard-rime puncture mechanism, which guides the population search through the\nweighted position of the dominant populations, thus enhancing the global search\nability of RIME in the early stage. Finally, a new stagnation indicator is\nproposed, and a stochastic covariance learning strategy is used to update the\nstagnant individuals in the population when the algorithm gets stagnant, thus\nenhancing the ability to jump out of the local optimal solution. The proposed\nMRIME-CD algorithm is subjected to a series of validations on the CEC2017 test\nset, the CEC2022 test set, and the experimental results are analyzed using the\nFriedman test, the Wilcoxon rank sum test, and the Kruskal Wallis test. The\nresults show that MRIME-CD can effectively improve the performance of basic\nRIME and has obvious superiorities in terms of solution accuracy, convergence\nspeed and stability.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09529v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09529v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.328,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09530",
      "title": "DualTrack: Sensorless 3D Ultrasound needs Local and Global Context",
      "authors": [
        "Paul F. R. Wilson",
        "Matteo Ronchetti",
        "Rüdiger Göbl",
        "Viktoria Markova",
        "Sebastian Rosenzweig",
        "Raphael Prevost",
        "Parvin Mousavi",
        "Oliver Zettinig"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Three-dimensional ultrasound (US) offers many clinical advantages over\nconventional 2D imaging, yet its widespread adoption is limited by the cost and\ncomplexity of traditional 3D systems. Sensorless 3D US, which uses deep\nlearning to estimate a 3D probe trajectory from a sequence of 2D US images, is\na promising alternative. Local features, such as speckle patterns, can help\npredict frame-to-frame motion, while global features, such as coarse shapes and\nanatomical structures, can situate the scan relative to anatomy and help\npredict its general shape. In prior approaches, global features are either\nignored or tightly coupled with local feature extraction, restricting the\nability to robustly model these two complementary aspects. We propose\nDualTrack, a novel dual-encoder architecture that leverages decoupled local and\nglobal encoders specialized for their respective scales of feature extraction.\nThe local encoder uses dense spatiotemporal convolutions to capture\nfine-grained features, while the global encoder utilizes an image backbone\n(e.g., a 2D CNN or foundation model) and temporal attention layers to embed\nhigh-level anatomical features and long-range dependencies. A lightweight\nfusion module then combines these features to estimate the trajectory.\nExperimental results on a large public benchmark show that DualTrack achieves\nstate-of-the-art accuracy and globally consistent 3D reconstructions,\noutperforming previous methods and yielding an average reconstruction error\nbelow 5 mm.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09530v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09530v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.395,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09541",
      "title": "Compositional Concept Generalization with Variational Quantum Circuits",
      "authors": [
        "Hala Hawashin",
        "Mina Abbaszadeh",
        "Nicholas Joseph",
        "Beth Pearson",
        "Martha Lewis",
        "Mehrnoosh sadrzadeh"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Compositional generalization is a key facet of human cognition, but lacking\nin current AI tools such as vision-language models. Previous work examined\nwhether a compositional tensor-based sentence semantics can overcome the\nchallenge, but led to negative results. We conjecture that the increased\ntraining efficiency of quantum models will improve performance in these tasks.\nWe interpret the representations of compositional tensor-based models in\nHilbert spaces and train Variational Quantum Circuits to learn these\nrepresentations on an image captioning task requiring compositional\ngeneralization. We used two image encoding techniques: a multi-hot encoding\n(MHE) on binary image vectors and an angle/amplitude encoding on image vectors\ntaken from the vision-language model CLIP. We achieve good proof-of-concept\nresults using noisy MHE encodings. Performance on CLIP image vectors was more\nmixed, but still outperformed classical compositional models.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09541v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.354,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using Variational Quantum Circuits to enhance compositional generalization in tasks like image captioning, focusing on quantum-based tensor representations and learning from datasets. It does not mention or utilize diffusion models, iterative refinement processes, or any mechanism for multi-step logical reasoning as described in the topic. Therefore, there is no overlap with diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09547",
      "title": "Improving Video Diffusion Transformer Training by Multi-Feature Fusion\n  and Alignment from Self-Supervised Vision Encoders",
      "authors": [
        "Dohun Lee",
        "Hyeonho Jeong",
        "Jiwook Kim",
        "Duygu Ceylan",
        "Jong Chul Ye"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Video diffusion models have advanced rapidly in the recent years as a result\nof series of architectural innovations (e.g., diffusion transformers) and use\nof novel training objectives (e.g., flow matching). In contrast, less attention\nhas been paid to improving the feature representation power of such models. In\nthis work, we show that training video diffusion models can benefit from\naligning the intermediate features of the video generator with feature\nrepresentations of pre-trained vision encoders. We propose a new metric and\nconduct an in-depth analysis of various vision encoders to evaluate their\ndiscriminability and temporal consistency, thereby assessing their suitability\nfor video feature alignment. Based on the analysis, we present Align4Gen which\nprovides a novel multi-feature fusion and alignment method integrated into\nvideo diffusion model training. We evaluate Align4Gen both for unconditional\nand class-conditional video generation tasks and show that it results in\nimproved video generation as quantified by various metrics. Full video results\nare available on our project page: https://align4gen.github.io/align4gen/",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09547v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09547v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.545,
      "distributed_training_score": 0.4,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing video diffusion models for generation tasks through feature alignment with pre-trained encoders, without any involvement in iterative refinement for logical reasoning, chain-of-thought processes, or solving complex logical tasks. It is solely about generative video synthesis.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node machine learning techniques. While it mentions \"accelerated training\" as a benefit of the proposed method, this refers to efficiency in video diffusion training, not strategies for partitioning data or computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09552",
      "title": "An improved educational competition optimizer with multi-covariance\n  learning operators for global optimization problems",
      "authors": [
        "Baoqi Zhao",
        "Xiong Yang",
        "Hoileong Lee",
        "Bowen Dong"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)"
      ],
      "abstract": "The educational competition optimizer is a recently introduced metaheuristic\nalgorithm inspired by human behavior, originating from the dynamics of\neducational competition within society. Nonetheless, ECO faces constraints due\nto an imbalance between exploitation and exploration, rendering it susceptible\nto local optima and demonstrating restricted effectiveness in addressing\ncomplex optimization problems. To address these limitations, this study\npresents an enhanced educational competition optimizer (IECO-MCO) utilizing\nmulti-covariance learning operators. In IECO, three distinct covariance\nlearning operators are introduced to improve the performance of ECO. Each\noperator effectively balances exploitation and exploration while preventing\npremature convergence of the population. The effectiveness of IECO is assessed\nthrough benchmark functions derived from the CEC 2017 and CEC 2022 test suites,\nand its performance is compared with various basic and improved algorithms\nacross different categories. The results demonstrate that IECO-MCO surpasses\nthe basic ECO and other competing algorithms in convergence speed, stability,\nand the capability to avoid local optima. Furthermore, statistical analyses,\nincluding the Friedman test, Kruskal-Wallis test, and Wilcoxon rank-sum test,\nare conducted to validate the superiority of IECO-MCO over the compared\nalgorithms. Compared with the basic algorithm (improved algorithm), IECO-MCO\nachieved an average ranking of 2.213 (2.488) on the CE2017 and CEC2022 test\nsuites. Additionally, the practical applicability of the proposed IECO-MCO\nalgorithm is verified by solving constrained optimization problems. The\nexperimental outcomes demonstrate the superior performance of IECO-MCO in\ntackling intricate optimization problems, underscoring its robustness and\npractical effectiveness in real-world scenarios.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09552v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09552v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.289,
      "distributed_training_score": 0.318,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09555",
      "title": "InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction\n  Generation",
      "authors": [
        "Sirui Xu",
        "Dongting Li",
        "Yucheng Zhang",
        "Xiyan Xu",
        "Qi Long",
        "Ziyin Wang",
        "Yunzhi Lu",
        "Shuchang Dong",
        "Hezi Jiang",
        "Akshat Gupta",
        "Yu-Xiong Wang",
        "Liang-Yan Gui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While large-scale human motion capture datasets have advanced human motion\ngeneration, modeling and generating dynamic 3D human-object interactions (HOIs)\nremain challenging due to dataset limitations. Existing datasets often lack\nextensive, high-quality motion and annotation and exhibit artifacts such as\ncontact penetration, floating, and incorrect hand motions. To address these\nissues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset\nand methodological advancements. First, we consolidate and standardize 21.81\nhours of HOI data from diverse sources, enriching it with detailed textual\nannotations. Second, we propose a unified optimization framework to enhance\ndata quality by reducing artifacts and correcting hand motions. Leveraging the\nprinciple of contact invariance, we maintain human-object relationships while\nintroducing motion variations, expanding the dataset to 30.70 hours. Third, we\ndefine six benchmarking tasks and develop a unified HOI generative modeling\nperspective, achieving state-of-the-art performance. Extensive experiments\nvalidate the utility of our dataset as a foundational resource for advancing 3D\nhuman-object interaction generation. To support continued research in this\narea, the dataset is publicly available at\nhttps://github.com/wzyabcas/InterAct, and will be actively maintained.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09555v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09555v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.32,
      "datasets_score": 0.429,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of InterAct, a large-scale 3D human-object interaction dataset, along with methodologies for curation, standardization, and augmentation to address artifacts. It also includes benchmarking tasks and evaluations, directly aligning with research on creating, analyzing, and benchmarking datasets for machine learning and AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces InterAct, a large-scale benchmark for 3D human-object interaction (HOI) generation, which consolidates and enhances existing datasets by standardizing data, adding detailed annotations, and applying a unified optimization framework to correct artifacts like penetrations and floating contacts while expanding the dataset through contact invariance techniques. The core objectives are to address limitations in current HOI datasets, enable comprehensive benchmarking across six tasks, and achieve state-of-the-art performance in generative modeling, with key findings demonstrating improved data quality, expanded dataset size from 21.81 to 30.70 hours, and superior results that establish InterAct as a foundational resource for future research in areas like robotics and animation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new large-scale dataset and a unified optimization framework that significantly advances the state-of-the-art in 3D human-object interaction generation by addressing key limitations in existing datasets.",
      "impact_score": "High",
      "impact_justification": "The work provides a publicly available, comprehensive benchmark that could influence a wide range of future research and applications in computer vision, robotics, and animation by offering high-quality data and standardized tasks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to 3D HOI generation with its innovative dataset and benchmarks, making it essential for researchers in the field to be aware of and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6da250eb3006fc9a067a19e6a88caccab81f7a9e",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 17,
      "average_h_index": 3.0833333333333335,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Sirui Xu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/8775961"
        },
        {
          "name": "Dongting Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371142522"
        },
        {
          "name": "Yucheng Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371139535"
        },
        {
          "name": "Xiyan Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371146470"
        },
        {
          "name": "Qi Long",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371137523"
        },
        {
          "name": "Ziyin Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2293764321"
        },
        {
          "name": "Yunzhi Lu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371376135"
        },
        {
          "name": "Shuchang Dong",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371247576"
        },
        {
          "name": "Hezi Jiang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371146758"
        },
        {
          "name": "Akshat Gupta",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371146690"
        },
        {
          "name": "Yu-Xiong Wang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2236683644"
        },
        {
          "name": "Liangyan Gui",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2587808"
        }
      ]
    },
    {
      "id": "2509.09558",
      "title": "Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in\n  MRI-based Alzheimer's Disease Classification",
      "authors": [
        "Akshit Achara",
        "Esther Puyol Anton",
        "Alexander Hammers",
        "Andrew P. King"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep\nlearning (DL) algorithms have been proposed to aid in the diagnosis of diseases\nsuch as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can\nsuffer from shortcut learning, in which spurious features, not directly related\nto the output label, are used for prediction. When these features are related\nto protected attributes, they can lead to performance bias against\nunderrepresented protected groups, such as those defined by race and sex. In\nthis work, we explore the potential for shortcut learning and demographic bias\nin DL based AD diagnosis from MRI. We first investigate if DL algorithms can\nidentify race or sex from 3D brain MRI scans to establish the presence or\notherwise of race and sex based distributional shifts. Next, we investigate\nwhether training set imbalance by race or sex can cause a drop in model\nperformance, indicating shortcut learning and bias. Finally, we conduct a\nquantitative and qualitative analysis of feature attributions in different\nbrain regions for both the protected attribute and AD classification tasks.\nThrough these experiments, and using multiple datasets and DL models (ResNet\nand SwinTransformer), we demonstrate the existence of both race and sex based\nshortcut learning and bias in DL based AD classification. Our work lays the\nfoundation for fairer DL diagnostic tools in brain MRI. The code is provided at\nhttps://github.com/acharaakshit/ShortMR",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09558v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09558v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.364,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves creating and analyzing datasets by constructing baseline and biased versions with imbalances in race or sex to study shortcut learning in AD classification from MRI scans. This aligns with dataset curation methodologies and analysis for ML applications, as it evaluates how dataset imbalances affect model performance. However, the primary focus is on AI biases in DL models rather than datasets themselves, making it moderately relevant rather than central.",
      "llm_score_status": "completed",
      "summary": "This paper examines potential biases in deep learning (DL) models for Alzheimer's disease (AD) classification from MRI scans by investigating whether models can identify demographic attributes like race and sex, assessing the impact of dataset imbalances on performance, and analyzing feature attributions to detect shortcut learning. Using multiple datasets and DL models such as ResNet and SwinTransformer, the authors demonstrate that race and sex-based shortcuts exist, leading to biased AD classifications, and propose this as a foundation for developing fairer diagnostic tools in brain MRI.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying existing shortcut learning concepts to demographic biases in MRI-based AD classification through empirical experiments on real datasets, though it does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI in healthcare and medical imaging, as it highlights critical biases in DL diagnostics, but its influence may remain confined to these specific areas rather than having widespread effects.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality contribution by revealing important biases in medical AI, making it essential for researchers in AI ethics and healthcare to be aware of, though it is not groundbreaking enough to be a must-read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1842537bb0f2ac3678d287cac5153d7b7351c7d3",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 2,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Akshit Achara",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2184669155"
        },
        {
          "name": "E. P. Anton",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/120264429"
        },
        {
          "name": "Alexander Hammers",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2371136890"
        },
        {
          "name": "Andrew P. King",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2333979653"
        }
      ]
    },
    {
      "id": "2509.09560",
      "title": "Boosting Embodied AI Agents through Perception-Generation Disaggregation\n  and Asynchronous Pipeline Execution",
      "authors": [
        "Shulai Zhang",
        "Ao Xu",
        "Quan Chen",
        "Han Zhao",
        "Weihao Cui",
        "Ningxin Zheng",
        "Haibin Lin",
        "Xin Liu",
        "Minyi Guo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Embodied AI systems operate in dynamic environments, requiring seamless\nintegration of perception and generation modules to process high-frequency\ninput and output demands. Traditional sequential computation patterns, while\neffective in ensuring accuracy, face significant limitations in achieving the\nnecessary \"thinking\" frequency for real-world applications. In this work, we\npresent Auras, an algorithm-system co-designed inference framework to optimize\nthe inference frequency of embodied AI agents. Auras disaggregates the\nperception and generation and provides controlled pipeline parallelism for them\nto achieve high and stable throughput. Faced with the data staleness problem\nthat appears when the parallelism is increased, Auras establishes a public\ncontext for perception and generation to share, thereby promising the accuracy\nof embodied agents. Experimental results show that Auras improves throughput by\n2.54x on average while achieving 102.7% of the original accuracy, demonstrating\nits efficacy in overcoming the constraints of sequential computation and\nproviding high throughput.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09560v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09560v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.497,
      "distributed_training_score": 0.451,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on optimizing inference for embodied AI through disaggregation and asynchronous pipeline execution, with no mention of human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper references diffusion-based algorithms as part of generative models in embodied AI, but its main contribution is the Auras framework for improving throughput and accuracy, not multi-step logical reasoning or iterative refinement of a chain-of-thought.",
      "distributed_training_justification": "The paper uses parallel computing techniques like pipeline parallelism and asynchronous execution for inference optimization, which overlaps with distributed computing concepts, but it addresses inference in embodied AI systems rather than model training across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Auras, a framework designed to enhance the performance of embodied AI agents by disaggregating perception and generation modules and implementing asynchronous pipeline execution to address the limitations of sequential computation in dynamic environments. By sharing a public context to mitigate data staleness and optimizing parallelism, Auras achieves an average throughput improvement of 2.54x while maintaining 102.7% of the original accuracy, as demonstrated through experimental results on auto-regressive and diffusion-based models.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining pipeline parallelism and perception-generation disaggregation to enhance throughput in embodied AI, building on existing techniques in a clever way to address real-world frequency demands. While not introducing an entirely new problem, it offers a practical and effective adaptation that advances system efficiency.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in embodied AI subfields by providing a scalable solution for high-frequency processing in applications like robotics, potentially leading to citations and further developments in hardware efficiency. However, its applicability may be limited to specific domains rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a strong, practical contribution with demonstrated improvements in AI system performance, making it valuable for researchers in embodied AI and related areas. It is not essential for all readers but offers insightful techniques that could inspire further work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/615f1a22cd632792cdbbb410897a77dddf6f52a6",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 10,
      "average_h_index": 5.0,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Shulai Zhang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2107941972"
        },
        {
          "name": "Ao Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372272905"
        },
        {
          "name": "Quan Chen",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2285397328"
        },
        {
          "name": "Han Zhao",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2112674260"
        },
        {
          "name": "Weihao Cui",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/1492129615"
        },
        {
          "name": "Ningxin Zheng",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2305682698"
        },
        {
          "name": "Haibin Lin",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2257447831"
        },
        {
          "name": "Xin Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380572295"
        },
        {
          "name": "Minyi Guo",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2323332373"
        }
      ]
    },
    {
      "id": "2509.09572",
      "title": "PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient\n  Fine-Tuning for Remote Sensing Change Detection",
      "authors": [
        "Sijun Dong",
        "Yuxuan Hu",
        "LiBo Wang",
        "Geng Chen",
        "Xiaoliang Meng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "To tackle the prevalence of pseudo changes, the scarcity of labeled samples,\nand the difficulty of cross-domain generalization in multi-temporal and\nmulti-source remote sensing imagery, we propose PeftCD, a change detection\nframework built upon Vision Foundation Models (VFMs) with Parameter-Efficient\nFine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese\nencoder derived from a VFM, into which LoRA and Adapter modules are seamlessly\nintegrated. This design enables highly efficient task adaptation by training\nonly a minimal set of additional parameters. To fully unlock the potential of\nVFMs, we investigate two leading backbones: the Segment Anything Model v2\n(SAM2), renowned for its strong segmentation priors, and DINOv3, a\nstate-of-the-art self-supervised representation learner. The framework is\ncomplemented by a deliberately lightweight decoder, ensuring the focus remains\non the powerful feature representations from the backbones. Extensive\nexperiments demonstrate that PeftCD achieves state-of-the-art performance\nacross multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD\n(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and\nLEVIR-CD (85.62%), with notably precise boundary delineation and strong\nsuppression of pseudo-changes. In summary, PeftCD presents an optimal balance\nof accuracy, efficiency, and generalization. It offers a powerful and scalable\nparadigm for adapting large-scale VFMs to real-world remote sensing change\ndetection applications. The code and pretrained models will be released at\nhttps://github.com/dyzy41/PeftCD.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09572v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09572v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.376,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09584",
      "title": "Visual Grounding from Event Cameras",
      "authors": [
        "Lingdong Kong",
        "Dongyue Lu",
        "Ao Liang",
        "Rong Li",
        "Yuhao Dong",
        "Tianshuai Hu",
        "Lai Xing Ng",
        "Wei Tsang Ooi",
        "Benoit R. Cottereau"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Event cameras capture changes in brightness with microsecond precision and\nremain reliable under motion blur and challenging illumination, offering clear\nadvantages for modeling highly dynamic scenes. Yet, their integration with\nnatural language understanding has received little attention, leaving a gap in\nmultimodal perception. To address this, we introduce Talk2Event, the first\nlarge-scale benchmark for language-driven object grounding using event data.\nBuilt on real-world driving scenarios, Talk2Event comprises 5,567 scenes,\n13,458 annotated objects, and more than 30,000 carefully validated referring\nexpressions. Each expression is enriched with four structured attributes --\nappearance, status, relation to the viewer, and relation to surrounding objects\n-- that explicitly capture spatial, temporal, and relational cues. This\nattribute-centric design supports interpretable and compositional grounding,\nenabling analysis that moves beyond simple object recognition to contextual\nreasoning in dynamic environments. We envision Talk2Event as a foundation for\nadvancing multimodal and temporally-aware perception, with applications\nspanning robotics, human-AI interaction, and so on.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09584v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09584v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.309,
      "datasets_score": 0.429,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of Talk2Event, a new large-scale benchmark dataset for language-driven object grounding using event cameras. It details the dataset creation process, including curation from real-world driving scenarios, annotation of 5,567 scenes and 13,458 objects with over 30,000 referring expressions, and the use of structured attributes for analysis. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications, as it establishes a new platform for multimodal perception tasks.",
      "llm_score_status": "completed",
      "summary": "The paper introduces Talk2Event, the first large-scale benchmark for language-driven object grounding using event camera data from real-world driving scenarios, addressing the gap in integrating event-based sensing with natural language understanding. It comprises 5,567 scenes, 13,458 annotated objects, and over 30,000 referring expressions enriched with four structured attributes—appearance, status, relation to the viewer, and relation to surrounding objects—to enable interpretable, compositional, and contextual reasoning in dynamic environments, with potential applications in robotics and human-AI interaction.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new problem by creating the first benchmark for visual grounding with event cameras, significantly advancing the state-of-the-art in multimodal perception for dynamic scenes. This represents a novel integration of event data with natural language, which has not been explored before.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence a wide range of future research in computer vision, robotics, and human-AI interaction by providing a foundational dataset for temporally-aware multimodal perception. Its applications in real-world dynamic environments, such as driving scenarios, suggest broad potential for both academic and commercial advancements.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by establishing a new benchmark that advances multimodal perception, making it essential for researchers in computer vision and robotics to be aware of. While highly insightful, it may not be groundbreaking for all audiences, positioning it as important but not absolutely essential.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8862f8b6511be59ca20f5fccab0334e2cefae55c",
      "total_authors": 9,
      "authors_found": 8,
      "highest_h_index": 5,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Lingdong Kong",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2335574081"
        },
        {
          "name": "Dongyue Lu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ao Liang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372674762"
        },
        {
          "name": "Rong Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372403246"
        },
        {
          "name": "Yuhao Dong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373409787"
        },
        {
          "name": "Tianshuai Hu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374364190"
        },
        {
          "name": "Lai Xing Ng",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2225267749"
        },
        {
          "name": "Wei Tsang Ooi",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2300285563"
        },
        {
          "name": "Benoit R. Cottereau",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2252672743"
        }
      ]
    },
    {
      "id": "2509.09593",
      "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models",
      "authors": [
        "Bangzhao Shu",
        "Isha Joshi",
        "Melissa Karnaze",
        "Anh C. Pham",
        "Ishita Kakkar",
        "Sindhu Kothe",
        "Arpine Hovasapian",
        "Mai ElSherief"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The versatility of Large Language Models (LLMs) in natural language\nunderstanding has made them increasingly popular in mental health research.\nWhile many studies explore LLMs' capabilities in emotion recognition, a\ncritical gap remains in evaluating whether LLMs align with human emotions at a\nfine-grained level. Existing research typically focuses on classifying emotions\ninto predefined, limited categories, overlooking more nuanced expressions. To\naddress this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit\ncommunities featuring 251 fine-grained, self-disclosed emotion labels. Our\ncomprehensive evaluation framework examines predicted emotion terms and\ndecomposes them into eight basic emotions using established emotion theories,\nenabling a fine-grained comparison. Systematic testing of prevalent LLMs under\nvarious prompt settings reveals that accurately predicting emotions that align\nwith human self-disclosed emotions remains challenging. Qualitative analysis\nfurther shows that while certain LLMs generate emotion terms consistent with\nestablished emotion theories and definitions, they sometimes fail to capture\ncontextual cues as effectively as human self-disclosures. These findings\nhighlight the limitations of LLMs in fine-grained emotion alignment and offer\ninsights for future research aimed at enhancing their contextual understanding.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09593v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09593v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.455,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.347,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on evaluating LLMs for emotion recognition using a new dataset and prompting strategies, but it does not involve training models with human feedback, reward models, or reinforcement learning techniques. There is no mention of aligning AI with human preferences through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses LLMs and prompting strategies like Chain of Thought for emotion prediction, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning adapted from diffusion techniques. The core contributions are benchmarking and dataset creation, not diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and evaluation of a new benchmark dataset (EXPRESS) for emotion recognition in LLMs, including dataset curation, analysis, and benchmarking. This directly aligns with research on creating, analyzing, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of Large Language Models (LLMs) in fine-grained emotion recognition by introducing EXPRESS, a benchmark dataset derived from Reddit self-disclosures with 251 detailed emotion labels, to evaluate how well LLMs align with human emotions. The methodology involves masking emotion words in texts, prompting LLMs to predict them, and decomposing emotions into basic components using Plutchik’s Wheel, revealing that while LLMs can capture underlying basic emotions, they often fail to accurately reflect human contextual nuances, thus highlighting areas for improvement in emotion-aware AI systems.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper introduces a new benchmark dataset and evaluation framework for fine-grained emotion recognition, which is a clever combination of existing emotion theories and techniques to address gaps in current research. However, it builds on established concepts rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in emotion recognition for LLMs, particularly in subfields like mental health applications and NLP, by providing a new benchmark for future studies. While it has potential for broader applications, its impact is primarily confined to specific AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a strong contribution with a novel benchmark and insightful findings on LLM limitations, making it valuable for researchers in AI and computational linguistics. It is not essential for all readers but is significant for those focused on emotion-aware technologies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0d7a74da5c1b42b1f509dcd7c3fde67f23cf9e56",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 4,
      "average_h_index": 0.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Bangzhao Shu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380031317"
        },
        {
          "name": "Isha Joshi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380030817"
        },
        {
          "name": "Melissa Karnaze",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380030655"
        },
        {
          "name": "Anh C. Pham",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2329739966"
        },
        {
          "name": "Ishita Kakkar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2329739330"
        },
        {
          "name": "Sindhu Kothe",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380031185"
        },
        {
          "name": "Arpine Hovasapian",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/6453166"
        },
        {
          "name": "Mai ElSherief",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380030807"
        }
      ]
    },
    {
      "id": "2509.09594",
      "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation",
      "authors": [
        "Sourav Garg",
        "Dustin Craggs",
        "Vineeth Bhat",
        "Lachlan Mares",
        "Stefan Podgorski",
        "Madhava Krishna",
        "Feras Dayoub",
        "Ian Reid"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Visual navigation using only a single camera and a topological map has\nrecently become an appealing alternative to methods that require additional\nsensors and 3D maps. This is typically achieved through an \"image-relative\"\napproach to estimating control from a given pair of current observation and\nsubgoal image. However, image-level representations of the world have\nlimitations because images are strictly tied to the agent's pose and\nembodiment. In contrast, objects, being a property of the map, offer an\nembodiment- and trajectory-invariant world representation. In this work, we\npresent a new paradigm of learning \"object-relative\" control that exhibits\nseveral desirable characteristics: a) new routes can be traversed without\nstrictly requiring to imitate prior experience, b) the control prediction\nproblem can be decoupled from solving the image matching problem, and c) high\ninvariance can be achieved in cross-embodiment deployment for variations across\nboth training-testing and mapping-execution settings. We propose a topometric\nmap representation in the form of a \"relative\" 3D scene graph, which is used to\nobtain more informative object-level global path planning costs. We train a\nlocal controller, dubbed \"ObjectReact\", conditioned directly on a high-level\n\"WayObject Costmap\" representation that eliminates the need for an explicit RGB\ninput. We demonstrate the advantages of learning object-relative control over\nits image-relative counterpart across sensor height variations and multiple\nnavigation tasks that challenge the underlying spatial understanding\ncapability, e.g., navigating a map trajectory in the reverse direction. We\nfurther show that our sim-only policy is able to generalize well to real-world\nindoor environments. Code and supplementary material are accessible via project\npage: https://object-react.github.io/",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09594v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09594v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.263,
      "datasets_score": 0.235,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09595",
      "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis",
      "authors": [
        "Yikang Ding",
        "Jiwen Liu",
        "Wenyuan Zhang",
        "Zekun Wang",
        "Wentao Hu",
        "Liyuan Cui",
        "Mingming Lao",
        "Yingchao Shao",
        "Hui Liu",
        "Xiaohan Li",
        "Ming Chen",
        "Xiaoqiang Liu",
        "Yu-Shen Liu",
        "Pengfei Wan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in audio-driven avatar video generation have significantly\nenhanced audio-visual realism. However, existing methods treat instruction\nconditioning merely as low-level tracking driven by acoustic or visual cues,\nwithout modeling the communicative purpose conveyed by the instructions. This\nlimitation compromises their narrative coherence and character expressiveness.\nTo bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that\nunifies multimodal instruction understanding with photorealistic portrait\ngeneration. Our approach adopts a two-stage pipeline. In the first stage, we\ndesign a multimodal large language model (MLLM) director that produces a\nblueprint video conditioned on diverse instruction signals, thereby governing\nhigh-level semantics such as character motion and emotions. In the second\nstage, guided by blueprint keyframes, we generate multiple sub-clips in\nparallel using a first-last frame strategy. This global-to-local framework\npreserves fine-grained details while faithfully encoding the high-level intent\nbehind multimodal instructions. Our parallel architecture also enables fast and\nstable generation of long-duration videos, making it suitable for real-world\napplications such as digital human livestreaming and vlogging. To\ncomprehensively evaluate our method, we construct a benchmark of 375 curated\nsamples covering diverse instructions and challenging scenarios. Extensive\nexperiments demonstrate that Kling-Avatar is capable of generating vivid,\nfluent, long-duration videos at up to 1080p and 48 fps, achieving superior\nperformance in lip synchronization accuracy, emotion and dynamic\nexpressiveness, instruction controllability, identity preservation, and\ncross-domain generalization. These results establish Kling-Avatar as a new\nbenchmark for semantically grounded, high-fidelity audio-driven avatar\nsynthesis.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09595v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09595v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.339,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes Video Diffusion Transformers (DiT) for generating avatar animations, which involves iterative refinement in video synthesis. However, this application focuses on visual content creation rather than adapting diffusion for complex logical tasks or holistic Chain-of-Thought reasoning. The MLLM Director handles instruction understanding, but there is no evidence of diffusion being used for multi-step logical reasoning, making the paper only loosely connected to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09597",
      "title": "Graph Alignment via Dual-Pass Spectral Encoding and Latent Space\n  Communication",
      "authors": [
        "Maysam Behmanesh",
        "Erkan Turan",
        "Maks Ovsjanikov"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Graph alignment, the problem of identifying corresponding nodes across\nmultiple graphs, is fundamental to numerous applications. Most existing\nunsupervised methods embed node features into latent representations to enable\ncross-graph comparison without ground-truth correspondences. However, these\nmethods suffer from two critical limitations: the degradation of node\ndistinctiveness due to oversmoothing in GNN-based embeddings, and the\nmisalignment of latent spaces across graphs caused by structural noise, feature\nheterogeneity, and training instability, ultimately leading to unreliable node\ncorrespondences. We propose a novel graph alignment framework that\nsimultaneously enhances node distinctiveness and enforces geometric consistency\nacross latent spaces. Our approach introduces a dual-pass encoder that combines\nlow-pass and high-pass spectral filters to generate embeddings that are both\nstructure-aware and highly discriminative. To address latent space\nmisalignment, we incorporate a geometry-aware functional map module that learns\nbijective and isometric transformations between graph embeddings, ensuring\nconsistent geometric relationships across different representations. Extensive\nexperiments on graph benchmarks demonstrate that our method consistently\noutperforms existing unsupervised alignment baselines, exhibiting superior\nrobustness to structural inconsistencies and challenging alignment scenarios.\nAdditionally, comprehensive evaluation on vision-language benchmarks using\ndiverse pretrained models shows that our framework effectively generalizes\nbeyond graph domains, enabling unsupervised alignment of vision and language\nrepresentations.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09597v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09597v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.376,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09610",
      "title": "Mechanistic Learning with Guided Diffusion Models to Predict\n  Spatio-Temporal Brain Tumor Growth",
      "authors": [
        "Daria Laslo",
        "Efthymios Georgiou",
        "Marius George Linguraru",
        "Andreas Rauschecker",
        "Sabine Muller",
        "Catherine R. Jutzeler",
        "Sarah Bruningk"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Predicting the spatio-temporal progression of brain tumors is essential for\nguiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic\nlearning framework that combines a mathematical tumor growth model with a\nguided denoising diffusion implicit model (DDIM) to synthesize anatomically\nfeasible future MRIs from preceding scans. The mechanistic model, formulated as\na system of ordinary differential equations, captures temporal tumor dynamics\nincluding radiotherapy effects and estimates future tumor burden. These\nestimates condition a gradient-guided DDIM, enabling image synthesis that\naligns with both predicted growth and patient anatomy. We train our model on\nthe BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices\nof in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our\nframework generates realistic follow-up scans based on spatial similarity\nmetrics. It also introduces tumor growth probability maps, which capture both\nclinically relevant extent and directionality of tumor growth as shown by 95th\npercentile Hausdorff Distance. The method enables biologically informed image\ngeneration in data-limited scenarios, offering generative-space-time\npredictions that account for mechanistic priors.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09610v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09610v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.553,
      "distributed_training_score": 0.323,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using a guided DDIM for synthesizing future MRI images of brain tumor growth, combined with a mechanistic ODE model. This is focused on generative image synthesis for medical predictions, not on adapting the diffusion process for multi-step logical reasoning or solving complex logical tasks, such as treating a Chain-of-Thought as a holistic entity for iterative correction. Therefore, it does not align with the topic's emphasis on logical reasoning applications.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09614",
      "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
      "authors": [
        "Jielin Qiu",
        "Zuxin Liu",
        "Zhiwei Liu",
        "Rithesh Murthy",
        "Jianguo Zhang",
        "Haolin Chen",
        "Shiyu Wang",
        "Ming Zhu",
        "Liangwei Yang",
        "Juntao Tan",
        "Zhepeng Cen",
        "Cheng Qian",
        "Shelby Heinecke",
        "Weiran Yao",
        "Silvio Savarese",
        "Caiming Xiong",
        "Huan Wang"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09614v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09614v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.377,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces LoCoBench, a benchmark for evaluating long-context large language models in software engineering tasks, focusing on code understanding, refactoring, and related capabilities. It does not mention or involve diffusion-based models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09616",
      "title": "Explaining Concept Drift through the Evolution of Group Counterfactuals",
      "authors": [
        "Ignacy Stępka",
        "Jerzy Stefanowski"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Machine learning models in dynamic environments often suffer from concept\ndrift, where changes in the data distribution degrade performance. While\ndetecting this drift is a well-studied topic, explaining how and why the\nmodel's decision-making logic changes still remains a significant challenge. In\nthis paper, we introduce a novel methodology to explain concept drift by\nanalyzing the temporal evolution of group-based counterfactual explanations\n(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their\nassociated counterfactual action vectors before and after a drift. These\nevolving GCEs act as an interpretable proxy, revealing structural changes in\nthe model's decision boundary and its underlying rationale. We operationalize\nthis analysis within a three-layer framework that synergistically combines\ninsights from the data layer (distributional shifts), the model layer\n(prediction disagreement), and our proposed explanation layer. We show that\nsuch holistic view allows for a more comprehensive diagnosis of drift, making\nit possible to distinguish between different root causes, such as a spatial\ndata shift versus a re-labeling of concepts.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09616v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09616v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.433,
      "distributed_training_score": 0.297,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on explaining concept drift using group counterfactual explanations (GCEs) in machine learning models, focusing on temporal evolution and XAI techniques. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09631",
      "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for\n  Low-Latency Zero-Shot Text-To-Speech",
      "authors": [
        "Ngoc-Son Nguyen",
        "Hieu-Nghia Huynh-Nguyen",
        "Thanh V. T. Tran",
        "Truong-Son Hy",
        "Van Nguyen"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that\nmimics the voice of an unseen speaker using only a short reference sample,\nrequiring not only speaker adaptation but also accurate modeling of prosodic\nattributes. Recent approaches based on language models, diffusion, and flow\nmatching have shown promising results in zero-shot TTS, but still suffer from\nslow inference and repetition artifacts. Discrete codec representations have\nbeen widely adopted for speech synthesis, and recent works have begun to\nexplore diffusion models in purely discrete settings, suggesting the potential\nof discrete generative modeling for speech synthesis. However, existing\nflow-matching methods typically embed these discrete tokens into a continuous\nspace and apply continuous flow matching, which may not fully leverage the\nadvantages of discrete representations. To address these challenges, we\nintroduce DiFlow-TTS, which, to the best of our knowledge, is the first model\nto explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS\nexplicitly models factorized speech attributes within a compact and unified\narchitecture. It leverages in-context learning by conditioning on textual\ncontent, along with prosodic and acoustic attributes extracted from a reference\nspeech, enabling effective attribute cloning in a zero-shot setting. In\naddition, the model employs a factorized flow prediction mechanism with\ndistinct heads for prosody and acoustic details, allowing it to learn\naspect-specific distributions. Experimental results demonstrate that DiFlow-TTS\nachieves promising performance in several key metrics, including naturalness,\nprosody, preservation of speaker style, and energy control. It also maintains a\ncompact model size and achieves low-latency inference, generating speech up to\n25.8 times faster than the latest existing baselines.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09631v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09631v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.485,
      "distributed_training_score": 0.398,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on DiFlow-TTS, a model using Discrete Flow Matching for zero-shot Text-to-Speech synthesis, which involves iterative generative processes similar to diffusion models. However, it applies this to speech generation and attribute cloning, not to complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity for holistic correction. Thus, while there is a loose connection through generative modeling techniques, the paper does not address diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09651",
      "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio\n  Regulations",
      "authors": [
        "Zakaria El Kassimi",
        "Fares Fourati",
        "Mohamed-Slim Alouini"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "We study question answering in the domain of radio regulations, a legally\nsensitive and high-stakes area. We propose a telecom-specific\nRetrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,\nthe first multiple-choice evaluation set for this domain, constructed from\nauthoritative sources using automated filtering and human validation. To assess\nretrieval quality, we define a domain-specific retrieval metric, under which\nour retriever achieves approximately 97% accuracy. Beyond retrieval, our\napproach consistently improves generation accuracy across all tested models. In\nparticular, while naively inserting documents without structured retrieval\nyields only marginal gains for GPT-4o (less than 1%), applying our pipeline\nresults in nearly a 12% relative improvement. These findings demonstrate that\ncarefully targeted grounding provides a simple yet strong baseline and an\neffective domain-specific solution for regulatory question answering. All code\nand evaluation scripts, along with our derived question-answer dataset, are\navailable at https://github.com/Zakaria010/Radio-RAG.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09651v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09651v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.351,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a Retrieval-Augmented Generation (RAG) pipeline for radio regulations, involving dataset creation with human validation, but does not involve training models using reinforcement learning or a reward model based on human-ranked data. There is no mention of RLHF techniques for aligning AI models with human preferences.",
      "weak_supervision_justification": "The paper uses automated filtering combined with human validation to create a dataset from authoritative sources, which partially aligns with weak supervision's use of programmatic label generation. However, the main contribution is the RAG pipeline and evaluation, not training models on noisy or imprecise labels as in weak supervision.",
      "diffusion_reasoning_justification": "The paper describes a RAG pipeline for retrieval and generation in radio regulations, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistic entity. It focuses solely on retrieval and accuracy improvements, not diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09655",
      "title": "Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for\n  Medicaid Care Management",
      "authors": [
        "Sanjay Basu",
        "Sadiq Y. Patel",
        "Parth Sheth",
        "Bhairavi Muralidharan",
        "Namrata Elamaran",
        "Aakriti Kinra",
        "Rajaie Batniji"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)",
        "stat.AP (Applications)"
      ],
      "abstract": "We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning\n(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds\nto reduce harm while equalizing a chosen fairness target (coverage or harm)\nacross protected subgroups. Using de-identified longitudinal trajectories from\na Medicaid population health management program, we evaluate FG-FARL against\nbehavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global\nconformal safety baseline). We report off-policy value estimates with bootstrap\n95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL\nachieves comparable value to baselines while improving fairness metrics,\ndemonstrating a practical path to safer and more equitable decision support.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09655v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09655v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.494,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.29,
      "distributed_training_score": 0.337,
      "datasets_score": 0.269,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces FG-FARL, an offline reinforcement learning method using logged trajectories from a Medicaid program to enhance safety and fairness, but it does not involve human feedback. Specifically, there is no mention of training a reward model from human-ranked data or using human preferences to fine-tune the model, which are essential components of RLHF. Instead, the approach relies on existing logged data and focuses on subgroup fairness and safety thresholds, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09658",
      "title": "Measuring Epistemic Humility in Multimodal Large Language Models",
      "authors": [
        "Bingkui Tong",
        "Jiaer Xia",
        "Sifeng Shang",
        "Kaiyang Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Hallucinations in multimodal large language models (MLLMs) -- where the model\ngenerates content inconsistent with the input image -- pose significant risks\nin real-world applications, from misinformation in visual question answering to\nunsafe errors in decision-making. Existing benchmarks primarily test\nrecognition accuracy, i.e., evaluating whether models can select the correct\nanswer among distractors. This overlooks an equally critical capability for\ntrustworthy AI: recognizing when none of the provided options are correct, a\nbehavior reflecting epistemic humility. We present HumbleBench, a new\nhallucination benchmark designed to evaluate MLLMs' ability to reject plausible\nbut incorrect answers across three hallucination types: object, relation, and\nattribute. Built from a panoptic scene graph dataset, we leverage fine-grained\nscene graph annotations to extract ground-truth entities and relations, and\nprompt GPT-4-Turbo to generate multiple-choice questions, followed by a\nrigorous manual filtering process. Each question includes a \"None of the above\"\noption, requiring models not only to recognize correct visual information but\nalso to identify when no provided answer is valid. We evaluate a variety of\nstate-of-the-art MLLMs -- including both general-purpose and specialized\nreasoning models -- on HumbleBench and share valuable findings and insights\nwith the community. By incorporating explicit false-option rejection,\nHumbleBench fills a key gap in current evaluation suites, providing a more\nrealistic measure of MLLM reliability in safety-critical settings. Our code and\ndataset are released publicly and can be accessed at\nhttps://github.com/maifoundations/HumbleBench.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09658v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09658v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.465,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.349,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on creating and evaluating a benchmark for MLLMs to assess epistemic humility and hallucination rejection, using GPT-4-Turbo for question generation and manual filtering for data validation. It does not involve training or fine-tuning models with human feedback to align preferences, as required for RLHF. There is no mention of a reward model or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper employs programmatic generation of questions and distractors using GPT-4-Turbo, combined with manual filtering, to create a large-scale benchmark from existing scene graph annotations. This approach aligns with weak supervision by using noisy or high-level sources for label generation rather than perfect hand-labeling. However, the paper's main contribution is benchmark evaluation, not a primary focus on weak supervision for model training.",
      "diffusion_reasoning_justification": "The paper evaluates MLLMs on a multiple-choice benchmark for hallucination detection and epistemic humility, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described. It does not adapt diffusion techniques for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces HumbleBench, a new benchmark designed to evaluate epistemic humility in multimodal large language models (MLLMs) by assessing their ability to reject plausible but incorrect answers in multiple-choice questions, addressing a gap in existing hallucination detection methods. Using a panoptic scene graph dataset, the authors leverage GPT-4-Turbo to generate 22,831 questions with a \"None of the above\" option, followed by manual filtering, and evaluate various state-of-the-art MLLMs, revealing that models struggle with false-option rejection and that scaling alone doesn't enhance robustness.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark focused on epistemic humility through false-option rejection, significantly advancing state-of-the-art evaluation methods for MLLMs by addressing an overlooked aspect of hallucination detection.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research in trustworthy AI and commercial applications by providing a robust tool for evaluating MLLM reliability, potentially leading to safer models in safety-critical settings.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with practical insights and a valuable dataset for AI researchers, making it essential for those working on MLLM reliability and hallucination benchmarks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/33dbdcb057dac3ec84f8160a982227abb6ed19c0",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 1.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Bingkui Tong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372256146"
        },
        {
          "name": "Jiaer Xia",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2182293108"
        },
        {
          "name": "Sifeng Shang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362430889"
        },
        {
          "name": "Kaiyang Zhou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2364091729"
        }
      ]
    },
    {
      "id": "2509.09666",
      "title": "Unified Multimodal Model as Auto-Encoder",
      "authors": [
        "Zhiyuan Yan",
        "Kaiqing Lin",
        "Zongjian Li",
        "Junyan Ye",
        "Hui Han",
        "Zhendong Wang",
        "Hao Liu",
        "Bin Lin",
        "Hao Li",
        "Xue Xu",
        "Xinyan Xiao",
        "Jingdong Wang",
        "Haifeng Wang",
        "Li Yuan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The pursuit of unified multimodal models (UMMs) has long been hindered by a\nfundamental schism between multimodal understanding and generation. Current\napproaches typically disentangle the two and treat them as separate endeavors\nwith disjoint objectives, missing the mutual benefits. We argue that true\nunification requires more than just merging two tasks. It requires a unified,\nfoundational objective that intrinsically links them. In this paper, we\nintroduce an insightful paradigm through the Auto-Encoder lens, i.e., regarding\nunderstanding as the encoder (I2T) that compresses images into text, and\ngeneration as the decoder (T2I) that reconstructs images from that text. To\nimplement this, we propose UAE, where we begin by pre-training the decoder with\nthe proposed 700k long-context image-caption pairs to direct it to \"understand\"\nthe fine-grained and complex semantics from the text. We then propose\nUnified-GRPO via reinforcement learning (RL) to unify the two, which covers two\ncomplementary stages: (1) Generation for Understanding, where the encoder is\ntrained to generate informative captions that maximize the decoder's\nreconstruction quality, enhancing its visual perception; (2) Understanding for\nGeneration, where the decoder is refined to reconstruct from these captions,\nforcing it to leverage every detail and improving its long-context instruction\nfollowing and generation fidelity. Our empirical results suggest that\nunderstanding can largely enhance generation (verified on GenEval), while\ngeneration, in turn, notably strengthens fine-grained visual perception like\nsmall object and color recognition (verified on MMT-Bench). This bidirectional\nimprovement reveals a deep synergy: under the unified reconstruction objective,\ngeneration and understanding can mutually benefit each other, moving closer to\ntruly unified multimodal intelligence.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09666v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09666v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.351,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a unified multimodal model using an auto-encoder framework and reinforcement learning to mutually enhance understanding and generation tasks. It references diffusion-based objectives in existing works but does not adapt the iterative refinement process of diffusion for multi-step logical reasoning or chain-of-thought tasks. Therefore, there is no clear component involving diffusion for reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09667",
      "title": "Geometric Neural Distance Fields for Learning Human Motion Priors",
      "authors": [
        "Zhengdi Yu",
        "Simone Foti",
        "Linguang Zhang",
        "Amy Zhao",
        "Cem Keskin",
        "Stefanos Zafeiriou",
        "Tolga Birdal"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative\nhuman motion prior that enables robust, temporally consistent, and physically\nplausible 3D motion recovery. Unlike existing VAE or diffusion-based methods,\nour higher-order motion prior explicitly models the human motion in the zero\nlevel set of a collection of neural distance fields (NDFs) corresponding to\npose, transition (velocity), and acceleration dynamics. Our framework is\nrigorous in the sense that our NDFs are constructed on the product space of\njoint rotations, their angular velocities, and angular accelerations,\nrespecting the geometry of the underlying articulations. We further introduce:\n(i) a novel adaptive-step hybrid algorithm for projecting onto the set of\nplausible motions, and (ii) a novel geometric integrator to \"roll out\"\nrealistic motion trajectories during test-time-optimization and generation. Our\nexperiments show significant and consistent gains: trained on the AMASS\ndataset, NRMF remarkably generalizes across multiple input modalities and to\ndiverse tasks ranging from denoising to motion in-betweening and fitting to\npartial 2D / 3D observations.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09667v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09667v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.367,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on generative models for 3D human motion using neural distance fields and does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper references diffusion-based methods as existing approaches for motion modeling but does not adapt diffusion for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity; instead, it critiques these methods and proposes an alternative for human motion tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09671",
      "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from\n  Reference-Scoped Exploration",
      "authors": [
        "Sirui Xu",
        "Yu-Wei Chao",
        "Liuyu Bian",
        "Arsalan Mousavian",
        "Yu-Xiong Wang",
        "Liang-Yan Gui",
        "Wei Yang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Hand-object motion-capture (MoCap) repositories offer large-scale,\ncontact-rich demonstrations and hold promise for scaling dexterous robotic\nmanipulation. Yet demonstration inaccuracies and embodiment gaps between human\nand robot hands limit the straightforward use of these data. Existing methods\nadopt a three-stage workflow, including retargeting, tracking, and residual\ncorrection, which often leaves demonstrations underused and compound errors\nacross stages. We introduce Dexplore, a unified single-loop optimization that\njointly performs retargeting and tracking to learn robot control policies\ndirectly from MoCap at scale. Rather than treating demonstrations as ground\ntruth, we use them as soft guidance. From raw trajectories, we derive adaptive\nspatial scopes, and train with reinforcement learning to keep the policy\nin-scope while minimizing control effort and accomplishing the task. This\nunified formulation preserves demonstration intent, enables robot-specific\nstrategies to emerge, improves robustness to noise, and scales to large\ndemonstration corpora. We distill the scaled tracking policy into a\nvision-based, skill-conditioned generative controller that encodes diverse\nmanipulation skills in a rich latent representation, supporting generalization\nacross objects and real-world deployment. Taken together, these contributions\nposition Dexplore as a principled bridge that transforms imperfect\ndemonstrations into effective training signals for dexterous manipulation.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09671v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09671v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.385,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with human MoCap data as soft guidance, which involves learning from human demonstrations. However, it does not involve training a separate reward model on human-ranked data or fine-tuning based on explicit human preferences, as required for RLHF. Thus, it is only tangentially related through the general use of human data in RL.",
      "weak_supervision_justification": "The paper treats human MoCap data as noisy, imprecise sources for generating adaptive spatial scopes and training signals, rather than relying on perfectly labeled data. This aligns directly with weak supervision, as it programmatically derives labels from high-level demonstrations to train the policy at scale.",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning and policy distillation for dexterous manipulation, with no mention of diffusion models, iterative refinement for logical reasoning, or treating a chain-of-thought as a holistic entity. It lacks any component for multi-step logical reasoning using diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "Dexplore presents a unified single-loop optimization framework to learn dexterous manipulation policies directly from human motion-capture (MoCap) data, addressing embodiment mismatches by treating demonstrations as soft references with adaptive spatial scopes in reinforcement learning. The methodology involves deriving spatial envelopes from raw trajectories, training policies to stay within these scopes while minimizing control effort, and distilling the resulting tracker into a vision-based, skill-conditioned generative controller that supports generalization across objects and real-world deployment on robotic hands.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new unified single-loop optimization technique that integrates retargeting and tracking with adaptive spatial scopes, significantly advancing the state-of-the-art in learning from MoCap data for dexterous manipulation. This approach avoids the pitfalls of traditional multi-stage workflows, allowing for more robust and scalable policy learning.",
      "impact_score": "High",
      "impact_justification": "The work could influence a wide range of future research in robotics and computer vision by providing a scalable method to utilize large MoCap datasets for real-world dexterous manipulation tasks. Its demonstration of real-world deployment suggests potential for practical applications in areas like automation and assisted living.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution with real-world validation, making it valuable for researchers in robotics and computer vision to understand advancements in manipulation learning. However, while insightful, it may not be essential for those outside the specific subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ff55393f87cd7d93b5103cdc0e7f8ee2384d1b76",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 32,
      "average_h_index": 9.285714285714286,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Sirui Xu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/8775961"
        },
        {
          "name": "Yu-Wei Chao",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2306062337"
        },
        {
          "name": "Liuyu Bian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380030949"
        },
        {
          "name": "A. Mousavian",
          "h_index": 32,
          "profile_url": "https://www.semanticscholar.org/author/3040583"
        },
        {
          "name": "Yu-Xiong Wang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2236683644"
        },
        {
          "name": "Liangyan Gui",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2587808"
        },
        {
          "name": "Wei Yang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2266191697"
        }
      ]
    },
    {
      "id": "2509.09672",
      "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
      "authors": [
        "Artem Lukoianov",
        "Chenyang Yuan",
        "Justin Solomon",
        "Vincent Sitzmann"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Among generative models, diffusion models are uniquely intriguing due to the\nexistence of a closed-form optimal minimizer of their training objective, often\nreferred to as the optimal denoiser. However, diffusion using this optimal\ndenoiser merely reproduces images in the training set and hence fails to\ncapture the behavior of deep diffusion models. Recent work has attempted to\ncharacterize this gap between the optimal denoiser and deep diffusion models,\nproposing analytical, training-free models that can generate images that\nresemble those generated by a trained UNet. The best-performing method\nhypothesizes that shift equivariance and locality inductive biases of\nconvolutional neural networks are the cause of the performance gap, hence\nincorporating these assumptions into its analytical model. In this work, we\npresent evidence that the locality in deep diffusion models emerges as a\nstatistical property of the image dataset, not due to the inductive bias of\nconvolutional neural networks. Specifically, we demonstrate that an optimal\nparametric linear denoiser exhibits similar locality properties to the deep\nneural denoisers. We further show, both theoretically and experimentally, that\nthis locality arises directly from the pixel correlations present in natural\nimage datasets. Finally, we use these insights to craft an analytical denoiser\nthat better matches scores predicted by a deep diffusion model than the prior\nexpert-crafted alternative.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09672v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09672v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.553,
      "distributed_training_score": 0.345,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on the statistical properties of image diffusion models, specifically how locality emerges from data correlations in generative image tasks. It does not involve adapting diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks, which are central to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09674",
      "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
      "authors": [
        "Haozhan Li",
        "Yuxin Zuo",
        "Jiale Yu",
        "Yuhao Zhang",
        "Zhaohui Yang",
        "Kaiyan Zhang",
        "Xuekai Zhu",
        "Yuchen Zhang",
        "Tianxing Chen",
        "Ganqu Cui",
        "Dehui Wang",
        "Dingxiang Luo",
        "Yuchen Fan",
        "Youbang Sun",
        "Jia Zeng",
        "Jiangmiao Pang",
        "Shanghang Zhang",
        "Yu Wang",
        "Yao Mu",
        "Bowen Zhou",
        "Ning Ding"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Vision-Language-Action (VLA) models have recently emerged as a powerful\nparadigm for robotic manipulation. Despite substantial progress enabled by\nlarge-scale pretraining and supervised fine-tuning (SFT), these models face two\nfundamental challenges: (i) the scarcity and high cost of large-scale\nhuman-operated robotic trajectories required for SFT scaling, and (ii) limited\ngeneralization to tasks involving distribution shift. Recent breakthroughs in\nLarge Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can\ndramatically enhance step-by-step reasoning capabilities, raising a natural\nquestion: Can RL similarly improve the long-horizon step-by-step action\nplanning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL\nframework tailored for VLA models. Building upon veRL, we introduce\nVLA-specific trajectory sampling, scalable parallelization, multi-environment\nrendering, and optimized loss computation. When applied to OpenVLA-OFT,\nSimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\\pi_0$\non RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce.\nSimpleVLA-RL not only reduces dependence on large-scale data and enables robust\ngeneralization, but also remarkably surpasses SFT in real-world tasks.\nMoreover, we identify a novel phenomenon ``pushcut'' during RL training,\nwherein the policy discovers previously unseen patterns beyond those seen in\nthe previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09674v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09674v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.477,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.409,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on applying reinforcement learning (RL) to Vision-Language-Action (VLA) models using environmental rewards and exploration strategies, without any mention of human feedback, a reward model trained on human-ranked data, or alignment with human preferences. It relies on rule-based or outcome-driven rewards, which differs from RLHF.",
      "weak_supervision_justification": "The paper addresses data efficiency in RL for VLA models, such as using a single demonstration per task to improve generalization, which indirectly relates to reducing reliance on high-quality labeled data. However, it does not primarily involve programmatically generating noisy labels or weak supervision techniques, focusing instead on RL frameworks.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper explicitly describes extending veRL with scalable parallelization, multi-environment rendering, and an integrated framework for faster sampling and distributed training, directly aligning with distributed training concepts like parallel computing across multiple nodes to accelerate RL processes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "SimpleVLA-RL introduces an efficient reinforcement learning framework for Vision-Language-Action (VLA) models to address challenges in scaling training and improving generalization, building upon the veRL framework with VLA-specific enhancements like trajectory sampling, parallelization, and optimized loss computation. The methodology involves applying RL to VLA models to reduce dependence on large-scale human-operated data, resulting in state-of-the-art performance on benchmarks such as LIBERO and RoboTwin, significant improvements in success rates and generalization, and effective real-world deployment, while also identifying a novel \"pushcut\" phenomenon where policies discover new patterns.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting reinforcement learning to VLA models, combining existing RL techniques with VLA-specific innovations to address known challenges, though it builds on prior work like veRL rather than introducing a entirely new paradigm. This clever adaptation enhances scalability and generalization but does not constitute a groundbreaking new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in robotics and AI by providing a more efficient training method for VLA models, potentially leading to broader adoption in subfields dealing with robotic manipulation and generalization. However, its impact may be confined to specific applications rather than causing widespread changes across diverse areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable contributions in scaling VLA training and achieving strong empirical results, making it a significant advancement for researchers in robotics and AI. While essential for those working directly in VLA or RL applications, it is not universally groundbreaking enough to be a must-read for all.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bca7f4dd4db559d6772b470d9fe5391e3608cc8c",
      "total_authors": 21,
      "authors_found": 19,
      "highest_h_index": 32,
      "average_h_index": 5.2631578947368425,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Hao-Si Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2297408582"
        },
        {
          "name": "Yuxin Zuo",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2329372501"
        },
        {
          "name": "Jiale Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380082677"
        },
        {
          "name": "Yuhao Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380087001"
        },
        {
          "name": "Zhaohui Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380373767"
        },
        {
          "name": "Kaiyan Zhang",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/2153281320"
        },
        {
          "name": "Xuekai Zhu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2145238612"
        },
        {
          "name": "Yuchen Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2324066000"
        },
        {
          "name": "Tianxing Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2345191789"
        },
        {
          "name": "Ganqu Cui",
          "h_index": 26,
          "profile_url": "https://www.semanticscholar.org/author/52297757"
        },
        {
          "name": "Dehui Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371073078"
        },
        {
          "name": "Dingxiang Luo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380436784"
        },
        {
          "name": "Yuchen Fan",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2295138535"
        },
        {
          "name": "Youbang Sun",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2329730696"
        },
        {
          "name": "Jia Zeng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jiangmiao Pang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377561990"
        },
        {
          "name": "Shanghang Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2288248563"
        },
        {
          "name": "Yu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380133886"
        },
        {
          "name": "Yao Mu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2348161293"
        },
        {
          "name": "Bowen Zhou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ning Ding",
          "h_index": 32,
          "profile_url": "https://www.semanticscholar.org/author/46649145"
        }
      ]
    },
    {
      "id": "2509.09675",
      "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning\n  in Large Language Models",
      "authors": [
        "Runpeng Dai",
        "Linfeng Song",
        "Haolin Liu",
        "Zhenwen Liang",
        "Dian Yu",
        "Haitao Mi",
        "Zhaopeng Tu",
        "Rui Liu",
        "Tong Zheng",
        "Hongtu Zhu",
        "Dong Yu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09675v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09675v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.484,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.507,
      "distributed_training_score": 0.379,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces Curiosity-Driven Exploration (CDE) for Reinforcement Learning with Verifiable Rewards (RLVR), which relies on automated rewards based on final-answer correctness, not a separate reward model trained on human-ranked data. Since no human feedback is used, it does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning techniques for LLMs, specifically CDE within RLVR, and does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity over multiple steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09676",
      "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
      "authors": [
        "Jiahao Wang",
        "Yufeng Yuan",
        "Rujie Zheng",
        "Youtian Lin",
        "Jian Gao",
        "Lin-Zhuo Chen",
        "Yajie Bao",
        "Yi Zhang",
        "Chang Zeng",
        "Yanxi Zhou",
        "Xiaoxiao Long",
        "Hao Zhu",
        "Zhaoxiang Zhang",
        "Xun Cao",
        "Yao Yao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Significant progress has been made in spatial intelligence, spanning both\nspatial reconstruction and world exploration. However, the scalability and\nreal-world fidelity of current models remain severely constrained by the\nscarcity of large-scale, high-quality training data. While several datasets\nprovide camera pose information, they are typically limited in scale,\ndiversity, and annotation richness, particularly for real-world dynamic scenes\nwith ground-truth camera motion. To this end, we collect \\textbf{SpatialVID}, a\ndataset consists of a large corpus of in-the-wild videos with diverse scenes,\ncamera movements and dense 3D annotations such as per-frame camera poses,\ndepth, and motion instructions. Specifically, we collect more than 21,000 hours\nof raw video, and process them into 2.7 million clips through a hierarchical\nfiltering pipeline, totaling 7,089 hours of dynamic content. A subsequent\nannotation pipeline enriches these clips with detailed spatial and semantic\ninformation, including camera poses, depth maps, dynamic masks, structured\ncaptions, and serialized motion instructions. Analysis of SpatialVID's data\nstatistics reveals a richness and diversity that directly foster improved model\ngeneralization and performance, establishing it as a key asset for the video\nand 3D vision research community.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09676v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09676v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.344,
      "datasets_score": 0.408,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new large-scale video dataset called SpatialVID, including its curation methodology, annotation processes, and statistical analysis. This directly aligns with research on creating, analyzing, and evaluating datasets for machine learning and AI, as it focuses on dataset development for 3D vision and video applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces SpatialVID, a large-scale dataset designed to advance spatial intelligence in computer vision by providing over 7,000 hours of high-quality video clips derived from more than 21,000 hours of raw internet footage. Through a hierarchical filtering and annotation pipeline, the authors enrich these clips with comprehensive spatial and semantic annotations, including camera poses, depth maps, dynamic masks, structured captions, and motion instructions, aiming to bridge the gap between semantic video data and geometric ground truth, thereby enhancing model training for 3D reconstruction and world simulation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new large-scale dataset with dense 3D annotations, addressing the scarcity of high-quality training data for spatial intelligence and significantly advancing the state-of-the-art in video and 3D vision research.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence a wide range of future research in computer vision, particularly in 3D reconstruction and world modeling, by providing a rich resource that improves model generalization and enables new applications in areas like autonomous navigation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a significant and valuable contribution through its innovative dataset, making it essential for researchers in computer vision and 3D vision to be aware of for advancing their work, though not universally critical for all AI fields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/df7655c7fd98fdd0f026364f4c98c2cc23fabff9",
      "total_authors": 15,
      "authors_found": 14,
      "highest_h_index": 7,
      "average_h_index": 1.0714285714285714,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Jiahao Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380094386"
        },
        {
          "name": "Yufeng Yuan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380048042"
        },
        {
          "name": "Rujie Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380353451"
        },
        {
          "name": "Youtian Lin",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2268430574"
        },
        {
          "name": "Jian Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380567159"
        },
        {
          "name": "Lin-Zhuo Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2344844868"
        },
        {
          "name": "Yajie Bao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yi Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380088162"
        },
        {
          "name": "Chang Zeng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380434785"
        },
        {
          "name": "Yanxi Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380558617"
        },
        {
          "name": "Xiaoxiao Long",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2325298178"
        },
        {
          "name": "Hao Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380020246"
        },
        {
          "name": "Zhaoxiang Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380128782"
        },
        {
          "name": "Xun Cao",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2268453713"
        },
        {
          "name": "Yao Yao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380551629"
        }
      ]
    },
    {
      "id": "2509.09677",
      "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMs",
      "authors": [
        "Akshit Sinha",
        "Arvindh Arun",
        "Shashwat Goel",
        "Steffen Staab",
        "Jonas Geiping"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Does continued scaling of large language models (LLMs) yield diminishing\nreturns? In this work, we show that short-task benchmarks may give an illusion\nof slowing progress, as even marginal gains in single-step accuracy can\ncompound into exponential improvements in the length of tasks a model can\nsuccessfully complete. Then, we argue that failures of LLMs when simple tasks\nare made longer arise from mistakes in execution, rather than an inability to\nreason. So, we propose isolating execution capability, by explicitly providing\nthe knowledge and plan needed to solve a long-horizon task. First, we find that\nlarger models can correctly execute significantly more turns even when small\nmodels have near-perfect single-turn accuracy. We then observe that the\nper-step accuracy of models degrades as the number of steps increases. This is\nnot just due to long-context limitations -- curiously, we observe a\nself-conditioning effect -- models become more likely to make mistakes when the\ncontext contains their errors from prior turns. Self-conditioning does not\nreduce by just scaling the model size. But, we find that thinking mitigates\nself-conditioning, and also enables execution of much longer tasks in a single\nturn. We conclude by benchmarking frontier thinking models on the length of\ntasks they can execute in a single turn. Overall, by focusing on the ability to\nexecute, we hope to reconcile debates on how LLMs can solve complex reasoning\nproblems yet fail at simple tasks when made longer, and highlight the massive\nbenefits of scaling model size and sequential test-time compute for\nlong-horizon tasks.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09677v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09677v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.505,
      "distributed_training_score": 0.428,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating long-horizon execution in LLMs through scaling and prompting techniques, without any mention of human feedback, reward models, or fine-tuning via reinforcement learning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses chain-of-thought prompting and execution in LLMs but does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning.",
      "distributed_training_justification": "The paper examines the effects of scaling model size on LLM performance but does not address distributed training techniques, parallel computing algorithms, or systems for multi-node machine learning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09679",
      "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
      "authors": [
        "Bingxin Xu",
        "Zhen Dong",
        "Oussama Elachqar",
        "Yuzhang Shang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. In this work, we propose ButterflyQuant, which\nreplaces Hadamard rotations with learnable butterfly transforms parameterized\nby continuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and thus prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP.\n\\href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09679v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09679v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.371,
      "datasets_score": 0.247,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09680",
      "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
      "authors": [
        "Rongyao Fang",
        "Aldrich Yu",
        "Chengqi Duan",
        "Linjiang Huang",
        "Shuai Bai",
        "Yuxuan Cai",
        "Kun Wang",
        "Si Liu",
        "Xihui Liu",
        "Hongsheng Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The advancement of open-source text-to-image (T2I) models has been hindered\nby the absence of large-scale, reasoning-focused datasets and comprehensive\nevaluation benchmarks, resulting in a performance gap compared to leading\nclosed-source systems. To address this challenge, We introduce FLUX-Reason-6M\nand PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).\nFLUX-Reason-6M is a massive dataset consisting of 6 million high-quality\nFLUX-generated images and 20 million bilingual (English and Chinese)\ndescriptions specifically designed to teach complex reasoning. The image are\norganized according to six key characteristics: Imagination, Entity, Text\nrendering, Style, Affection, and Composition, and design explicit Generation\nChain-of-Thought (GCoT) to provide detailed breakdowns of image generation\nsteps. The whole data curation takes 15,000 A100 GPU days, providing the\ncommunity with a resource previously unattainable outside of large industrial\nlabs. PRISM-Bench offers a novel evaluation standard with seven distinct\ntracks, including a formidable Long Text challenge using GCoT. Through\ncarefully designed prompts, it utilizes advanced vision-language models for\nnuanced human-aligned assessment of prompt-image alignment and image\naesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench\nreveals critical performance gaps and highlights specific areas requiring\nimprovement. Our dataset, benchmark, and evaluation code are released to\ncatalyze the next wave of reasoning-oriented T2I generation. Project page:\nhttps://flux-reason-6m.github.io/ .",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09680v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09680v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.533,
      "distributed_training_score": 0.381,
      "datasets_score": 0.438,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a dataset and benchmark for text-to-image (T2I) models, which may involve diffusion-based generation (e.g., using FLUX), but it does not develop or evaluate models that adapt the iterative refinement process of diffusion for multi-step logical reasoning tasks. While Generation Chain-of-Thought (GCoT) is mentioned for image generation steps, it is not presented as a diffusion-specific reasoning mechanism.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation of FLUX-Reason-6M, a large-scale dataset with 6 million images and 20 million captions, along with curation methodologies, annotation processes, and the PRISM-Bench for evaluation. This directly aligns with research on dataset creation, benchmarking, and analysis for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces FLUX-Reason-6M, a large-scale dataset comprising 6 million high-quality images and 20 million bilingual descriptions designed to enhance reasoning capabilities in text-to-image (T2I) models, and PRISM-Bench, a comprehensive benchmark with seven tracks for evaluating T2I performance. The methodology involves synthesizing images using advanced models, organizing them around six key characteristics with Generation Chain-of-Thought (GCoT) annotations, and employing vision-language models for nuanced evaluation; key findings reveal performance gaps in leading models and underscore the need for improvements in reasoning-oriented T2I generation, with the resources released to the community for broader impact.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new large-scale dataset and benchmark focused on reasoning in T2I models, including innovative elements like GCoT, which significantly advances the state-of-the-art by addressing gaps in existing resources.",
      "impact_score": "High",
      "impact_justification": "The work provides a foundational dataset and benchmark that could influence a wide range of future T2I research and applications by enabling better model training and evaluation, especially in open-source communities.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to T2I research with its innovative dataset and benchmark, making it essential for researchers in computer vision and language to be aware of for advancing their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1801e22c16090ae6b11489efaba5f8c51ac123ea",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 13,
      "average_h_index": 3.1,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Rongyao Fang",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/144484147"
        },
        {
          "name": "Aldrich Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380115008"
        },
        {
          "name": "Chengqi Duan",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2326302007"
        },
        {
          "name": "Linjiang Huang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2292210560"
        },
        {
          "name": "Shuai Bai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380470739"
        },
        {
          "name": "Yuxuan Cai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380535641"
        },
        {
          "name": "Kun Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380211459"
        },
        {
          "name": "Si Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2205666900"
        },
        {
          "name": "Xihui Liu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2286520347"
        },
        {
          "name": "Hongsheng Li",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2268799275"
        }
      ]
    },
    {
      "id": "2509.09742",
      "title": "Images in Motion?: A First Look into Video Leakage in Collaborative Deep\n  Learning",
      "authors": [
        "Md Fazle Rasul",
        "Alanood Alqobaisi",
        "Bruhadeshwar Bezawada",
        "Indrakshi Ray"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Federated learning (FL) allows multiple entities to train a shared model\ncollaboratively. Its core, privacy-preserving principle is that participants\nonly exchange model updates, such as gradients, and never their raw, sensitive\ndata. This approach is fundamental for applications in domains where privacy\nand confidentiality are important. However, the security of this very mechanism\nis threatened by gradient inversion attacks, which can reverse-engineer private\ntraining data directly from the shared gradients, defeating the purpose of FL.\nWhile the impact of these attacks is known for image, text, and tabular data,\ntheir effect on video data remains an unexamined area of research. This paper\npresents the first analysis of video data leakage in FL using gradient\ninversion attacks. We evaluate two common video classification approaches: one\nemploying pre-trained feature extractors and another that processes raw video\nframes with simple transformations. Our initial results indicate that the use\nof feature extractors offers greater resilience against gradient inversion\nattacks. We also demonstrate that image super-resolution techniques can enhance\nthe frames extracted through gradient inversion attacks, enabling attackers to\nreconstruct higher-quality videos. Our experiments validate this across\nscenarios where the attacker has access to zero, one, or more reference frames\nfrom the target environment. We find that although feature extractors make\nattacks more challenging, leakage is still possible if the classifier lacks\nsufficient complexity. We, therefore, conclude that video data leakage in FL is\na viable threat, and the conditions under which it occurs warrant further\ninvestigation.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09742v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09742v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.376,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09744",
      "title": "Structure Matters: Brain Graph Augmentation via Learnable Edge Masking\n  for Data-efficient Psychiatric Diagnosis",
      "authors": [
        "Mujie Liu",
        "Chenze Wang",
        "Liping Chen",
        "Nguyen Linh Dan Le",
        "Niharika Tewari",
        "Ting Dang",
        "Jiangang Ma",
        "Feng Xia"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The limited availability of labeled brain network data makes it challenging\nto achieve accurate and interpretable psychiatric diagnoses. While\nself-supervised learning (SSL) offers a promising solution, existing methods\noften rely on augmentation strategies that can disrupt crucial structural\nsemantics in brain graphs. To address this, we propose SAM-BG, a two-stage\nframework for learning brain graph representations with structural semantic\npreservation. In the pre-training stage, an edge masker is trained on a small\nlabeled subset to capture key structural semantics. In the SSL stage, the\nextracted structural priors guide a structure-aware augmentation process,\nenabling the model to learn more semantically meaningful and robust\nrepresentations. Experiments on two real-world psychiatric datasets demonstrate\nthat SAM-BG outperforms state-of-the-art methods, particularly in small-labeled\ndata settings, and uncovers clinically relevant connectivity patterns that\nenhance interpretability. Our code is available at\nhttps://github.com/mjliu99/SAM-BG.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09744v4",
      "pdf_url": "http://arxiv.org/pdf/2509.09744v4",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.34,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using a small labeled subset to train an edge masker, which then guides self-supervised learning on unlabeled data. This approach aligns closely with weak supervision, as it programmatically leverages limited, potentially noisy labels to generate meaningful representations without relying on large-scale, perfectly annotated datasets, enhancing data efficiency in psychiatric diagnosis.",
      "diffusion_reasoning_justification": "The paper focuses on graph augmentation and self-supervised learning for brain networks, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component that adapts diffusion for reasoning tasks, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis,\" addresses the challenge of limited labeled brain network data in psychiatric diagnosis by proposing SAM-BG, a two-stage framework that first pre-trains an edge masker on a small labeled subset to capture key structural semantics, then applies it in self-supervised learning for structure-aware augmentation to generate robust and interpretable representations. Experiments on real-world fMRI datasets demonstrate that SAM-BG outperforms state-of-the-art methods, especially in low-data scenarios, while revealing clinically relevant connectivity patterns that enhance diagnostic accuracy and interpretability.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining self-supervised learning with a learnable edge masker tailored to preserve structural semantics in brain graphs, offering a clever adaptation of existing techniques for psychiatric diagnosis rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in AI-driven psychiatric diagnosis by providing a data-efficient method for brain graph analysis, though its impact may be confined to specific subfields like medical graph learning rather than broader applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to data-efficient AI in healthcare with practical implications for psychiatric diagnosis, making it valuable for researchers in machine learning and neuroscience, though it is not essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1aaf6803264b7779d1e8cc69d4f34c64b83b5f89",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 2,
      "average_h_index": 0.625,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Mujie Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313189138"
        },
        {
          "name": "Chenze Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380460861"
        },
        {
          "name": "Liping Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372246434"
        },
        {
          "name": "Nguyen Linh Dan Le",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2367044229"
        },
        {
          "name": "Niharika Tewari",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380445106"
        },
        {
          "name": "Ting Dang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380445559"
        },
        {
          "name": "Jiangang Ma",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2275085408"
        },
        {
          "name": "Feng Xia",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2265653924"
        }
      ]
    },
    {
      "id": "2509.09747",
      "title": "D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for\n  Unimodal Inference",
      "authors": [
        "Leen Daher",
        "Zhaobo Wang",
        "Malcolm Mielle"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Cross-modal transfer learning is used to improve multi-modal classification\nmodels (e.g., for human activity recognition in human-robot collaboration).\nHowever, existing methods require paired sensor data at both training and\ninference, limiting deployment in resource-constrained environments where full\nsensor suites are not economically and technically usable. To address this, we\npropose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns\nmodality-specific representations without requiring joint sensor modality\nduring inference. Our approach combines a self-attention module for feature\nextraction with a novel cross-attention alignment loss, which enforces the\nalignment of sensors' feature spaces without requiring the coupling of the\nclassification pipelines of both modalities. We evaluate D-CAT on three\nmulti-modal human activity datasets (IMU, video, and audio) under both\nin-distribution and out-of-distribution scenarios, comparing against uni-modal\nmodels. Results show that in in-distribution scenarios, transferring from\nhigh-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains\nover uni-modal training. In out-of-distribution scenarios, even weaker source\nmodalities (e.g., IMU to video) improve target performance, as long as the\ntarget model isn't overfitted on the training data. By enabling single-sensor\ninference with cross-modal knowledge, D-CAT reduces hardware redundancy for\nperception systems while maintaining accuracy, which is critical for\ncost-sensitive or adaptive deployments (e.g., assistive robots in homes with\nvariable sensor availability). Code is available at\nhttps://github.com/Schindler-EPFL-Lab/D-CAT.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09747v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09747v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.403,
      "datasets_score": 0.433,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on cross-modal transfer learning and attention-based feature alignment, with no mention of generating labels from noisy or imprecise sources. It relies on standard supervised training with paired data during training, not weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper introduces a cross-attention alignment loss and self-attention for feature extraction, but does not involve diffusion models, iterative refinement for logical reasoning, or any multi-step reasoning processes as defined.",
      "distributed_training_justification": "The paper does not address parallel computing, multi-node training, or strategies for partitioning data/computation across processors. Its focus is on model architecture for modality transfer, not distributed systems.",
      "datasets_justification": "The paper evaluates its framework on existing multi-modal datasets (IMU, video, audio) through benchmarking in various scenarios, which aligns with dataset evaluation. However, it does not primarily focus on creating, analyzing, or curating datasets, making it only peripheral to this topic.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09750",
      "title": "A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO\n  Networks for Object Detection in Densely Packed Retail Images",
      "authors": [
        "Hossein Yazdanjouei",
        "Arash Mansouri",
        "Mohammad Shokouhifar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study proposes a semi-supervised co-training framework for object\ndetection in densely packed retail environments, where limited labeled data and\ncomplex conditions pose major challenges. The framework combines Faster R-CNN\n(utilizing a ResNet backbone) for precise localization with YOLO (employing a\nDarknet backbone) for global context, enabling mutual pseudo-label exchange\nthat improves accuracy in scenes with occlusion and overlapping objects. To\nstrengthen classification, it employs an ensemble of XGBoost, Random Forest,\nand SVM, utilizing diverse feature representations for higher robustness.\nHyperparameters are optimized using a metaheuristic-driven algorithm, enhancing\nprecision and efficiency across models. By minimizing reliance on manual\nlabeling, the approach reduces annotation costs and adapts effectively to\nfrequent product and layout changes common in retail. Experiments on the\nSKU-110k dataset demonstrate strong performance, highlighting the scalability\nand practicality of the proposed framework for real-world retail applications\nsuch as automated inventory tracking, product monitoring, and checkout systems.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09750v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09750v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.304,
      "distributed_training_score": 0.38,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper proposes a semi-supervised framework that uses mutual pseudo-label exchange between Faster R-CNN and YOLO networks, which aligns closely with weak supervision by programmatically generating labels from model predictions rather than relying solely on hand-labeled data. This approach reduces annotation costs and handles noisy or imprecise labels, directly matching the definition of training models with high-level, noisy sources for large-scale labeling.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper proposes a semi-supervised co-training framework that integrates Faster R-CNN for precise localization and YOLO for global context to enhance object detection in densely packed retail images, addressing challenges like limited labeled data and occlusions. The methodology involves mutual pseudo-label exchange between the models, an ensemble of classifiers (XGBoost, Random Forest, and SVM) for robust classification, and hyperparameter optimization via a metaheuristic algorithm, resulting in improved accuracy, reduced annotation costs, and strong performance demonstrated on the SKU-110k dataset for real-world retail applications such as inventory tracking and checkout systems.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining Faster R-CNN and YOLO in a co-training framework with pseudo-label exchange, offering a clever adaptation for semi-supervised object detection in retail settings, though it builds on existing techniques rather than introducing a entirely new architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research and applications in retail-specific computer vision, such as automated inventory systems, by providing a practical framework that reduces labeling needs, but its applicability may be limited to niche areas like densely packed object detection.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality contribution with practical innovations for object detection in retail, making it valuable for researchers in computer vision and AI to understand advancements in semi-supervised methods, though it may not be essential for those outside the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/dc8b46699487bc9311dc0946025072454e40f920",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Hossein Yazdanjouei",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/9501594"
        },
        {
          "name": "Arash Mansouri",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380442574"
        },
        {
          "name": "Mohammad Shokouhifar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2268358737"
        }
      ]
    },
    {
      "id": "2509.09751",
      "title": "Meta-Learning Reinforcement Learning for Crypto-Return Prediction",
      "authors": [
        "Junqiao Wang",
        "Zhaoyang Guan",
        "Guanyu Liu",
        "Tianze Xia",
        "Xianzhi Li",
        "Shuo Yin",
        "Xinyuan Song",
        "Chuhan Cheng",
        "Tianyu Shi",
        "Alex Lee"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Predicting cryptocurrency returns is notoriously difficult: price movements\nare driven by a fast-shifting blend of on-chain activity, news flow, and social\nsentiment, while labeled training data are scarce and expensive. In this paper,\nwe present Meta-RL-Crypto, a unified transformer-based architecture that\nunifies meta-learning and reinforcement learning (RL) to create a fully\nself-improving trading agent. Starting from a vanilla instruction-tuned LLM,\nthe agent iteratively alternates between three roles-actor, judge, and\nmeta-judge-in a closed-loop architecture. This learning process requires no\nadditional human supervision. It can leverage multimodal market inputs and\ninternal preference feedback. The agent in the system continuously refines both\nthe trading policy and evaluation criteria. Experiments across diverse market\nregimes demonstrate that Meta-RL-Crypto shows good performance on the technical\nindicators of the real market and outperforming other LLM-based baselines.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09751v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09751v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.479,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.363,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a self-improving system using internal preference feedback and a Meta-Judge for refinement, without any reliance on human-ranked data or human supervision. RLHF specifically requires training with human feedback to align models with human preferences, which is not present here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a meta-learning and reinforcement learning framework with iterative roles (Actor, Judge, Meta-Judge) for trading predictions, but it does not involve diffusion models, iterative refinement for Chain-of-Thought reasoning, or any diffusion-based processes for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09754",
      "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
      "authors": [
        "Yiqun Shen",
        "Song Yuan",
        "Zhengze Zhang",
        "Xiaoliang Wang",
        "Daxin Jiang",
        "Nguyen Cam-Tu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09754v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09754v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.382,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on KV cache eviction and compression techniques for LLMs to optimize memory and inference efficiency in Transformer models. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09775",
      "title": "Executable Ontologies: Synthesizing Event Semantics with Dataflow\n  Architecture",
      "authors": [
        "Aleksandr Boldachev"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.FL (Formal Languages and Automata Theory)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "This paper presents boldsea, Boldachev's semantic-event approach -- an\narchitecture for modeling complex dynamic systems using executable ontologies\n-- semantic models that act as dynamic structures, directly controlling process\nexecution. We demonstrate that integrating event semantics with a dataflow\narchitecture addresses the limitations of traditional Business Process\nManagement (BPM) systems and object-oriented semantic technologies. The paper\npresents the formal BSL (boldsea Semantic Language), including its BNF grammar,\nand outlines the boldsea-engine's architecture, which directly interprets\nsemantic models as executable algorithms without compilation. It enables the\nmodification of event models at runtime, ensures temporal transparency, and\nseamlessly merges data and business logic within a unified semantic framework.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09775v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09775v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.255,
      "weak_supervision_score": 0.267,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.233,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09785",
      "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds\n  Classification via Token Purging",
      "authors": [
        "Moslem Yazdanpanah",
        "Ali Bahri",
        "Mehrdad Noori",
        "Sahar Dastani",
        "Gustavo Adolfo Vargas Hakim",
        "David Osowiechi",
        "Ismail Ben Ayed",
        "Christian Desrosiers"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Test-time adaptation (TTA) is crucial for mitigating performance degradation\ncaused by distribution shifts in 3D point cloud classification. In this work,\nwe introduce Token Purging (PG), a novel backpropagation-free approach that\nremoves tokens highly affected by domain shifts before they reach attention\nlayers. Unlike existing TTA methods, PG operates at the token level, ensuring\nrobust adaptation without iterative updates. We propose two variants: PG-SP,\nwhich leverages source statistics, and PG-SF, a fully source-free version\nrelying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,\nShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of\n+10.3\\% higher accuracy than state-of-the-art backpropagation-free methods,\nwhile PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is\n12.4 times faster and 5.5 times more memory efficient than our baseline, making\nit suitable for real-world deployment. Code is available at\n\\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09785v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09785v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.41,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on test-time adaptation for 3D point cloud classification using a backpropagation-free method called Token Purging, which aims to remove noisy tokens during inference to handle domain shifts. It does not discuss distributed training, parallel computing, multi-node machine learning, or any strategies for partitioning data or computation across processors during model training. The core contributions are related to inference efficiency and adaptation, making it unrelated to the topic of distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09787",
      "title": "ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full\n  Version)",
      "authors": [
        "Nojan Sheybani",
        "Alessandro Pegoraro",
        "Jonathan Knauer",
        "Phillip Rieger",
        "Elissa Mollakuqe",
        "Farinaz Koushanfar",
        "Ahmad-Reza Sadeghi"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Split Learning (SL) is a distributed learning approach that enables\nresource-constrained clients to collaboratively train deep neural networks\n(DNNs) by offloading most layers to a central server while keeping in- and\noutput layers on the client-side. This setup enables SL to leverage server\ncomputation capacities without sharing data, making it highly effective in\nresource-constrained environments dealing with sensitive data. However, the\ndistributed nature enables malicious clients to manipulate the training\nprocess. By sending poisoned intermediate gradients, they can inject backdoors\ninto the shared DNN. Existing defenses are limited by often focusing on\nserver-side protection and introducing additional overhead for the server. A\nsignificant challenge for client-side defenses is enforcing malicious clients\nto correctly execute the defense algorithm.\n  We present ZORRO, a private, verifiable, and robust SL defense scheme.\nThrough our novel design and application of interactive zero-knowledge proofs\n(ZKPs), clients prove their correct execution of a client-located defense\nalgorithm, resulting in proofs of computational integrity attesting to the\nbenign nature of locally trained DNN portions. Leveraging the frequency\nrepresentation of model partitions enables ZORRO to conduct an in-depth\ninspection of the locally trained models in an untrusted environment, ensuring\nthat each client forwards a benign checkpoint to its succeeding client. In our\nextensive evaluation, covering different model architectures as well as various\nattack strategies and data scenarios, we show ZORRO's effectiveness, as it\nreduces the attack success rate to less than 6\\% while causing even for models\nstoring \\numprint{1000000} parameters on the client-side an overhead of less\nthan 10 seconds.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09787v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09787v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.452,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution, ZORRO, builds directly on Split Learning (SL), a distributed training paradigm that partitions DNN architectures across clients and a server to accelerate training while preserving privacy. This aligns closely with the topic, as SL involves strategic partitioning of model computation across multiple nodes, addressing distributed training challenges like efficiency and security in resource-constrained environments.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces ZORRO, a novel defense mechanism for Split Learning (SL) that addresses backdoor attacks from malicious clients by leveraging interactive zero-knowledge proofs (ZKPs) to verify the correct execution of a client-side defense algorithm. It employs frequency-domain analysis to inspect locally trained model partitions, ensuring only benign checkpoints are shared, and demonstrates through extensive evaluations that ZORRO reduces attack success rates to under 6% with less than 10 seconds of overhead for models with up to 1,000,000 parameters, while maintaining privacy and computational efficiency.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new client-side defense mechanism using zero-knowledge proofs and frequency-domain analysis for Split Learning, significantly advancing the state-of-the-art in securing distributed learning against backdoor attacks.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within subfields like secure AI and cryptography, as it enhances privacy and robustness in collaborative training, though its influence may be limited to specific applications of distributed learning.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality, innovative contribution to AI security by introducing an effective defense for Split Learning, making it valuable for researchers focused on privacy-preserving machine learning techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9e35c103edff840fd77b8f0b1a6093455ca68e0b",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 65,
      "average_h_index": 13.285714285714286,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Nojan Sheybani",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2025342076"
        },
        {
          "name": "Alessandro Pegoraro",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2213466309"
        },
        {
          "name": "Jonathan Knauer",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2319415396"
        },
        {
          "name": "P. Rieger",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/1753285317"
        },
        {
          "name": "Elissa Mollakuqe",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349109049"
        },
        {
          "name": "F. Koushanfar",
          "h_index": 65,
          "profile_url": "https://www.semanticscholar.org/author/3018662"
        },
        {
          "name": "Ahmad Sadeghi",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2253408026"
        }
      ]
    },
    {
      "id": "2509.09790",
      "title": "How well can LLMs provide planning feedback in grounded environments?",
      "authors": [
        "Yuxuan Li",
        "Victor Zhong"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Learning to plan in grounded environments typically requires carefully\ndesigned reward functions or high-quality annotated demonstrations. Recent\nworks show that pretrained foundation models, such as large language models\n(LLMs) and vision language models (VLMs), capture background knowledge helpful\nfor planning, which reduces the amount of reward design and demonstrations\nneeded for policy learning. We evaluate how well LLMs and VLMs provide feedback\nacross symbolic, language, and continuous control environments. We consider\nprominent types of feedback for planning including binary feedback, preference\nfeedback, action advising, goal advising, and delta action feedback. We also\nconsider inference methods that impact feedback performance, including\nin-context learning, chain-of-thought, and access to environment dynamics. We\nfind that foundation models can provide diverse high-quality feedback across\ndomains. Moreover, larger and reasoning models consistently provide more\naccurate feedback, exhibit less bias, and benefit more from enhanced inference\nmethods. Finally, feedback quality degrades for environments with complex\ndynamics or continuous state spaces and action spaces.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09790v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09790v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.57,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.488,
      "distributed_training_score": 0.34,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates LLMs and VLMs for providing feedback in planning environments, which relates to reward modeling in reinforcement learning, but it primarily uses AI-generated feedback rather than human feedback. While it mentions works involving human feedback (e.g., refining reward functions), the main contribution focuses on automated feedback from models, not aligning AI with human preferences as defined in RLHF.",
      "weak_supervision_justification": "The paper's use of LLMs and VLMs to generate feedback as a substitute for high-quality annotated demonstrations aligns with weak supervision, as it involves programmatically derived, potentially noisy labels from pre-trained models rather than precise hand-labeled data. However, the paper does not explicitly frame this as weak supervision, focusing instead on feedback for planning, making it moderately relevant rather than central.",
      "diffusion_reasoning_justification": "The paper discusses reasoning techniques like chain-of-thought and in-context learning for feedback generation, but it does not involve diffusion models or adapt the iterative refinement process of diffusion for multi-step logical reasoning. There is no mention of treating reasoning paths as entities for holistic correction, so it does not meet the criteria for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper evaluates the effectiveness of large language models (LLMs) and vision language models (VLMs) in providing various types of feedback, such as binary, preference, action advising, goal advising, and delta action, for planning in grounded environments including symbolic games, language-based tasks, and continuous control scenarios. The methodology involves comparing model-generated feedback against ground truth from optimal policies across different model sizes, reasoning capabilities, and inference techniques like in-context learning and chain-of-thought, revealing that larger, reasoning-capable models deliver higher accuracy with less bias, though feedback quality diminishes in environments with complex dynamics or continuous spaces.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new comprehensive analysis of LLMs and VLMs for providing diverse feedback types in planning, advancing the state-of-the-art by demonstrating their potential to reduce reliance on traditional reward functions and demonstrations.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI planning and robotics, as it highlights practical applications of foundation models for feedback, though its influence may be limited to specific domains rather than widespread commercial adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers significant insights into the capabilities and limitations of LLMs for planning feedback, making it a valuable contribution that researchers in artificial intelligence should be aware of for advancing policy learning techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/def0655b1d0d434442b3bbcb8475d4cc6e750718",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yuxuan Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380467211"
        },
        {
          "name": "Victor Zhong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380444927"
        }
      ]
    },
    {
      "id": "2509.09792",
      "title": "Loc$^2$: Interpretable Cross-View Localization via Depth-Lifted Local\n  Feature Matching",
      "authors": [
        "Zimin Xia",
        "Chenghao Xu",
        "Alexandre Alahi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We propose an accurate and interpretable fine-grained cross-view localization\nmethod that estimates the 3 Degrees of Freedom (DoF) pose of a ground-level\nimage by matching its local features with a reference aerial image. Unlike\nprior approaches that rely on global descriptors or bird's-eye-view (BEV)\ntransformations, our method directly learns ground-aerial image-plane\ncorrespondences using weak supervision from camera poses. The matched ground\npoints are lifted into BEV space with monocular depth predictions, and\nscale-aware Procrustes alignment is then applied to estimate camera rotation,\ntranslation, and optionally the scale between relative depth and the aerial\nmetric space. This formulation is lightweight, end-to-end trainable, and\nrequires no pixel-level annotations. Experiments show state-of-the-art accuracy\nin challenging scenarios such as cross-area testing and unknown orientation.\nFurthermore, our method offers strong interpretability: correspondence quality\ndirectly reflects localization accuracy and enables outlier rejection via\nRANSAC, while overlaying the re-scaled ground layout on the aerial image\nprovides an intuitive visual cue of localization accuracy.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09792v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09792v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.33,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09794",
      "title": "A Modular and Multimodal Generative AI Framework for Urban Building\n  Energy Data: Generating Synthetic Homes",
      "authors": [
        "Jackson Eshbaugh",
        "Chetan Tiwari",
        "Jorge Silveyra"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Computational models have emerged as powerful tools for energy modeling\nresearch, touting scalability and quantitative results. However, these models\nrequire a plethora of data, some of which is inaccessible, expensive, or raises\nprivacy concerns. We introduce a modular multimodal framework to produce this\ndata from publicly accessible residential information and images using\ngenerative artificial intelligence (AI). Additionally, we provide a pipeline\ndemonstrating this framework, and we evaluate its generative AI components. Our\nexperiments show that our framework's use of AI avoids common issues with\ngenerative models. Our framework produces realistic, labeled data. By reducing\ndependence on costly or restricted data sources, we pave a path towards more\naccessible and reproducible research.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09794v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09794v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.379,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a framework for generating synthetic data using generative AI, such as LLMs and CNNs, but does not mention or utilize diffusion models for iterative refinement in logical reasoning tasks. There is no component involving multi-step Chain-of-Thought processing or holistic correction via diffusion, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of synthetic datasets for urban building energy modeling, using a multimodal framework to generate realistic, labeled data from public sources. This directly aligns with research on dataset creation, curation, and evaluation for AI applications, as it addresses the need for accessible datasets and assesses their quality.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a modular multimodal generative AI framework designed to generate synthetic data for urban building energy modeling, addressing challenges such as data inaccessibility, cost, and privacy concerns by utilizing publicly available residential information and images. The methodology involves a pipeline that combines pretrained large language models with simulation tools to produce realistic, labeled synthetic data, which is evaluated to minimize issues like hallucinations, ultimately enhancing the accessibility and reproducibility of energy research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing generative AI techniques and multimodal data processing to address data scarcity in energy modeling, offering a notable improvement over traditional methods without introducing an entirely new problem or architecture. While it builds on prior work like digital twins and AI-based data generation, the modular framework provides a practical, integrated approach that advances application in this domain.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI for energy modeling, as it provides a scalable solution for synthetic data generation that could improve research accessibility. However, its influence may be limited to specific applications in urban energy and policy, rather than broader commercial or interdisciplinary fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper represents a strong, valuable contribution by offering a practical framework for generating synthetic data in energy research, making it essential for researchers in AI and urban studies to be aware of. While not groundbreaking, its focus on real-world challenges like data privacy enhances its relevance and utility.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c89b6797df8903bf6bd37af0564a7ebd8224a13b",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jackson Eshbaugh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2367196142"
        },
        {
          "name": "Chetan Tiwari",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380443021"
        },
        {
          "name": "Jorge Silveyra",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380442673"
        }
      ]
    },
    {
      "id": "2509.09801",
      "title": "HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and\n  Accuracy of Language Model Reasoning",
      "authors": [
        "Brennen Hill"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The adaptation of large language models (LLMs) to specialized reasoning tasks\nis fundamentally constrained by computational resources. Parameter-Efficient\nFine-Tuning (PEFT) methods have emerged as a powerful solution, yet the\nlandscape of these techniques is diverse, with distinct methods operating in\neither the model's weight space or its representation space. This paper\ninvestigates the hypothesis that a synergistic combination of these paradigms\ncan unlock superior performance and efficiency. We introduce HEFT (Hierarchical\nEfficient Fine-Tuning), a novel hierarchical adaptation strategy that composes\ntwo distinct PEFT methods in a coarse-to-fine manner: first, a broad,\nfoundational adaptation in the weight space using Low-Rank Adaptation (LoRA),\nfollowed by a precise, surgical refinement of internal activations using\nRepresentation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a\nLlama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential\nreasoning. Our results reveal a profound synergistic effect. A model fine-tuned\nfor only three epochs with our HEFT strategy achieves an accuracy of 85.17\\%,\nexceeding the performance of models trained for 20 epochs with either LoRA-only\n(85.05\\%) or ReFT-only (83.36\\%) methodologies. This work demonstrates that the\nthoughtful composition of PEFT methods is a potent algorithmic innovation,\noffering a more efficient and effective path toward advancing the reasoning\ncapabilities of language models. By achieving superior results with a fraction\nof the computational budget, our findings present a principled approach to\novercoming the obstacles inherent in adapting large-scale models for complex\ncognitive tasks.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09801v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09801v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.511,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.493,
      "distributed_training_score": 0.433,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on parameter-efficient fine-tuning methods (HEFT, combining LoRA and ReFT) for adapting language models to reasoning tasks, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a hierarchical fine-tuning strategy for efficiency and accuracy in language model adaptation, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "The paper addresses computational efficiency through parameter-efficient fine-tuning (PEFT) methods, but it does not discuss distributed training, parallel computing, multi-node systems, or strategies for partitioning data/computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09808",
      "title": "Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye\n  Reflex Test",
      "authors": [
        "Judith Massmann",
        "Alexander Lichtenstein",
        "Francisco M. López"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Numerous visual impairments can be detected in red-eye reflex images from\nyoung children. The so-called Bruckner test is traditionally performed by\nophthalmologists in clinical settings. Thanks to the recent technological\nadvances in smartphones and artificial intelligence, it is now possible to\nrecreate the Bruckner test using a mobile device. In this paper, we present a\nfirst study conducted during the development of KidsVisionCheck, a free\napplication that can perform vision screening with a mobile device using\nred-eye reflex images. The underlying model relies on deep neural networks\ntrained on children's pupil images collected and labeled by an ophthalmologist.\nWith an accuracy of 90% on unseen test data, our model provides highly reliable\nperformance without the necessity of specialist equipment. Furthermore, we can\nidentify the optimal conditions for data collection, which can in turn be used\nto provide immediate feedback to the users. In summary, this work marks a first\nstep toward accessible pediatric vision screenings and early intervention for\nvision abnormalities worldwide.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09808v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09808v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.317,
      "distributed_training_score": 0.3,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09810",
      "title": "Towards a Common Framework for Autoformalization",
      "authors": [
        "Agnieszka Mensfelt",
        "David Tena Cucala",
        "Santiago Franco",
        "Angeliki Koutsoukou-Argyraki",
        "Vince Trencsenyi",
        "Kostas Stathis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autoformalization has emerged as a term referring to the automation of\nformalization - specifically, the formalization of mathematics using\ninteractive theorem provers (proof assistants). Its rapid development has been\ndriven by progress in deep learning, especially large language models (LLMs).\nMore recently, the term has expanded beyond mathematics to describe the broader\ntask of translating informal input into formal logical representations. At the\nsame time, a growing body of research explores using LLMs to translate informal\nlanguage into formal representations for reasoning, planning, and knowledge\nrepresentation - often without explicitly referring to this process as\nautoformalization. As a result, despite addressing similar tasks, the largely\nindependent development of these research areas has limited opportunities for\nshared methodologies, benchmarks, and theoretical frameworks that could\naccelerate progress. The goal of this paper is to review - explicit or implicit\n- instances of what can be considered autoformalization and to propose a\nunified framework, encouraging cross-pollination between different fields to\nadvance the development of next generation AI systems.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09810v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09810v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.352,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses autoformalization and the use of LLMs for translating informal language into formal representations, with a brief mention of reinforcement learning in the context of improving reasoning models. However, it does not specifically address Reinforcement Learning from Human Feedback, which requires training a reward model using human-ranked data for fine-tuning. There is no evidence of human feedback mechanisms in the paper's content.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper centers on autoformalization, semantic parsing, and the application of LLMs for formal logical representations, including techniques like chain-of-thought prompting. It does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09823",
      "title": "SoilSound: Smartphone-based Soil Moisture Estimation",
      "authors": [
        "Yixuan Gao",
        "Tanvir Ahmed",
        "Shuang He",
        "Zhongqi Cheng",
        "Rajalakshmi Nandakumar"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)",
        "cs.HC (Human-Computer Interaction)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Soil moisture monitoring is essential for agriculture and environmental\nmanagement, yet existing methods require either invasive probes disturbing the\nsoil or specialized equipment, limiting access to the public. We present\nSoilSound, an ubiquitous accessible smartphone-based acoustic sensing system\nthat can measure soil moisture without disturbing the soil. We leverage the\nbuilt-in speaker and microphone to perform a vertical scan mechanism to\naccurately measure moisture without any calibration. Unlike existing work that\nuse transmissive properties, we propose an alternate model for acoustic\nreflections in soil based on the surface roughness effect to enable moisture\nsensing without disturbing the soil. The system works by sending acoustic\nchirps towards the soil and recording the reflections during a vertical scan,\nwhich are then processed and fed to a convolutional neural network for\non-device soil moisture estimation with negligible computational, memory, or\npower overhead. We evaluated the system by training with curated soils in boxes\nin the lab and testing in the outdoor fields and show that SoilSound achieves a\nmean absolute error (MAE) of 2.39% across 10 different locations. Overall, the\nevaluation shows that SoilSound can accurately track soil moisture levels\nranging from 15.9% to 34.0% across multiple soil types, environments, and\nusers; without requiring any calibration or disturbing the soil, enabling\nwidespread moisture monitoring for home gardeners, urban farmers, citizen\nscientists, and agricultural communities in resource-limited settings.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09823v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09823v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.27,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.253,
      "distributed_training_score": 0.264,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09828",
      "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception",
      "authors": [
        "Tim Broedermannn",
        "Christos Sakaridis",
        "Luigi Piccinelli",
        "Wim Abbeloos",
        "Luc Van Gool"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Robust semantic perception for autonomous vehicles relies on effectively\ncombining multiple sensors with complementary strengths and weaknesses.\nState-of-the-art sensor fusion approaches to semantic perception often treat\nsensor data uniformly across the spatial extent of the input, which hinders\nperformance when faced with challenging conditions. By contrast, we propose a\nnovel depth-guided multimodal fusion method that upgrades condition-aware\nfusion by integrating depth information. Our network, DGFusion, poses\nmultimodal segmentation as a multi-task problem, utilizing the lidar\nmeasurements, which are typically available in outdoor sensor suites, both as\none of the model's inputs and as ground truth for learning depth. Our\ncorresponding auxiliary depth head helps to learn depth-aware features, which\nare encoded into spatially varying local depth tokens that condition our\nattentive cross-modal fusion. Together with a global condition token, these\nlocal depth tokens dynamically adapt sensor fusion to the spatially varying\nreliability of each sensor across the scene, which largely depends on depth. In\naddition, we propose a robust loss for our depth, which is essential for\nlearning from lidar inputs that are typically sparse and noisy in adverse\nconditions. Our method achieves state-of-the-art panoptic and semantic\nsegmentation performance on the challenging MUSES and DELIVER datasets. Code\nand models will be available at https://github.com/timbroed/DGFusion",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09828v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09828v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.375,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on depth-guided sensor fusion for semantic perception in autonomous vehicles, emphasizing multimodal integration, depth estimation, and segmentation robustness. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09836",
      "title": "CoDiCodec: Unifying Continuous and Discrete Compressed Representations\n  of Audio",
      "authors": [
        "Marco Pasini",
        "Stefan Lattner",
        "George Fazekas"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Efficiently representing audio signals in a compressed latent space is\ncritical for latent generative modelling. However, existing autoencoders often\nforce a choice between continuous embeddings and discrete tokens. Furthermore,\nachieving high compression ratios while maintaining audio fidelity remains a\nchallenge. We introduce CoDiCodec, a novel audio autoencoder that overcomes\nthese limitations by both efficiently encoding global features via summary\nembeddings, and by producing both compressed continuous embeddings at ~ 11 Hz\nand discrete tokens at a rate of 2.38 kbps from the same trained model,\noffering unprecedented flexibility for different downstream generative tasks.\nThis is achieved through Finite Scalar Quantization (FSQ) and a novel\nFSQ-dropout technique, and does not require additional loss terms beyond the\nsingle consistency loss used for end-to-end training. CoDiCodec supports both\nautoregressive decoding and a novel parallel decoding strategy, with the latter\nachieving superior audio quality and faster decoding. CoDiCodec outperforms\nexisting continuous and discrete autoencoders at similar bitrates in terms of\nreconstruction audio quality. Our work enables a unified approach to audio\ncompression, bridging the gap between continuous and discrete generative\nmodelling paradigms.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09836v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09836v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.286,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.327,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09838",
      "title": "Revisiting Actor-Critic Methods in Discrete Action Off-Policy\n  Reinforcement Learning",
      "authors": [
        "Reza Asad",
        "Reza Babanezhad",
        "Sharan Vaswani"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Value-based approaches such as DQN are the default methods for off-policy\nreinforcement learning with discrete-action environments such as Atari. Common\npolicy-based methods are either on-policy and do not effectively learn from\noff-policy data (e.g. PPO), or have poor empirical performance in the\ndiscrete-action setting (e.g. SAC). Consequently, starting from discrete SAC\n(DSAC), we revisit the design of actor-critic methods in this setting. First,\nwe determine that the coupling between the actor and critic entropy is the\nprimary reason behind the poor performance of DSAC. We demonstrate that by\nmerely decoupling these components, DSAC can have comparable performance as\nDQN. Motivated by this insight, we introduce a flexible off-policy actor-critic\nframework that subsumes DSAC as a special case. Our framework allows using an\nm-step Bellman operator for the critic update, and enables combining standard\npolicy optimization methods with entropy regularization to instantiate the\nresulting actor objective. Theoretically, we prove that the proposed methods\ncan guarantee convergence to the optimal regularized value function in the\ntabular setting. Empirically, we demonstrate that these methods can approach\nthe performance of DQN on standard Atari games, and do so even without entropy\nregularization or explicit exploration.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09838v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09838v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.333,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses improvements to actor-critic methods in discrete action off-policy reinforcement learning, with a brief mention of RLHF as a potential application area (e.g., citing ouyang2022training in the introduction). However, the main contributions focus on algorithmic enhancements for standard RL environments like Atari games, without involving human feedback, reward models, or fine-tuning based on human preferences. Thus, it is only tangentially related as a general RL advancement that might indirectly support RLHF, but it does not directly address RLHF systems.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09841",
      "title": "Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning\n  Framework",
      "authors": [
        "Chengyu Yang",
        "Rishik Reddy Yesgari",
        "Chengjun Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Rosacea, which is a chronic inflammatory skin condition that manifests with\nfacial redness, papules, and visible blood vessels, often requirs precise and\nearly detection for significantly improving treatment effectiveness. This paper\npresents new patch-based automatic rosacea detection strategies using the\nResNet-18 deep learning framework. The contributions of the proposed strategies\ncome from the following aspects. First, various image pateches are extracted\nfrom the facial images of people in different sizes, shapes, and locations.\nSecond, a number of investigation studies are carried out to evaluate how the\nlocalized visual information influences the deep learing model performance.\nThird, thorough experiments are implemented to reveal that several patch-based\nautomatic rosacea detection strategies achieve competitive or superior accuracy\nand sensitivity than the full-image based methods. And finally, the proposed\npatch-based strategies, which use only localized patches, inherently preserve\npatient privacy by excluding any identifiable facial features from the data.\nThe experimental results indicate that the proposed patch-based strategies\nguide the deep learning model to focus on clinically relevant regions, enhance\nrobustness and interpretability, and protect patient privacy. As a result, the\nproposed strategies offer practical insights for improving automated\ndermatological diagnostics.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09841v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09841v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.306,
      "distributed_training_score": 0.324,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09843",
      "title": "HGEN: Heterogeneous Graph Ensemble Networks",
      "authors": [
        "Jiajun Shen",
        "Yufei Jin",
        "Yi He",
        "Xingquan Zhu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents HGEN that pioneers ensemble learning for heterogeneous\ngraphs. We argue that the heterogeneity in node types, nodal features, and\nlocal neighborhood topology poses significant challenges for ensemble learning,\nparticularly in accommodating diverse graph learners. Our HGEN framework\nensembles multiple learners through a meta-path and transformation-based\noptimization pipeline to uplift classification accuracy. Specifically, HGEN\nuses meta-path combined with random dropping to create Allele Graph Neural\nNetworks (GNNs), whereby the base graph learners are trained and aligned for\nlater ensembling. To ensure effective ensemble learning, HGEN presents two key\ncomponents: 1) a residual-attention mechanism to calibrate allele GNNs of\ndifferent meta-paths, thereby enforcing node embeddings to focus on more\ninformative graphs to improve base learner accuracy, and 2) a\ncorrelation-regularization term to enlarge the disparity among embedding\nmatrices generated from different meta-paths, thereby enriching base learner\ndiversity. We analyze the convergence of HGEN and attest its higher\nregularization magnitude over simple voting. Experiments on five heterogeneous\nnetworks validate that HGEN consistently outperforms its state-of-the-art\ncompetitors by substantial margin.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09843v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09843v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.355,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09844",
      "title": "Privacy-Preserving Automated Rosacea Detection Based on Medically\n  Inspired Region of Interest Selection",
      "authors": [
        "Chengyu Yang",
        "Rishik Reddy Yesgari",
        "Chengjun Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Rosacea is a common but underdiagnosed inflammatory skin condition that\nprimarily affects the central face and presents with subtle redness, pustules,\nand visible blood vessels. Automated detection remains challenging due to the\ndiffuse nature of symptoms, the scarcity of labeled datasets, and privacy\nconcerns associated with using identifiable facial images. A novel\nprivacy-preserving automated rosacea detection method inspired by clinical\npriors and trained entirely on synthetic data is presented in this paper.\nSpecifically, the proposed method, which leverages the observation that rosacea\nmanifests predominantly through central facial erythema, first constructs a\nfixed redness-informed mask by selecting regions with consistently high red\nchannel intensity across facial images. The mask thus is able to focus on\ndiagnostically relevant areas such as the cheeks, nose, and forehead and\nexclude identity-revealing features. Second, the ResNet-18 deep learning\nmethod, which is trained on the masked synthetic images, achieves superior\nperformance over the full-face baselines with notable gains in terms of\naccuracy, recall and F1 score when evaluated using the real-world test data.\nThe experimental results demonstrate that the synthetic data and clinical\npriors can jointly enable accurate and ethical dermatological AI systems,\nespecially for privacy sensitive applications in telemedicine and large-scale\nscreening.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09844v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09844v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.304,
      "distributed_training_score": 0.322,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09848",
      "title": "Towards an AI-based knowledge assistant for goat farmers based on\n  Retrieval-Augmented Generation",
      "authors": [
        "Nana Han",
        "Dong Liu",
        "Tomas Norton"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) are increasingly being recognised as valuable\nknowledge communication tools in many industries. However, their application in\nlivestock farming remains limited, being constrained by several factors not\nleast the availability, diversity and complexity of knowledge sources. This\nstudy introduces an intelligent knowledge assistant system designed to support\nhealth management in farmed goats. Leveraging the Retrieval-Augmented\nGeneration (RAG), two structured knowledge processing methods, table\ntextualization and decision-tree textualization, were proposed to enhance large\nlanguage models' (LLMs) understanding of heterogeneous data formats. Based on\nthese methods, a domain-specific goat farming knowledge base was established to\nimprove LLM's capacity for cross-scenario generalization. The knowledge base\nspans five key domains: Disease Prevention and Treatment, Nutrition Management,\nRearing Management, Goat Milk Management, and Basic Farming Knowledge.\nAdditionally, an online search module is integrated to enable real-time\nretrieval of up-to-date information. To evaluate system performance, six\nablation experiments were conducted to examine the contribution of each\ncomponent. The results demonstrated that heterogeneous knowledge fusion method\nachieved the best results, with mean accuracies of 87.90% on the validation set\nand 84.22% on the test set. Across the text-based, table-based, decision-tree\nbased Q&A tasks, accuracy consistently exceeded 85%, validating the\neffectiveness of structured knowledge fusion within a modular design. Error\nanalysis identified omission as the predominant error category, highlighting\nopportunities to further improve retrieval coverage and context integration. In\nconclusion, the results highlight the robustness and reliability of the\nproposed system for practical applications in goat farming.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09848v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09848v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.32,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on building a knowledge assistant using Retrieval-Augmented Generation (RAG) and LLMs for goat farming, with no mention of human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes RAG and structured knowledge processing for LLMs, but does not involve diffusion models, iterative refinement, or multi-step logical reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09849",
      "title": "Investigating the Impact of Various Loss Functions and Learnable Wiener\n  Filter for Laparoscopic Image Desmoking",
      "authors": [
        "Chengyu Yang",
        "Chengjun Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "To rigorously assess the effectiveness and necessity of individual components\nwithin the recently proposed ULW framework for laparoscopic image desmoking,\nthis paper presents a comprehensive ablation study. The ULW approach combines a\nU-Net based backbone with a compound loss function that comprises mean squared\nerror (MSE), structural similarity index (SSIM) loss, and perceptual loss. The\nframework also incorporates a differentiable, learnable Wiener filter module.\nIn this study, each component is systematically ablated to evaluate its\nspecific contribution to the overall performance of the whole framework. The\nanalysis includes: (1) removal of the learnable Wiener filter, (2) selective\nuse of individual loss terms from the composite loss function. All variants are\nbenchmarked on a publicly available paired laparoscopic images dataset using\nquantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative\nvisual comparisons.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09849v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09849v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.241,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09853",
      "title": "SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under\n  Resource Constraints",
      "authors": [
        "Zhiyu Fan",
        "Kirill Vasilevski",
        "Dayi Lin",
        "Boyuan Chen",
        "Yihao Chen",
        "Zhiqing Zhong",
        "Jie M. Zhang",
        "Pinjia He",
        "Ahmed E. Hassan"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The advancement of large language models (LLMs) and code agents has\ndemonstrated significant potential to assist software engineering (SWE) tasks,\nsuch as autonomous issue resolution and feature addition. Existing AI for\nsoftware engineering leaderboards (e.g., SWE-bench) focus solely on solution\naccuracy, ignoring the crucial factor of effectiveness in a\nresource-constrained world. This is a universal problem that also exists beyond\nsoftware engineering tasks: any AI system should be more than correct - it must\nalso be cost-effective. To address this gap, we introduce SWE-Effi, a set of\nnew metrics to re-evaluate AI systems in terms of holistic effectiveness\nscores. We define effectiveness as the balance between the accuracy of outcome\n(e.g., issue resolve rate) and the resources consumed (e.g., token and time).\nIn this paper, we specifically focus on the software engineering scenario by\nre-ranking popular AI systems for issue resolution on a subset of the SWE-bench\nbenchmark using our new multi-dimensional metrics. We found that AI system's\neffectiveness depends not just on the scaffold itself, but on how well it\nintegrates with the base model, which is key to achieving strong performance in\na resource-efficient manner. We also identified systematic challenges such as\nthe \"token snowball\" effect and, more significantly, a pattern of \"expensive\nfailures\". In these cases, agents consume excessive resources while stuck on\nunsolvable tasks - an issue that not only limits practical deployment but also\ndrives up the cost of failed rollouts during RL training. Lastly, we observed a\nclear trade-off between effectiveness under the token budget and effectiveness\nunder the time budget, which plays a crucial role in managing project budgets\nand enabling scalable reinforcement learning, where fast responses are\nessential.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09853v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09853v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.384,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses Reinforcement Learning (RL) in the context of software engineering tasks, emphasizing how resource efficiency impacts RL training for AI agents, as seen in examples like DeepResearch Agent and SkyRL. However, it does not specifically address RLHF, which involves using human feedback to train a reward model and fine-tune models. The focus is on evaluation metrics rather than RLHF methods.",
      "weak_supervision_justification": "The paper introduces metrics for evaluating AI systems under resource constraints in software engineering, with mentions of RL for training, but it does not involve weak supervision techniques, such as programmatically generating labels for model training. The content is centered on performance assessment, not on training methodologies like weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09859",
      "title": "WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector",
      "authors": [
        "Razvan Stefanescu",
        "Ethan Oh",
        "Ruben Vazquez",
        "Chris Mesterharm",
        "Constantin Serban",
        "Ritu Chadha"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and\nacoustic signals for robust real-life UAV object detection. Our approach fuses\nvisual and acoustic features in a unified object detector model relying on the\nDeformable DETR and Wav2Vec2 architectures, achieving strong performance under\nchallenging environmental conditions. Our work leverage the existing\nDrone-vs-Bird dataset and the newly generated ARDrone dataset containing more\nthan 7,500 synchronized images and audio segments. We show how the acoustic\ninformation is used to improve the performance of the Deformable DETR object\ndetector on the real ARDrone dataset. We developed, trained and tested four\ndifferent fusion configurations based on a gated mechanism, linear layer, MLP\nand cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi\nresolution feature mappings of the Deformable DETR and enhance the object\ndetection performance over all drones dimensions. The best performer is the\ngated fusion approach, which improves the mAP of the Deformable DETR object\ndetector on our in-distribution and out-of-distribution ARDrone datasets by\n11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.\nThe mAP scores for medium and large drones are also enhanced, with overall\ngains across all drone sizes ranging from 3.27% to 5.84%.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09859v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09859v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.331,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09864",
      "title": "Latency and Token-Aware Test-Time Compute",
      "authors": [
        "Jenny Y. Huang",
        "Mehul Damani",
        "Yousef El-Kurdi",
        "Ramon Astudillo",
        "Wei Sun"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Inference-time scaling has emerged as a powerful way to improve large\nlanguage model (LLM) performance by generating multiple candidate responses and\nselecting among them. However, existing work on dynamic allocation for\ntest-time compute typically considers only parallel generation methods such as\nbest-of-N, overlooking incremental decoding methods like beam search, and has\nlargely ignored latency, focusing only on token usage. We formulate\ninference-time scaling as a problem of dynamic compute allocation and method\nselection, where the system must decide which strategy to apply and how much\ncompute to allocate on a per-query basis. Our framework explicitly incorporates\nboth token cost and wall-clock latency, the latter being critical for user\nexperience and particularly for agentic workflows where models must issue\nmultiple queries efficiently. Experiments on reasoning benchmarks show that our\napproach consistently outperforms static strategies, achieving favorable\naccuracy-cost trade-offs while remaining practical for deployment.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09864v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09864v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.454,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on inference-time scaling for LLMs, involving strategies like best-of-N and beam search to optimize compute and latency. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning through diffusion-based methods.",
      "distributed_training_justification": "The paper deals with dynamic compute allocation during inference for LLMs, emphasizing latency and token usage, rather than distributed training techniques, parallel computing for model training, or partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09867",
      "title": "LLMs as Agentic Cooperative Players in Multiplayer UNO",
      "authors": [
        "Yago Romano Matinez",
        "Jesse Roberts"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "LLMs promise to assist humans -- not just by answering questions, but by\noffering useful guidance across a wide range of tasks. But how far does that\nassistance go? Can a large language model based agent actually help someone\naccomplish their goal as an active participant? We test this question by\nengaging an LLM in UNO, a turn-based card game, asking it not to win but\ninstead help another player to do so. We built a tool that allows decoder-only\nLLMs to participate as agents within the RLCard game environment. These models\nreceive full game-state information and respond using simple text prompts under\ntwo distinct prompting strategies. We evaluate models ranging from small (1B\nparameters) to large (70B parameters) and explore how model scale impacts\nperformance. We find that while all models were able to successfully outperform\na random baseline when playing UNO, few were able to significantly aid another\nplayer.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09867v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09867v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.315,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained LLMs as agents in UNO games through prompting, without any mention of training models with human feedback, reward models, or reinforcement learning techniques to align AI with human preferences. It evaluates existing models in a game environment, not RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs standard prompting strategies like cloze and counterfactual prompting for LLMs in UNO, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning treated as a holistic entity for correction. There is no adaptation of diffusion techniques for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09869",
      "title": "Surrogate Supervision for Robust and Generalizable Deformable Image\n  Registration",
      "authors": [
        "Yihao Liu",
        "Junyu Chen",
        "Lianrui Zuo",
        "Shuwen Wei",
        "Brian D. Boyd",
        "Carmen Andreescu",
        "Olusola Ajilore",
        "Warren D. Taylor",
        "Aaron Carass",
        "Bennett A. Landman"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Objective: Deep learning-based deformable image registration has achieved\nstrong accuracy, but remains sensitive to variations in input image\ncharacteristics such as artifacts, field-of-view mismatch, or modality\ndifference. We aim to develop a general training paradigm that improves the\nrobustness and generalizability of registration networks. Methods: We introduce\nsurrogate supervision, which decouples the input domain from the supervision\ndomain by applying estimated spatial transformations to surrogate images. This\nallows training on heterogeneous inputs while ensuring supervision is computed\nin domains where similarity is well defined. We evaluate the framework through\nthree representative applications: artifact-robust brain MR registration,\nmask-agnostic lung CT registration, and multi-modal MR registration. Results:\nAcross tasks, surrogate supervision demonstrated strong resilience to input\nvariations including inhomogeneity field, inconsistent field-of-view, and\nmodality differences, while maintaining high performance on well-curated data.\nConclusions: Surrogate supervision provides a principled framework for training\nrobust and generalizable deep learning-based registration models without\nincreasing complexity. Significance: Surrogate supervision offers a practical\npathway to more robust and generalizable medical image registration, enabling\nbroader applicability in diverse biomedical imaging scenarios.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09869v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09869v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.468,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.353,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces surrogate supervision, a method that uses alternative or derived images (e.g., preprocessed versions or different modalities) for training deformable image registration models, rather than relying solely on direct input comparisons. This approach aligns with weak supervision by programmatically generating supervision signals from high-level or indirect sources, such as warped surrogates, to handle noisy or heterogeneous data. However, it is not a general weak supervision framework but is specifically applied to image registration, making it moderately relevant rather than highly so.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces surrogate supervision, a novel training paradigm for deep learning-based deformable image registration, aimed at enhancing robustness and generalizability by decoupling the input domain from the supervision domain through the use of surrogate images where similarity metrics are reliable. The methodology is evaluated across applications such as artifact-robust brain MR registration, mask-agnostic lung CT registration, and multi-modal MR registration, demonstrating improved resilience to input variations like inhomogeneity fields, field-of-view inconsistencies, and modality differences while maintaining high performance on well-curated data, thus providing a practical framework for broader biomedical imaging scenarios.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing surrogate supervision as a clever combination and generalization of existing techniques like those in VoxelMorph and SynthMorph, effectively addressing known challenges in robust image registration without creating entirely new problems or architectures.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of medical image analysis due to its practical approach for improving registration robustness, though its influence may remain confined to specialized applications in computer vision and AI for biomedical imaging.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and innovative contribution to training robust registration models, making it essential for researchers in medical AI to be aware of its methods and potential applications in diverse imaging scenarios.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/39618f37e0268cf9eef8408fa4b065962e40daac",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 46,
      "average_h_index": 8.222222222222221,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yihao Liu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2284725492"
        },
        {
          "name": "Junyu Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356366544"
        },
        {
          "name": "Lianrui Zuo",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/49934300"
        },
        {
          "name": "Shuwen Wei",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Brian D Boyd",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2354357641"
        },
        {
          "name": "Carmen Andreescu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2276352356"
        },
        {
          "name": "O. Ajilore",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2244550351"
        },
        {
          "name": "Warren D. Taylor",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2287249068"
        },
        {
          "name": "A. Carass",
          "h_index": 46,
          "profile_url": "https://www.semanticscholar.org/author/1755681"
        },
        {
          "name": "Bennett A. Landman",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2289702593"
        }
      ]
    },
    {
      "id": "2509.09870",
      "title": "Vibe Check: Understanding the Effects of LLM-Based Conversational\n  Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks",
      "authors": [
        "Hasibur Rahman",
        "Smit Desai"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large language models (LLMs) enable conversational agents (CAs) to express\ndistinctive personalities, raising new questions about how such designs shape\nuser perceptions. This study investigates how personality expression levels and\nuser-agent personality alignment influence perceptions in goal-oriented tasks.\nIn a between-subjects experiment (N=150), participants completed travel\nplanning with CAs exhibiting low, medium, or high expression across the Big\nFive traits, controlled via our novel Trait Modulation Keys framework. Results\nrevealed an inverted-U relationship: medium expression produced the most\npositive evaluations across Intelligence, Enjoyment, Anthropomorphism,\nIntention to Adopt, Trust, and Likeability, significantly outperforming both\nextremes. Personality alignment further enhanced outcomes, with Extraversion\nand Emotional Stability emerging as the most influential traits. Cluster\nanalysis identified three distinct compatibility profiles, with \"Well-Aligned\"\nusers reporting substantially positive perceptions. These findings demonstrate\nthat personality expression and strategic trait alignment constitute optimal\ndesign targets for CA personality, offering design implications as LLM-based\nCAs become increasingly prevalent.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09870v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09870v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.473,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.288,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an experimental study on how personality expression and alignment in LLM-based conversational agents affect user perceptions in goal-oriented tasks. It introduces a prompting framework (Trait Modulation Keys) for controlling personality traits and analyzes user responses through surveys and cluster analysis. There is no mention of reinforcement learning, human feedback for training AI models, reward models, or fine-tuning processes based on human-ranked data. Thus, the paper does not address or relate to RLHF systems.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09871",
      "title": "Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic\n  Survey Responses for the Chilean Case",
      "authors": [
        "Bastián González-Bustamante",
        "Nando Verelst",
        "Carla Cisternas"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) offer promising avenues for methodological and\napplied innovations in survey research by using synthetic respondents to\nemulate human answers and behaviour, potentially mitigating measurement and\nrepresentation errors. However, the extent to which LLMs recover aggregate item\ndistributions remains uncertain and downstream applications risk reproducing\nsocial stereotypes and biases inherited from training data. We evaluate the\nreliability of LLM-generated synthetic survey responses against ground-truth\nhuman responses from a Chilean public opinion probabilistic survey.\nSpecifically, we benchmark 128 prompt-model-question triplets, generating\n189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,\nprecision, recall, and F1-score) in a meta-analysis across 128\nquestion-subsample pairs to test for biases along key sociodemographic\ndimensions. The evaluation spans OpenAI's GPT family and o-series reasoning\nmodels, as well as Llama and Qwen checkpoints. Three results stand out. First,\nsynthetic responses achieve excellent performance on trust items (F1-score and\naccuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform\ncomparably on this task. Third, synthetic-human alignment is highest among\nrespondents aged 45-59. Overall, LLM-based synthetic samples approximate\nresponses from a probabilistic sample, though with substantial item-level\nheterogeneity. Capturing the full nuance of public opinion remains challenging\nand requires careful calibration and additional distributional tests to ensure\nalgorithmic fidelity and reduce errors.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09871v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09871v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.336,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates LLMs for generating synthetic survey responses but does not discuss or implement RLHF. It focuses on using pre-trained models like GPT for simulation, without any mention of training with human feedback or reward models.",
      "weak_supervision_justification": "The paper involves programmatically generating synthetic responses based on demographic traits, which loosely relates to weak supervision by creating noisy or indirect labels. However, it does not focus on training models with these labels, emphasizing evaluation against human data instead.",
      "diffusion_reasoning_justification": "The paper uses LLMs for generating survey responses but does not involve diffusion models or iterative refinement processes for multi-step logical reasoning. It benchmarks standard LLMs without any reference to diffusion-based techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09873",
      "title": "From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI\n  Ecosystem",
      "authors": [
        "James Jewitt",
        "Hao Li",
        "Bram Adams",
        "Gopi Krishnan Rajbahadur",
        "Ahmed E. Hassan"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Hidden license conflicts in the open-source AI ecosystem pose serious legal\nand ethical risks, exposing organizations to potential litigation and users to\nundisclosed risk. However, the field lacks a data-driven understanding of how\nfrequently these conflicts occur, where they originate, and which communities\nare most affected. We present the first end-to-end audit of licenses for\ndatasets and models on Hugging Face, as well as their downstream integration\ninto open-source software applications, covering 364 thousand datasets, 1.6\nmillion models, and 140 thousand GitHub projects. Our empirical analysis\nreveals systemic non-compliance in which 35.5% of model-to-application\ntransitions eliminate restrictive license clauses by relicensing under\npermissive terms. In addition, we prototype an extensible rule engine that\nencodes almost 200 SPDX and model-specific clauses for detecting license\nconflicts, which can solve 86.4% of license conflicts in software applications.\nTo support future research, we release our dataset and the prototype engine.\nOur study highlights license compliance as a critical governance challenge in\nopen-source AI and provides both the data and tools necessary to enable\nautomated, AI-aware compliance at scale.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09873v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09873v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.377,
      "datasets_score": 0.43,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on auditing licenses and compliance in the open-source AI ecosystem, with no mention of reinforcement learning, human feedback, reward models, or any related techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper analyzes licenses for 364,000 datasets on Hugging Face, including their propagation and integration, which involves dataset analysis and auditing. However, its primary emphasis is on legal compliance rather than core aspects like creation, benchmarking, or ML-specific evaluation.",
      "llm_score_status": "completed",
      "summary": "This paper presents the first end-to-end empirical audit of license drift in the open-source AI ecosystem, analyzing licenses for 364,000 datasets and 1.6 million models on Hugging Face and their integration into 140,000 GitHub projects, revealing that 35.5% of model-to-application transitions involve relicensing that removes restrictive clauses, indicating widespread non-compliance. It introduces LicenseRec, a prototype AI-aware framework that detects license conflicts and recommends solutions, covering 86.4% of cases, and releases the dataset and tools to support future compliance efforts.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new problem by conducting the first comprehensive end-to-end audit of license propagation in the AI supply chain and develops a novel AI-aware framework for detecting conflicts, significantly advancing understanding and tools in this area.",
      "impact_score": "High",
      "impact_justification": "The work could influence future research and commercial applications by highlighting critical governance issues in open-source AI and providing publicly released datasets and tools for automated compliance.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to AI and software engineering by quantifying license risks and offering practical tools, making it essential for researchers and practitioners in open-source AI, though not universally critical for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cda7bce7e0581d06bab45405887daa21df39d69b",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 12,
      "average_h_index": 4.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "James Jewitt",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380441727"
        },
        {
          "name": "Hao Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2367444515"
        },
        {
          "name": "Bram Adams",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2254329941"
        },
        {
          "name": "Gopi Krishnan Rajbahadur",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/19248159"
        },
        {
          "name": "Ahmed E. Hassan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2326253700"
        }
      ]
    },
    {
      "id": "2509.09880",
      "title": "Automated Tuning for Diffusion Inverse Problem Solvers without\n  Generative Prior Retraining",
      "authors": [
        "Yaşar Utku Alçalar",
        "Junno Yun",
        "Mehmet Akçakaya"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Diffusion/score-based models have recently emerged as powerful generative\npriors for solving inverse problems, including accelerated MRI reconstruction.\nWhile their flexibility allows decoupling the measurement model from the\nlearned prior, their performance heavily depends on carefully tuned data\nfidelity weights, especially under fast sampling schedules with few denoising\nsteps. Existing approaches often rely on heuristics or fixed weights, which\nfail to generalize across varying measurement conditions and irregular timestep\nschedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling\n(ZADS), a test-time optimization method that adaptively tunes fidelity weights\nacross arbitrary noise schedules without requiring retraining of the diffusion\nprior. ZADS treats the denoising process as a fixed unrolled sampler and\noptimizes fidelity weights in a self-supervised manner using only undersampled\nmeasurements. Experiments on the fastMRI knee dataset demonstrate that ZADS\nconsistently outperforms both traditional compressed sensing and recent\ndiffusion-based methods, showcasing its ability to deliver high-fidelity\nreconstructions across varying noise schedules and acquisition settings.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09880v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09880v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.514,
      "distributed_training_score": 0.396,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for solving inverse problems in MRI reconstruction by adaptively tuning fidelity weights, without any involvement in multi-step logical reasoning or treating a Chain-of-Thought as an entity. It deals with image generation and denoising priors, which do not align with the topic's emphasis on complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09893",
      "title": "Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe\n  Self-augmentation with Demonstrator-annotated Precision",
      "authors": [
        "Hanbit Oh",
        "Masaki Murooka",
        "Tomohiro Motoda",
        "Ryoichi Nakajo",
        "Yukiyasu Domae"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Imitation learning is a promising paradigm for training robot agents;\nhowever, standard approaches typically require substantial data acquisition --\nvia numerous demonstrations or random exploration -- to ensure reliable\nperformance. Although exploration reduces human effort, it lacks safety\nguarantees and often results in frequent collisions -- particularly in\nclearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual\nenvironmental resets and imposing additional human burden. This study proposes\nSelf-Augmented Robot Trajectory (SART), a framework that enables policy\nlearning from a single human demonstration, while safely expanding the dataset\nthrough autonomous augmentation. SART consists of two stages: (1) human\nteaching only once, where a single demonstration is provided and precision\nboundaries -- represented as spheres around key waypoints -- are annotated,\nfollowed by one environment reset; (2) robot self-augmentation, where the robot\ngenerates diverse, collision-free trajectories within these boundaries and\nreconnects to the original demonstration. This design improves the data\ncollection efficiency by minimizing human effort while ensuring safety.\nExtensive evaluations in simulation and real-world manipulation tasks show that\nSART achieves substantially higher success rates than policies trained solely\non human-collected demonstrations. Video results available at\nhttps://sites.google.com/view/sart-il .",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.09893v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09893v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.346,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on imitation learning with a single human demonstration and autonomous data augmentation, without involving a reward model trained on human-ranked data or reinforcement learning for fine-tuning. RLHF specifically requires human feedback to shape preferences via RL, which is not present here.",
      "weak_supervision_justification": "The paper uses a single human demonstration with annotated boundaries to programmatically generate additional training data via robot augmentation, reducing reliance on extensive hand-labeled data. This aligns with weak supervision's use of high-level or imprecise sources for label generation, though it still incorporates direct human input, making it only moderately relevant.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Self-Augmented Robot Trajectory (SART), a framework for imitation learning that enables robots to learn from a single human demonstration by autonomously generating safe, diverse trajectories within human-annotated precision boundaries, thereby minimizing human effort and ensuring safety in clearance-limited tasks. The methodology involves two stages: first, collecting a demonstration with annotated spheres around key waypoints, and second, the robot creating collision-free variations within these boundaries to expand the dataset; evaluations in simulation and real-world tasks demonstrate that SART achieves higher success rates and reduces human involvement compared to traditional methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining human-annotated precision boundaries with autonomous data augmentation, offering a safer alternative to existing imitation learning methods that rely on extensive demonstrations or risky exploration. While it builds on prior ideas, it introduces a clever mechanism for efficiency and safety, advancing the field without fundamentally redefining it.",
      "impact_score": "Moderate",
      "impact_justification": "This work is likely to be cited and built upon in robotics and AI subfields, particularly for tasks requiring safe manipulation, as it addresses key challenges in data efficiency and safety. However, its influence may be confined to specific applications like clearance-limited tasks, limiting broader commercial adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a strong, practical contribution to imitation learning by enhancing efficiency and safety, making it valuable for researchers focused on robotic automation. While not essential for all, it offers insights that could inspire further innovations in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c331f6f62536d4f391db6dcea181a16b40df7839",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 12,
      "average_h_index": 3.4,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Hanbit Oh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2367643082"
        },
        {
          "name": "Masaki Murooka",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2350756597"
        },
        {
          "name": "Tomohiro Motoda",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2328411976"
        },
        {
          "name": "Ryoichi Nakajo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/3411577"
        },
        {
          "name": "Y. Domae",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2512607"
        }
      ]
    },
    {
      "id": "2509.10569",
      "title": "MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of\n  Latent Diffusion Models",
      "authors": [
        "Leyi Pan",
        "Sheng Guan",
        "Zheyu Fu",
        "Luyang Si",
        "Zian Wang",
        "Xuming Hu",
        "Irwin King",
        "Philip S. Yu",
        "Aiwei Liu",
        "Lijie Wen"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "We introduce MarkDiffusion, an open-source Python toolkit for generative\nwatermarking of latent diffusion models. It comprises three key components: a\nunified implementation framework for streamlined watermarking algorithm\nintegrations and user-friendly interfaces; a mechanism visualization suite that\nintuitively showcases added and extracted watermark patterns to aid public\nunderstanding; and a comprehensive evaluation module offering standard\nimplementations of 24 tools across three essential aspects - detectability,\nrobustness, and output quality - plus 8 automated evaluation pipelines. Through\nMarkDiffusion, we seek to assist researchers, enhance public awareness and\nengagement in generative watermarking, and promote consensus while advancing\nresearch and applications.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.10569v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10569v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.512,
      "distributed_training_score": 0.319,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents MarkDiffusion, a toolkit for generative watermarking in latent diffusion models, focusing on embedding and detecting watermarks in generated images or videos. It does not involve adapting the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for multi-step reasoning. The core contributions are related to watermarking techniques, not logical reasoning applications.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10570",
      "title": "Large Foundation Models for Trajectory Prediction in Autonomous Driving:\n  A Comprehensive Survey",
      "authors": [
        "Wei Dai",
        "Shengen Wu",
        "Wei Wu",
        "Zhenhao Wang",
        "Sisuo Lyu",
        "Haicheng Liao",
        "Limin Yu",
        "Weiping Ding",
        "Runwei Guan",
        "Yutao Yue"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Trajectory prediction serves as a critical functionality in autonomous\ndriving, enabling the anticipation of future motion paths for traffic\nparticipants such as vehicles and pedestrians, which is essential for driving\nsafety. Although conventional deep learning methods have improved accuracy,\nthey remain hindered by inherent limitations, including lack of\ninterpretability, heavy reliance on large-scale annotated data, and weak\ngeneralization in long-tail scenarios. The rise of Large Foundation Models\n(LFMs) is transforming the research paradigm of trajectory prediction. This\nsurvey offers a systematic review of recent advances in LFMs, particularly\nLarge Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for\ntrajectory prediction. By integrating linguistic and scene semantics, LFMs\nfacilitate interpretable contextual reasoning, significantly enhancing\nprediction safety and generalization in complex environments. The article\nhighlights three core methodologies: trajectory-language mapping, multimodal\nfusion, and constraint-based reasoning. It covers prediction tasks for both\nvehicles and pedestrians, evaluation metrics, and dataset analyses. Key\nchallenges such as computational latency, data scarcity, and real-world\nrobustness are discussed, along with future research directions including\nlow-latency inference, causality-aware modeling, and motion foundation models.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.10570v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10570v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.385,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper is a survey on Large Foundation Models (LFMs) for trajectory prediction in autonomous driving, focusing on LLMs and MLLMs for reasoning and prediction tasks. It does not mention reinforcement learning, human feedback, reward models, or any alignment processes involving human-ranked data, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses Chain-of-Thought (CoT) reasoning in LLMs for trajectory prediction, which involves multi-step logical processes, but it does not reference diffusion models, iterative refinement for reasoning, or any adaptation of diffusion techniques. Thus, it lacks the required components for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10572",
      "title": "Quality Assessment of Tabular Data using Large Language Models and Code\n  Generation",
      "authors": [
        "Ashlesha Akella",
        "Akshar Kaul",
        "Krishnasuri Narayanam",
        "Sameep Mehta"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.DB (Databases)"
      ],
      "abstract": "Reliable data quality is crucial for downstream analysis of tabular datasets,\nyet rule-based validation often struggles with inefficiency, human\nintervention, and high computational costs. We present a three-stage framework\nthat combines statistical inliner detection with LLM-driven rule and code\ngeneration. After filtering data samples through traditional clustering, we\niteratively prompt LLMs to produce semantically valid quality rules and\nsynthesize their executable validators through code-generating LLMs. To\ngenerate reliable quality rules, we aid LLMs with retrieval-augmented\ngeneration (RAG) by leveraging external knowledge sources and domain-specific\nfew-shot examples. Robust guardrails ensure the accuracy and consistency of\nboth rules and code snippets. Extensive evaluations on benchmark datasets\nconfirm the effectiveness of our approach.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.10572v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10572v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.481,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.345,
      "datasets_score": 0.436,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on a framework for tabular data quality assessment using LLMs and code generation, without any mention of human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "The paper uses retrieval-augmented generation (RAG) with few-shot examples for LLM prompting, which involves noisy or programmatic sources, but it does not primarily focus on training models with weakly supervised labels.",
      "diffusion_reasoning_justification": "The paper employs iterative LLM prompting for rule generation but does not involve diffusion models, multi-step logical reasoning via diffusion, or holistic chain-of-thought refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper evaluates its framework on benchmark datasets for data quality assessment, which aligns with benchmarking and evaluating datasets for AI applications, though it is not primarily about creating or analyzing new datasets.",
      "llm_score_status": "completed",
      "summary": "This paper presents a three-stage framework for assessing the quality of tabular data by integrating statistical inlier detection through clustering with large language models (LLMs) to generate semantically valid quality rules and corresponding code validators. The methodology leverages retrieval-augmented generation (RAG) for incorporating external knowledge and domain-specific examples, ensures accuracy via robust guardrails, and demonstrates effectiveness through evaluations on benchmark datasets, addressing limitations of traditional and existing LLM-based approaches by balancing adaptability, efficiency, and semantic understanding.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of statistical clustering and LLM-driven rule generation with code synthesis, offering a notable improvement over existing methods by incorporating RAG for better semantic accuracy, but it does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in AI-driven data quality assessment and database management by providing a scalable framework, potentially leading to citations and adaptations in subfields like software engineering and AI, though its broader commercial impact may be limited to specific applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a high-quality contribution with a practical framework that advances data quality techniques using LLMs, making it essential for researchers in AI and databases to understand its innovations and potential applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/606fb4f7dab820e79cb2a6697b6db6a6ae617ced",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 2.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ashlesha Akella",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/152426178"
        },
        {
          "name": "Akshar Kaul",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/29154944"
        },
        {
          "name": "Krishnasuri Narayanam",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2405247"
        },
        {
          "name": "Sameep Mehta",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2355162048"
        }
      ]
    },
    {
      "id": "2509.10575",
      "title": "Gene-R1: Reasoning with Data-Augmented Lightweight LLMs for Gene Set\n  Analysis",
      "authors": [
        "Zhizheng Wang",
        "Yifan Yang",
        "Qiao Jin",
        "Zhiyong Lu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The gene set analysis (GSA) is a foundational approach for uncovering the\nmolecular functions associated with a group of genes. Recently, LLM-powered\nmethods have emerged to annotate gene sets with biological functions together\nwith coherent explanatory insights. However, existing studies primarily focus\non proprietary models, which have been shown to outperform their open-source\ncounterparts despite concerns over cost and data privacy. Furthermore, no\nresearch has investigated the application of advanced reasoning strategies to\nthe GSA task. To address this gap, we introduce Gene-R1, a data-augmented\nlearning framework that equips lightweight and open-source LLMs with\nstep-by-step reasoning capabilities tailored to GSA. Experiments on 1,508\nin-distribution gene sets demonstrate that Gene-R1 achieves substantial\nperformance gains, matching commercial LLMs. On 106 out-of-distribution gene\nsets, Gene-R1 performs comparably to both commercial and large-scale LLMs,\nexhibiting robust generalizability across diverse gene sources.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.10575v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10575v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.324,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Gene-R1, a framework that enhances lightweight LLMs with step-by-step reasoning for gene set analysis, but it does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a holistically corrected entity. There is no evidence of adapting diffusion techniques for logical tasks, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10576",
      "title": "Aesthetic Experience and Educational Value in Co-creating Art with\n  Generative AI: Evidence from a Survey of Young Learners",
      "authors": [
        "Chengyuan Zhang",
        "Suzhe Xu"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study investigates the aesthetic experience and educational value of\ncollaborative artmaking with generative artificial intelligence (AI) among\nyoung learners and art students. Based on a survey of 112 participants, we\nexamine how human creators renegotiate their roles, how conventional notions of\noriginality are challenged, how the creative process is transformed, and how\naesthetic judgment is formed in human--AI co-creation. Empirically,\nparticipants generally view AI as a partner that stimulates ideation and\nexpands creative boundaries rather than a passive tool, while simultaneously\nvoicing concerns about stylistic homogenization and the erosion of traditional\nauthorship. Theoretically, we synthesize Dewey's aesthetics of experience,\nIhde's postphenomenology, and actor--network theory (ANT) into a single\nanalytical framework to unpack the dynamics between human creators and AI as a\nnon-human actant. Findings indicate (i) a fluid subjectivity in which creators\nshift across multiple stances (director, dialogic partner, discoverer); (ii) an\niterative, dialogic workflow (intent--generate--select--refine) that centers\ncritical interpretation; and (iii) an educational value shift from technical\nskill training toward higher-order competencies such as critical judgment,\ncross-modal ideation, and reflexivity. We argue that arts education should\ncultivate a \\emph{critical co-creation} stance toward technology, guiding\nlearners to collaborate with AI while preserving human distinctiveness in\nconcept formation, judgment, and meaning-making.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.10576v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10576v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.271,
      "datasets_score": 0.393,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper examines aesthetic experiences, educational value, and human-AI collaboration in art creation through surveys and theoretical frameworks, without any discussion of training AI models using human feedback, reward models, or reinforcement learning techniques. It focuses on user perceptions and applications of existing generative AI, not on AI development methods like RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10577",
      "title": "The Coding Limits of Robust Watermarking for Generative Models",
      "authors": [
        "Danilo Francati",
        "Yevin Nikhel Goonatilake",
        "Shubham Pawar",
        "Daniele Venturi",
        "Giuseppe Ateniese"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We prove a sharp threshold for the robustness of cryptographic watermarking\nfor generative models. This is achieved by introducing a coding abstraction,\nwhich we call messageless secret-key codes, that formalizes sufficient and\nnecessary requirements of robust watermarking: soundness, tamper detection, and\npseudorandomness. Thus, we establish that robustness has a precise limit: For\nbinary outputs no scheme can survive if more than half of the encoded bits are\nmodified, and for an alphabet of size q the corresponding threshold is\n$(1-1/q)$ of the symbols.\n  Complementing this impossibility, we give explicit constructions that meet\nthe bound up to a constant slack. For every ${\\delta} > 0$, assuming\npseudorandom functions and access to a public counter, we build linear-time\ncodes that tolerate up to $(1/2)(1-{\\delta})$ errors in the binary case and\n$(1-1/q)(1-{\\delta})$ errors in the $q$-ary case. Together with the lower\nbound, these yield the maximum robustness achievable under standard\ncryptographic assumptions.\n  We then test experimentally whether this limit appears in practice by looking\nat the recent watermarking for images of Gunn, Zhao, and Song (ICLR 2025). We\nshow that a simple crop and resize operation reliably flipped about half of the\nlatent signs and consistently prevented belief-propagation decoding from\nrecovering the codeword, erasing the watermark while leaving the image visually\nintact.\n  These results provide a complete characterization of robust watermarking,\nidentifying the threshold at which robustness fails, constructions that achieve\nit, and an experimental confirmation that the threshold is already reached in\npractice.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.10577v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10577v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.292,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10582",
      "title": "LearnLens: An AI-Enhanced Dashboard to Support Teachers in Open-Ended\n  Classrooms",
      "authors": [
        "Namrata Srivastava",
        "Shruti Jain",
        "Clayton Cohn",
        "Naveeduddin Mohammed",
        "Umesh Timalsina",
        "Gautam Biswas"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Exploratory learning environments (ELEs), such as simulation-based platforms\nand open-ended science curricula, promote hands-on exploration and\nproblem-solving but make it difficult for teachers to gain timely insights into\nstudents' conceptual understanding. This paper presents LearnLens, a generative\nAI (GenAI)-enhanced teacher-facing dashboard designed to support problem-based\ninstruction in middle school science. LearnLens processes students' open-ended\nresponses from digital assessments to provide various insights, including\nsample responses, word clouds, bar charts, and AI-generated summaries. These\nfeatures elucidate students' thinking, enabling teachers to adjust their\ninstruction based on emerging patterns of understanding. The dashboard was\ninformed by teacher input during professional development sessions and\nimplemented within a middle school Earth science curriculum. We report insights\nfrom teacher interviews that highlight the dashboard's usability and potential\nto guide teachers' instruction in the classroom.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.10582v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10582v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.314,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes LearnLens, a dashboard using generative AI for analyzing student responses, with design informed by teacher input. However, it does not involve training AI models with human-ranked data or using reinforcement learning for fine-tuning, which are core to RLHF. The human feedback mentioned is for dashboard design, not AI alignment via RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generative AI for summarizing and visualizing student responses in educational settings, but it does not mention diffusion models, iterative refinement for multi-step logical reasoning, or treating a chain-of-thought as a holistic entity. There is no component involving diffusion-based processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12247",
      "title": "Modular, On-Site Solutions with Lightweight Anomaly Detection for\n  Sustainable Nutrient Management in Agriculture",
      "authors": [
        "Abigail R. Cohen",
        "Yuming Sun",
        "Zhihao Qin",
        "Harsh S. Muriki",
        "Zihao Xiao",
        "Yeonju Lee",
        "Matthew Housley",
        "Andrew F. Sharkey",
        "Rhuanito S. Ferrarezi",
        "Jing Li",
        "Lu Gan",
        "Yongsheng Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Efficient nutrient management is critical for crop growth and sustainable\nresource consumption (e.g., nitrogen, energy). Current approaches require\nlengthy analyses, preventing real-time optimization; similarly, imaging\nfacilitates rapid phenotyping but can be computationally intensive, preventing\ndeployment under resource constraints. This study proposes a flexible, tiered\npipeline for anomaly detection and status estimation (fresh weight, dry mass,\nand tissue nutrients), including a comprehensive energy analysis of approaches\nthat span the efficiency-accuracy spectrum. Using a nutrient depletion\nexperiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer\nstrength) and multispectral imaging (MSI), we developed a hierarchical pipeline\nusing an autoencoder (AE) for early warning. Further, we compared two status\nestimation modules of different complexity for more detailed analysis:\nvegetation index (VI) features with machine learning (Random Forest, RF) and\nraw whole-image deep learning (Vision Transformer, ViT). Results demonstrated\nhigh-efficiency anomaly detection (73% net detection of T3 samples 9 days after\ntransplanting) at substantially lower energy than embodied energy in wasted\nnitrogen. The state estimation modules show trade-offs, with ViT outperforming\nRF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at\nhigher energy cost. With our modular pipeline, this work opens opportunities\nfor edge diagnostics and practical opportunities for agricultural\nsustainability.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.12247v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12247v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.298,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.286,
      "distributed_training_score": 0.365,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13338",
      "title": "Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks",
      "authors": [
        "Hassan Gharoun",
        "Mohammad Sadegh Khorshidi",
        "Kasra Ranjbarigderi",
        "Fang Chen",
        "Amir H. Gandomi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.NE (Neural and Evolutionary Computing)"
      ],
      "abstract": "This work proposes an evidence-retrieval mechanism for uncertainty-aware\ndecision-making that replaces a single global cutoff with an\nevidence-conditioned, instance-adaptive criterion. For each test instance,\nproximal exemplars are retrieved in an embedding space; their predictive\ndistributions are fused via Dempster-Shafer theory. The resulting fused belief\nacts as a per-instance thresholding mechanism. Because the supporting evidences\nare explicit, decisions are transparent and auditable. Experiments on\nCIFAR-10/100 with BiT and ViT backbones show higher or comparable\nuncertainty-aware performance with materially fewer confidently incorrect\noutcomes and a sustainable review load compared with applying threshold on\nprediction entropy. Notably, only a few evidences are sufficient to realize\nthese gains; increasing the evidence set yields only modest changes. These\nresults indicate that evidence-conditioned tagging provides a more reliable and\ninterpretable alternative to fixed prediction entropy thresholds for\noperational uncertainty-aware decision-making.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.13338v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13338v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.423,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.345,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is an evidence-retrieval mechanism for uncertainty-aware decision-making using proximal exemplars and Dempster-Shafer theory, which focuses on inference and decision transparency. It does not involve training models with programmatically generated, noisy, or imprecise labels, as it relies on existing trained models and historical data for evaluation, not weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper proposes a proximity-based evidence retrieval method for uncertainty quantification, involving exemplar retrieval and belief fusion, but it does not use diffusion models or iterative refinement processes for multi-step logical reasoning. There is no component related to treating a Chain-of-Thought as a single entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13339",
      "title": "Position: AI Safety Must Embrace an Antifragile Perspective",
      "authors": [
        "Ming Jin",
        "Hyunin Lee"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This position paper contends that modern AI research must adopt an\nantifragile perspective on safety -- one in which the system's capacity to\nguarantee long-term AI safety such as handling rare or out-of-distribution\n(OOD) events expands over time. Conventional static benchmarks and single-shot\nrobustness tests overlook the reality that environments evolve and that models,\nif left unchallenged, can drift into maladaptation (e.g., reward hacking,\nover-optimization, or atrophy of broader capabilities). We argue that an\nantifragile approach -- Rather than striving to rapidly reduce current\nuncertainties, the emphasis is on leveraging those uncertainties to better\nprepare for potentially greater, more unpredictable uncertainties in the future\n-- is pivotal for the long-term reliability of open-ended ML systems. In this\nposition paper, we first identify key limitations of static testing, including\nscenario diversity, reward hacking, and over-alignment. We then explore the\npotential of antifragile solutions to manage rare events. Crucially, we\nadvocate for a fundamental recalibration of the methods used to measure,\nbenchmark, and continually improve AI safety over the long term, complementing\nexisting robustness approaches by providing ethical and practical guidelines\ntowards fostering an antifragile AI safety community.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.13339v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13339v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.462,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.398,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses AI safety and antifragility, mentioning issues like reward hacking and over-optimization as examples of maladaptation in AI systems. While reward hacking is a known risk in reinforcement learning contexts, including RLHF, the paper does not specifically address or propose using human feedback to train a reward model for fine-tuning AI models. Its main focus is on broader AI safety strategies for handling uncertainties and evolving environments, making it only indirectly related to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13341",
      "title": "Imagined Autocurricula",
      "authors": [
        "Ahmet H. Güzel",
        "Matthew Thomas Jackson",
        "Jarek Luca Liesen",
        "Tim Rocktäschel",
        "Jakob Nicolaus Foerster",
        "Ilija Bogunovic",
        "Jack Parker-Holder"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Training agents to act in embodied environments typically requires vast\ntraining data or access to accurate simulation, neither of which exists for\nmany cases in the real world. Instead, world models are emerging as an\nalternative leveraging offline, passively collected data, they make it possible\nto generate diverse worlds for training agents in simulation. In this work, we\nharness world models to generate imagined environments to train robust agents\ncapable of generalizing to novel task variations. One of the challenges in\ndoing this is ensuring the agent trains on useful generated data. We thus\npropose a novel approach, IMAC (Imagined Autocurricula), leveraging\nUnsupervised Environment Design (UED), which induces an automatic curriculum\nover generated worlds. In a series of challenging, procedurally generated\nenvironments, we show it is possible to achieve strong transfer performance on\nheld-out environments, having trained only inside a world model learned from a\nnarrower dataset. We believe this opens the path to utilizing larger-scale,\nfoundation world models for generally capable agents.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.13341v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13341v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.436,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.384,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on training agents using world models and offline RL data from environments, without any involvement of human preferences, feedback, or a reward model based on human rankings. It relies solely on automated processes and passively collected data.",
      "weak_supervision_justification": "The paper uses offline, passively collected data to train world models, but it does not involve programmatically generating training labels from high-level or noisy sources. Instead, it works with existing state-action-reward sequences, which is not the core mechanism of weak supervision.",
      "diffusion_reasoning_justification": "Although the paper employs a diffusion world model for generating environments and simulations, it does not adapt diffusion for multi-step logical reasoning or chain-of-thought refinement. The diffusion model is used for state transitions and environment creation, not for solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14251",
      "title": "Unified Crew Planning and Replanning Optimization in Multi-Line Metro\n  Systems Considering Workforce Heterogeneity",
      "authors": [
        "Qihang Chen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Metro crew planning is a key component of smart city development as it\ndirectly impacts the operational efficiency and service reliability of public\ntransportation. With the rapid expansion of metro networks, effective\nmulti-line scheduling and emergency management have become essential for\nlarge-scale seamless operations. However, current research focuses primarily on\nindividual metro lines,with insufficient attention on cross-line coordination\nand rapid replanning during disruptions. Here, a unified optimization framework\nis presented for multi-line metro crew planning and replanning with\nheterogeneous workforce. Specifically, a hierarchical time-space network model\nis proposed to represent the unified crew action space, and computationally\nefficient constraints and formulations are derived for the crew's heterogeneous\nqualifications and preferences. Solution algorithms based on column generation\nand shortest path adjustment are further developed, utilizing the proposed\nnetwork model. Experiments with real data from Shanghai and Beijing Metro\ndemonstrate that the proposed methods outperform benchmark heuristics in both\ncost reduction and task completion,and achieve notable efficiency gains by\nincorporating cross-line operations, particularly for urgent tasks during\ndisruptions. This work highlights the role of global optimization and\ncross-line coordination in multi-line metro system operations, providing\ninsights into the efficient and reliable functioning of public transportation\nin smart cities.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.14251v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14251v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.266,
      "diffusion_reasoning_score": 0.3,
      "distributed_training_score": 0.359,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14252",
      "title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive\n  Architectures",
      "authors": [
        "Hai Huang",
        "Yann LeCun",
        "Randall Balestriero"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Model (LLM) pretraining, finetuning, and evaluation rely on\ninput-space reconstruction and generative capabilities. Yet, it has been\nobserved in vision that embedding-space training objectives, e.g., with Joint\nEmbedding Predictive Architectures (JEPAs), are far superior to their\ninput-space counterpart. That mismatch in how training is achieved between\nlanguage and vision opens up a natural question: {\\em can language training\nmethods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is\na testimony of the challenge in designing such objectives for language. In this\nwork, we propose a first step in that direction where we develop LLM-JEPA, a\nJEPA based solution for LLMs applicable both to finetuning and pretraining.\nThus far, LLM-JEPA is able to outperform the standard LLM training objectives\nby a significant margin across models, all while being robust to overfiting.\nThose findings are observed across numerous datasets (NL-RX, GSM8K, Spider,\nRottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo\nfamilies. Code: https://github.com/rbalestr-lab/llm-jepa.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.14252v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14252v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.369,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on adapting Joint Embedding Predictive Architectures (JEPAs) for Large Language Models (LLMs) to improve representation learning through embedding-space objectives, without any mention of reinforcement learning, human feedback, reward models, or alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces LLM-JEPA, which uses JEPA for LLM training by predicting embeddings from different views, and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14253",
      "title": "CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt\n  Tuning",
      "authors": [
        "Ahmad Pouramini",
        "Hesham Faili"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Prompt tuning offers a parameter-efficient way to adapt large pre-trained\nlanguage models to new tasks, but most existing approaches are designed for\nsingle-task settings, failing to share knowledge across related tasks. We\npropose Cross-task Prompt Tuning (CrossPT), a modular framework for multi-task\nprompt tuning that enables controlled knowledge transfer while maintaining\ntask-specific specialization. CrossPT decomposes each target prompt into\nshared, pre-trained source prompts and task-specific private prompts, combined\nvia a learned attention mechanism. To support robust transfer, we\nsystematically investigate key design factors including prompt initialization,\nbalancing shared and private prompts, number of source prompts, learning rates,\ntask prefixes, and label semantics. Empirical results on GLUE and related\nbenchmarks show that CrossPT achieves higher accuracy and robustness compared\nto traditional prompt tuning and related methods, particularly in low-resource\nscenarios, while maintaining strong parameter efficiency.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.14253v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14253v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.419,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on multi-task prompt tuning for language models, emphasizing knowledge transfer via shared and private prompts with attention mechanisms. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks, such as treating a Chain-of-Thought as a holistic entity for multi-step correction.",
      "distributed_training_justification": "The paper discusses parameter-efficient adaptation of language models through prompt tuning, which is about optimizing model parameters for multiple tasks, but it does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors. The emphasis is on efficiency within a single model, not distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14254",
      "title": "Hallucination Detection with the Internal Layers of LLMs",
      "authors": [
        "Martin Preiß"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have succeeded in a variety of natural language\nprocessing tasks [Zha+25]. However, they have notable limitations. LLMs tend to\ngenerate hallucinations, a seemingly plausible yet factually unsupported output\n[Hua+24], which have serious real-world consequences [Kay23; Rum+24]. Recent\nwork has shown that probing-based classifiers that utilize LLMs' internal\nrepresentations can detect hallucinations [AM23; Bei+24; Bur+24; DYT24; Ji+24;\nSMZ24; Su+24]. This approach, since it does not involve model training, can\nenhance reliability without significantly increasing computational costs.\n  Building upon this approach, this thesis proposed novel methods for\nhallucination detection using LLM internal representations and evaluated them\nacross three benchmarks: TruthfulQA, HaluEval, and ReFact. Specifically, a new\narchitecture that dynamically weights and combines internal LLM layers was\ndeveloped to improve hallucination detection performance. Throughout extensive\nexperiments, two key findings were obtained: First, the proposed approach was\nshown to achieve superior performance compared to traditional probing methods,\nthough generalization across benchmarks and LLMs remains challenging. Second,\nthese generalization limitations were demonstrated to be mitigated through\ncross-benchmark training and parameter freezing. While not consistently\nimproving, both techniques yielded better performance on individual benchmarks\nand reduced performance degradation when transferred to other benchmarks. These\nfindings open new avenues for improving LLM reliability through internal\nrepresentation analysis.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.14254v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14254v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.344,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on hallucination detection using internal representations of LLMs without any involvement of human feedback, reward models, or reinforcement learning for model alignment. It explicitly states that the approach does not involve model training, which is a core aspect of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks. Instead, it proposes methods for combining internal LLM layers for hallucination detection, which lacks any connection to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16221",
      "title": "Evaluation of Ensemble Learning Techniques for handwritten OCR\n  Improvement",
      "authors": [
        "Martin Preiß"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "For the bachelor project 2021 of Professor Lippert's research group,\nhandwritten entries of historical patient records needed to be digitized using\nOptical Character Recognition (OCR) methods. Since the data will be used in the\nfuture, a high degree of accuracy is naturally required. Especially in the\nmedical field this has even more importance. Ensemble Learning is a method that\ncombines several machine learning models and is claimed to be able to achieve\nan increased accuracy for existing methods. For this reason, Ensemble Learning\nin combination with OCR is investigated in this work in order to create added\nvalue for the digitization of the patient records. It was possible to discover\nthat ensemble learning can lead to an increased accuracy for OCR, which methods\nwere able to achieve this and that the size of the training data set did not\nplay a role here.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.16221v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16221v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.257,
      "distributed_training_score": 0.277,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16223",
      "title": "mRadNet: A Compact Radar Object Detector with MetaFormer",
      "authors": [
        "Huaiyu Chen",
        "Fahed Hassanat",
        "Robert Laganiere",
        "Martin Bouchard"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Frequency-modulated continuous wave radars have gained increasing popularity\nin the automotive industry. Its robustness against adverse weather conditions\nmakes it a suitable choice for radar object detection in advanced driver\nassistance systems. These real-time embedded systems have requirements for the\ncompactness and efficiency of the model, which have been largely overlooked in\nprevious work. In this work, we propose mRadNet, a novel radar object detection\nmodel with compactness in mind. mRadNet employs a U-net style architecture with\nMetaFormer blocks, in which separable convolution and attention token mixers\nare used to capture both local and global features effectively. More efficient\ntoken embedding and merging strategies are introduced to further facilitate the\nlightweight design. The performance of mRadNet is validated on the CRUW\ndataset, improving state-of-the-art performance with the least number of\nparameters and FLOPs.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.16223v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16223v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.259,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.322,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18126",
      "title": "Anomaly Detection in Electric Vehicle Charging Stations Using Federated\n  Learning",
      "authors": [
        "Bishal K C",
        "Amr Hilal",
        "Pawan Thapa"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Federated Learning (FL) is a decentralized training framework widely used in\nIoT ecosystems that preserves privacy by keeping raw data local, making it\nideal for IoT-enabled cyber-physical systems with sensing and communication\nlike Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric\nVehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle\ninfrastructure, securing these IoT-based charging stations against cyber\nthreats has become critical. Centralized Intrusion Detection Systems (IDS)\nraise privacy concerns due to sensitive network and user data, making FL a\npromising alternative. However, current FL-based IDS evaluations overlook\npractical challenges such as system heterogeneity and non-IID data. To address\nthese challenges, we conducted experiments to evaluate the performance of\nfederated learning for anomaly detection in EV charging stations under system\nand data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization\napproaches, to analyze their effectiveness in anomaly detection. Under IID\nsettings, FedAvg achieves superior performance to centralized models using the\nsame neural network. However, performance degrades with non-IID data and system\nheterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous\nsettings, showing better convergence and higher anomaly detection accuracy. Our\nresults demonstrate that FL can handle heterogeneity in IoT-based EVCS without\nsignificant performance loss, with FedAvgM as a promising solution for robust,\nprivacy-preserving EVCS security.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.18126v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18126v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.402,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves evaluating Federated Learning (FL) algorithms, such as FedAvg and FedAvgM, for anomaly detection in EV charging stations. FL is a form of distributed training that partitions data and computation across multiple decentralized devices, aligning directly with distributed training concepts like parallel computing and multi-node machine learning. The study addresses challenges in heterogeneous settings, demonstrating how FL accelerates model training while preserving privacy, making it highly relevant to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper evaluates the use of Federated Learning (FL) for anomaly detection in Electric Vehicle Charging Stations (EVCS), addressing challenges like system heterogeneity and non-IID data distributions to enhance privacy and security. The authors conduct experiments using FL algorithms FedAvg and FedAvgM, comparing their performance to centralized models, and find that while FedAvg excels in IID settings, FedAvgM provides better accuracy and convergence in heterogeneous environments, demonstrating FL's viability for robust, privacy-preserving EVCS security.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying existing FL techniques to anomaly detection in EVCS while addressing overlooked challenges like heterogeneity, rather than introducing a entirely new problem or architecture. However, it builds on prior works, making it a clever combination rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in IoT security and EV infrastructure by highlighting FL's effectiveness in heterogeneous settings, potentially leading to citations and further developments in subfields like smart grids and cybersecurity. Nonetheless, its applicability is somewhat niche and may not broadly affect commercial applications immediately.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to FL applications in EVCS security, providing valuable insights into handling real-world heterogeneity that are relevant for researchers in AI and IoT. While not essential for all, it is a high-quality work that advances understanding in a timely and important area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/701d2c6c6c2767143d6be9f5a1d252b79b6d3e0e",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "C. BishalK",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381729369"
        },
        {
          "name": "Amr Hilal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381727662"
        },
        {
          "name": "Pawan Thapa",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381803703"
        }
      ]
    },
    {
      "id": "2509.18127",
      "title": "Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language\n  Models via Sparse Autoencoder Interpretation Framework",
      "authors": [
        "Jiaqi Weng",
        "Han Zheng",
        "Hanyu Zhang",
        "Qinqin He",
        "Jialing Tao",
        "Hui Xue",
        "Zhixuan Chu",
        "Xiting Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Increasing deployment of large language models (LLMs) in real-world\napplications raises significant safety concerns. Most existing safety research\nfocuses on evaluating LLM outputs or specific safety tasks, limiting their\nability to address broader, undefined risks. Sparse Autoencoders (SAEs)\nfacilitate interpretability research to clarify model behavior by explaining\nsingle-meaning atomic features decomposed from entangled signals. jHowever,\nprior applications on SAEs do not interpret features with fine-grained\nsafety-related concepts, thus inadequately addressing safety-critical\nbehaviors, such as generating toxic responses and violating safety regulations.\nFor rigorous safety analysis, we must extract a rich and diverse set of\nsafety-relevant features that effectively capture these high-risk behaviors,\nyet face two challenges: identifying SAEs with the greatest potential for\ngenerating safety concept-specific neurons, and the prohibitively high cost of\ndetailed feature explanation. In this paper, we propose Safe-SAIL, a framework\nfor interpreting SAE features within LLMs to advance mechanistic understanding\nin safety domains. Our approach systematically identifies SAE with best\nconcept-specific interpretability, explains safety-related neurons, and\nintroduces efficient strategies to scale up the interpretation process. We will\nrelease a comprehensive toolkit including SAE checkpoints and human-readable\nneuron explanations, which supports empirical analysis of safety risks to\npromote research on LLM safety.",
      "published_date": "2025-09-11",
      "arxiv_url": "http://arxiv.org/abs/2509.18127v2",
      "pdf_url": "http://arxiv.org/pdf/2509.18127v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.449,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.387,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Sparse Autoencoders for interpreting LLM features related to safety, without any mention of reinforcement learning, human feedback, reward models, or aligning models with human preferences.",
      "weak_supervision_justification": "The paper's main contribution is an interpretability framework using SAEs, which are unsupervised, and does not involve training models with programmatically generated labels from noisy sources as in weak supervision.",
      "diffusion_reasoning_justification": "The paper deals with SAE-based interpretability for safety analysis in LLMs and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 185,
  "date": "2025-09-11"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
