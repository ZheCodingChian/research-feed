<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 28 August 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 28 August 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 28 August 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2508.20325",
      "title": "GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak\n  Diagnostics for LLMs",
      "authors": [
        "Haibo Jin",
        "Ruoxi Chen",
        "Peiyan Zhang",
        "Andy Zhou",
        "Yang Zhang",
        "Haohan Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "As Large Language Models become increasingly integral to various domains,\ntheir potential to generate harmful responses has prompted significant societal\nand regulatory concerns. In response, governments have issued ethics guidelines\nto promote the development of trustworthy AI. However, these guidelines are\ntypically high-level demands for developers and testers, leaving a gap in\ntranslating them into actionable testing questions to verify LLM compliance.\n  To address this challenge, we introduce GUARD (\\textbf{G}uideline\n\\textbf{U}pholding Test through \\textbf{A}daptive \\textbf{R}ole-play and\nJailbreak \\textbf{D}iagnostics), a testing method designed to operationalize\nguidelines into specific guideline-violating questions that assess LLM\nadherence. To implement this, GUARD uses automated generation of\nguideline-violating questions based on government-issued guidelines, thereby\ntesting whether responses comply with these guidelines. When responses directly\nviolate guidelines, GUARD reports inconsistencies. Furthermore, for responses\nthat do not directly violate guidelines, GUARD integrates the concept of\n``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that\nprovoke unethical or guideline-violating responses, effectively identifying\npotential scenarios that could bypass built-in safety mechanisms. Our method\nfinally culminates in a compliance report, delineating the extent of adherence\nand highlighting any violations.\n  We have empirically validated the effectiveness of GUARD on seven LLMs,\nincluding Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,\nGPT-4o, and Claude-3.7, by testing compliance under three government-issued\nguidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can\ntransfer jailbreak diagnostics to vision-language models, demonstrating its\nusage in promoting reliable LLM-based applications.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20325v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20325v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.479,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.345,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a testing method for LLMs to check compliance with ethical guidelines using automated role-play and jailbreak diagnostics, but it does not involve training or fine-tuning models with human feedback, reward models, or reinforcement learning. There is no mention of aligning AI with human preferences through human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generating guideline-violating questions and testing LLMs through adaptive role-play and jailbreak techniques, but it does not adapt diffusion processes for multi-step logical reasoning or treat Chain-of-Thought as a holistically refined entity. No diffusion models or iterative refinement for complex tasks are described.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20328",
      "title": "Multi-View Graph Convolution Network for Internal Talent Recommendation\n  Based on Enterprise Emails",
      "authors": [
        "Soo Hyun Kim",
        "Jang-Hyun Kim"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Internal talent recommendation is a critical strategy for organizational\ncontinuity, yet conventional approaches suffer from structural limitations,\noften overlooking qualified candidates by relying on the narrow perspective of\na few managers. To address this challenge, we propose a novel framework that\nmodels two distinct dimensions of an employee's position fit from email data:\nWHAT they do (semantic similarity of tasks) and HOW they work (structural\ncharacteristics of their interactions and collaborations). These dimensions are\nrepresented as independent graphs and adaptively fused using a Dual Graph\nConvolutional Network (GCN) with a gating mechanism. Experiments show that our\nproposed gating-based fusion model significantly outperforms other fusion\nstrategies and a heuristic baseline, achieving a top performance of 40.9% on\nHit@100. Importantly, it is worth noting that the model demonstrates high\ninterpretability by learning distinct, context-aware fusion strategies for\ndifferent job families. For example, it learned to prioritize relational (HOW)\ndata for 'sales and marketing' job families while applying a balanced approach\nfor 'research' job families. This research offers a quantitative and\ncomprehensive framework for internal talent discovery, minimizing the risk of\ncandidate omission inherent in traditional methods. Its primary contribution\nlies in its ability to empirically determine the optimal fusion ratio between\ntask alignment (WHAT) and collaborative patterns (HOW), which is required for\nemployees to succeed in the new positions, thereby offering important practical\nimplications.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20328v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20328v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.33,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20333",
      "title": "Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in\n  LLMs",
      "authors": [
        "Md Abdullah Al Mamun",
        "Ihsen Alouani",
        "Nael Abu-Ghazaleh"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Large Language Models (LLMs) are aligned to meet ethical standards and safety\nrequirements by training them to refuse answering harmful or unsafe prompts. In\nthis paper, we demonstrate how adversaries can exploit LLMs' alignment to\nimplant bias, or enforce targeted censorship without degrading the model's\nresponsiveness to unrelated topics. Specifically, we propose Subversive\nAlignment Injection (SAI), a poisoning attack that leverages the alignment\nmechanism to trigger refusal on specific topics or queries predefined by the\nadversary. Although it is perhaps not surprising that refusal can be induced\nthrough overalignment, we demonstrate how this refusal can be exploited to\ninject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning\ndefenses including LLM state forensics, as well as robust aggregation\ntechniques that are designed to detect poisoning in FL settings. We demonstrate\nthe practical dangers of this attack by illustrating its end-to-end impacts on\nLLM-powered application pipelines. For chat based applications such as\nChatDoctor, with 1% data poisoning, the system refuses to answer healthcare\nquestions to targeted racial category leading to high bias ($\\Delta DP$ of\n23%). We also show that bias can be induced in other NLP tasks: for a resume\nselection pipeline aligned to refuse to summarize CVs from a selected\nuniversity, high bias in selection ($\\Delta DP$ of 27%) results. Even higher\nbias ($\\Delta DP$~38%) results on 9 other chat based downstream applications.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20333v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20333v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.489,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.349,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses attacks on LLM alignment processes, which often involve RLHF for training models to refuse harmful prompts, but it does not directly focus on or contribute to RLHF methods like using a reward model with human feedback. Instead, it centers on poisoning alignment data to induce bias, making the connection indirect.",
      "weak_supervision_justification": "The paper's main contribution is on adversarial poisoning attacks to manipulate LLM alignment and inject bias, which does not involve programmatically generating labels from noisy sources as in weak supervision. There is no discussion or relation to weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20340",
      "title": "Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce\n  Formula Generators",
      "authors": [
        "Maolin Sun",
        "Yibiao Yang",
        "Yuming Zhou"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.PL (Programming Languages)"
      ],
      "abstract": "Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems\nand programming languages research, providing the foundation for tasks like\nsymbolic execution and automated verification. Because these solvers sit on the\ncritical path, their correctness is essential, and high-quality test formulas\nare key to uncovering bugs. However, while prior testing techniques performed\nwell on earlier solver versions, they struggle to keep pace with rapidly\nevolving features. Recent approaches based on Large Language Models (LLMs) show\npromise in exploring advanced solver capabilities, but two obstacles remain:\nnearly half of the generated formulas are syntactically invalid, and iterative\ninteractions with the LLMs introduce substantial computational overhead. In\nthis study, we present Chimera, a novel LLM-assisted fuzzing framework that\naddresses both issues by shifting from direct formula generation to the\nsynthesis of reusable term (i.e., logical expression) generators. Particularly,\nChimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for\nSMT theories, including solver-specific extensions, from documentation, and (2)\nsynthesize composable Boolean term generators that adhere to these grammars.\nDuring fuzzing, Chimera populates structural skeletons derived from existing\nformulas with the terms iteratively produced by the LLM-synthesized generators.\nThis design ensures syntactic validity while promoting semantic diversity.\nNotably, Chimera requires only one-time LLM interaction investment,\ndramatically reducing runtime cost. We evaluated Chimera on two leading SMT\nsolvers: Z3 and cvc5. Our experiments show that Chimera has identified 43\nconfirmed bugs, 40 of which have already been fixed by developers.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20340v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20340v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.297,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20345",
      "title": "MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical\n  Vision Language Foundation Models",
      "authors": [
        "Xiao Li",
        "Yanfan Zhu",
        "Ruining Deng",
        "Wei-Qi Wei",
        "Yu Wang",
        "Shilin Zhao",
        "Yaohong Wang",
        "Haichun Yang",
        "Yuankai Huo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Recent advances in medical vision-language models (VLMs) open up remarkable\nopportunities for clinical applications such as automated report generation,\ncopilots for physicians, and uncertainty quantification. However, despite their\npromise, medical VLMs introduce serious security concerns, most notably risks\nof Protected Health Information (PHI) exposure, data leakage, and vulnerability\nto cyberthreats - which are especially critical in hospital environments. Even\nwhen adopted for research or non-clinical purposes, healthcare organizations\nmust exercise caution and implement safeguards. To address these challenges, we\npresent MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)\nenables physicians to manually select and use different models without\nprogramming expertise, (2) supports engineers in efficiently deploying medical\nVLMs in a plug-and-play fashion, with seamless integration of Hugging Face\nopen-source models, and (3) ensures privacy-preserving inference through\nDocker-orchestrated, operating system agnostic deployment. MedFoundationHub\nrequires only an offline local workstation equipped with a single NVIDIA A6000\nGPU, making it both secure and accessible within the typical resources of\nacademic research labs. To evaluate current capabilities, we engaged\nboard-certified pathologists to deploy and assess five state-of-the-art VLMs\n(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and\nLLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,\nyielding 1015 clinician-model scoring events. These assessments revealed\nrecurring limitations, including off-target answers, vague reasoning, and\ninconsistent pathology terminology.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20345v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20345v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.37,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of MedFoundationHub, a toolkit for secure and user-friendly deployment of medical vision-language models, along with evaluations of existing models by pathologists. It does not involve training or fine-tuning AI models using human feedback to create a reward model or apply reinforcement learning techniques. The human evaluations mentioned are for assessing model performance, not for aligning models with human preferences via RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20368",
      "title": "AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal\n  Multi-Objective Reinforcement Learning",
      "authors": [
        "Lang Mei",
        "Zhihan Yang",
        "Chong Chen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent studies have explored integrating Large Language Models (LLMs) with\nsearch engines to leverage both the LLMs' internal pre-trained knowledge and\nexternal information. Specially, reinforcement learning (RL) has emerged as a\npromising paradigm for enhancing LLM reasoning through multi-turn interactions\nwith search engines. However, existing RL-based search agents rely on a single\nLLM to handle both search planning and question-answering (QA) tasks in an\nend-to-end manner, which limits their ability to optimize both capabilities\nsimultaneously. In practice, sophisticated AI search systems often employ a\nlarge, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a\nmore effective and efficient approach is to utilize a small, trainable LLM\ndedicated to search planning. In this paper, we propose\n\\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to\nenhance the performance of frozen QA models by focusing on search planning.\nSpecifically, our approach introduces three key innovations: 1) Decoupling the\nArchitecture of the Search Planner and Generator, 2) Dual-Reward Alignment for\nSearch Planning, and 3) Pareto Optimization of Planning Utility and Cost, to\nachieve the objectives. Extensive experiments on real-world datasets\ndemonstrate that AI SearchPlanner outperforms existing RL-based search agents\nin both effectiveness and efficiency, while exhibiting strong generalization\ncapabilities across diverse frozen QA models and data domains.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20368v3",
      "pdf_url": "http://arxiv.org/pdf/2508.20368v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.461,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.476,
      "distributed_training_score": 0.387,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes an RL framework for search planning using rewards based on performance metrics (e.g., outcome and process rewards from QA accuracy and trajectory rationality), but it does not involve training a reward model from human-ranked data or incorporate human feedback for alignment. Thus, it does not align with RLHF principles.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for search planning and multi-turn interactions, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for correction. It lacks any components related to diffusion-based methods for reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20370",
      "title": "Adaptive Root Cause Localization for Microservice Systems with\n  Multi-Agent Recursion-of-Thought",
      "authors": [
        "Lingzhe Zhang",
        "Tong Jia",
        "Kangjin Wang",
        "Weijie Hong",
        "Chiming Duan",
        "Minghua He",
        "Ying Li"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As contemporary microservice systems become increasingly popular and\ncomplex-often comprising hundreds or even thousands of fine-grained,\ninterdependent subsystems-they are facing more frequent failures. Ensuring\nsystem reliability thus demands accurate root cause localization. While traces\nand metrics have proven to be effective data sources for this task, existing\nmethods either heavily rely on pre-defined schemas, which struggle to adapt to\nevolving operational contexts, or lack interpretability in their reasoning\nprocess, thereby leaving Site Reliability Engineers (SREs) confused. In this\npaper, we conduct a comprehensive study on how SREs localize the root cause of\nfailures, drawing insights from multiple professional SREs across different\norganizations. Our investigation reveals that human root cause analysis\nexhibits three key characteristics: recursiveness, multi-dimensional expansion,\nand cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,\nan adaptive root cause localization method for microservice systems that\nleverages a multi-agent recursion-of-thought framework. RCLAgent employs a\nnovel recursion-of-thought strategy to guide the LLM's reasoning process,\neffectively integrating data from multiple agents and tool-assisted analysis to\naccurately pinpoint the root cause. Experimental evaluations on various public\ndatasets demonstrate that RCLAgent achieves superior performance by localizing\nthe root cause using only a single request-outperforming state-of-the-art\nmethods that depend on aggregating multiple requests. These results underscore\nthe effectiveness of RCLAgent in enhancing the efficiency and precision of root\ncause localization in complex microservice environments.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20370v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20370v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.367,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20371",
      "title": "P2C: Path to Counterfactuals",
      "authors": [
        "Sopam Dasgupta",
        "Sadaf MD Halim",
        "Joaquín Arias",
        "Elmer Salazar",
        "Gopal Gupta"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "Machine-learning models are increasingly driving decisions in high-stakes\nsettings, such as finance, law, and hiring, thus, highlighting the need for\ntransparency. However, the key challenge is to balance transparency --\nclarifying `why' a decision was made -- with recourse: providing actionable\nsteps on `how' to achieve a favourable outcome from an unfavourable outcome.\nCounterfactual explanations reveal `why' an undesired outcome occurred and\n`how' to reverse it through targeted feature changes (interventions).\n  Current counterfactual approaches have limitations: 1) they often ignore\ncausal dependencies between features, and 2) they typically assume all\ninterventions can happen simultaneously, an unrealistic assumption in practical\nscenarios where actions are typically taken in a sequence. As a result, these\ncounterfactuals are often not achievable in the real world.\n  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that\nproduces a plan (ordered sequence of actions) converting an unfavourable\noutcome to a causally consistent favourable outcome. P2C addresses both\nlimitations by 1) Explicitly modelling causal relationships between features\nand 2) Ensuring that each intermediate state in the plan is feasible and\ncausally valid. P2C uses the goal-directed Answer Set Programming system\ns(CASP) to generate the plan accounting for feature changes that happen\nautomatically due to causal dependencies. Furthermore, P2C refines cost\n(effort) computation by only counting changes actively made by the user,\nresulting in realistic cost estimates. Finally, P2C highlights how its causal\nplanner outperforms standard planners, which lack causal knowledge and thus can\ngenerate illegal actions.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20371v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20371v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.275,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework called P2C for generating sequential counterfactual explanations using Answer Set Programming (ASP) and causal modeling to address limitations in existing approaches. It focuses on creating ordered plans for feature changes while respecting causal dependencies, but it does not involve diffusion models, iterative refinement processes, or treating a 'Chain-of-Thought' as a holistic entity for correction. There is no mention of adapting diffusion techniques for logical tasks, making this paper unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20373",
      "title": "Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems",
      "authors": [
        "Yuyao Wang",
        "Bowen Liu",
        "Jianheng Tang",
        "Nuo Chen",
        "Yuhan Li",
        "Qifan Zhang",
        "Jia Li"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reasoning Large Language Models (RLLMs) have recently achieved remarkable\nprogress on complex reasoning tasks, largely enabled by their long\nchain-of-thought (Long CoT) capabilities. However, developing these Long CoT\nbehaviors relies heavily on post-training with high-quality datasets, which are\ntypically costly and human-curated (e.g., mathematics and code), leaving\nscalable alternatives unexplored. In this work, we introduce NP-hard (NPH)\ngraph problems as a novel synthetic training corpus, as they inherently require\ndeep reasoning, extensive exploration, and reflective strategies, which are\ncore characteristics of Long CoT reasoning. Building on this insight, we\ndevelop a two-stage post-training framework: (i) Long CoT Supervised\nFine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially\nenhances reasoning depth, and (ii) Reinforcement Learning (RL) with a\nfine-grained reward design, which sharpens reasoning efficiency. Our flagship\nmodel, Graph-R1-7B, demonstrates strong generalization across mathematics,\ncoding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both\naccuracy and reasoning efficiency. These results position NPH graph problems as\nan effective and scalable resource for advancing Long CoT reasoning in LLMs,\nopening a new frontier for LLM post-training. Our implementation is available\nat https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted\nin our Hugging Face collection HKUST-DSAIL/Graph-R1.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20373v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20373v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.542,
      "distributed_training_score": 0.405,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper involves a Reinforcement Learning (RL) stage in its two-stage framework, using a fine-grained reward design to improve reasoning efficiency in LLMs. However, the rewards are based on algorithmic criteria (e.g., repetition penalty, solution quality) rather than human-ranked data or a separate reward model trained on human feedback, which is core to RLHF. Thus, it touches on RL concepts but does not fully align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using NP-hard graph problems for Supervised Fine-Tuning and RL to enhance Long CoT reasoning in LLMs, with no mention of diffusion models, iterative refinement processes, or treating chains-of-thought as entities for holistic correction. There is no component involving multi-step logical reasoning via diffusion techniques.",
      "distributed_training_justification": "The paper describes a training framework involving SFT and RL but does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors. While it mentions Group Relative Policy Optimization (GRPO) for computational efficiency, this is not framed as distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20374",
      "title": "TCIA: A Task-Centric Instruction Augmentation Method for Instruction\n  Finetuning",
      "authors": [
        "Simin Ma",
        "Shujian Liu",
        "Jun Tan",
        "Yebowen Hu",
        "Song Wang",
        "Sathish Reddy Indurthi",
        "Sanqiang Zhao",
        "Liwei Wu",
        "Jianbing Han",
        "Kaiqiang Song"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Diverse instruction data is vital for effective instruction tuning of large\nlanguage models, as it enables the model to generalize across different types\nof inputs . Building such diversified instruction dataset is an essential step\nin this process. Existing approaches often leverage large language models to\nautomatically explore and generate diverse instructions, ensuring both data\ndiversity and quality. However, they tend to overlook an important factor in\nreal-world applications: on-task relevance. In practice, only a few real-world\napplications require a truly general-purpose model; most benefit from\ntask-specific knowledge tailored to their particular use case. Therefore, it is\nvital to develop instruction augmentation methods that not only maintain\ndiversity but are also optimized for specific, real-world scenarios.\n  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework\nthat systematically expands instructions while preserving both diversity and\ntask alignment. By representing instructions in a discrete query-constraints\nspace, TCIA creates a rich set of task-relevant instructions and enables models\nto generalize to these task-specific instructions without sacrificing overall\nperformance. Experiments show that TCIA improves open-source LLMs' performance\nby an average of 8.7% across four real-world, task-specific applications, and\nin some cases outperforming leading closed-source models. These improvements do\nnot compromise general instruction-following ability, making TCIA a scalable\nand efficient solution for adapting LLMs to real-world, task-focused\napplications.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20374v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20374v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.36,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the introduction of TCIA, a framework for augmenting instructions to improve supervised fine-tuning of large language models, focusing on diversity and task relevance. It involves techniques like instruction decomposition, retrieval, and generation, but does not mention human feedback, reward models, or reinforcement learning algorithms. Since RLHF specifically requires training on human-ranked data and using reinforcement learning for alignment, this paper lacks any direct or indirect connection to those elements.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20376",
      "title": "Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task\n  Dense Prediction",
      "authors": [
        "Mang Cao",
        "Sanping Zhou",
        "Yizhe Li",
        "Ye Deng",
        "Wenli Huang",
        "Le Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Sufficient cross-task interaction is crucial for success in multi-task dense\nprediction. However, sufficient interaction often results in high computational\ncomplexity, forcing existing methods to face the trade-off between interaction\ncompleteness and computational efficiency. To address this limitation, this\nwork proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel\nscanning mechanisms to adapt the Mamba modeling approach for multi-task dense\nprediction. On the one hand, we introduce a novel Bidirectional Interaction\nScan (BI-Scan) mechanism, which constructs task-specific representations as\nbidirectional sequences during interaction. By integrating task-first and\nposition-first scanning modes within a unified linear complexity architecture,\nBI-Scan efficiently preserves critical cross-task information. On the other\nhand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve\nmulti-granularity scene modeling. This design not only meets the diverse\ngranularity requirements of various tasks but also enhances nuanced cross-task\nfeature interactions. Extensive experiments on two challenging benchmarks,\n\\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its\nstate-of-the-art competitors.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20376v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20376v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.399,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing the Mamba decoder for multi-task dense prediction in computer vision, using bidirectional and multi-scale scanning mechanisms to improve cross-task interactions. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no mention of adapting diffusion for chain-of-thought reasoning, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20379",
      "title": "Audio-Guided Visual Editing with Complex Multi-Modal Prompts",
      "authors": [
        "Hyeonyu Kim",
        "Seokhoon Jeong",
        "Seonghee Han",
        "Chanhyuk Choi",
        "Taehwan Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual editing with diffusion models has made significant progress but often\nstruggles with complex scenarios that textual guidance alone could not\nadequately describe, highlighting the need for additional non-text editing\nprompts. In this work, we introduce a novel audio-guided visual editing\nframework that can handle complex editing tasks with multiple text and audio\nprompts without requiring additional training. Existing audio-guided visual\nediting methods often necessitate training on specific datasets to align audio\nwith text, limiting their generalization to real-world situations. We leverage\na pre-trained multi-modal encoder with strong zero-shot capabilities and\nintegrate diverse audio into visual editing tasks, by alleviating the\ndiscrepancy between the audio encoder space and the diffusion model's prompt\nencoder space. Additionally, we propose a novel approach to handle complex\nscenarios with multiple and multi-modal editing prompts through our separate\nnoise branching and adaptive patch selection. Our comprehensive experiments on\ndiverse editing tasks demonstrate that our framework excels in handling\ncomplicated editing scenarios by incorporating rich information from audio,\nwhere text-only approaches fail.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20379v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20379v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.314,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on audio-guided visual editing using diffusion models for image and video manipulation, emphasizing prompt integration and editing tasks. It does not involve adapting the diffusion process for multi-step logical reasoning, chain-of-thought processing, or solving complex logical tasks. Instead, it applies diffusion models to generative visual editing, lacking any component for holistic reasoning correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20381",
      "title": "More Reliable Pseudo-labels, Better Performance: A Generalized Approach\n  to Single Positive Multi-label Learning",
      "authors": [
        "Luong Tran",
        "Thieu Vo",
        "Anh Nguyen",
        "Sang Dinh",
        "Van Nguyen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multi-label learning is a challenging computer vision task that requires\nassigning multiple categories to each image. However, fully annotating\nlarge-scale datasets is often impractical due to high costs and effort,\nmotivating the study of learning from partially annotated data. In the extreme\ncase of Single Positive Multi-Label Learning (SPML), each image is provided\nwith only one positive label, while all other labels remain unannotated.\nTraditional SPML methods that treat missing labels as unknown or negative tend\nto yield inaccuracies and false negatives, and integrating various\npseudo-labeling strategies can introduce additional noise. To address these\nchallenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a\nnovel loss function that effectively learns from diverse pseudo-labels while\nmitigating noise. Complementing this, we introduce a simple yet effective\nDynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these\ncontributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling\n(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate\nthat our framework significantly advances multi-label classification, achieving\nstate-of-the-art results.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20381v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20381v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.517,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.394,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on pseudo-labeling techniques and loss functions for multi-label learning in computer vision, with no mention of human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "The paper addresses learning from partially annotated data using pseudo-labels generated from pre-trained models, which aligns with weak supervision by relying on noisy, programmatically derived labels rather than fully hand-annotated data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper tackles the challenges of Single Positive Multi-Label Learning (SPML), where images are annotated with only one positive label, by introducing the Generalized Pseudo-Label Robust Loss (GPR Loss) to mitigate noise in pseudo-labels and the Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique for dynamic, reliable pseudo-label generation using Vision-Language Models. These components form the Adaptive and Efficient Vision-Language Pseudo-Labeling (AEVLP) framework, which is evaluated through extensive experiments on four benchmark datasets, demonstrating state-of-the-art performance in multi-label classification by effectively handling noisy and uncertain labels.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new loss function (GPR Loss) and a dynamic pseudo-labeling technique (DAMP), which significantly advance the state-of-the-art in SPML by addressing noise in pseudo-labels and improving learning from partial annotations.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of multi-label learning and computer vision, given its state-of-the-art results on SPML, though its influence may be limited to scenarios involving partial annotations rather than broader applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution to SPML with effective new methods and empirical validation, making it valuable for researchers focused on computer vision and partial supervision techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/99d90f5b01805628cace7758b26206932062eabe",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 8,
      "average_h_index": 1.8,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Luong Tran",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2329094191"
        },
        {
          "name": "T. Vo",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2196372534"
        },
        {
          "name": "Anh Nguyen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380437120"
        },
        {
          "name": "Sang Dinh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377791108"
        },
        {
          "name": "Van Nguyen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375982022"
        }
      ]
    },
    {
      "id": "2508.20384",
      "title": "Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for\n  Reasoning LLM",
      "authors": [
        "Yongfu Zhu",
        "Lin Sun",
        "Guangxiang Zhao",
        "Weihong Lin",
        "Xiangzheng Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this work, we introduce Entropy Area Score (EAS), a simple yet effective\nmetric to quantify uncertainty in the answer generation process of reasoning\nlarge language models (LLMs). EAS requires neither external models nor repeated\nsampling, it integrates token-level predictive entropy from the model itself to\ncapture the evolution of uncertainty during generation. Empirical results show\nthat EAS is strongly correlated with answer entropy across models and datasets.\nIn training data selection, EAS identifies high-potential samples and\nconsistently outperforms Pass Rate filtering under equal sample budgets,\nimproving student model accuracy on math benchmarks. EAS is both efficient and\ninterpretable, offering a practical tool for uncertainty modeling and data\nquality assessment in LLM training.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20384v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20384v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.483,
      "distributed_training_score": 0.337,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces EAS for uncertainty quantification in LLMs, which is used to select training data based on model-generated signals (e.g., entropy), aligning with weak supervision's use of noisy or programmatic labels. While EAS improves data selection for training without relying on perfect labels, it does not directly generate labels, making it moderately relevant rather than highly so.",
      "diffusion_reasoning_justification": "The paper focuses on entropy-based uncertainty metrics for LLMs and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. There is no mention of adapting diffusion mechanisms for reasoning tasks, making it entirely unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces the Entropy Area Score (EAS), a novel metric for quantifying uncertainty in reasoning large language models (LLMs) by integrating token-level predictive entropy during answer generation, without requiring external models or repeated sampling. The methodology demonstrates strong correlations with answer entropy across various models and datasets, and empirical results show that EAS outperforms traditional methods like Pass Rate filtering in training data selection, leading to improved student model accuracy on math benchmarks while being efficient and interpretable.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing entropy-based ideas to create EAS, which tracks uncertainty evolution in a new way for reasoning LLMs, but it does not introduce a entirely new problem or technique beyond refining uncertainty quantification.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like LLM training and data selection due to its practical efficiency and demonstrated improvements, though its influence may remain confined to specific applications in AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a high-quality, practical contribution to uncertainty modeling in LLMs with clear empirical benefits, making it valuable for researchers focused on model training and evaluation.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/62372cdcce62f54c2fdd61a109df76d4c066897c",
      "total_authors": 5,
      "authors_found": 4,
      "highest_h_index": 2,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yongfu Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349258629"
        },
        {
          "name": "Lin Sun",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2346059467"
        },
        {
          "name": "Guangxiang Zhao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2346123786"
        },
        {
          "name": "Weihong Lin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349427454"
        },
        {
          "name": "Xiangzheng Zhang",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2508.20392",
      "title": "Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent\n  Integrate-and-Fire Neuron Model for Objects Detection",
      "authors": [
        "Chengjun Zhang",
        "Yuhao Zhang",
        "Jie Yang",
        "Mohamad Sawan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spiking Neural Networks (SNNs), inspired by the brain, are characterized by\nminimal power consumption and swift inference capabilities on neuromorphic\nhardware, and have been widely applied to various visual perception tasks.\nCurrent ANN-SNN conversion methods have achieved excellent results in\nclassification tasks with ultra-low time-steps, but their performance in visual\ndetection tasks remains suboptimal. In this paper, we propose a delay-spike\napproach to mitigate the issue of residual membrane potential caused by\nheterogeneous spiking patterns. Furthermore, we propose a novel\ntemporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This\nenables Integrate-and-fire (IF) neurons to dynamically adjust their\naccumulation and firing behaviors based on the temporal order of time-steps.\nOur method enables spikes to exhibit distinct temporal properties, rather than\nrelying solely on frequency-based representations. Moreover, the tdIF neuron\nmaintains energy consumption on par with traditional IF neuron. We demonstrate\nthat our method achieves more precise feature representation with lower\ntime-steps, enabling high performance and ultra-low latency in visual detection\ntasks. In this study, we conduct extensive evaluation of the tdIF method across\ntwo critical vision tasks: object detection and lane line detection. The\nresults demonstrate that the proposed method surpasses current ANN-SNN\nconversion approaches, achieving state-of-the-art performance with ultra-low\nlatency (within 5 time-steps).",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20392v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20392v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.394,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20395",
      "title": "Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction",
      "authors": [
        "Xu Guo"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advancements in large language models (LLMs) often rely on generating\nintermediate reasoning steps to enhance accuracy. However, little work has\nexamined how reasoning utility contributes to the final answer's correctness.\nDue to the stochastic nature of autoregressive generation, generating more\ncontext does not guarantee increased confidence in the answer. If we could\npredict, during generation, whether a reasoning step will be useful, we could\nstop early or prune ineffective steps, avoiding distractions in the final\ndecision.\n  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to\ngenerate reasoning chains, and then employing a separate model (Qwen3-8B) to\nquantify the utility of these chains for final accuracy. Specifically, we\nmeasure the model's uncertainty on the answer span Y at each reasoning step\nusing conditional entropy (expected negative log-likelihood over the\nvocabulary) with context expanding step by step. Our results show a clear\npattern: conditional entropy that decreases over steps is strongly associated\nwith correct answers, whereas flat or increasing entropy often results in wrong\nanswers. We also corroborate that incorrect reasoning paths tend to be longer\nthan correct ones, suggesting that longer reasoning does not necessarily yield\nbetter outcomes. These findings serve as a foundation to inspire future work on\ndesigning efficient reasoning pipelines that detect and avoid unproductive\nreasoning early.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20395v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20395v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.581,
      "distributed_training_score": 0.299,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on measuring reasoning utility in LLMs using conditional entropy, without any mention of human feedback, reward models, or reinforcement learning techniques. It is an empirical study on generated reasoning chains, not involving alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines Chain-of-Thought reasoning in LLMs and measures uncertainty via conditional entropy, but it does not adapt diffusion models or their iterative refinement processes for logical tasks. There is no component involving holistic correction of reasoning paths using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20398",
      "title": "TF-TransUNet1D: Time-Frequency Guided Transformer U-Net for Robust ECG\n  Denoising in Digital Twin",
      "authors": [
        "Shijie Wang",
        "Lei Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Electrocardiogram (ECG) signals serve as a foundational data source for\ncardiac digital twins, yet their diagnostic utility is frequently compromised\nby noise and artifacts. To address this issue, we propose TF-TransUNet1D, a\nnovel one-dimensional deep neural network that integrates a U-Net-based\nencoder-decoder architecture with a Transformer encoder, guided by a hybrid\ntime-frequency domain loss. The model is designed to simultaneously capture\nlocal morphological features and long-range temporal dependencies, which are\ncritical for preserving the diagnostic integrity of ECG signals. To enhance\ndenoising robustness, we introduce a dual-domain loss function that jointly\noptimizes waveform reconstruction in the time domain and spectral fidelity in\nthe frequency domain. In particular, the frequency-domain component effectively\nsuppresses high-frequency noise while maintaining the spectral structure of the\nsignal, enabling recovery of subtle but clinically significant waveform\ncomponents. We evaluate TF-TransUNet1D using synthetically corrupted signals\nfrom the MIT-BIH Arrhythmia Database and the Noise Stress Test Database\n(NSTDB). Comparative experiments against state-of-the-art baselines demonstrate\nconsistent superiority of our model in terms of SNR improvement and error\nmetrics, achieving a mean absolute error of 0.1285 and Pearson correlation\ncoefficient of 0.9540. By delivering high-precision denoising, this work\nbridges a critical gap in pre-processing pipelines for cardiac digital twins,\nenabling more reliable real-time monitoring and personalized modeling.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20398v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20398v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.296,
      "distributed_training_score": 0.345,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20400",
      "title": "MPFormer: Adaptive Framework for Industrial Multi-Task Personalized\n  Sequential Retriever",
      "authors": [
        "Yijia Sun",
        "Shanshan Huang",
        "Linxiao Che",
        "Haitao Lu",
        "Qiang Luo",
        "Kun Gai",
        "Guorui Zhou"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Modern industrial recommendation systems encounter a core challenge of\nmulti-stage optimization misalignment: a significant semantic gap exists\nbetween the multi-objective optimization paradigm widely used in the ranking\nphase and the single-objective modeling in the retrieve phase. Although the\nmainstream industry solution achieves multi-objective coverage through parallel\nmulti-path single-objective retrieval, this approach leads to linear growth of\ntraining and serving resources with the number of objectives and has inherent\nlimitations in handling loosely coupled objectives. This paper proposes the\nMPFormer, a dynamic multi-task Transformer framework, which systematically\naddresses the aforementioned issues through three innovative mechanisms. First,\nan objective-conditioned transformer that jointly encodes user behavior\nsequences and multi-task semantics through learnable attention modulation;\nsecond, personalized target weights are introduced to achieve dynamic\nadjustment of retrieval results; finally, user personalization information is\nincorporated into token representations and the Transformer structure to\nfurther enhance the model's representation ability. This framework has been\nsuccessfully integrated into Kuaishou short video recommendation system, stably\nserving over 400 million daily active users. It significantly improves user\ndaily engagement and system operational efficiency. Practical deployment\nverification shows that, compared with traditional solutions, it effectively\noptimizes the iterative paradigm of multi-objective retrieval while maintaining\nservice response speed, providing a scalable multi-objective solution for\nindustrial recommendation systems.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20400v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20400v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.417,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a multi-task Transformer framework for recommendation systems, emphasizing retrieval and optimization techniques, but it does not involve reinforcement learning, human feedback, or training models with human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses efficiency improvements in an industrial-scale recommendation system, such as reducing training overhead and GPU memory consumption, which may imply distributed computing elements, but it does not primarily address distributed training algorithms, parallel computing strategies, or multi-node architectures.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20404",
      "title": "AWorld: Orchestrating the Training Recipe for Agentic AI",
      "authors": [
        "Chengyue Yu",
        "Siyuan Lu",
        "Chenyi Zhuang",
        "Dong Wang",
        "Qintong Wu",
        "Zongyue Li",
        "Runsheng Gan",
        "Chunfeng Wang",
        "Siqi Hou",
        "Gaochi Huang",
        "Wenlong Yan",
        "Lifeng Hong",
        "Aohui Xue",
        "Yanfeng Wang",
        "Jinjie Gu",
        "David Tsai",
        "Tao Lin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The learning from practice paradigm is crucial for developing capable Agentic\nAI systems, yet it is severely hampered by inefficient experience generation, a\nbottleneck especially pronounced in complex benchmarks like GAIA. To address\nthis, we introduce AWorld, an open-source system engineered for large-scale\nagent-environment interaction. By distributing tasks across a cluster, AWorld\naccelerates experience collection by 14.6x compared to standard single-node,\nsequential execution. This critical speedup makes extensive reinforcement\nlearning practical and scalable. Leveraging this capability, we trained a\nQwen3-32B-based agent that achieves pass@1 accuracy of 32.23% on the GAIA test\nset, which surpasses GPT-4o (27.91%) and rivals DeepSeek-V3 (31.89%). Our\nopen-source system and the resulting agent provide a practical blueprint for a\ncomplete agentic AI training pipeline, from efficient interaction to\ndemonstrable model improvement.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20404v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20404v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.427,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.457,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on reinforcement learning for agent-environment interactions to generate experiences and train agents, but it does not involve human feedback, such as using a reward model trained on human-ranked data. While it integrates with frameworks like OpenRLHF, the core method relies on environmental interactions, not human preferences.",
      "weak_supervision_justification": "The paper involves generating experiences through agent-environment interactions, which could indirectly relate to programmatic data creation, but it does not emphasize training models with noisy or imprecise labels from high-level sources. The primary focus is on reinforcement learning and distributed interaction, not weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is AWorld, a system that distributes tasks across clusters to accelerate experience collection by 14.6x, directly addressing distributed training and parallel computing for reinforcement learning. It optimizes multi-node execution for large-scale agent training, aligning closely with the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "AWorld is an open-source framework designed to enhance the training of Agentic AI systems by addressing inefficiencies in experience generation, particularly in complex benchmarks like GAIA, through distributed task execution that accelerates data collection by 14.6x compared to sequential methods. The paper details AWorld's architecture, which supports agent construction, communication, and integration with reinforcement learning frameworks, and demonstrates its effectiveness by training a Qwen3-32B-based agent that achieves a 32.23% pass@1 accuracy on GAIA, outperforming models like GPT-4o and rivaling DeepSeek-V3, thereby providing a scalable blueprint for agentic AI development.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing AWorld, a distributed system that cleverly combines existing ideas for scalable agent-environment interactions, addressing a known bottleneck in reinforcement learning without introducing an entirely new problem or technique. While it advances the state-of-the-art in efficiency, it builds on prior concepts rather than creating a fundamentally novel architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of Agentic AI and reinforcement learning due to its open-source framework and demonstrated performance gains, potentially influencing tools for scalable training. However, its applicability may remain niche, primarily benefiting researchers focused on complex benchmarks like GAIA rather than broader commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper represents a strong, valuable contribution with a practical open-source framework and empirical results that advance agent training methodologies, making it essential for researchers in AI and reinforcement learning. While not groundbreaking enough to be a must-read, it offers insights that could inform future work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d4e4dfcad0d47aa5962dcca07c93dba9b1bde982",
      "total_authors": 17,
      "authors_found": 17,
      "highest_h_index": 6,
      "average_h_index": 1.0588235294117647,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Chengyue Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2278612576"
        },
        {
          "name": "Siyuan Lu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2326998432"
        },
        {
          "name": "Chenyi Zhuang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2240539453"
        },
        {
          "name": "Dong Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378195481"
        },
        {
          "name": "Qintong Wu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2155452030"
        },
        {
          "name": "Zongyue Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378896726"
        },
        {
          "name": "Runsheng Gan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377791308"
        },
        {
          "name": "Chunfeng Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377908706"
        },
        {
          "name": "Siqi Hou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377959227"
        },
        {
          "name": "Gaochi Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377945769"
        },
        {
          "name": "Wenlong Yan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377926917"
        },
        {
          "name": "Lifeng Hong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377984480"
        },
        {
          "name": "Aohui Xue",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377790199"
        },
        {
          "name": "Yanfeng Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377930717"
        },
        {
          "name": "Jinjie Gu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2266811973"
        },
        {
          "name": "David Tsai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377791131"
        },
        {
          "name": "Tao Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377937898"
        }
      ]
    },
    {
      "id": "2508.20411",
      "title": "Governable AI: Provable Safety Under Extreme Threat Models",
      "authors": [
        "Donglin Wang",
        "Weiyun Liang",
        "Chunyuan Chen",
        "Jing Xu",
        "Yulong Fu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "As AI rapidly advances, the security risks posed by AI are becoming\nincreasingly severe, especially in critical scenarios, including those posing\nexistential risks. If AI becomes uncontrollable, manipulated, or actively\nevades safety mechanisms, it could trigger systemic disasters. Existing AI\nsafety approaches-such as model enhancement, value alignment, and human\nintervention-suffer from fundamental, in-principle limitations when facing AI\nwith extreme motivations and unlimited intelligence, and cannot guarantee\nsecurity. To address this challenge, we propose a Governable AI (GAI) framework\nthat shifts from traditional internal constraints to externally enforced\nstructural compliance based on cryptographic mechanisms that are\ncomputationally infeasible to break, even for future AI, under the defined\nthreat model and well-established cryptographic assumptions.The GAI framework\nis composed of a simple yet reliable, fully deterministic, powerful, flexible,\nand general-purpose rule enforcement module (REM); governance rules; and a\ngovernable secure super-platform (GSSP) that offers end-to-end protection\nagainst compromise or subversion by AI. The decoupling of the governance rules\nand the technical platform further enables a feasible and generalizable\ntechnical pathway for the safety governance of AI. REM enforces the bottom line\ndefined by governance rules, while GSSP ensures non-bypassability,\ntamper-resistance, and unforgeability to eliminate all identified attack\nvectors. This paper also presents a rigorous formal proof of the security\nproperties of this mechanism and demonstrates its effectiveness through a\nprototype implementation evaluated in representative high-stakes scenarios.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20411v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20411v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.357,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of a Governable AI (GAI) framework focused on cryptographic mechanisms for AI safety, addressing threats like manipulation and active attacks through external enforcement. It critiques existing methods such as model alignment but does not involve, discuss, or build upon Reinforcement Learning from Human Feedback (RLHF), which specifically entails training AI models using human feedback to align with preferences. Since the paper neither uses human feedback nor reinforcement learning techniques, it has no direct or indirect relevance to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20413",
      "title": "Assessing local deformation and computing scalar curvature with\n  nonlinear conformal regularization of decoders",
      "authors": [
        "Benjamin Couéraud",
        "Vikram Sunkara",
        "Christof Schütte"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "One aim of dimensionality reduction is to discover the main factors that\nexplain the data, and as such is paramount to many applications. When working\nwith high dimensional data, autoencoders offer a simple yet effective approach\nto learn low-dimensional representations. The two components of a general\nautoencoder consist first of an encoder that maps the observed data onto a\nlatent space; and second a decoder that maps the latent space back to the\noriginal observation space, which allows to learn a low-dimensional manifold\nrepresentation of the original data. In this article, we introduce a new type\nof geometric regularization for decoding maps approximated by deep neural\nnetworks, namely nonlinear conformal regularization. This regularization\nprocedure permits local variations of the decoder map and comes with a new\nscalar field called conformal factor which acts as a quantitative indicator of\nthe amount of local deformation sustained by the latent space when mapped into\nthe original data space. We also show that this regularization technique allows\nthe computation of the scalar curvature of the learned manifold. Implementation\nand experiments on the Swiss roll and CelebA datasets are performed to\nillustrate how to obtain these quantities from the architecture.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20413v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20413v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.311,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20414",
      "title": "Federated Learning for Large Models in Medical Imaging: A Comprehensive\n  Review",
      "authors": [
        "Mengyu Sun",
        "Ziyuan Yang",
        "Yongqiang Huang",
        "Hui Yu",
        "Yingyu Chen",
        "Shuren Qi",
        "Andrew Beng Jin Teoh",
        "Yi Zhang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Artificial intelligence (AI) has demonstrated considerable potential in the\nrealm of medical imaging. However, the development of high-performance AI\nmodels typically necessitates training on large-scale, centralized datasets.\nThis approach is confronted with significant challenges due to strict patient\nprivacy regulations and legal restrictions on data sharing and utilization.\nThese limitations hinder the development of large-scale models in medical\ndomains and impede continuous updates and training with new data. Federated\nLearning (FL), a privacy-preserving distributed training framework, offers a\nnew solution by enabling collaborative model development across fragmented\nmedical datasets. In this survey, we review FL's contributions at two stages of\nthe full-stack medical analysis pipeline. First, in upstream tasks such as CT\nor MRI reconstruction, FL enables joint training of robust reconstruction\nnetworks on diverse, multi-institutional datasets, alleviating data scarcity\nwhile preserving confidentiality. Second, in downstream clinical tasks like\ntumor diagnosis and segmentation, FL supports continuous model updating by\nallowing local fine-tuning on new data without centralizing sensitive images.\nWe comprehensively analyze FL implementations across the medical imaging\npipeline, from physics-informed reconstruction networks to diagnostic AI\nsystems, highlighting innovations that improve communication efficiency, align\nheterogeneous data, and ensure secure parameter aggregation. Meanwhile, this\npaper provides an outlook on future research directions, aiming to serve as a\nvaluable reference for the field's development.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20414v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20414v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.441,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on Federated Learning (FL), which is a form of distributed training where model training occurs across multiple decentralized nodes (e.g., medical institutions) without sharing raw data. It discusses how FL partitions computation by having clients perform local training and aggregate parameters on a central server, directly aligning with distributed training concepts like parallel computing and multi-node machine learning. The review's emphasis on FL implementations for medical imaging tasks, such as handling data heterogeneity and communication efficiency, underscores its relevance to accelerating model training across distributed environments.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This survey paper reviews the application of Federated Learning (FL) in medical imaging, highlighting its role in preserving patient privacy while enabling collaborative model training across decentralized datasets. It examines FL's contributions in upstream tasks like CT and MRI reconstruction and downstream tasks such as tumor diagnosis and segmentation, analyzes innovations in communication efficiency and data heterogeneity handling, compares with existing surveys to emphasize its comprehensive coverage of the full imaging pipeline, and proposes future research directions to advance secure and efficient FL frameworks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing FL concepts by integrating them across the full medical imaging pipeline, which bridges gaps in prior surveys, though it does not introduce entirely new problems or techniques.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of FL in medical imaging due to its comprehensive analysis and comparison with other surveys, potentially influencing research on privacy-preserving AI in healthcare.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, valuable synthesis of FL applications in medical imaging that addresses key gaps in existing literature, making it essential for researchers focused on privacy and AI in healthcare.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fa3b884c5d8fe4ef342aa9a350262fd373484733",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 9,
      "average_h_index": 4.875,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Mengyu Sun",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2293663904"
        },
        {
          "name": "Ziyuan Yang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2254317449"
        },
        {
          "name": "Yongqiang Huang",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2145525804"
        },
        {
          "name": "Hui Yu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2219968737"
        },
        {
          "name": "Yingyu Chen",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2281434962"
        },
        {
          "name": "Shuren Qi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378038509"
        },
        {
          "name": "Andrew Beng Jin Teoh",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2256992843"
        },
        {
          "name": "Yi Zhang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2202935041"
        }
      ]
    },
    {
      "id": "2508.20415",
      "title": "Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient\n  Object Detection",
      "authors": [
        "Yuqi Xiong",
        "Wuzhen Shi",
        "Yang Wen",
        "Ruhan Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In view of the problems that existing salient object detection (SOD) methods\nare prone to losing details, blurring edges, and insufficient fusion of\nsingle-modal information in complex scenes, this paper proposes a dynamic\nuncertainty propagation and multimodal collaborative reasoning network\n(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is\ndesigned to propagate uncertainty between layers through a sparse graph\nconstructed based on spatial semantic distance, and combined with channel\nadaptive interaction, it effectively improves the detection accuracy of small\nstructures and edge regions. Secondly, a multimodal collaborative fusion\nstrategy (MCF) is proposed, which uses learnable modality gating weights to\nweightedly fuse the attention maps of RGB, depth, and edge features. It can\ndynamically adjust the importance of each modality according to different\nscenes, effectively suppress redundant or interfering information, and\nstrengthen the semantic complementarity and consistency between\ncross-modalities, thereby improving the ability to identify salient regions\nunder occlusion, weak texture or background interference. Finally, the\ndetection performance at the pixel level and region level is optimized through\nmulti-scale BCE and IoU loss, cross-scale consistency constraints, and\nuncertainty-guided supervision mechanisms. Extensive experiments show that\nDUP-MCRNet outperforms various SOD methods on most common benchmark datasets,\nespecially in terms of edge clarity and robustness to complex backgrounds. Our\ncode is publicly available at https://github.com/YukiBear426/DUP-MCRNet.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20415v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20415v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.336,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves graph-based uncertainty propagation and multimodal fusion for salient object detection, focusing on techniques like dynamic uncertainty graph convolution and adaptive feature fusion. It does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20416",
      "title": "DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual\n  Dentistry Understanding",
      "authors": [
        "Hengchuan Zhu",
        "Yihuan Xu",
        "Yichen Li",
        "Zijie Meng",
        "Zuozhu Liu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)\nhave demonstrated strong performance on general medical benchmarks. However,\ntheir capabilities in specialized medical fields, such as dentistry which\nrequire deeper domain-specific knowledge, remain underexplored due to the lack\nof targeted evaluation resources. In this paper, we introduce DentalBench, the\nfirst comprehensive bilingual benchmark designed to evaluate and advance LLMs\nin the dental domain. DentalBench consists of two main components: DentalQA, an\nEnglish-Chinese question-answering (QA) benchmark with 36,597 questions\nspanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,\nhigh-quality corpus with 337.35 million tokens curated for dental domain\nadaptation, supporting both supervised fine-tuning (SFT) and\nretrieval-augmented generation (RAG). We evaluate 14 LLMs, covering\nproprietary, open-source, and medical-specific models, and reveal significant\nperformance gaps across task types and languages. Further experiments with\nQwen-2.5-3B demonstrate that domain adaptation substantially improves model\nperformance, particularly on knowledge-intensive and terminology-focused tasks,\nand highlight the importance of domain-specific benchmarks for developing\ntrustworthy and effective LLMs tailored to healthcare applications.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20416v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20416v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.383,
      "datasets_score": 0.443,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on creating a specialized benchmark and corpus for dentistry, involving supervised fine-tuning and retrieval-augmented generation, but does not discuss or employ weak supervision techniques, such as programmatically generating noisy labels for training. There is no mention of relying on high-level or imprecise sources for labels, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions involve creating and evaluating new datasets—DentalQA as a bilingual QA benchmark and DentalCorpus as a large-scale corpus—including their curation methodologies, benchmarking of LLMs, and performance analysis, which directly aligns with research on datasets for AI and machine learning applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces DentalBench, a comprehensive bilingual benchmark for evaluating large language models (LLMs) in dentistry, consisting of DentalQA—a dataset with 36,597 English-Chinese questions across four task types and 16 subfields—and DentalCorpus, a high-quality corpus with 337.35 million tokens for domain adaptation. The authors evaluate 14 LLMs, including proprietary, open-source, and medical-specific models, revealing significant performance gaps in knowledge-intensive and terminology-focused tasks, and demonstrate through experiments that fine-tuning with DentalCorpus substantially improves LLM capabilities, underscoring the need for specialized benchmarks in healthcare applications.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark and corpus specifically for dentistry, addressing an underexplored area in medical LLMs and significantly advancing evaluation resources for domain-specific AI. This represents a novel contribution by providing targeted tools that enable deeper understanding and improvement of LLMs in specialized fields.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research and development of LLMs in healthcare by offering essential benchmarks and corpora that can be built upon for more accurate and trustworthy AI applications in dentistry. Its bilingual focus and emphasis on domain adaptation could extend to other medical subfields, potentially impacting both academic and commercial AI advancements.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides high-quality, practical tools and insights for advancing LLMs in a specialized medical domain, making it valuable for researchers in AI and healthcare. While essential for those working on medical AI, it may not be critical for a broader audience outside this niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5d6a31c83e614565433833c77b46d750d03d475c",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 3,
      "average_h_index": 1.2,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Hengchuan Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377969932"
        },
        {
          "name": "Yihuan Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377917956"
        },
        {
          "name": "Yichen Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2301405716"
        },
        {
          "name": "Zijie Meng",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2278829306"
        },
        {
          "name": "Zuozhu Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377917810"
        }
      ]
    },
    {
      "id": "2508.20427",
      "title": "Rethinking Purity and Diversity in Multi-Behavior Sequential\n  Recommendation from the Frequency Perspective",
      "authors": [
        "Yongqiang Han",
        "Kai Cheng",
        "Kefan Wang",
        "Enhong Chen"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In recommendation systems, users often exhibit multiple behaviors, such as\nbrowsing, clicking, and purchasing. Multi-behavior sequential recommendation\n(MBSR) aims to consider these different behaviors in an integrated manner to\nimprove the recommendation performance of the target behavior. However, some\nbehavior data will also bring inevitable noise to the modeling of user\ninterests. Some research efforts focus on data denoising from the frequency\ndomain perspective to improve the accuracy of user preference prediction. These\nstudies indicate that low-frequency information tends to be valuable and\nreliable, while high-frequency information is often associated with noise. In\nthis paper, we argue that high-frequency information is by no means\ninsignificant. Further experimental results highlight that low frequency\ncorresponds to the purity of user interests, while high frequency corresponds\nto the diversity of user interests. Building upon this finding, we proposed our\nmodel PDB4Rec, which efficiently extracts information across various frequency\nbands and their relationships, and introduces Boostrapping Balancer mechanism\nto balance their contributions for improved recommendation performance.\nSufficient experiments on real-world datasets demonstrate the effectiveness and\nefficiency of our model.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20427v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20427v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.331,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is in multi-behavior sequential recommendation, focusing on frequency domain analysis to handle user behavior data and improve recommendation accuracy. It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20437",
      "title": "On Identifying Why and When Foundation Models Perform Well on\n  Time-Series Forecasting Using Automated Explanations and Rating",
      "authors": [
        "Michael Widener",
        "Kausik Lakkaraju",
        "John Aydin",
        "Biplav Srivastava"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Time-series forecasting models (TSFM) have evolved from classical statistical\nmethods to sophisticated foundation models, yet understanding why and when\nthese models succeed or fail remains challenging. Despite this known\nlimitation, time series forecasting models are increasingly used to generate\ninformation that informs real-world actions with equally real consequences.\nUnderstanding the complexity, performance variability, and opaque nature of\nthese models then becomes a valuable endeavor to combat serious concerns about\nhow users should interact with and rely on these models' outputs. This work\naddresses these concerns by combining traditional explainable AI (XAI) methods\nwith Rating Driven Explanations (RDE) to assess TSFM performance and\ninterpretability across diverse domains and use cases. We evaluate four\ndistinct model architectures: ARIMA, Gradient Boosting, Chronos (time-series\nspecific foundation model), Llama (general-purpose; both fine-tuned and base\nmodels) on four heterogeneous datasets spanning finance, energy,\ntransportation, and automotive sales domains. In doing so, we demonstrate that\nfeature-engineered models (e.g., Gradient Boosting) consistently outperform\nfoundation models (e.g., Chronos) in volatile or sparse domains (e.g., power,\ncar parts) while providing more interpretable explanations, whereas foundation\nmodels excel only in stable or trend-driven contexts (e.g., finance).",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20437v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20437v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.3,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on explainable AI techniques for time-series forecasting models, such as ARIMA, Gradient Boosting, Chronos, and Llama, to evaluate their performance and interpretability across various domains. It does not involve diffusion-based models, iterative refinement processes for logical tasks, or any form of multi-step reasoning akin to Chain-of-Thought corrections. As the paper lacks any components related to diffusion models or their adaptation for reasoning, it has no relevance to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20441",
      "title": "Uncovering the Spectral Bias in Diagonal State Space Models",
      "authors": [
        "Ruben Solozabal",
        "Velibor Bojkovic",
        "Hilal AlQuabeh",
        "Kentaro Inui",
        "Martin Takáč"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Current methods for initializing state space models (SSMs) parameters mainly\nrely on the \\textit{HiPPO framework}, which is based on an online approximation\nof orthogonal polynomials. Recently, diagonal alternatives have shown to reach\na similar level of performance while being significantly more efficient due to\nthe simplification in the kernel computation. However, the \\textit{HiPPO\nframework} does not explicitly study the role of its diagonal variants. In this\npaper, we take a further step to investigate the role of diagonal SSM\ninitialization schemes from the frequency perspective. Our work seeks to\nsystematically understand how to parameterize these models and uncover the\nlearning biases inherent in such diagonal state-space models. Based on our\nobservations, we propose a diagonal initialization on the discrete Fourier\ndomain \\textit{S4D-DFouT}. The insights in the role of pole placing in the\ninitialization enable us to further scale them and achieve state-of-the-art\nresults on the Long Range Arena benchmark, allowing us to train from scratch on\nvery large datasets as PathX-256.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20441v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20441v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.391,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20443",
      "title": "Towards Mitigating Excessive Forgetting in LLM Unlearning via\n  Entanglement-Aware Unlearning with Proxy Constraint",
      "authors": [
        "Zhihao Liu",
        "Jian Lou",
        "Yuke Hu",
        "Xiaochen Li",
        "Tailun Chen",
        "Yitian Chen",
        "Zhan Qin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) are trained on massive datasets that may include\nprivate or copyrighted content. Due to growing privacy and ownership concerns,\ndata owners may request the removal of their data from trained models. Machine\nunlearning provides a practical solution by removing the influence of specific\ndata without full retraining. However, most existing methods lack a sound\nforgetting boundary, causing some samples to be under-forgotten, leaving\nresidual leakage risks, while others remain over-forgotten at the expense of\ndegraded utility.\n  In this work, we propose EAGLE-PC (Entanglement-Awareness Guided Loss\nReweighting with Proxy Constraint), a novel unlearning framework that addresses\nthese limitations through two key components. First, entanglement-awareness\nguided loss reweighting determines the forgetting effort of each sample by\nmeasuring its similarity to retain samples in the embedding space, enabling\nmore targeted and effective unlearning. Second, a proxy constraint leveraging\nICL (In-Context Learning) generated test data softly regularizes the forgetting\nprocess, effectively mitigating over-forgetting. EAGLE-PC is compatible with\nexisting gradient-based objectives and serves as a plug-and-play enhancement.\nWe evaluate EAGLE-PC on the TOFU and MUSE benchmarks, showing consistent\nimprovements in the forgetting-utility trade-off across multiple LLMs. Combined\nwith the NPO+GD optimizer, it approaches full retraining performance, offering\na scalable and robust unlearning solution.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20443v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20443v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.435,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.389,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on machine unlearning for LLMs, specifically proposing methods to mitigate forgetting through entanglement-aware techniques and proxy constraints. It does not involve human feedback, reward models, or reinforcement learning for model alignment, which are core to RLHF.",
      "weak_supervision_justification": "The paper uses In-Context Learning (ICL) to generate proxy data for constraining the unlearning process, which involves programmatically created outputs that could be noisy, resembling aspects of weak supervision. However, the main contribution is unlearning, not training models with weak labels as in weak supervision paradigms.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20447",
      "title": "MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV\n  Features for Multi-view Pedestrian Detection",
      "authors": [
        "Taiga Yamane",
        "Satoshi Suzuki",
        "Ryo Masumura",
        "Shota Orihashi",
        "Tomohiro Tanaka",
        "Mana Ihori",
        "Naoki Makishima",
        "Naotaka Kawata"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form\nof a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end\ntrainable deep learning methods have progressed greatly. However, they often\nstruggle to detect pedestrians with consistently small or large scales in views\nor with vastly different scales between views. This is because they do not\nexploit multi-scale image features to generate the BEV feature and detect\npedestrians. To overcome this problem, we propose a novel MVPD method, called\nMulti-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV\nfeatures by projecting multi-scale image features extracted from individual\nviews into the BEV space, scale-by-scale. Each of these BEV features inherits\nthe properties of its corresponding scale image features from multiple views.\nTherefore, these BEV features help the precise detection of pedestrians with\nconsistently small or large scales in views. Then, MSMVD combines information\nat different scales of multiple views by processing the multi-scale BEV\nfeatures using a feature pyramid network. This improves the detection of\npedestrians with vastly different scales between views. Extensive experiments\ndemonstrate that exploiting multi-scale image features via multi-scale BEV\nfeatures greatly improves the detection performance, and MSMVD outperforms the\nprevious highest MODA by $4.5$ points on the GMVD dataset.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20447v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20447v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.257,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.341,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20449",
      "title": "A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time\n  Deepfake Detection",
      "authors": [
        "Libo Lv",
        "Tianyi Wang",
        "Mengxiao Huang",
        "Ruixia Liu",
        "Yinglong Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With the rapid advancement of real-time deepfake generation techniques,\nforged content is becoming increasingly realistic and widespread across\napplications like video conferencing and social media. Although\nstate-of-the-art detectors achieve high accuracy on standard benchmarks, their\nheavy computational cost hinders real-time deployment in practical\napplications. To address this, we propose the Spatial-Frequency Aware\nMulti-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture\nfor real-time deepfake detection. We design a spatial-frequency hybrid aware\nmodule that jointly leverages spatial textures and frequency artifacts through\na gated mechanism, enhancing sensitivity to subtle manipulations. A\ntoken-selective cross attention mechanism enables efficient multi-level feature\ninteraction, while a residual-enhanced blur pooling structure helps retain key\nsemantic cues during downsampling. Experiments on several benchmark datasets\nshow that SFMFNet achieves a favorable balance between accuracy and efficiency,\nwith strong generalization and practical value for real-time applications.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20449v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20449v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.393,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20452",
      "title": "Evaluating Differentially Private Generation of Domain-Specific Text",
      "authors": [
        "Yidan Sun",
        "Viktor Schlegel",
        "Srinivasan Nandakumar",
        "Iqra Zahid",
        "Yuping Wu",
        "Warren Del-Pinto",
        "Goran Nenadic",
        "Siew-Kei Lam",
        "Jie Zhang",
        "Anil A Bharath"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Generative AI offers transformative potential for high-stakes domains such as\nhealthcare and finance, yet privacy and regulatory barriers hinder the use of\nreal-world data. To address this, differentially private synthetic data\ngeneration has emerged as a promising alternative. In this work, we introduce a\nunified benchmark to systematically evaluate the utility and fidelity of text\ndatasets generated under formal Differential Privacy (DP) guarantees. Our\nbenchmark addresses key challenges in domain-specific benchmarking, including\nchoice of representative data and realistic privacy budgets, accounting for\npre-training and a variety of evaluation metrics. We assess state-of-the-art\nprivacy-preserving generation methods across five domain-specific datasets,\nrevealing significant utility and fidelity degradation compared to real data,\nespecially under strict privacy constraints. These findings underscore the\nlimitations of current approaches, outline the need for advanced\nprivacy-preserving data sharing methods and set a precedent regarding their\nevaluation in realistic scenarios.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20452v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20452v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.37,
      "datasets_score": 0.472,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on benchmarking differentially private synthetic text generation and does not involve training models using programmatically generated labels from noisy sources, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "The paper does not discuss or utilize diffusion models for multi-step logical reasoning or iterative refinement processes; it centers on evaluating differentially private text generation methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is introducing a unified benchmark for evaluating domain-specific text datasets generated under Differential Privacy, which directly involves creating, analyzing, and benchmarking datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a unified benchmark to evaluate the utility and fidelity of differentially private synthetic text generation for domain-specific datasets, addressing challenges such as prior exposure from pre-training and the representativeness of specialized data. By assessing state-of-the-art privacy-preserving methods on five gated-access datasets with various metrics and realistic privacy budgets, the authors demonstrate significant degradation in performance compared to real data under strict privacy constraints, highlighting the limitations of current approaches and the need for more advanced techniques.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a unified benchmark for evaluating differentially private text generation in domain-specific contexts, effectively combining existing ideas to address gaps in prior exposure and representativeness. While it doesn't introduce a entirely new problem, it offers a clever and systematic evaluation protocol that advances current practices.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI security and NLP, as it provides a practical benchmark for privacy-preserving data generation in sensitive domains. However, its influence may be limited to specialized research areas rather than broader commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution by highlighting critical limitations in differentially private text generation and proposing a robust benchmarking approach, making it essential for researchers in privacy and AI ethics. While not groundbreaking, it provides important insights that could guide future developments in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b63549e07aab627dd310d390a289f8baa953e011",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 5,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yidan Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2330235618"
        },
        {
          "name": "Viktor Schlegel",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2363499756"
        },
        {
          "name": "Srinivasan Nandakumar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377798561"
        },
        {
          "name": "Iqra Zahid",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/90788907"
        },
        {
          "name": "Yuping Wu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2304897005"
        },
        {
          "name": "Warren Del-Pinto",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2264380048"
        },
        {
          "name": "Goran Nenadic",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373717797"
        },
        {
          "name": "Siew-Kei Lam",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2243382041"
        },
        {
          "name": "Jie Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377809417"
        },
        {
          "name": "Anil A. Bharath",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2338058977"
        }
      ]
    },
    {
      "id": "2508.20461",
      "title": "Dual-Model Weight Selection and Self-Knowledge Distillation for Medical\n  Image Classification",
      "authors": [
        "Ayaka Tsutsumi",
        "Guang Li",
        "Ren Togo",
        "Takahiro Ogawa",
        "Satoshi Kondo",
        "Miki Haseyama"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We propose a novel medical image classification method that integrates\ndual-model weight selection with self-knowledge distillation (SKD). In\nreal-world medical settings, deploying large-scale models is often limited by\ncomputational resource constraints, which pose significant challenges for their\npractical implementation. Thus, developing lightweight models that achieve\ncomparable performance to large-scale models while maintaining computational\nefficiency is crucial. To address this, we employ a dual-model weight selection\nstrategy that initializes two lightweight models with weights derived from a\nlarge pretrained model, enabling effective knowledge transfer. Next, SKD is\napplied to these selected models, allowing the use of a broad range of initial\nweight configurations without imposing additional excessive computational cost,\nfollowed by fine-tuning for the target classification tasks. By combining\ndual-model weight selection with self-knowledge distillation, our method\novercomes the limitations of conventional approaches, which often fail to\nretain critical information in compact models. Extensive experiments on\npublicly available datasets-chest X-ray images, lung computed tomography scans,\nand brain magnetic resonance imaging scans-demonstrate the superior performance\nand robustness of our approach compared to existing methods.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20461v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20461v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.416,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on dual-model weight selection and self-knowledge distillation for medical image classification, relying on standard supervised learning with labeled datasets. It does not involve programmatically generating labels from noisy or imprecise sources, which is central to weak supervision.",
      "diffusion_reasoning_justification": "The paper's method involves weight selection and knowledge distillation for classification tasks, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not adapt diffusion techniques for reasoning paths.",
      "distributed_training_justification": "The paper addresses computational efficiency through lightweight models and knowledge transfer, but it does not discuss parallel computing, data partitioning across nodes, or strategies for multi-node machine learning. The focus is on single-model optimization, not distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20466",
      "title": "Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of\n  LiDAR Point Clouds",
      "authors": [
        "Pengpeng Yu",
        "Haoran Li",
        "Dingquan Li",
        "Runqing Jiang",
        "Jing Wang",
        "Liang Lin",
        "Yulan Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "LiDAR point clouds are fundamental to various applications, yet\nhigh-precision scans incur substantial storage and transmission overhead.\nExisting methods typically convert unordered points into hierarchical octree or\nvoxel structures for dense-to-sparse predictive coding. However, the extreme\nsparsity of geometric details hinders efficient context modeling, thereby\nlimiting their compression performance and speed. To address this challenge, we\npropose to generate compact features for efficient predictive coding. Our\nframework comprises two lightweight modules. First, the Geometry\nRe-Densification Module re-densifies encoded sparse geometry, extracts features\nat denser scale, and then re-sparsifies the features for predictive coding.\nThis module avoids costly computation on highly sparse details while\nmaintaining a lightweight prediction head. Second, the Cross-scale Feature\nPropagation Module leverages occupancy cues from multiple resolution levels to\nguide hierarchical feature propagation. This design facilitates information\nsharing across scales, thereby reducing redundant feature extraction and\nproviding enriched features for the Geometry Re-Densification Module. By\nintegrating these two modules, our method yields a compact feature\nrepresentation that provides efficient context modeling and accelerates the\ncoding process. Experiments on the KITTI dataset demonstrate state-of-the-art\ncompression ratios and real-time performance, achieving 26 FPS for both\nencoding and decoding at 12-bit quantization. Code is available at\nhttps://github.com/pengpeng-yu/FastPCC.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20466v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20466v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.392,
      "datasets_score": 0.271,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20469",
      "title": "Prediction of Distant Metastasis for Head and Neck Cancer Patients Using\n  Multi-Modal Tumor and Peritumoral Feature Fusion Network",
      "authors": [
        "Zizhao Tang",
        "Changhao Liu",
        "Nuo Tong",
        "Shuiping Gou",
        "Mei Shi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Metastasis remains the major challenge in the clinical management of head and\nneck squamous cell carcinoma (HNSCC). Reliable pre-treatment prediction of\nmetastatic risk is crucial for optimizing treatment strategies and prognosis.\nThis study develops a deep learning-based multimodal framework to predict\nmetastasis risk in HNSCC patients by integrating computed tomography (CT)\nimages, radiomics, and clinical data. 1497 HNSCC patients were included. Tumor\nand organ masks were derived from pretreatment CT images. A 3D Swin Transformer\nextracted deep features from tumor regions. Meanwhile, 1562 radiomics features\nwere obtained using PyRadiomics, followed by correlation filtering and random\nforest selection, leaving 36 features. Clinical variables including age, sex,\nsmoking, and alcohol status were encoded and fused with imaging-derived\nfeatures. Multimodal features were fed into a fully connected network to\npredict metastasis risk. Performance was evaluated using five-fold\ncross-validation with area under the curve (AUC), accuracy (ACC), sensitivity\n(SEN), and specificity (SPE). The proposed fusion model outperformed\nsingle-modality models. The 3D deep learning module alone achieved an AUC of\n0.715, and when combined with radiomics and clinical features, predictive\nperformance improved (AUC = 0.803, ACC = 0.752, SEN = 0.730, SPE = 0.758).\nStratified analysis showed generalizability across tumor subtypes. Ablation\nstudies indicated complementary information from different modalities.\nEvaluation showed the 3D Swin Transformer provided more robust representation\nlearning than conventional networks. This multimodal fusion model demonstrated\nhigh accuracy and robustness in predicting metastasis risk in HNSCC, offering a\ncomprehensive representation of tumor biology. The interpretable model has\npotential as a clinical decision-support tool for personalized treatment\nplanning.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20469v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20469v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.284,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.328,
      "datasets_score": 0.26,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20470",
      "title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
      "authors": [
        "Xiaochuan Li",
        "Guoguang Du",
        "Runze Zhang",
        "Liang Jin",
        "Qi Jia",
        "Lihua Lu",
        "Zhenhua Guo",
        "Yaqian Zhao",
        "Haiyang Liu",
        "Tianqi Wang",
        "Changsheng Li",
        "Xiaoli Gong",
        "Rengang Li",
        "Baoyu Fan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Scaling laws have validated the success and promise of large-data-trained\nmodels in creative generation across text, image, and video domains. However,\nthis paradigm faces data scarcity in the 3D domain, as there is far less of it\navailable on the internet compared to the aforementioned modalities.\nFortunately, there exist adequate videos that inherently contain commonsense\npriors, offering an alternative supervisory signal to mitigate the\ngeneralization bottleneck caused by limited native 3D data. On the one hand,\nvideos capturing multiple views of an object or scene provide a spatial\nconsistency prior for 3D generation. On the other hand, the rich semantic\ninformation contained within the videos enables the generated content to be\nmore faithful to the text prompts and semantically plausible. This paper\nexplores how to apply the video modality in 3D asset generation, spanning\ndatasets to models. We introduce Droplet3D-4M, the first large-scale video\ndataset with multi-view level annotations, and train Droplet3D, a generative\nmodel supporting both image and dense text input. Extensive experiments\nvalidate the effectiveness of our approach, demonstrating its ability to\nproduce spatially consistent and semantically plausible content. Moreover, in\ncontrast to the prevailing 3D solutions, our approach exhibits the potential\nfor extension to scene-level applications. This indicates that the commonsense\npriors from the videos significantly facilitate 3D creation. We have\nopen-sourced all resources including the dataset, code, technical framework,\nand model weights: https://dropletx.github.io/.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20470v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20470v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.364,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes a diffusion model (Droplet3D, fine-tuned from a video diffusion model) for 3D generation, which involves iterative refinement processes. However, this is applied to generative tasks like creating 3D assets from images and text, not to complex logical reasoning or Chain-of-Thought processes as defined in the topic. Thus, while diffusion mechanisms are present, they are not used for multi-step logical tasks, making the paper only tangentially relevant.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20471",
      "title": "Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving\n  Video Generation",
      "authors": [
        "Jiusi Li",
        "Jackson Jiang",
        "Jinyu Miao",
        "Miao Long",
        "Tuopu Wen",
        "Peijin Jia",
        "Shengxiang Liu",
        "Chunlei Yu",
        "Maolin Liu",
        "Yuzhan Cai",
        "Kun Jiang",
        "Mengmeng Yang",
        "Diange Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Corner cases are crucial for training and validating autonomous driving\nsystems, yet collecting them from the real world is often costly and hazardous.\nEditing objects within captured sensor data offers an effective alternative for\ngenerating diverse scenarios, commonly achieved through 3D Gaussian Splatting\nor image generative models. However, these approaches often suffer from limited\nvisual fidelity or imprecise pose control. To address these issues, we propose\nG^2Editor, a framework designed for photorealistic and precise object editing\nin driving videos. Our method leverages a 3D Gaussian representation of the\nedited object as a dense prior, injected into the denoising process to ensure\naccurate pose control and spatial consistency. A scene-level 3D bounding box\nlayout is employed to reconstruct occluded areas of non-target objects.\nFurthermore, to guide the appearance details of the edited object, we\nincorporate hierarchical fine-grained features as additional conditions during\ngeneration. Experiments on the Waymo Open Dataset demonstrate that G^2Editor\neffectively supports object repositioning, insertion, and deletion within a\nunified framework, outperforming existing methods in both pose controllability\nand visual quality, while also benefiting downstream data-driven tasks.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20471v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20471v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.328,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models for iterative denoising in image editing and generation, which involves refinement processes similar to those in diffusion-based methods. However, it applies this to visual synthesis and object editing in driving videos, not to multi-step logical reasoning or treating a 'Chain-of-Thought' as a holistic entity for complex tasks. Thus, while diffusion is a component, it lacks the core focus on reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20472",
      "title": "Photonic restricted Boltzmann machine for content generation tasks",
      "authors": [
        "Li Luo",
        "Yisheng Fang",
        "Wanyi Zhang",
        "Zhichao Ruan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The restricted Boltzmann machine (RBM) is a neural network based on the Ising\nmodel, well known for its ability to learn probability distributions and\nstochastically generate new content. However, the high computational cost of\nGibbs sampling in content generation tasks imposes significant bottlenecks on\nelectronic implementations. Here, we propose a photonic restricted Boltzmann\nmachine (PRBM) that leverages photonic computing to accelerate Gibbs sampling,\nenabling efficient content generation. By introducing an efficient encoding\nmethod, the PRBM eliminates the need for computationally intensive matrix\ndecomposition and reduces the computational complexity of Gibbs sampling from\n$O(N)$ to $O(1)$. Moreover, its non-Von Neumann photonic computing architecture\ncircumvents the memory storage of interaction matrices, providing substantial\nadvantages for large-scale RBMs. We experimentally validate the\nphotonic-accelerated Gibbs sampling by simulating a two-dimensional Ising\nmodel, where the observed phase transition temperature closely matches the\ntheoretical predictions. Beyond physics-inspired tasks, the PRBM demonstrates\nrobust capabilities in generating and restoring diverse content, including\nimages and temporal sequences, even in the presence of noise and aberrations.\nThe scalability and reduced training cost of the PRBM framework underscore its\npotential as a promising pathway for advancing photonic computing in generative\nartificial intelligence.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20472v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20472v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.375,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of a photonic restricted Boltzmann machine (PRBM) to accelerate Gibbs sampling for content generation tasks, such as image and sequence generation. It focuses on photonic computing, Ising models, and RBMs, with no mention of diffusion models, iterative refinement for logical tasks, or treating a Chain-of-Thought as a single entity for multi-step reasoning. Since the paper lacks any components related to diffusion-based approaches for logical reasoning, it is not relevant to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20475",
      "title": "Enhancing Corpus Callosum Segmentation in Fetal MRI via\n  Pathology-Informed Domain Randomization",
      "authors": [
        "Marina Grifell i Plana",
        "Vladyslav Zalevskyi",
        "Léa Schmidt",
        "Yvan Gomez",
        "Thomas Sanchez",
        "Vincent Dunet",
        "Mériam Koob",
        "Vanessa Siffredi",
        "Meritxell Bach Cuadra"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Accurate fetal brain segmentation is crucial for extracting biomarkers and\nassessing neurodevelopment, especially in conditions such as corpus callosum\ndysgenesis (CCD), which can induce drastic anatomical changes. However, the\nrarity of CCD severely limits annotated data, hindering the generalization of\ndeep learning models. To address this, we propose a pathology-informed domain\nrandomization strategy that embeds prior knowledge of CCD manifestations into a\nsynthetic data generation pipeline. By simulating diverse brain alterations\nfrom healthy data alone, our approach enables robust segmentation without\nrequiring pathological annotations.\n  We validate our method on a cohort comprising 248 healthy fetuses, 26 with\nCCD, and 47 with other brain pathologies, achieving substantial improvements on\nCCD cases while maintaining performance on both healthy fetuses and those with\nother pathologies. From the predicted segmentations, we derive clinically\nrelevant biomarkers, such as corpus callosum length (LCC) and volume, and show\ntheir utility in distinguishing CCD subtypes. Our pathology-informed\naugmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in\nhealthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these\nquantitative gains, our approach yields segmentations with improved topological\nconsistency relative to available ground truth, enabling more reliable\nshape-based analyses. Overall, this work demonstrates that incorporating\ndomain-specific anatomical priors into synthetic data pipelines can effectively\nmitigate data scarcity and enhance analysis of rare but clinically significant\nmalformations.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20475v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20475v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.366,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on pathology-informed domain randomization for fetal MRI segmentation, which involves simulating brain alterations using synthetic data techniques. While it briefly mentions diffusion models as part of existing methods for pathology simulation in the introduction, it does not employ or adapt diffusion models for multi-step logical reasoning tasks. The main contribution is about improving segmentation robustness through domain randomization, not iterative refinement processes for reasoning, so it lacks any clear component of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20476",
      "title": "Towards Inclusive Communication: A Unified LLM-Based Framework for Sign\n  Language, Lip Movements, and Audio Understanding",
      "authors": [
        "Jeong Hun Yeo",
        "Hyeongseop Rha",
        "Sungjune Park",
        "Junil Won",
        "Yong Man Ro"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)",
        "eess.AS (Audio and Speech Processing)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Audio is the primary modality for human communication and has driven the\nsuccess of Automatic Speech Recognition (ASR) technologies. However, such\nsystems remain inherently inaccessible to individuals who are deaf or hard of\nhearing. Visual alternatives such as sign language and lip reading offer\neffective substitutes, and recent advances in Sign Language Translation (SLT)\nand Visual Speech Recognition (VSR) have improved audio-less communication.\nYet, these modalities have largely been studied in isolation, and their\nintegration within a unified framework remains underexplored. In this paper, we\nintroduce the first unified framework capable of handling diverse combinations\nof sign language, lip movements, and audio for spoken-language text generation.\nWe focus on three main objectives: (i) designing a unified, modality-agnostic\narchitecture capable of effectively processing heterogeneous inputs; (ii)\nexploring the underexamined synergy among modalities, particularly the role of\nlip movements as non-manual cues in sign language comprehension; and (iii)\nachieving performance on par with or superior to state-of-the-art models\nspecialized for individual tasks. Building on this framework, we achieve\nperformance on par with or better than task-specific state-of-the-art models\nacross SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that\nexplicitly modeling lip movements as a separate modality significantly improves\nSLT performance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20476v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20476v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.348,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a unified framework for integrating sign language, lip movements, and audio using LLMs for tasks like SLT and ASR, focusing on multimodal processing and training strategies. It does not involve diffusion models, iterative refinement processes, or any adaptation for solving complex logical tasks through a Chain-of-Thought mechanism. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20478",
      "title": "Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding",
      "authors": [
        "Yuan Xie",
        "Tianshui Chen",
        "Zheng Ge",
        "Lionel Ni"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Long-form video understanding, characterized by long-range temporal\ndependencies and multiple events, remains a challenge. Existing methods often\nrely on static reasoning or external visual-language models (VLMs), which face\nissues like complexity and sub-optimal performance due to the lack of\nend-to-end training. In this paper, we propose Video-MTR, a reinforced\nmulti-turn reasoning framework designed to enable iterative key video segment\nselection and question comprehension. Unlike traditional video reasoning\npipeline, which generate predictions in a single turn, Video-MTR performs\nreasoning in multiple turns, selecting video segments progressively based on\nthe evolving understanding of previously processed segments and the current\nquestion. This iterative process allows for a more refined and contextually\naware analysis of the video. To ensure intermediate reasoning process, we\nintroduce a novel gated bi-level reward system, combining trajectory-level\nrewards based on answer correctness and turn-level rewards emphasizing\nframe-query relevance. This system optimizes both video segment selection and\nquestion comprehension, eliminating the need for external VLMs and allowing\nend-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,\nand EgoSchema demonstrate that Video-MTR outperforms existing methods in both\naccuracy and efficiency, advancing the state-of-the-art in long video\nunderstanding.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20478v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20478v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.481,
      "distributed_training_score": 0.344,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a reinforced multi-turn reasoning framework using reinforcement learning for long video understanding, focusing on iterative segment selection and a bi-level reward system. It does not involve diffusion models, the iterative refinement process of diffusion, or any adaptation of diffusion for logical tasks. Since the core contribution lacks any component related to diffusion-based reasoning, the paper is not relevant to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20488",
      "title": "Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object\n  Detection under Test-Time Shifts",
      "authors": [
        "Zixuan Hu",
        "Dongxiao Li",
        "Xinzhu Ma",
        "Shixiang Tang",
        "Xiaotong Li",
        "Wenhan Yang",
        "Ling-Yu Duan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical\napplications like autonomous driving, yet its reliability deteriorates\nsignificantly under real-world domain shifts caused by environmental or sensor\nvariations. To address these shifts, Test-Time Adaptation (TTA) methods have\nemerged, enabling models to adapt to target distributions during inference.\nWhile prior TTA approaches recognize the positive correlation between low\nuncertainty and high generalization ability, they fail to address the dual\nuncertainty inherent to M3OD: semantic uncertainty (ambiguous class\npredictions) and geometric uncertainty (unstable spatial localization). To\nbridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA\nframework designed to jointly minimize both uncertainties for robust M3OD.\nThrough a convex optimization lens, we introduce an innovative convex structure\nof the focal loss and further derive a novel unsupervised version, enabling\nlabel-agnostic uncertainty weighting and balanced learning for high-uncertainty\nobjects. In parallel, we design a semantic-aware normal field constraint that\npreserves geometric coherence in regions with clear semantic cues, reducing\nuncertainty from the unstable 3D representation. This dual-branch mechanism\nforms a complementary loop: enhanced spatial perception improves semantic\nclassification, and robust semantic predictions further refine spatial\nunderstanding. Extensive experiments demonstrate the superiority of DUO over\nexisting methods across various datasets and domain shift types.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20488v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20488v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.389,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Test-Time Adaptation for monocular 3D object detection, emphasizing uncertainty minimization through convex optimization and loss functions. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20491",
      "title": "CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball\n  Information",
      "authors": [
        "Seunghyeon Jung",
        "Seoyoung Hong",
        "Jiwoo Jeong",
        "Seungwon Jeong",
        "Jaerim Choi",
        "Hoki Kim",
        "Woojin Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in deep learning have led to more studies to enhance golfers'\nshot precision. However, these existing studies have not quantitatively\nestablished the relationship between swing posture and ball trajectory,\nlimiting their ability to provide golfers with the necessary insights for swing\nimprovement. In this paper, we propose a new dataset called CaddieSet, which\nincludes joint information and various ball information from a single shot.\nCaddieSet extracts joint information from a single swing video by segmenting it\ninto eight swing phases using a computer vision-based approach. Furthermore,\nbased on expert golf domain knowledge, we define 15 key metrics that influence\na golf swing, enabling the interpretation of swing outcomes through\nswing-related features. Through experiments, we demonstrated the feasibility of\nCaddieSet for predicting ball trajectories using various benchmarks. In\nparticular, we focus on interpretable models among several benchmarks and\nverify that swing feedback using our joint features is quantitatively\nconsistent with established domain knowledge. This work is expected to offer\nnew insight into golf swing analysis for both academia and the sports industry.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20491v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20491v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.295,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.291,
      "datasets_score": 0.404,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new dataset, CaddieSet, which includes golf swing joint features and ball information. It describes the dataset creation process, including video segmentation, feature extraction using computer vision, and the definition of 15 key metrics based on domain knowledge. The paper also benchmarks the dataset through experiments for predicting ball trajectories and compares model performances, directly aligning with research on creating, analyzing, and benchmarking datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces CaddieSet, a new dataset designed to analyze golf swings by integrating human joint features extracted from videos with ball trajectory data, aiming to establish quantitative relationships between swing posture and shot outcomes for improved golfer feedback. Using computer vision to segment swings into eight phases and define 15 key metrics based on domain expertise, the authors demonstrate through experiments that interpretable machine learning models trained on this dataset can accurately predict ball parameters like speed, spin axis, and direction angle, outperforming image-based approaches and providing actionable insights for swing correction.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a specialized dataset that combines existing computer vision techniques with golf domain knowledge to link swing joint features to ball trajectories, addressing a gap in prior research. While it builds on established methods, it offers a clever integration for a specific application rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI for sports, particularly golf swing analysis, due to its provision of a new dataset and interpretable models for practical feedback. However, its influence may be limited to niche applications rather than broadly affecting general computer vision or AI fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution for researchers in computer vision and sports AI by introducing a useful dataset and demonstrating effective swing analysis methods. It is essential for those working in golf-related tech but not broadly groundbreaking for the entire field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8d188d565f77248251737932bc0efdc3c01fd893",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 1,
      "average_h_index": 0.14285714285714285,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Seunghyeon Jung",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355895498"
        },
        {
          "name": "Seoyoung Hong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372233701"
        },
        {
          "name": "Jiwoo Jeong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374098122"
        },
        {
          "name": "Seungwon Jeong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372019349"
        },
        {
          "name": "Jaerim Choi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373323071"
        },
        {
          "name": "Hoki Kim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372153662"
        },
        {
          "name": "Woojin Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372121723"
        }
      ]
    },
    {
      "id": "2508.20492",
      "title": "IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based\n  Anomaly Detection",
      "authors": [
        "Xuanming Cao",
        "Chengyu Tao",
        "Yifeng Cheng",
        "Juan Du"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Surface anomaly detection is pivotal for ensuring product quality in\nindustrial manufacturing. While 2D image-based methods have achieved remarkable\nsuccess, 3D point cloud-based detection remains underexplored despite its\nricher geometric cues. We argue that the key bottleneck is the absence of\npowerful pretrained foundation backbones in 3D comparable to those in 2D. To\nbridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an\nensemble framework that synergizes 2D pretrained expert with 3D expert models.\nHowever, naively fusing predictions from disparate sources is non-trivial:\nexisting strategies can be affected by a poorly performing modality and thus\ndegrade overall accuracy. To address this challenge, We introduce an novel\nImportance-Aware Fusion (IAF) module that dynamically assesses the contribution\nof each source and reweights their anomaly scores. Furthermore, we devise\ncritical loss functions that explicitly guide the optimization of IAF, enabling\nit to combine the collective knowledge of the source experts but also preserve\ntheir unique strengths, thereby enhancing the overall performance of anomaly\ndetection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet\nachieves a new state-of-the-art with a markedly lower false positive rate,\nunderscoring its practical value for industrial deployment.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20492v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20492v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.336,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20505",
      "title": "Describe, Don't Dictate: Semantic Image Editing with Natural Language\n  Intent",
      "authors": [
        "En Ci",
        "Shanyan Guan",
        "Yanhao Ge",
        "Yilin Zhang",
        "Wei Li",
        "Zhenyu Zhang",
        "Jian Yang",
        "Ying Tai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite the progress in text-to-image generation, semantic image editing\nremains a challenge. Inversion-based algorithms unavoidably introduce\nreconstruction errors, while instruction-based models mainly suffer from\nlimited dataset quality and scale. To address these problems, we propose a\ndescriptive-prompt-based editing framework, named DescriptiveEdit. The core\nidea is to re-frame `instruction-based image editing' as `reference-image-based\ntext-to-image generation', which preserves the generative power of well-trained\nText-to-Image models without architectural modifications or inversion.\nSpecifically, taking the reference image and a prompt as input, we introduce a\nCross-Attentive UNet, which newly adds attention bridges to inject reference\nimage features into the prompt-to-edit-image generation process. Owing to its\ntext-to-image nature, DescriptiveEdit overcomes limitations in instruction\ndataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other\nextensions, and is more scalable. Experiments on the Emu Edit benchmark show it\nimproves editing accuracy and consistency.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20505v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20505v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.313,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on semantic image editing using diffusion models for generating and refining images based on descriptive prompts, but it does not adapt the diffusion process for multi-step logical reasoning or solving complex logical tasks. Instead, it employs iterative refinement solely for visual generation and editing, without any component for holistic Chain-of-Thought correction or logical inference.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20511",
      "title": "Languages Still Left Behind: Toward a Better Multilingual Machine\n  Translation Benchmark",
      "authors": [
        "Chihiro Taguchi",
        "Seng Mai",
        "Keita Kurabe",
        "Yusuke Sakai",
        "Georgina Agyei",
        "Soudabeh Eslami",
        "David Chiang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multilingual machine translation (MT) benchmarks play a central role in\nevaluating the capabilities of modern MT systems. Among them, the FLORES+\nbenchmark is widely used, offering English-to-many translation data for over\n200 languages, curated with strict quality control protocols. However, we study\ndata in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)\nand uncover critical shortcomings in the benchmark's suitability for truly\nmultilingual evaluation. Human assessments reveal that many translations fall\nbelow the claimed 90% quality standard, and the annotators report that source\nsentences are often too domain-specific and culturally biased toward the\nEnglish-speaking world. We further demonstrate that simple heuristics, such as\ncopying named entities, can yield non-trivial BLEU scores, suggesting\nvulnerabilities in the evaluation protocol. Notably, we show that MT models\ntrained on high-quality, naturalistic data perform poorly on FLORES+ while\nachieving significant gains on our domain-relevant evaluation set. Based on\nthese findings, we advocate for multilingual MT benchmarks that use\ndomain-general and culturally neutral source texts rely less on named entities,\nin order to better reflect real-world translation challenges.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20511v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20511v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.365,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves analyzing and evaluating the FLORES+ dataset, a key benchmark for multilingual machine translation, by assessing its quality, cultural biases, and evaluation metrics. This directly aligns with research on dataset analysis, benchmarking, and evaluation in AI applications, as it critiques existing datasets and proposes improvements for future ones.",
      "llm_score_status": "completed",
      "summary": "This paper critically evaluates the FLORES+ benchmark for multilingual machine translation by examining data in four languages—Asante Twi, Japanese, Jinghpaw, and South Azerbaijani—revealing that many translations fall short of the claimed 90% quality standard and are hindered by domain-specific, culturally biased English source sentences. Through human assessments, feedback from evaluators, and experiments demonstrating that simple heuristics like copying named entities can inflate BLEU scores, the authors highlight the benchmark's vulnerabilities and propose improvements, such as using domain-general, culturally neutral texts that minimize reliance on named entities to better reflect real-world translation challenges.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by identifying critical flaws in the FLORES+ benchmark and suggesting practical enhancements for multilingual evaluation, though it builds on existing ideas rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "This work is likely to influence future developments in multilingual machine translation benchmarks by highlighting the need for more culturally neutral and robust evaluations, potentially leading to citations and advancements within the subfield of computational linguistics and AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides high-quality insights into the limitations of existing MT benchmarks and offers actionable recommendations, making it a valuable read for researchers in multilingual translation to inform better evaluation practices.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7d9bb0b62f9ef2810440a3510c6690b3124d3f36",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 4,
      "average_h_index": 1.2857142857142858,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Chihiro Taguchi",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/6138197"
        },
        {
          "name": "Seng Mai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377797514"
        },
        {
          "name": "Keita Kurabe",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/83291824"
        },
        {
          "name": "Yusuke Sakai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377782624"
        },
        {
          "name": "Georgina Agyei",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377797417"
        },
        {
          "name": "Soudabeh Eslami",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302560545"
        },
        {
          "name": "David Chiang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2298041319"
        }
      ]
    },
    {
      "id": "2508.20516",
      "title": "DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and\n  Sample",
      "authors": [
        "Wenting Yin",
        "Han Sun",
        "Xinru Meng",
        "Ningzhong Liu",
        "Huiyu Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Continual test-time adaptation aims to continuously adapt a pre-trained model\nto a stream of target domain data without accessing source data. Without access\nto source domain data, the model focuses solely on the feature characteristics\nof the target data. Relying exclusively on these features can lead to confusion\nand introduce learning biases. Currently, many existing methods generate\npseudo-labels via model predictions. However, the quality of pseudo-labels\ncannot be guaranteed and the problem of error accumulation must be solved. To\naddress these challenges, we propose DCFS, a novel CTTA framework that\nintroduces dual-path feature consistency and confidence-aware sample learning.\nThis framework disentangles the whole feature representation of the target data\ninto semantic-related feature and domain-related feature using dual classifiers\nto learn distinct feature representations. By maintaining consistency between\nthe sub-features and the whole feature, the model can comprehensively capture\ndata features from multiple perspectives. Additionally, to ensure that the\nwhole feature information of the target domain samples is not overlooked, we\nset a adaptive threshold and calculate a confidence score for each sample to\ncarry out loss weighted self-supervised learning, effectively reducing the\nnoise of pseudo-labels and alleviating the problem of error accumulation. The\nefficacy of our proposed method is validated through extensive experimentation\nacross various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,\ndemonstrating consistent performance in continual test-time adaptation\nscenarios.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20516v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20516v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.418,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.423,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves generating and refining pseudo-labels from model predictions, which aligns with weak supervision as it uses noisy, programmatically derived labels rather than perfect ground truth. However, the primary focus is on continual test-time adaptation and feature consistency, not on developing weak supervision techniques themselves, making it moderately relevant rather than central.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node setups for accelerating model training. Its contributions are centered on feature disentanglement and adaptation techniques for a single model, with no mention of partitioning data or computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces DCFS, a framework for continual test-time adaptation (CTTA) that addresses challenges like feature entanglement and unreliable pseudo-labels by disentangling target data features into semantic-related and domain-related components using dual classifiers for consistency learning. It incorporates confidence-aware sample learning with adaptive thresholds to reduce errors from pseudo-labels, and experimental results on datasets such as CIFAR10-C, CIFAR100-C, and ImageNet-C demonstrate improved performance in adapting to evolving domains without source data access.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining feature disentanglement and confidence-aware learning to handle limitations in existing CTTA methods, though it builds on established techniques like self-training and pseudo-labeling.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in continual test-time adaptation within computer vision, as its empirical validation on standard benchmarks could lead to further developments in handling domain shifts.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to domain adaptation techniques with solid experimental evidence, making it important for researchers focused on continual learning and computer vision applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6fd211d8cce87e953beda0cc43ad9e9836f2045d",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 14,
      "average_h_index": 3.6,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Wenting Yin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378138361"
        },
        {
          "name": "Han Sun",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/40186176"
        },
        {
          "name": "Xinru Meng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2338329026"
        },
        {
          "name": "Ningzhong Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2287807023"
        },
        {
          "name": "Huiyu Zhou",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2243660539"
        }
      ]
    },
    {
      "id": "2508.20517",
      "title": "BridgeShield: Enhancing Security for Cross-chain Bridge Applications via\n  Heterogeneous Graph Mining",
      "authors": [
        "Dan Lin",
        "Shunfeng Lu",
        "Ziyan Liu",
        "Jiajing Wu",
        "Junyuan Fang",
        "Kaixin Lin",
        "Bowen Song",
        "Zibin Zheng"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Cross-chain bridges play a vital role in enabling blockchain\ninteroperability. However, due to the inherent design flaws and the enormous\nvalue they hold, they have become prime targets for hacker attacks. Existing\ndetection methods show progress yet remain limited, as they mainly address\nsingle-chain behaviors and fail to capture cross-chain semantics. To address\nthis gap, we leverage heterogeneous graph attention networks, which are\nwell-suited for modeling multi-typed entities and relations, to capture the\ncomplex execution semantics of cross-chain behaviors. We propose BridgeShield,\na detection framework that jointly models the source chain, off-chain\ncoordination, and destination chain within a unified heterogeneous graph\nrepresentation. BridgeShield incorporates intra-meta-path attention to learn\nfine-grained dependencies within cross-chain paths and inter-meta-path\nattention to highlight discriminative cross-chain patterns, thereby enabling\nprecise identification of attack behaviors. Extensive experiments on 51\nreal-world cross-chain attack events demonstrate that BridgeShield achieves an\naverage F1-score of 92.58%, representing a 24.39% improvement over\nstate-of-the-art baselines. These results validate the effectiveness of\nBridgeShield as a practical solution for securing cross-chain bridges and\nenhancing the resilience of multi-chain ecosystems.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20517v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20517v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.307,
      "distributed_training_score": 0.356,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20525",
      "title": "Enhancing Health Fact-Checking with LLM-Generated Synthetic Data",
      "authors": [
        "Jingze Zhang",
        "Jiahe Qian",
        "Yiliang Zhou",
        "Yifan Peng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Fact-checking for health-related content is challenging due to the limited\navailability of annotated training data. In this study, we propose a synthetic\ndata generation pipeline that leverages large language models (LLMs) to augment\ntraining data for health-related fact checking. In this pipeline, we summarize\nsource documents, decompose the summaries into atomic facts, and use an LLM to\nconstruct sentence-fact entailment tables. From the entailment relations in the\ntable, we further generate synthetic text-claim pairs with binary veracity\nlabels. These synthetic data are then combined with the original data to\nfine-tune a BERT-based fact-checking model. Evaluation on two public datasets,\nPubHealth and SciFact, shows that our pipeline improved F1 scores by up to\n0.019 and 0.049, respectively, compared to models trained only on the original\ndata. These results highlight the effectiveness of LLM-driven synthetic data\naugmentation in enhancing the performance of health-related fact-checkers.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20525v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20525v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.433,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.349,
      "datasets_score": 0.399,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using LLMs to programmatically generate synthetic data and labels for training a fact-checking model, which aligns directly with weak supervision. It relies on automated, potentially noisy sources (e.g., LLM-derived entailment tables) to create large quantities of training labels, addressing the scarcity of hand-labeled data, as seen in the improvements on PubHealth and SciFact datasets.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of limited annotated data for health-related fact-checking by proposing a synthetic data generation pipeline that utilizes large language models (LLMs) to summarize source documents, decompose them into atomic facts, create entailment tables, and generate synthetic text-claim pairs with veracity labels. The methodology involves fine-tuning a BERT-based model with a combination of original and synthetic data, resulting in improved F1 scores of up to 0.019 on the PubHealth dataset and 0.049 on the SciFact dataset, demonstrating the effectiveness of LLM-driven data augmentation in enhancing fact-checking performance for health claims.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing LLM techniques for data augmentation applied to health fact-checking, offering a notable improvement in handling domain-specific data scarcity. While not introducing a entirely new problem, it innovatively integrates summarization and entailment tables to generate synthetic pairs, advancing the state-of-the-art in this niche.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in AI-driven fact-checking and health informatics subfields, as it demonstrates practical improvements in model performance through synthetic data. However, its influence may be limited to specialized applications rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to AI for fact-checking, particularly in health domains, making it important for researchers in related areas to be aware of the methodology and results. While not essential for all, its practical insights and empirical improvements warrant attention from those working on data augmentation techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/619369089618ac6ffab78b0c004302c03030f449",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 2.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jingze Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375151509"
        },
        {
          "name": "Jiahe Qian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378172898"
        },
        {
          "name": "Yiliang Zhou",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2261543341"
        },
        {
          "name": "Yifan Peng",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2293320120"
        }
      ]
    },
    {
      "id": "2508.20526",
      "title": "Adam SLAM - the last mile of camera calibration with 3DGS",
      "authors": [
        "Matthieu Gendrin",
        "Stéphane Pateux",
        "Xiaoran Jiang",
        "Théo Ladune",
        "Luce Morin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The quality of the camera calibration is of major importance for evaluating\nprogresses in novel view synthesis, as a 1-pixel error on the calibration has a\nsignificant impact on the reconstruction quality. While there is no ground\ntruth for real scenes, the quality of the calibration is assessed by the\nquality of the novel view synthesis. This paper proposes to use a 3DGS model to\nfine tune calibration by backpropagation of novel view color loss with respect\nto the cameras parameters. The new calibration alone brings an average\nimprovement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine\ntuning may be long and its suitability depends on the criticity of training\ntime, but for calibration of reference scenes, such as Mip-NeRF 360, the stake\nof novel view quality is the most important.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20526v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20526v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.312,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20528",
      "title": "Learning What is Worth Learning: Active and Sequential Domain Adaptation\n  for Multi-modal Gross Tumor Volume Segmentation",
      "authors": [
        "Jingyun Yang",
        "Guoqing Zhang",
        "Jingge Wang",
        "Yang Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate gross tumor volume segmentation on multi-modal medical data is\ncritical for radiotherapy planning in nasopharyngeal carcinoma and\nglioblastoma. Recent advances in deep neural networks have brought promising\nresults in medical image segmentation, leading to an increasing demand for\nlabeled data. Since labeling medical images is time-consuming and\nlabor-intensive, active learning has emerged as a solution to reduce annotation\ncosts by selecting the most informative samples to label and adapting\nhigh-performance models with as few labeled samples as possible. Previous\nactive domain adaptation (ADA) methods seek to minimize sample redundancy by\nselecting samples that are farthest from the source domain. However, such\none-off selection can easily cause negative transfer, and access to source\nmedical data is often limited. Moreover, the query strategy for multi-modal\nmedical data remains unexplored. In this work, we propose an active and\nsequential domain adaptation framework for dynamic multi-modal sample selection\nin ADA. We derive a query strategy to prioritize labeling and training on the\nmost valuable samples based on their informativeness and representativeness.\nEmpirical validation on diverse gross tumor volume segmentation tasks\ndemonstrates that our method achieves favorable segmentation performance,\nsignificantly outperforming state-of-the-art ADA methods. Code is available at\nthe git repository: \\href{https://github.com/Hiyoochan/mmActS}{mmActS}.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20528v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20528v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.363,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20530",
      "title": "Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for\n  Unsupervised 3D Object Detection",
      "authors": [
        "Mingqian Ji",
        "Jian Yang",
        "Shanshan Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing LiDAR-based 3D object detectors typically rely on manually annotated\nlabels for training to achieve good performance. However, obtaining\nhigh-quality 3D labels is time-consuming and labor-intensive. To address this\nissue, recent works explore unsupervised 3D object detection by introducing RGB\nimages as an auxiliary modal to assist pseudo-box generation. However, these\nmethods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB\nimages. Yet, such a label-level fusion strategy brings limited improvements to\nthe quality of pseudo-boxes, as it overlooks the complementary nature in terms\nof LiDAR and RGB image data. To overcome the above limitations, we propose a\nnovel data-level fusion framework that integrates RGB images and LiDAR data at\nan early stage. Specifically, we utilize vision foundation models for instance\nsegmentation and depth estimation on images and introduce a bi-directional\nfusion method, where real points acquire category labels from the 2D space,\nwhile 2D pixels are projected onto 3D to enhance real point density. To\nmitigate noise from depth and segmentation estimations, we propose a local and\nglobal filtering method, which applies local radius filtering to suppress depth\nestimation errors and global statistical filtering to remove\nsegmentation-induced outliers. Furthermore, we propose a data-level fusion\nbased dynamic self-evolution strategy, which iteratively refines pseudo-boxes\nunder a dense representation, significantly improving localization accuracy.\nExtensive experiments on the nuScenes dataset demonstrate that the detector\ntrained by our method significantly outperforms that trained by previous\nstate-of-the-art methods with 28.4$\\%$ mAP on the nuScenes validation\nbenchmark.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20530v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20530v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.432,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.371,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves generating pseudo-boxes for 3D object detection using LiDAR and RGB data via vision foundation models, which aligns directly with weak supervision. It programmatically creates training labels from noisy sources like instance segmentation and depth estimation, avoiding manual annotations, thus exemplifying weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces DFU3D, a novel framework for unsupervised 3D object detection that addresses the limitations of existing methods by employing data-level fusion of LiDAR point clouds and RGB images to generate higher-quality pseudo-boxes. The methodology involves using vision foundation models for instance segmentation and depth estimation, a bi-directional fusion technique to enrich point density and assign labels, noise-filtering methods to mitigate estimation errors, and a dynamic self-evolution strategy for iterative refinement, resulting in significant performance improvements with 28.4% mAP on the nuScenes dataset compared to prior state-of-the-art approaches.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing data-level fusion and bi-directional techniques as a clever combination of existing LiDAR and RGB modalities, advancing unsupervised 3D detection beyond label-level methods without introducing an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in multi-modal fusion for 3D object detection, particularly in autonomous driving, by providing a more effective unsupervised approach that could be built upon in specific subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to unsupervised 3D object detection with innovative fusion techniques and strong empirical results, making it essential for researchers focused on computer vision and autonomous systems to stay informed.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/259f19e52da80a0070f7ecc52ab9bb6cc043fc90",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 5,
      "average_h_index": 3.3333333333333335,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Mingqian Ji",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2279220591"
        },
        {
          "name": "Jian Yang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2279867887"
        },
        {
          "name": "Shanshan Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2284518907"
        }
      ]
    },
    {
      "id": "2508.20532",
      "title": "Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale\n  Biomedical Semantic Indexing and Question Answering",
      "authors": [
        "Anastasios Nentidis",
        "Georgios Katsimpras",
        "Anastasia Krithara",
        "Salvador Lima-López",
        "Eulàlia Farré-Maduell",
        "Martin Krallinger",
        "Natalia Loukachevitch",
        "Vera Davydova",
        "Elena Tutubalina",
        "Georgios Paliouras"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "This is an overview of the twelfth edition of the BioASQ challenge in the\ncontext of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ\nis a series of international challenges promoting advances in large-scale\nbiomedical semantic indexing and question answering. This year, BioASQ\nconsisted of new editions of the two established tasks b and Synergy, and two\nnew tasks: a) MultiCardioNER on the adaptation of clinical entity detection to\nthe cardiology domain in a multilingual setting, and b) BIONNE on nested NER in\nRussian and English. In this edition of BioASQ, 37 competing teams participated\nwith more than 700 distinct submissions in total for the four different shared\ntasks of the challenge. Similarly to previous editions, most of the\nparticipating systems achieved competitive performance, suggesting the\ncontinuous advancement of the state-of-the-art in the field.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20532v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20532v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.246,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.29,
      "distributed_training_score": 0.303,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper provides an overview of the BioASQ 2024 challenge, which includes the development and description of new benchmark datasets for tasks like biomedical semantic indexing, question answering, MultiCardioNER, and BIONNE. It discusses dataset creation for AI applications, such as entity detection and NER in multilingual settings, and their use in benchmarking and evaluation, directly aligning with research on creating, analyzing, and benchmarking datasets for machine learning and AI.",
      "llm_score_status": "completed",
      "summary": "The BioASQ 2024 challenge, as detailed in this paper, aims to advance large-scale biomedical semantic indexing and question answering by organizing shared tasks with benchmark datasets, including established ones like tasks b and Synergy, and two new tasks: MultiCardioNER for multilingual clinical entity detection in cardiology and BIONNE for nested named entity recognition in Russian and English. The paper overviews the tasks, datasets, participating systems from 37 teams with over 700 submissions, and their performance evaluations, demonstrating competitive results that reflect ongoing progress in the field.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper introduces two new tasks that adapt existing techniques to specific domains and languages, representing a clever combination of ideas for the BioASQ challenge, but it primarily serves as an overview rather than introducing a truly groundbreaking problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "This work is likely to be cited and built upon within the subfield of biomedical NLP and AI, as it provides updated benchmarks and fosters collaboration, though its influence may be limited to specialized research rather than widespread applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality overview of the BioASQ 2024 challenge, making it valuable for researchers in computational linguistics, AI, and information retrieval to stay informed about advancements and datasets in biomedical semantic indexing and question answering.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/75696d9221c7d972e222b90e071ad23115849b16",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 20,
      "average_h_index": 9.1,
      "notable_authors_count": 7,
      "author_h_indexes": [
        {
          "name": "A. Nentidis",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/23133070"
        },
        {
          "name": "Georgios Katsimpras",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2566909"
        },
        {
          "name": "Anastasia Krithara",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/1927081"
        },
        {
          "name": "Salvador Lima-López",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2082347840"
        },
        {
          "name": "Eulàlia Farré-Maduell",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2050518711"
        },
        {
          "name": "Martin Krallinger",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2261405497"
        },
        {
          "name": "Natalia V. Loukachevitch",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2301580883"
        },
        {
          "name": "Vera Davydova",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2141570394"
        },
        {
          "name": "Elena Tutubalina",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2141764359"
        },
        {
          "name": "Georgios Paliouras",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2247480883"
        }
      ]
    },
    {
      "id": "2508.20534",
      "title": "Digital Scale: Open-Source On-Device BMI Estimation from Smartphone\n  Camera Images Trained on a Large-Scale Real-World Dataset",
      "authors": [
        "Frederik Rajiv Manichand",
        "Robin Deuber",
        "Robert Jakob",
        "Steve Swerling",
        "Jamie Rosen",
        "Elgar Fleisch",
        "Patrick Langer"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Estimating Body Mass Index (BMI) from camera images with machine learning\nmodels enables rapid weight assessment when traditional methods are unavailable\nor impractical, such as in telehealth or emergency scenarios. Existing computer\nvision approaches have been limited to datasets of up to 14,500 images. In this\nstudy, we present a deep learning-based BMI estimation method trained on our\nWayBED dataset, a large proprietary collection of 84,963 smartphone images from\n25,353 individuals. We introduce an automatic filtering method that uses\nposture clustering and person detection to curate the dataset by removing\nlow-quality images, such as those with atypical postures or incomplete views.\nThis process retained 71,322 high-quality images suitable for training. We\nachieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test\nset (WayBED data) using full-body images, the lowest value in the published\nliterature to the best of our knowledge. Further, we achieve a MAPE of 13% on\nthe completely unseen~(during training) VisualBodyToBMI dataset, comparable\nwith state-of-the-art approaches trained on it, demonstrating robust\ngeneralization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a\nMAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the\nfull pipeline, including image filtering and BMI estimation, on Android devices\nusing the CLAID framework. We release our complete code for model training,\nfiltering, and the CLAID package for mobile deployment as open-source\ncontributions.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20534v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20534v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.291,
      "distributed_training_score": 0.348,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20537",
      "title": "Domain Adaptation Techniques for Natural and Medical Image\n  Classification",
      "authors": [
        "Ahmad Chaddad",
        "Yihang Wu",
        "Reem Kateb",
        "Christian Desrosiers"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Domain adaptation (DA) techniques have the potential in machine learning to\nalleviate distribution differences between training and test sets by leveraging\ninformation from source domains. In image classification, most advances in DA\nhave been made using natural images rather than medical data, which are harder\nto work with. Moreover, even for natural images, the use of mainstream datasets\ncan lead to performance bias. {With the aim of better understanding the\nbenefits of DA for both natural and medical images, this study performs 557\nsimulation studies using seven widely-used DA techniques for image\nclassification in five natural and eight medical datasets that cover various\nscenarios, such as out-of-distribution, dynamic data streams, and limited\ntraining samples.} Our experiments yield detailed results and insightful\nobservations highlighting the performance and medical applicability of these\ntechniques. Notably, our results have shown the outstanding performance of the\nDeep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved\nfeasible classification accuracy (91.2\\%) in the COVID-19 dataset using\nResnet50 and showed an important accuracy improvement in the dynamic data\nstream DA scenario (+6.7\\%) compared to the baseline. Our results also\ndemonstrate that DSAN exhibits remarkable level of explainability when\nevaluated on COVID-19 and skin cancer datasets. These results contribute to the\nunderstanding of DA techniques and offer valuable insight into the effective\nadaptation of models to medical data.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20537v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20537v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.371,
      "datasets_score": 0.429,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses existing public datasets (five natural and eight medical) to evaluate domain adaptation techniques, including analyzing their characteristics such as data scales, task complexities, and modalities. While it involves benchmarking and evaluating these datasets in various scenarios (e.g., out-of-distribution, limited samples), the primary focus is on assessing DA methods rather than creating, curating, or deeply analyzing datasets themselves. Thus, datasets are a supporting element rather than the core contribution.",
      "llm_score_status": "completed",
      "summary": "This paper evaluates seven domain adaptation techniques for image classification, focusing on both natural and medical datasets, by conducting 557 simulation studies to assess their performance in various scenarios such as out-of-distribution data, dynamic data streams, and limited training samples. The core objectives include understanding the benefits of these techniques for medical images, where data scarcity is an issue, and the methodology involves testing algorithms like Deep Subdomain Adaptation Network (DSAN), which demonstrated outstanding results, including 91.2% accuracy on a COVID-19 dataset and improved explainability on medical data, ultimately providing insights into effective model adaptation and future directions in the field.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by conducting extensive simulations on diverse natural and medical datasets, cleverly combining existing DA techniques to address underrepresented scenarios like medical imaging. However, it does not introduce a entirely new problem or technique, focusing instead on comparative analysis.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like medical image analysis due to its comprehensive evaluation of DA techniques on challenging datasets, potentially guiding future applications in healthcare. Nonetheless, its influence may be limited to specific domains rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, valuable contribution through its detailed analysis and insights into DA for medical images, making it essential for researchers in computer vision and healthcare AI. While insightful, it is not groundbreaking enough to be a must-read for all.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8735e2e3c9c9b769beae5663052f7c27cc7ce9dc",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 6,
      "average_h_index": 4.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Ahmad Chaddad",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2275606211"
        },
        {
          "name": "Yihang Wu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2260821946"
        },
        {
          "name": "Reem Kateb",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/31850798"
        },
        {
          "name": "Christian Desrosiers",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2268844765"
        }
      ]
    },
    {
      "id": "2508.20546",
      "title": "MM-HSD: Multi-Modal Hate Speech Detection in Videos",
      "authors": [
        "Berta Céspedes-Sarrias",
        "Carlos Collado-Capell",
        "Pablo Rodenas-Ruiz",
        "Olena Hrynenko",
        "Andrea Cavallaro"
      ],
      "categories": [
        "cs.MM (Multimedia)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While hate speech detection (HSD) has been extensively studied in text,\nexisting multi-modal approaches remain limited, particularly in videos. As\nmodalities are not always individually informative, simple fusion methods fail\nto fully capture inter-modal dependencies. Moreover, previous work often omits\nrelevant modalities such as on-screen text and audio, which may contain subtle\nhateful content and thus provide essential cues, both individually and in\ncombination with others. In this paper, we present MM-HSD, a multi-modal model\nfor HSD in videos that integrates video frames, audio, and text derived from\nspeech transcripts and from frames (i.e.~on-screen text) together with features\nextracted by Cross-Modal Attention (CMA). We are the first to use CMA as an\nearly feature extractor for HSD in videos, to systematically compare query/key\nconfigurations, and to evaluate the interactions between different modalities\nin the CMA block. Our approach leads to improved performance when on-screen\ntext is used as a query and the rest of the modalities serve as a key.\nExperiments on the HateMM dataset show that MM-HSD outperforms state-of-the-art\nmethods on M-F1 score (0.874), using concatenation of transcript, audio, video,\non-screen text, and CMA for feature extraction on raw embeddings of the\nmodalities. The code is available at https://github.com/idiap/mm-hsd",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20546v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20546v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.308,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20547",
      "title": "SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes",
      "authors": [
        "Yunpeng Mei",
        "Hongjie Cao",
        "Yinqiu Xia",
        "Wei Xiao",
        "Zhaohan Feng",
        "Gang Wang",
        "Jie Chen"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Real-time interactive grasp synthesis for dynamic objects remains challenging\nas existing methods fail to achieve low-latency inference while maintaining\npromptability. To bridge this gap, we propose SPGrasp (spatiotemporal\nprompt-driven dynamic grasp synthesis), a novel framework extending segment\nanything model v2 (SAMv2) for video stream grasp estimation. Our core\ninnovation integrates user prompts with spatiotemporal context, enabling\nreal-time interaction with end-to-end latency as low as 59 ms while ensuring\ntemporal consistency for dynamic objects. In benchmark evaluations, SPGrasp\nachieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on\nJacquard. On the challenging GraspNet-1Billion dataset under continuous\ntracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency,\nrepresenting a 58.5% reduction compared to the prior state-of-the-art\npromptable method RoG-SAM while maintaining competitive accuracy. Real-world\nexperiments involving 13 moving objects demonstrate a 94.8% success rate in\ninteractive grasping scenarios. These results confirm SPGrasp effectively\nresolves the latency-interactivity trade-off in dynamic grasp synthesis.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20547v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20547v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.301,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20549",
      "title": "MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via\n  Generative Reward Learning",
      "authors": [
        "Weihai Zhi",
        "Jiayan Guo",
        "Shangyang Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The application of Vision-Language Models (VLMs) in medicine is critically\nhampered by the scarcity of high-quality, expert-annotated data. Supervised\nFine-Tuning (SFT) on existing datasets often leads to poor generalization on\nunseen modalities and tasks, while Reinforcement Learning (RL), a promising\nalternative, is stymied by the lack of reliable reward signals in this\ndata-scarce domain. To break this impasse, we introduce Generative Reward\nLearning for Medical Reasoning (MedGR$^2$), a novel framework that creates a\nself-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a\nreward model, enabling the automated, continuous creation of high-quality,\nmulti-modal medical data that serves as both a superior training source for SFT\nand RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data\nalready surpasses baselines trained on large-scale, human-curated datasets.\nCrucially, when leveraging this data for RL via Group Relative Policy\nOptimization (GRPO), our model achieves state-of-the-art cross-modality and\ncross-task generalization, significantly outperforming specialized RL-based\nmethods. Furthermore, our compact model, empowered by MedGR$^2$, achieves\nperformance competitive with foundation models possessing over 10 times more\nparameters. MedGR$^2$ presents a new paradigm for data-efficient learning in\nhigh-stakes domains, transforming the problem from data scarcity to data\ngeneration and unlocking the full potential of RL for building truly\ngeneralizable medical AI.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20549v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20549v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.452,
      "weak_supervision_score": 0.438,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.385,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces MedGR², which uses a reward model for RL but does not rely on human-ranked data for training it. Instead, the reward model is derived from expert criteria and evolves autonomously, contrasting with RLHF's requirement for ongoing human feedback. Thus, it does not align with RLHF's core definition.",
      "weak_supervision_justification": "The paper's MedGR² framework programmatically generates and filters large quantities of high-quality medical data using a data generator and reward model, aligning directly with weak supervision by creating labels from automated, noisy, or imprecise sources rather than relying on hand-annotated data.",
      "diffusion_reasoning_justification": "The paper focuses on generative reward learning, SFT, and RL for data creation and model training, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "MedGR² addresses the scarcity of high-quality medical data for Vision-Language Models by introducing a generative reward learning framework that autonomously generates and refines multimodal medical data through a self-improving cycle involving a data generator and a reward model. The methodology enables superior training for Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), with key findings showing that models trained on this generated data outperform baselines on benchmarks, achieve state-of-the-art generalization across modalities and tasks, and demonstrate parameter efficiency by rivaling much larger foundation models.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework, MedGR², that combines generative data creation with reward learning to autonomously address data scarcity in medical AI, significantly advancing beyond existing methods by establishing a self-improving cycle for data generation and model training. This represents a true innovation in handling high-stakes domains like medicine, where data limitations have been a persistent barrier.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to broadly influence medical AI research and applications by enabling data-efficient learning and improving model generalization, which could lead to more scalable and effective clinical tools. Its demonstrated performance gains and parameter efficiency suggest it could transform how AI is developed in data-scarce, high-stakes fields like healthcare.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality, innovative contribution that advances medical AI through a new framework, making it valuable for researchers in AI and healthcare to understand and build upon. While exceptional, it is not entirely paradigm-shifting on its own, positioning it as essential but not absolutely mandatory reading.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/52cc026f99fa930f679a8151ffab97250740632b",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Weihai Zhi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377791643"
        },
        {
          "name": "Jiayan Guo",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2216202739"
        },
        {
          "name": "Shangyang Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378026083"
        }
      ]
    },
    {
      "id": "2508.20551",
      "title": "Contrastive Learning through Auxiliary Branch for Video Object Detection",
      "authors": [
        "Lucas Rakotoarivony"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video object detection is a challenging task because videos often suffer from\nimage deterioration such as motion blur, occlusion, and deformable shapes,\nmaking it significantly more difficult than detecting objects in still images.\nPrior approaches have improved video object detection performance by employing\nfeature aggregation and complex post-processing techniques, though at the cost\nof increased computational demands. To improve robustness to image degradation\nwithout additional computational load during inference, we introduce a\nstraightforward yet effective Contrastive Learning through Auxiliary Branch\n(CLAB) method. First, we implement a constrastive auxiliary branch using a\ncontrastive loss to enhance the feature representation capability of the video\nobject detector's backbone. Next, we propose a dynamic loss weighting strategy\nthat emphasizes auxiliary feature learning early in training while gradually\nprioritizing the detection task as training converges. We validate our approach\nthrough comprehensive experiments and ablation studies, demonstrating\nconsistent performance gains. Without bells and whistles, CLAB reaches a\nperformance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,\nrespectively, on the ImageNet VID dataset, thus achieving state-of-the-art\nperformance for CNN-based models without requiring additional post-processing\nmethods.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20551v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20551v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.315,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20554",
      "title": "Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale\n  Biomedical Semantic Indexing and Question Answering",
      "authors": [
        "Anastasios Nentidis",
        "Georgios Katsimpras",
        "Anastasia Krithara",
        "Martin Krallinger",
        "Miguel Rodríguez-Ortega",
        "Eduard Rodriguez-López",
        "Natalia Loukachevitch",
        "Andrey Sakhovskiy",
        "Elena Tutubalina",
        "Dimitris Dimitriadis",
        "Grigorios Tsoumakas",
        "George Giannakoulas",
        "Alexandra Bekiaridou",
        "Athanasios Samaras",
        "Giorgio Maria Di Nunzio",
        "Nicola Ferro",
        "Stefano Marchesin",
        "Marco Martinelli",
        "Gianmaria Silvello",
        "Georgios Paliouras"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "This is an overview of the thirteenth edition of the BioASQ challenge in the\ncontext of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ\nis a series of international challenges promoting advances in large-scale\nbiomedical semantic indexing and question answering. This year, BioASQ\nconsisted of new editions of the two established tasks, b and Synergy, and four\nnew tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task\nBioNNE-L on nested named entity linking in Russian and English. c) Task\nELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain\ninterplay information extraction. In this edition of BioASQ, 83 competing teams\nparticipated with more than 1000 distinct submissions in total for the six\ndifferent shared tasks of the challenge. Similar to previous editions, several\nparticipating systems achieved competitive performance, indicating the\ncontinuous advancement of the state-of-the-art in the field.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20554v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20554v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.251,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.303,
      "distributed_training_score": 0.304,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20557",
      "title": "Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data",
      "authors": [
        "Jiahao Xiao",
        "Jiangming Liu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The widespread success of pre-trained language models has established a new\ntraining paradigm, where a global PLM is fine-tuned using task-specific data\nfrom local clients. The local data are highly different from each other and can\nnot capture the global distribution of the whole data in real world. To address\nthe challenges of non-IID data in real environments, privacy-preserving\nfederated distillation has been proposed and highly investigated. However,\nprevious experimental non-IID scenarios are primarily identified with the label\n(output) diversity, without considering the diversity of language domains\n(input) that is crucial in natural language processing. In this paper, we\nintroduce a comprehensive set of multi-domain non-IID scenarios and propose a\nunified benchmarking framework that includes diverse data. The benchmark can be\nused to evaluate the federated learning framework in a real environment. To\nthis end, we propose an Adaptive Federated Distillation (AdaFD) framework\ndesigned to address multi-domain non-IID challenges in both homogeneous and\nheterogeneous settings. Experimental results demonstrate that our models\ncapture the diversity of local clients and achieve better performance compared\nto the existing works. The code for this paper is available at:\nhttps://github.com/jiahaoxiao1228/AdaFD.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20557v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20557v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.485,
      "datasets_score": 0.421,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on federated learning and distillation for non-IID textual data, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper addresses federated learning with non-IID data but does not involve programmatically generating labels or using noisy supervision sources; it centers on model aggregation and distillation.",
      "diffusion_reasoning_justification": "The paper deals with federated distillation for NLP tasks and does not incorporate diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "The paper's core contribution is on federated learning, a form of distributed training, where it proposes strategies for handling non-IID data across clients, including model aggregation and communication-efficient methods.",
      "datasets_justification": "The paper introduces a unified benchmarking framework with diverse multi-domain non-IID datasets for NLP, focusing on creation, analysis, and evaluation to advance federated learning research.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of non-IID data in federated learning for natural language processing by introducing multi-domain scenarios that account for both input (domain) and output (label) diversity, and proposes the Adaptive Federated Distillation (AdaFD) framework to handle these in both homogeneous and heterogeneous settings. AdaFD incorporates adaptive ensemble strategies with weighting methods based on training loss and adaptive distillation, demonstrating superior performance over existing methods in experiments, while also providing a unified benchmarking framework for evaluating federated learning in real environments.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by extending non-IID federated learning to include multi-domain input diversity alongside label diversity, which is a clever combination of existing ideas for NLP applications. However, it builds on established federated distillation techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of federated learning for NLP due to its benchmarking framework and improved handling of real-world data heterogeneity. Nonetheless, its influence may be limited to specific applications rather than broadly affecting general AI or commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a strong, valuable contribution to federated learning in NLP by addressing practical non-IID challenges, making it essential for researchers in this area to be aware of its methods and benchmarks. While insightful, it is not groundbreaking enough to be considered must-read for the broader AI community.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/27043305e94e05513221f34f722928989ac32d8c",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jiahao Xiao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374277666"
        },
        {
          "name": "Jiangming Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378038419"
        }
      ]
    },
    {
      "id": "2508.20563",
      "title": "AI and Agile Software Development: A Research Roadmap from the XP2025\n  Workshop",
      "authors": [
        "Zheying Zhang",
        "Tomas Herda",
        "Victoria Pichler",
        "Pekka Abrahamsson",
        "Geir K. Hanssen",
        "Joshua Kerievsky",
        "Alex Polyakov",
        "Mohit Chandna",
        "Marius Irgens",
        "Kai-Kristian Kemell",
        "Ayman Asad Khan",
        "Crystal Kwok",
        "Evan Leybourn",
        "Munish Malik",
        "Dorota Mleczko",
        "Morteza Moalagh",
        "Christopher Morales",
        "Yuliia Pieskova",
        "Daniel Planötscher",
        "Mika Saari",
        "Anastasiia Tkalich",
        "Karl Josef Gstettner",
        "Xiaofeng Wang"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper synthesizes the key findings from a full-day XP2025 workshop on\n\"AI and Agile: From Frustration to Success\", held in Brugg-Windisch,\nSwitzerland. The workshop brought together over 30 interdisciplinary academic\nresearchers and industry practitioners to tackle the concrete challenges and\nemerging opportunities at the intersection of Generative Artificial\nIntelligence (GenAI) and agile software development. Through structured,\ninteractive breakout sessions, participants identified shared pain points like\ntool fragmentation, governance, data quality, and critical skills gaps in AI\nliteracy and prompt engineering. These issues were further analyzed, revealing\nunderlying causes and cross-cutting concerns. The workshop concluded by\ncollaboratively co-creating a multi-thematic research roadmap, articulating\nboth short-term, implementable actions and visionary, long-term research\ndirections. This cohesive agenda aims to guide future investigation and drive\nthe responsible, human-centered integration of GenAI into agile practices.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20563v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20563v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.376,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a research roadmap for integrating Generative AI into agile software development, focusing on challenges like tool fragmentation, governance, and skills gaps. It discusses general AI applications in agile practices but does not mention, involve, or relate to Reinforcement Learning from Human Feedback (RLHF), which specifically pertains to fine-tuning AI models using human-ranked data and reinforcement learning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20570",
      "title": "Towards Mechanistic Defenses Against Typographic Attacks in CLIP",
      "authors": [
        "Lorenz Hufe",
        "Constantin Venhoff",
        "Maximilian Dreyer",
        "Sebastian Lapuschkin",
        "Wojciech Samek"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Typographic attacks exploit multi-modal systems by injecting text into\nimages, leading to targeted misclassifications, malicious content generation\nand even Vision-Language Model jailbreaks. In this work, we analyze how CLIP\nvision encoders behave under typographic attacks, locating specialized\nattention heads in the latter half of the model's layers that causally extract\nand transmit typographic information to the cls token. Building on these\ninsights, we introduce a method to defend CLIP models against typographic\nattacks by selectively ablating a typographic circuit, consisting of attention\nheads. Without requiring finetuning, our method improves performance by up to\n19.6% on a typographic variant of ImageNet-100, while reducing standard\nImageNet-100 accuracy by less than 1%. Notably, our training-free approach\nremains competitive with current state-of-the-art typographic defenses that\nrely on finetuning. To this end, we release a family of dyslexic CLIP models\nwhich are significantly more robust against typographic attacks. These models\nserve as suitable drop-in replacements for a broad range of safety-critical\napplications, where the risks of text-based manipulation outweigh the utility\nof text recognition.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20570v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20570v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.339,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is developing defenses against typographic attacks in CLIP models by identifying and ablating attention heads, focusing on vision-language model vulnerabilities. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as it centers on image classification robustness rather than reasoning adaptations.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20577",
      "title": "MERIT: Maximum-normalized Element-wise Ratio for Language Model\n  Large-batch Training",
      "authors": [
        "Yang Luo",
        "Zangwei Zheng",
        "Ziheng Qin",
        "Zirui Zhu",
        "Yong Liu",
        "Yang You"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large-batch training has become a cornerstone in accelerating the training of\ndeep neural networks, yet it poses challenges in optimization and\ngeneralization. Existing optimizers like AdamW present performance degradation\nduring language models' large-batch training, due to the information bottleneck\nin attention layers caused by the sharp increase of max attention logit. While\nthe LAMB optimizer partially addresses this issue, some attention layers still\nface this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are\nless effective in directly influencing the max value of query/key weights.\nFurthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks\nrelationships of weight values within rows or columns. Building on these\nobservations, we propose a novel optimizer, MERIT, which leverages the max-norm\nto calculate the trust ratio to constrain the max attention logit more\neffectively. Moreover, we further construct element-wise trust ratios to\nprovide more robust update scaling by focusing on local weight structures.\nExtensive experiments of large-batch training across various sizes of GPT-2\nmodels demonstrate the superior performance of MERIT. Notably, during the\ntraining of GPT-2 Medium, MERIT enables a 6k batch size without any performance\ndegradation compared to the standard batch size (480) with 48B training tokens.\nThis work highlights the importance of considering the max attention logit and\nfiner-granularity trust ratio in large-batch training. It successfully improves\nthe training stability and paves the way for larger batch usage, enabling\nfaster development and iteration of large language models. Code is available at\nhttps://github.com/NUS-HPC-AI-Lab/MERIT.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20577v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20577v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.492,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing a new optimizer for large-batch training of language models, specifically addressing issues in attention layers and optimization stability. It does not involve reinforcement learning, human feedback, reward models, or any mechanisms for aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a novel optimizer for large-batch training, which directly relates to distributed training by enhancing stability and performance when processing data in parallel across hardware, such as GPU clusters. It discusses data parallelism, batch size scaling, and overcoming limitations in multi-node environments, making it highly pertinent to parallel computing and acceleration techniques in machine learning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces MERIT, a novel optimizer designed to address performance degradation in large-batch training of language models by effectively constraining the max attention logit through max-norm-based and element-wise trust ratios, building on the limitations of existing optimizers like AdamW and LAMB. The methodology involves calculating trust ratios using max-norm to limit extreme values in query/key weights and element-wise ratios to focus on local weight structures, with extensive experiments on various GPT-2 models demonstrating that MERIT enables stable training with much larger batch sizes, such as 6k for GPT-2 Medium, without loss in generalization compared to standard batch sizes.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing max-norm-based and element-wise trust ratios in the MERIT optimizer, which cleverly addresses the limitations of existing methods like LAMB for large-batch training, rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of optimization for large language models, as it enhances training efficiency and stability, potentially influencing future developments in scalable AI training practices.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a strong, practical contribution to optimizer design for large-batch training, making it valuable for researchers in machine learning and AI who focus on improving model training efficiency and stability.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/71434d21bf0e4e939c7448618379de042b357b41",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 13,
      "average_h_index": 4.333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yang Luo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2218050858"
        },
        {
          "name": "Zangwei Zheng",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/2109654065"
        },
        {
          "name": "Ziheng Qin",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2198208950"
        },
        {
          "name": "Zirui Zhu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2286899874"
        },
        {
          "name": "Yong Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2287037867"
        },
        {
          "name": "Yang You",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2286900390"
        }
      ]
    },
    {
      "id": "2508.20578",
      "title": "Human-AI Collaborative Bot Detection in MMORPGs",
      "authors": [
        "Jaeman Son",
        "Hyunsoo Kim"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling\nbots exploit automated programs to level up characters at scale, undermining\ngameplay balance and fairness. Detecting such bots is challenging, not only\nbecause they mimic human behavior, but also because punitive actions require\nexplainable justification to avoid legal and user experience issues. In this\npaper, we present a novel framework for detecting auto-leveling bots by\nleveraging contrastive representation learning and clustering techniques in a\nfully unsupervised manner to identify groups of characters with similar\nlevel-up patterns. To ensure reliable decisions, we incorporate a Large\nLanguage Model (LLM) as an auxiliary reviewer to validate the clustered groups,\neffectively mimicking a secondary human judgment. We also introduce a growth\ncurve-based visualization to assist both the LLM and human moderators in\nassessing leveling behavior. This collaborative approach improves the\nefficiency of bot detection workflows while maintaining explainability, thereby\nsupporting scalable and accountable bot regulation in MMORPGs.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20578v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20578v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.345,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for unsupervised bot detection in MMORPGs using contrastive representation learning, clustering, and LLMs for verification. It does not involve training an AI model with human-ranked data to create a reward model or fine-tune via reinforcement learning, which are core elements of RLHF. The LLM is used solely for auxiliary review, not for aligning models with human preferences through feedback loops.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20579",
      "title": "GLaRE: A Graph-based Landmark Region Embedding Network for Emotion\n  Recognition",
      "authors": [
        "Debasis Maji",
        "Debaditya Barman"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Facial expression recognition (FER) is a crucial task in computer vision with\nwide range of applications including human computer interaction, surveillance,\nand assistive technologies. However, challenges such as occlusion, expression\nvariability, and lack of interpretability hinder the performance of traditional\nFER systems. Graph Neural Networks (GNNs) offer a powerful alternative by\nmodeling relational dependencies between facial landmarks, enabling structured\nand interpretable learning. In this paper, we propose GLaRE, a novel\nGraph-based Landmark Region Embedding network for emotion recognition. Facial\nlandmarks are extracted using 3D facial alignment, and a quotient graph is\nconstructed via hierarchical coarsening to preserve spatial structure while\nreducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet\nand 94.24 percentage on FERG, outperforming several existing baselines.\nAdditionally, ablation studies have demonstrated that region-level embeddings\nfrom quotient graphs have contributed to improved prediction performance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20579v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20579v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.323,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20583",
      "title": "A Graph Talks, But Who's Listening? Rethinking Evaluations for\n  Graph-Language Models",
      "authors": [
        "Soham Petkar",
        "Hari Aakash K",
        "Anirudh Vempati",
        "Akshit Sinha",
        "Ponnurangam Kumarauguru",
        "Chirag Agarwal"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Developments in Graph-Language Models (GLMs) aim to integrate the structural\nreasoning capabilities of Graph Neural Networks (GNNs) with the semantic\nunderstanding of Large Language Models (LLMs). However, we demonstrate that\ncurrent evaluation benchmarks for GLMs, which are primarily repurposed\nnode-level classification datasets, are insufficient to assess multimodal\nreasoning. Our analysis reveals that strong performance on these benchmarks is\nachievable using unimodal information alone, suggesting that they do not\nnecessitate graph-language integration. To address this evaluation gap, we\nintroduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed\nto evaluate multimodal reasoning at various complexity levels. Our benchmark\nemploys a synthetic graph generation pipeline paired with questions that\nrequire joint reasoning over structure and textual semantics. We perform a\nthorough evaluation of representative GLM architectures and find that\nsoft-prompted LLM baselines perform on par with GLMs that incorporate a full\nGNN backbone. This result calls into question the architectural necessity of\nincorporating graph structure into LLMs. We further show that GLMs exhibit\nsignificant performance degradation in tasks that require structural reasoning.\nThese findings highlight limitations in the graph reasoning capabilities of\ncurrent GLMs and provide a foundation for advancing the community toward\nexplicit multimodal reasoning involving graph structure and language.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20583v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20583v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.484,
      "distributed_training_score": 0.357,
      "datasets_score": 0.42,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating Graph-Language Models (GLMs) and introduces a new benchmark for graph-language reasoning, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. There is no mention of adapting diffusion for complex tasks, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes introducing and evaluating the CLEGR benchmark, a new synthetic dataset for assessing graph-language reasoning, as well as analyzing existing datasets for GLMs. This directly aligns with creating, benchmarking, and evaluating datasets for AI applications, fitting the topic's focus on dataset introduction and analysis.",
      "llm_score_status": "completed",
      "summary": "The paper critiques current evaluation benchmarks for Graph-Language Models (GLMs), which integrate Graph Neural Networks and Large Language Models, by showing that these benchmarks can be solved using unimodal information alone, thus failing to assess true multimodal reasoning. To address this, the authors introduce the CLEGR benchmark, a synthetic graph-question answering dataset designed to require joint structural and semantic reasoning, and evaluate various GLM architectures, revealing that GLMs often perform no better than unimodal baselines and struggle with tasks demanding graph reasoning, thereby highlighting the need for better integration of graph and language modalities.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the CLEGR benchmark and demonstrating flaws in existing evaluations, offering a clever combination of synthetic data generation and analysis to address a known issue in GLM assessments. However, it builds on existing ideas in graph-language integration rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of graph-language models, as it provides a new benchmark that could improve future evaluations and model designs. While its influence may be confined to AI and computational language research, it addresses a critical gap that could enhance multimodal reasoning capabilities.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by exposing limitations in current GLM evaluations and proposing a practical new benchmark, making it essential for researchers in graph and language models to understand and build upon. It is not a must-read for the broader field but provides important insights for those working in multimodal AI.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f622b158261f02772cbbe99bf7e68224a447a500",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 44,
      "average_h_index": 6.571428571428571,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Soham Petkar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2349637998"
        },
        {
          "name": "Hari Aakash",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377803768"
        },
        {
          "name": "Anirudh Vempati",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377798252"
        },
        {
          "name": "Akshit Sinha",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2304753404"
        },
        {
          "name": "P. Kumaraguru",
          "h_index": 44,
          "profile_url": "https://www.semanticscholar.org/author/1734731"
        },
        {
          "name": "Chirag Agarwal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377797719"
        },
        {
          "name": "Cora CiteSeer",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377802943"
        }
      ]
    },
    {
      "id": "2508.20584",
      "title": "Flowing Straighter with Conditional Flow Matching for Accurate Speech\n  Enhancement",
      "authors": [
        "Mattias Cross",
        "Anton Ragni"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Current flow-based generative speech enhancement methods learn curved\nprobability paths which model a mapping between clean and noisy speech. Despite\nimpressive performance, the implications of curved probability paths are\nunknown. Methods such as Schrodinger bridges focus on curved paths, where\ntime-dependent gradients and variance do not promote straight paths. Findings\nin machine learning research suggest that straight paths, such as conditional\nflow matching, are easier to train and offer better generalisation. In this\npaper we quantify the effect of path straightness on speech enhancement\nquality. We report experiments with the Schrodinger bridge, where we show that\ncertain configurations lead to straighter paths. Conversely, we propose\nindependent conditional flow-matching for speech enhancement, which models\nstraight paths between noisy and clean speech. We demonstrate empirically that\na time-independent variance has a greater effect on sample quality than the\ngradient. Although conditional flow matching improves several speech quality\nmetrics, it requires multiple inference steps. We rectify this with a one-step\nsolution by inferring the trained flow-based model as if it was directly\npredictive. Our work suggests that straighter time-independent probability\npaths improve generative speech enhancement over curved time-dependent paths.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20584v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20584v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.303,
      "datasets_score": 0.24,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on flow-based generative methods for speech enhancement, specifically comparing probability paths in models like Schrodinger bridges and conditional flow matching to improve audio quality. While it mentions diffusion models (DM) briefly in the context of training continuous normalizing flows (CNF), it does not adapt iterative refinement processes for solving complex logical tasks, treating Chain-of-Thought as an entity, or performing multi-step logical reasoning. Thus, there is no connection to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20586",
      "title": "FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable\n  Diffusion Models",
      "authors": [
        "Zheng Chong",
        "Yanwei Lei",
        "Shiyue Zhang",
        "Zhuandi He",
        "Zhen Wang",
        "Xujie Zhang",
        "Xiao Dong",
        "Yiling Wu",
        "Dongmei Jiang",
        "Xiaodan Liang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite its great potential, virtual try-on technology is hindered from\nreal-world application by two major challenges: the inability of current\nmethods to support multi-reference outfit compositions (including garments and\naccessories), and their significant inefficiency caused by the redundant\nre-computation of reference features in each denoising step. To address these\nchallenges, we propose FastFit, a high-speed multi-reference virtual try-on\nframework based on a novel cacheable diffusion architecture. By employing a\nSemi-Attention mechanism and substituting traditional timestep embeddings with\nclass embeddings for reference items, our model fully decouples reference\nfeature encoding from the denoising process with negligible parameter overhead.\nThis allows reference features to be computed only once and losslessly reused\nacross all steps, fundamentally breaking the efficiency bottleneck and\nachieving an average 3.5x speedup over comparable methods. Furthermore, to\nfacilitate research on complex, multi-reference virtual try-on, we introduce\nDressCode-MR, a new large-scale dataset. It comprises 28,179 sets of\nhigh-quality, paired images covering five key categories (tops, bottoms,\ndresses, shoes, and bags), constructed through a pipeline of expert models and\nhuman feedback refinement. Extensive experiments on the VITON-HD, DressCode,\nand our DressCode-MR datasets show that FastFit surpasses state-of-the-art\nmethods on key fidelity metrics while offering its significant advantage in\ninference efficiency.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20586v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20586v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.399,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for accelerating image generation in virtual try-on tasks, specifically through a cacheable architecture to improve efficiency in multi-reference outfit synthesis. While it employs the iterative refinement process of diffusion models for denoising and image creation, it does not adapt this process for solving complex logical tasks, such as treating a 'Chain-of-Thought' as an entity for holistic reasoning. There is no component involving multi-step logical reasoning, making the paper's contributions unrelated to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20594",
      "title": "UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted\n  Traffic Signage Sketching",
      "authors": [
        "Yuqi Han",
        "Songqian Zhang",
        "Weijian Su",
        "Ke Li",
        "Jiayu Yang",
        "Jinli Suo",
        "Qiang Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The thermal camera excels at perceiving outdoor environments under low-light\nconditions, making it ideal for applications such as nighttime autonomous\ndriving and unmanned navigation. However, thermal cameras encounter challenges\nwhen capturing signage from objects made of similar materials, which can pose\nsafety risks for accurately understanding semantics in autonomous driving\nsystems. In contrast, the neuromorphic vision camera, also known as an event\ncamera, detects changes in light intensity asynchronously and has proven\neffective in high-speed, low-light traffic environments. Recognizing the\ncomplementary characteristics of these two modalities, this paper proposes\nUTA-Sign, an unsupervised thermal-event video augmentation for traffic signage\nin low-illumination environments, targeting elements such as license plates and\nroadblock indicators. To address the signage blind spots of thermal imaging and\nthe non-uniform sampling of event cameras, we developed a dual-boosting\nmechanism that fuses thermal frames and event signals for consistent signage\nrepresentation over time. The proposed method utilizes thermal frames to\nprovide accurate motion cues as temporal references for aligning the uneven\nevent signals. At the same time, event signals contribute subtle signage\ncontent to the raw thermal frames, enhancing the overall understanding of the\nenvironment. The proposed method is validated on datasets collected from\nreal-world scenarios, demonstrating superior quality in traffic signage\nsketching and improved detection accuracy at the perceptual level.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20594v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20594v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.327,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20595",
      "title": "Disruptive Attacks on Face Swapping via Low-Frequency Perceptual\n  Perturbations",
      "authors": [
        "Mengxiao Huang",
        "Minglei Shu",
        "Shuwang Zhou",
        "Zhaoyang Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deepfake technology, driven by Generative Adversarial Networks (GANs), poses\nsignificant risks to privacy and societal security. Existing detection methods\nare predominantly passive, focusing on post-event analysis without preventing\nattacks. To address this, we propose an active defense method based on\nlow-frequency perceptual perturbations to disrupt face swapping manipulation,\nreducing the performance and naturalness of generated content. Unlike prior\napproaches that used low-frequency perturbations to impact classification\naccuracy,our method directly targets the generative process of deepfake\ntechniques. We combine frequency and spatial domain features to strengthen\ndefenses. By introducing artifacts through low-frequency perturbations while\npreserving high-frequency details, we ensure the output remains visually\nplausible. Additionally, we design a complete architecture featuring an\nencoder, a perturbation generator, and a decoder, leveraging discrete wavelet\ntransform (DWT) to extract low-frequency components and generate perturbations\nthat disrupt facial manipulation models. Experiments on CelebA-HQ and LFW\ndemonstrate significant reductions in face-swapping effectiveness, improved\ndefense success rates, and preservation of visual quality.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20595v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20595v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.327,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20600",
      "title": "GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac\n  MRI Reconstruction",
      "authors": [
        "Kian Anvari Hamedani",
        "Narges Razizadeh",
        "Shahabedin Nabavi",
        "Mohsen Ebrahimi Moghaddam"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction\nremains a critical challenge due to the trade-off between scan time and image\nquality, particularly when generalizing across diverse acquisition settings. We\npropose GENRE-CMR, a generative adversarial network (GAN)-based architecture\nemploying a residual deep unrolled reconstruction framework to enhance\nreconstruction fidelity and generalization. The architecture unrolls iterative\noptimization into a cascade of convolutional subnetworks, enriched with\nresidual connections to enable progressive feature propagation from shallow to\ndeeper stages. To further improve performance, we integrate two loss functions:\n(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on\nstructurally informative regions and helps prevent common reconstruction\nblurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which\nregularizes the feature space across diverse data distributions via a symmetric\nKL divergence formulation. Extensive experiments confirm that GENRE-CMR\nsurpasses state-of-the-art methods on training and unseen data, achieving\n0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various\nacceleration factors and sampling trajectories. Ablation studies confirm the\ncontribution of each proposed component to reconstruction quality and\ngeneralization. Our framework presents a unified and robust solution for\nhigh-quality CMR reconstruction, paving the way for clinically adaptable\ndeployment across heterogeneous acquisition protocols.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20600v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20600v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.411,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a GAN-based architecture for MRI reconstruction and mentions diffusion models briefly in the related work (e.g., MRPD), but it does not incorporate diffusion-based iterative refinement for logical reasoning tasks. The main contribution involves unrolled optimization and loss functions for image reconstruction, not multi-step reasoning via diffusion models.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node machine learning. It discusses the architecture and training of a single GAN-based model for MRI reconstruction, with no mention of partitioning data, architecture, or computation across multiple processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20604",
      "title": "Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion",
      "authors": [
        "Zheng Qin",
        "Yabing Wang",
        "Minghui Yang",
        "Sanping Zhou",
        "Ming Yang",
        "Le Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Generating 3D human motions from text is a challenging yet valuable task. The\nkey aspects of this task are ensuring text-motion consistency and achieving\ngeneration diversity. Although recent advancements have enabled the generation\nof precise and high-quality human motions from text, achieving diversity in the\ngenerated motions remains a significant challenge. In this paper, we aim to\novercome the above challenge by designing a simple yet effective text-to-motion\ngeneration method, \\textit{i.e.}, Diverse-T2M. Our method introduces\nuncertainty into the generation process, enabling the generation of highly\ndiverse motions while preserving the semantic consistency of the text.\nSpecifically, we propose a novel perspective that utilizes noise signals as\ncarriers of diversity information in transformer-based methods, facilitating a\nexplicit modeling of uncertainty. Moreover, we construct a latent space where\ntext is projected into a continuous representation, instead of a rigid\none-to-one mapping, and integrate a latent space sampler to introduce\nstochastic sampling into the generation process, thereby enhancing the\ndiversity and uncertainty of the outputs. Our results on text-to-motion\ngeneration benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our\nmethod significantly enhances diversity while maintaining state-of-the-art\nperformance in text consistency.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20604v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20604v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.497,
      "distributed_training_score": 0.346,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a two-stage text-to-motion generation method that introduces uncertainty via noise signals and latent space sampling to enhance diversity in 3D human motion. It briefly references diffusion-based models as part of related work (e.g., MDM, MotionDiffuse), but does not adapt diffusion for multi-step logical reasoning or chain-of-thought processes. The focus is on generative motion tasks, not solving complex logical tasks through iterative diffusion refinement, so it lacks any clear component of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20605",
      "title": "Optimization-Based Calibration for Intravascular Ultrasound Volume\n  Reconstruction",
      "authors": [
        "Karl-Philippe Beaudet",
        "Sidaty El Hadramy",
        "Philippe C Cattin",
        "Juan Verde",
        "Stéphane Cotin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Intraoperative ultrasound images are inherently challenging to interpret in\nliver surgery due to the limited field of view and complex anatomical\nstructures. Bridging the gap between preoperative and intraoperative data is\ncrucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS)\noffers a potential solution by enabling the reconstruction of the entire organ,\nwhich facilitates registration between preoperative computed tomography (CT)\nscans and intraoperative IVUS images. In this work, we propose an\noptimization-based calibration method using a 3D-printed phantom for accurate\n3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise\nalignment of tracked IVUS data with preoperative CT images, improving\nintraoperative navigation. We validated our method using in vivo swine liver\nimages, achieving a calibration error from 0.88 to 1.80 mm and a registration\nerror from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT\nscan. Our method provides a reliable and accurate means of calibration and\nvolume reconstruction. It can be used to register intraoperative ultrasound\nimages with preoperative CT images in the context of liver surgery, and enhance\nintraoperative guidance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20605v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20605v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.218,
      "weak_supervision_score": 0.263,
      "diffusion_reasoning_score": 0.228,
      "distributed_training_score": 0.218,
      "datasets_score": 0.209,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20612",
      "title": "Physics Informed Generative Models for Magnetic Field Images",
      "authors": [
        "Aye Phyu Phyu Aung",
        "Lucas Lum",
        "Zhansen Shi",
        "Wen Qiu",
        "Bernice Zee",
        "JM Chin",
        "Yeow Kheng Lim",
        "J. Senthilnath"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In semiconductor manufacturing, defect detection and localization are\ncritical to ensuring product quality and yield. While X-ray imaging is a\nreliable non-destructive testing method, it is memory-intensive and\ntime-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a\nmore efficient means to localize regions of interest (ROI) for targeted X-ray\nscanning. However, the limited availability of MFI datasets due to proprietary\nconcerns presents a significant bottleneck for training machine learning (ML)\nmodels using MFI. To address this challenge, we consider an ML-driven approach\nleveraging diffusion models with two physical constraints. We propose Physics\nInformed Generative Models for Magnetic Field Images (PI-GenMFI) to generate\nsynthetic MFI samples by integrating specific physical information. We generate\nMFI images for the most common defect types: power shorts. These synthetic\nimages will serve as training data for ML algorithms designed to localize\ndefect areas efficiently. To evaluate generated MFIs, we compare our model to\nSOTA generative models from both variational autoencoder (VAE) and diffusion\nmethods. We present a domain expert evaluation to assess the generated samples.\nIn addition, we present qualitative and quantitative evaluation using various\nmetrics used for image generation and signal processing, showing promising\nresults to optimize the defect localization process.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20612v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20612v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.359,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for generating synthetic Magnetic Field Images (MFI) with physical constraints for defect detection in semiconductors. While it employs the iterative refinement process of diffusion models for image generation, it does not adapt this process to solve complex logical tasks or perform multi-step logical reasoning, such as treating a 'Chain-of-Thought' as an entity. The core application is generative modeling for visual data, not reasoning, so it does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20613",
      "title": "Revisiting the Privacy Risks of Split Inference: A GAN-Based Data\n  Reconstruction Attack via Progressive Feature Optimization",
      "authors": [
        "Yixiang Qiu",
        "Yanhan Liu",
        "Hongyao Yu",
        "Hao Fang",
        "Bin Chen",
        "Shu-Tao Xia",
        "Ke Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "The growing complexity of Deep Neural Networks (DNNs) has led to the adoption\nof Split Inference (SI), a collaborative paradigm that partitions computation\nbetween edge devices and the cloud to reduce latency and protect user privacy.\nHowever, recent advances in Data Reconstruction Attacks (DRAs) reveal that\nintermediate features exchanged in SI can be exploited to recover sensitive\ninput data, posing significant privacy risks. Existing DRAs are typically\neffective only on shallow models and fail to fully leverage semantic priors,\nlimiting their reconstruction quality and generalizability across datasets and\nmodel architectures. In this paper, we propose a novel GAN-based DRA framework\nwith Progressive Feature Optimization (PFO), which decomposes the generator\ninto hierarchical blocks and incrementally refines intermediate representations\nto enhance the semantic fidelity of reconstructed images. To stabilize the\noptimization and improve image realism, we introduce an L1-ball constraint\nduring reconstruction. Extensive experiments show that our method outperforms\nprior attacks by a large margin, especially in high-resolution scenarios,\nout-of-distribution settings, and against deeper and more complex DNNs.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20613v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20613v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.413,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a GAN-based data reconstruction attack using progressive feature optimization for Split Inference, focusing on privacy risks in DNNs. It does not involve diffusion models, iterative refinement for logical tasks, or any form of Chain-of-Thought reasoning.",
      "distributed_training_justification": "The paper discusses Split Inference, which partitions DNN computation between edge devices and the cloud for inference purposes, but it does not address distributed training, parallel computing for model training, or multi-node machine learning algorithms. The focus is on inference and security, not training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20615",
      "title": "EmoCAST: Emotional Talking Portrait via Emotive Text Description",
      "authors": [
        "Yiguo Jiang",
        "Xiaodong Cun",
        "Yong Zhang",
        "Yudian Zheng",
        "Fan Tang",
        "Chi-Man Pun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Emotional talking head synthesis aims to generate talking portrait videos\nwith vivid expressions. Existing methods still exhibit limitations in control\nflexibility, motion naturalness, and expression quality. Moreover, currently\navailable datasets are primarily collected in lab settings, further\nexacerbating these shortcomings. Consequently, these limitations substantially\nhinder practical applications in real-world scenarios. To address these\nchallenges, we propose EmoCAST, a diffusion-based framework with two key\nmodules for precise text-driven emotional synthesis. In appearance modeling,\nemotional prompts are integrated through a text-guided decoupled emotive\nmodule, enhancing the spatial knowledge to improve emotion comprehension. To\nimprove the relationship between audio and emotion, we introduce an emotive\naudio attention module to capture the interplay between controlled emotion and\ndriving audio, generating emotion-aware features to guide more precise facial\nmotion synthesis. Additionally, we construct an emotional talking head dataset\nwith comprehensive emotive text descriptions to optimize the framework's\nperformance. Based on the proposed dataset, we propose an emotion-aware\nsampling training strategy and a progressive functional training strategy that\nfurther improve the model's ability to capture nuanced expressive features and\nachieve accurate lip-synchronization. Overall, EmoCAST achieves\nstate-of-the-art performance in generating realistic, emotionally expressive,\nand audio-synchronized talking-head videos. Project Page:\nhttps://github.com/GVCLab/EmoCAST",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20615v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20615v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.294,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a diffusion-based framework for generating emotional talking portraits, focusing on visual synthesis with text and audio guidance. While it uses diffusion models for iterative refinement in video generation, it does not involve adapting diffusion for complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity. The core application is generative modeling for visuals, not logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20621",
      "title": "Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI\n  Classification",
      "authors": [
        "Smriti Joshi",
        "Lidia Garrucho",
        "Richard Osuala",
        "Oliver Diaz",
        "Karim Lekadir"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Breast cancer is one of the leading causes of cancer-related mortality in\nwomen, and early detection is essential for improving outcomes. Magnetic\nresonance imaging (MRI) is a highly sensitive tool for breast cancer detection,\nparticularly in women at high risk or with dense breast tissue, where\nmammography is less effective. The ODELIA consortium organized a multi-center\nchallenge to foster AI-based solutions for breast cancer diagnosis and\nclassification. The dataset included 511 studies from six European centers,\nacquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study\nwas labeled for the left and right breast as no lesion, benign lesion, or\nmalignant lesion. We developed a SwinUNETR-based deep learning framework that\nincorporates breast region masking, extensive data augmentation, and ensemble\nlearning to improve robustness and generalizability. Our method achieved second\nplace on the challenge leaderboard, highlighting its potential to support\nclinical breast MRI interpretation. We publicly share our codebase at\nhttps://github.com/smriti-joshi/bcnaim-odelia-challenge.git.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20621v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20621v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.258,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.353,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20622",
      "title": "Masked Autoencoders for Ultrasound Signals: Robust Representation\n  Learning for Downstream Applications",
      "authors": [
        "Immanuel Roßteutscher",
        "Klaus S. Drese",
        "Thorsten Uphues"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We investigated the adaptation and performance of Masked Autoencoders (MAEs)\nwith Vision Transformer (ViT) architectures for self-supervised representation\nlearning on one-dimensional (1D) ultrasound signals. Although MAEs have\ndemonstrated significant success in computer vision and other domains, their\nuse for 1D signal analysis, especially for raw ultrasound data, remains largely\nunexplored. Ultrasound signals are vital in industrial applications such as\nnon-destructive testing (NDT) and structural health monitoring (SHM), where\nlabeled data are often scarce and signal processing is highly task-specific. We\npropose an approach that leverages MAE to pre-train on unlabeled synthetic\nultrasound signals, enabling the model to learn robust representations that\nenhance performance in downstream tasks, such as time-of-flight (ToF)\nclassification. This study systematically investigated the impact of model\nsize, patch size, and masking ratio on pre-training efficiency and downstream\naccuracy. Our results show that pre-trained models significantly outperform\nmodels trained from scratch and strong convolutional neural network (CNN)\nbaselines optimized for the downstream task. Additionally, pre-training on\nsynthetic data demonstrates superior transferability to real-world measured\nsignals compared with training solely on limited real datasets. This study\nunderscores the potential of MAEs for advancing ultrasound signal analysis\nthrough scalable, self-supervised learning.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20622v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20622v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.333,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on self-supervised learning using Masked Autoencoders for ultrasound signals, where models are pre-trained on unlabeled synthetic data to learn representations for downstream tasks. While this approach reduces reliance on labeled data, similar to weak supervision's goal of using programmatically generated or noisy labels, the paper does not involve creating labels from high-level or imprecise sources. Instead, it relies on internal data reconstruction (e.g., predicting masked patches), making it only indirectly related to weak supervision concepts.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20623",
      "title": "AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View\n  Images",
      "authors": [
        "Shiqi Xin",
        "Xiaolin Zhang",
        "Yanbin Liu",
        "Peng Zhang",
        "Caifeng Shan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in Gaussian Splatting have significantly boosted the\nreconstruction of head avatars, enabling high-quality facial modeling by\nrepresenting an 3D avatar as a collection of 3D Gaussians. However, existing\nmethods predominantly rely on frontal-view images, leaving the back-head poorly\nconstructed. This leads to geometric inconsistencies, structural blurring, and\nreduced realism in the rear regions, ultimately limiting the fidelity of\nreconstructed avatars. To address this challenge, we propose AvatarBack, a\nnovel plug-and-play framework specifically designed to reconstruct complete and\nconsistent 3D Gaussian avatars by explicitly modeling the missing back-head\nregions. AvatarBack integrates two core technical innovations,i.e., the\nSubject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy\n(ASA). The former leverages a generative prior to synthesize\nidentity-consistent, plausible back-view pseudo-images from sparse frontal\ninputs, providing robust multi-view supervision. To achieve precise geometric\nalignment between these synthetic views and the 3D Gaussian representation, the\nlater employs learnable transformation matrices optimized during training,\neffectively resolving inherent pose and coordinate discrepancies. Extensive\nexperiments on NeRSemble and K-hairstyle datasets, evaluated using geometric,\nphotometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack\nsignificantly enhances back-head reconstruction quality while preserving\nfrontal fidelity. Moreover, the reconstructed avatars maintain consistent\nvisual realism under diverse motions and remain fully animatable.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20623v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20623v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.311,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20626",
      "title": "ArtFace: Towards Historical Portrait Face Identification via Model\n  Adaptation",
      "authors": [
        "Francois Poh",
        "Anjith George",
        "Sébastien Marcel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Identifying sitters in historical paintings is a key task for art historians,\noffering insight into their lives and how they chose to be seen. However, the\nprocess is often subjective and limited by the lack of data and stylistic\nvariations. Automated facial recognition is capable of handling challenging\nconditions and can assist, but while traditional facial recognition models\nperform well on photographs, they struggle with paintings due to domain shift\nand high intra-class variation. Artistic factors such as style, skill, intent,\nand influence from other works further complicate recognition. In this work, we\ninvestigate the potential of foundation models to improve facial recognition in\nartworks. By fine-tuning foundation models and integrating their embeddings\nwith those from conventional facial recognition networks, we demonstrate\nnotable improvements over current state-of-the-art methods. Our results show\nthat foundation models can bridge the gap where traditional methods are\nineffective. Paper page at https://www.idiap.ch/paper/artface/",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20626v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20626v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.295,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.309,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20637",
      "title": "GDS Agent: A Graph Algorithmic Reasoning Agent",
      "authors": [
        "Borun Shi",
        "Ioannis Panagiotas"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable multimodal information\nprocessing and reasoning ability. When equipped with tools through function\ncalling and enhanced with retrieval-augmented techniques, compound LLM-based\nsystems can access closed data sources and answer questions about them.\nHowever, they still struggle to process and reason over large-scale\ngraph-structure data. We introduce the GDS (Graph Data Science) agent in this\ntechnical report. The GDS agent introduces a comprehensive set of graph\nalgorithms as tools, together with preprocessing (retrieval) and postprocessing\nof algorithm results, in a model context protocol (MCP) server. The server can\nbe used with any modern LLM out-of-the-box. GDS agent allows users to ask any\nquestion that implicitly and intrinsically requires graph algorithmic reasoning\nabout their data, and quickly obtain accurate and grounded answers. We also\nintroduce a new benchmark that evaluates intermediate tool calls as well as\nfinal responses. The results indicate that GDS agent is able to solve a wide\nspectrum of graph tasks. We also provide detailed case studies for more\nopen-ended tasks and study scenarios where the agent struggles. Finally, we\ndiscuss the remaining challenges and the future roadmap.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20637v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20637v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.392,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a GDS Agent for graph algorithmic reasoning using LLMs and tools, focusing on retrieval-augmented techniques and graph processing. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The main contribution is unrelated to treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20640",
      "title": "CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via\n  Facial-Preserving Diffusion Models",
      "authors": [
        "Ayan Banerjee",
        "Fernando Vilariño",
        "Josep Lladós"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Preserving facial identity under extreme stylistic transformation remains a\nmajor challenge in generative art. In graffiti, a high-contrast, abstract\nmedium, subtle distortions to the eyes, nose, or mouth can erase the subject's\nrecognizability, undermining both personal and cultural authenticity. We\npresent CraftGraffiti, an end-to-end text-guided graffiti generation framework\ndesigned with facial feature preservation as a primary objective. Given an\ninput image and a style and pose descriptive prompt, CraftGraffiti first\napplies graffiti style transfer via LoRA-fine-tuned pretrained diffusion\ntransformer, then enforces identity fidelity through a face-consistent\nself-attention mechanism that augments attention layers with explicit identity\nembeddings. Pose customization is achieved without keypoints, using CLIP-guided\nprompt extension to enable dynamic re-posing while retaining facial coherence.\nWe formally justify and empirically validate the \"style-first, identity-after\"\nparadigm, showing it reduces attribute drift compared to the reverse order.\nQuantitative results demonstrate competitive facial feature consistency and\nstate-of-the-art aesthetic and human preference scores, while qualitative\nanalyses and a live deployment at the Cruilla Festival highlight the system's\nreal-world creative impact. CraftGraffiti advances the goal of\nidentity-respectful AI-assisted artistry, offering a principled approach for\nblending stylistic freedom with recognizability in creative AI applications.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20640v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20640v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.326,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models for iterative image generation and refinement in creating graffiti-style portraits, which involves a process similar to iterative refinement. However, it does not adapt this process for solving complex logical tasks or treating a 'Chain-of-Thought' as a holistic entity for correction. Instead, the focus is on artistic generation, style transfer, and facial preservation, lacking any component for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20655",
      "title": "Improving Alignment in LVLMs with Debiased Self-Judgment",
      "authors": [
        "Sihan Yang",
        "Chenhang Cui",
        "Zihao Zhao",
        "Yiyang Zhou",
        "Weilong Yan",
        "Ying Wei",
        "Huaxiu Yao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The rapid advancements in Large Language Models (LLMs) and Large\nVisual-Language Models (LVLMs) have opened up new opportunities for integrating\nvisual and linguistic modalities. However, effectively aligning these\nmodalities remains challenging, often leading to hallucinations--where\ngenerated outputs are not grounded in the visual input--and raising safety\nconcerns across various domains. Existing alignment methods, such as\ninstruction tuning and preference tuning, often rely on external datasets,\nhuman annotations, or complex post-processing, which limit scalability and\nincrease costs. To address these challenges, we propose a novel approach that\ngenerates the debiased self-judgment score, a self-evaluation metric created\ninternally by the model without relying on external resources. This enables the\nmodel to autonomously improve alignment. Our method enhances both decoding\nstrategies and preference tuning processes, resulting in reduced\nhallucinations, enhanced safety, and improved overall capability. Empirical\nresults show that our approach significantly outperforms traditional methods,\noffering a more effective solution for aligning LVLMs.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20655v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20655v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.51,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.351,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses preference tuning using an internal debiased self-judgment score, which may resemble aspects of RLHF in improving model alignment. However, it explicitly avoids human feedback or external resources, relying instead on self-generated evaluations, so it does not meet the core criteria of RLHF involving human-ranked data and a separate reward model.",
      "weak_supervision_justification": "The paper's use of an internally generated, debiased self-judgment score as a self-evaluation metric aligns with weak supervision by providing noisy or programmatic labels for tuning without relying on precise human annotations. This enables model improvement through internal signals, though it is not explicitly framed as weak supervision for training on large-scale noisy data.",
      "diffusion_reasoning_justification": "The paper focuses on generating and using a debiased self-judgment score for alignment in LVLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper addresses the challenges of aligning visual and linguistic modalities in Large Visual-Language Models (LVLMs), such as hallucinations and safety issues, by introducing a debiased self-judgment score that leverages the model's internal confidence without external resources. This method debias the score to guide decoding and preference tuning, resulting in reduced hallucinations, enhanced safety, and improved overall performance, with empirical experiments demonstrating superior results compared to traditional approaches.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining self-reflection techniques with debiasing for LVLMs, offering a new way to achieve autonomous alignment without external dependencies, though it builds on existing ideas in LLMs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in LVLMs and multimodal AI by providing a cost-effective alignment method, potentially leading to broader adoption in subfields focused on safety and efficiency.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality contribution with practical implications for improving LVLMs, making it essential for researchers in computer vision and language to consider for advancing alignment techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/391f6206350c70d6d8eab39704320b385bdd732d",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 10,
      "average_h_index": 2.857142857142857,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Sihan Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302782859"
        },
        {
          "name": "Chenhang Cui",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2249763944"
        },
        {
          "name": "Zihao Zhao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2372446314"
        },
        {
          "name": "Yiyang Zhou",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2250759613"
        },
        {
          "name": "Weilong Yan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377926919"
        },
        {
          "name": "Ying Wei",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375098604"
        },
        {
          "name": "Huaxiu Yao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378212884"
        }
      ]
    },
    {
      "id": "2508.20664",
      "title": "Task-Oriented Edge-Assisted Cross-System Design for Real-Time\n  Human-Robot Interaction in Industrial Metaverse",
      "authors": [
        "Kan Chen",
        "Zhen Meng",
        "Xiangmin Xu",
        "Jiaming Yang",
        "Emma Li",
        "Philip G. Zhao"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.GR (Graphics)"
      ],
      "abstract": "Real-time human-device interaction in industrial Metaverse faces challenges\nsuch as high computational load, limited bandwidth, and strict latency. This\npaper proposes a task-oriented edge-assisted cross-system framework using\ndigital twins (DTs) to enable responsive interactions. By predicting operator\nmotions, the system supports: 1) proactive Metaverse rendering for visual\nfeedback, and 2) preemptive control of remote devices. The DTs are decoupled\ninto two virtual functions-visual display and robotic control-optimizing both\nperformance and adaptability. To enhance generalizability, we introduce the\nHuman-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which\ndynamically adjusts prediction horizons. Evaluation on two tasks demonstrates\nthe framework's effectiveness: in a Trajectory-Based Drawing Control task, it\nreduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene\nrepresentation task for nuclear decommissioning, it achieves a PSNR of 22.11,\nSSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's\ncapability to ensure spatial precision and visual fidelity in real-time,\nhigh-risk industrial environments.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20664v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20664v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.418,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces HITL-MAML, which incorporates human-in-the-loop adaptation for prediction horizons, involving human feedback to adjust the model. However, it does not use a separate reward model trained on human-ranked data for fine-tuning via reinforcement learning, which is the core of RLHF. Thus, it only touches on human feedback in learning peripherally.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on edge-assisted frameworks for real-time interactions, including task offloading and cloud-edge collaboration, but does not discuss distributed training, parallel computing for ML model training, or strategies for partitioning data/computation across multiple nodes. It is centered on operational deployment, not training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20665",
      "title": "Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for\n  Symbolic Music",
      "authors": [
        "Hongju Su",
        "Ke Li",
        "Lan Yang",
        "Honggang Zhang",
        "Yi-Zhe Song"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Existing state-of-the-art symbolic music generation models predominantly\nadopt autoregressive or hierarchical autoregressive architectures, modelling\nsymbolic music as a sequence of attribute tokens with unidirectional temporal\ndependencies, under the assumption of a fixed, strict dependency structure\namong these attributes. However, we observe that using different attributes as\nthe initial token in these models leads to comparable performance. This\nsuggests that the attributes of a musical note are, in essence, a concurrent\nand unordered set, rather than a temporally dependent sequence. Based on this\ninsight, we introduce Amadeus, a novel symbolic music generation framework.\nAmadeus adopts a two-level architecture: an autoregressive model for note\nsequences and a bidirectional discrete diffusion model for attributes. To\nenhance performance, we propose Music Latent Space Discriminability Enhancement\nStrategy(MLSDES), incorporating contrastive learning constraints that amplify\ndiscriminability of intermediate music representations. The Conditional\nInformation Enhancement Module (CIEM) simultaneously strengthens note latent\nvector representation via attention mechanisms, enabling more precise note\ndecoding. We conduct extensive experiments on unconditional and\ntext-conditioned generation tasks. Amadeus significantly outperforms SOTA\nmodels across multiple metrics while achieving at least 4$\\times$ speed-up.\nFurthermore, we demonstrate training-free, fine-grained note attribute control\nfeasibility using our model. To explore the upper performance bound of the\nAmadeus architecture, we compile the largest open-source symbolic music dataset\nto date, AMD (Amadeus MIDI Dataset), supporting both pre-training and\nfine-tuning.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20665v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20665v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.297,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a bidirectional discrete diffusion model for generating and refining music attributes in symbolic music generation, involving iterative processes. However, this is applied to creative tasks like music production, not to solving complex logical tasks or refining a 'Chain-of-Thought' for reasoning. There is no component for multi-step logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20670",
      "title": "\"Humor, Art, or Misinformation?\": A Multimodal Dataset for Intent-Aware\n  Synthetic Image Detection",
      "authors": [
        "Anastasios Skoularikis",
        "Stefanos-Iordanis Papadopoulos",
        "Symeon Papadopoulos",
        "Panagiotis C. Petrantonakis"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Recent advances in multimodal AI have enabled progress in detecting synthetic\nand out-of-context content. However, existing efforts largely overlook the\nintent behind AI-generated images. To fill this gap, we introduce S-HArM, a\nmultimodal dataset for intent-aware classification, comprising 9,576 \"in the\nwild\" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,\nor Misinformation. Additionally, we explore three prompting strategies\n(image-guided, description-guided, and multimodally-guided) to construct a\nlarge-scale synthetic training dataset with Stable Diffusion. We conduct an\nextensive comparative study including modality fusion, contrastive learning,\nreconstruction networks, attention mechanisms, and large vision-language\nmodels. Our results show that models trained on image- and multimodally-guided\ndata generalize better to \"in the wild\" content, due to preserved visual\ncontext. However, overall performance remains limited, highlighting the\ncomplexity of inferring intent and the need for specialized architectures.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20670v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20670v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.335,
      "datasets_score": 0.468,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on creating a dataset for intent-aware classification of synthetic images and evaluating models using Stable Diffusion, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses Stable Diffusion for image generation to create synthetic training data, but it does not adapt diffusion models for multi-step logical reasoning or treat a Chain-of-Thought as an entity for iterative correction; it is solely for visual content generation.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and curation of the S-HArM dataset, including methodologies for collecting, annotating, and generating synthetic data, as well as benchmarking models on it, which directly aligns with research on dataset creation, analysis, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces S-HArM, a multimodal dataset comprising 9,576 \"in the wild\" image-text pairs from social media, labeled as Humor/Satire, Art, or Misinformation, to enable intent-aware classification of AI-generated images. The authors generate synthetic training data using Stable Diffusion with three prompting strategies and conduct a comparative study of various models, finding that image-guided and multimodally-guided approaches generalize better to real-world content, though overall performance is limited, highlighting the need for specialized multimodal architectures to infer subtle intents.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new dataset and prompting strategies for intent-aware synthetic image detection, combining existing multimodal AI techniques in a clever way to address a previously overlooked aspect of content classification. However, it builds on established methods like Stable Diffusion rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in multimodal AI and content moderation within subfields like computer vision and misinformation detection, as it provides a new dataset and insights into intent classification. Nonetheless, its applicability is somewhat limited to specific areas and may not broadly affect commercial applications immediately.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution by addressing an important gap in synthetic image detection through a new dataset and empirical study, making it essential for researchers in AI ethics and multimodal learning to be aware of. While not groundbreaking for all audiences, it provides useful insights that could guide future work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/347ebb813fb412af2b78af6e2db69761f9c7befd",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 15,
      "average_h_index": 7.25,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Anastasios Skoularikis",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377792895"
        },
        {
          "name": "Stefanos-Iordanis Papadopoulos",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/1725431945"
        },
        {
          "name": "Symeon Papadopoulos",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2237546962"
        },
        {
          "name": "P. Petrantonakis",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/2733886"
        }
      ]
    },
    {
      "id": "2508.20674",
      "title": "Bridging Minds and Machines: Toward an Integration of AI and Cognitive\n  Science",
      "authors": [
        "Rui Mao",
        "Qian Liu",
        "Xiao Li",
        "Erik Cambria",
        "Amir Hussain"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Cognitive Science has profoundly shaped disciplines such as Artificial\nIntelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and\nCulture. Many breakthroughs in AI trace their roots to cognitive theories,\nwhile AI itself has become an indispensable tool for advancing cognitive\nresearch. This reciprocal relationship motivates a comprehensive review of the\nintersections between AI and Cognitive Science. By synthesizing key\ncontributions from both perspectives, we observe that AI progress has largely\nemphasized practical task performance, whereas its cognitive foundations remain\nconceptually fragmented. We argue that the future of AI within Cognitive\nScience lies not only in improving performance but also in constructing systems\nthat deepen our understanding of the human mind. Promising directions include\naligning AI behaviors with cognitive frameworks, situating AI in embodiment and\nculture, developing personalized cognitive models, and rethinking AI ethics\nthrough cognitive co-evaluation.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20674v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20674v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.34,
      "datasets_score": 0.418,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "The paper focuses on a general review of AI and Cognitive Science intersections, emphasizing inspiration from cognitive theories and future directions like alignment with cognitive frameworks. It does not discuss or mention Reinforcement Learning from Human Feedback (RLHF), such as training models with human-ranked data for reward modeling and fine-tuning. There is no evidence of human feedback mechanisms in AI training as defined.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper reviews AI technologies inspired by cognitive science, including attention mechanisms and LLMs, but does not address diffusion-based models for multi-step logical reasoning. It lacks any description of iterative refinement processes, chain-of-thought as a holistic entity, or diffusion adaptations for complex tasks, focusing instead on broader AI-cognitive integrations.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper is a comprehensive review of AI and Cognitive Science relationships, synthesizing existing contributions without introducing, analyzing, benchmarking, or evaluating new datasets. It references general AI tools like LLMs for research but does not cover dataset creation, curation methodologies, or related evaluations as a main focus.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20688",
      "title": "Task Allocation for Autonomous Machines using Computational Intelligence\n  and Deep Reinforcement Learning",
      "authors": [
        "Thanh Thi Nguyen",
        "Quoc Viet Hung Nguyen",
        "Jonathan Kua",
        "Imran Razzak",
        "Dung Nguyen",
        "Saeid Nahavandi"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Enabling multiple autonomous machines to perform reliably requires the\ndevelopment of efficient cooperative control algorithms. This paper presents a\nsurvey of algorithms that have been developed for controlling and coordinating\nautonomous machines in complex environments. We especially focus on task\nallocation methods using computational intelligence (CI) and deep reinforcement\nlearning (RL). The advantages and disadvantages of the surveyed methods are\nanalysed thoroughly. We also propose and discuss in detail various future\nresearch directions that shed light on how to improve existing algorithms or\ncreate new methods to enhance the employability and performance of autonomous\nmachines in real-world applications. The findings indicate that CI and deep RL\nmethods provide viable approaches to addressing complex task allocation\nproblems in dynamic and uncertain environments. The recent development of deep\nRL has greatly contributed to the literature on controlling and coordinating\nautonomous machines, and it has become a growing trend in this area. It is\nenvisaged that this paper will provide researchers and engineers with a\ncomprehensive overview of progress in machine learning research related to\nautonomous machines. It also highlights underexplored areas, identifies\nemerging methodologies, and suggests new avenues for exploration in future\nresearch within this domain.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20688v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20688v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.435,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.4,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a survey of computational intelligence and deep reinforcement learning for task allocation in autonomous machines, emphasizing general deep RL applications without any mention of human feedback, reward models based on human-ranked data, or alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses task allocation for multiple autonomous machines, which involves coordination and potentially distributed systems (e.g., consensus-based algorithms), but it does not specifically address distributed training, parallel computing, or multi-node machine learning for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20691",
      "title": "MobileCLIP2: Improving Multi-Modal Reinforced Training",
      "authors": [
        "Fartash Faghri",
        "Pavan Kumar Anasosalu Vasu",
        "Cem Koc",
        "Vaishaal Shankar",
        "Alexander Toshev",
        "Oncel Tuzel",
        "Hadi Pouransari"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Foundation image-text models such as CLIP with zero-shot capabilities enable\na wide array of applications. MobileCLIP is a recent family of image-text\nmodels at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot\naccuracy. The main ingredients in MobileCLIP were its low-latency and light\narchitectures and a novel multi-modal reinforced training that made knowledge\ndistillation from multiple caption-generators and CLIP teachers efficient,\nscalable, and reproducible. In this paper, we improve the multi-modal\nreinforced training of MobileCLIP through: 1) better CLIP teacher ensembles\ntrained on the DFN dataset, 2) improved captioner teachers trained on the DFN\ndataset and fine-tuned on a diverse selection of high-quality image-caption\ndatasets. We discover new insights through ablations such as the importance of\ntemperature tuning in contrastive knowledge distillation, the effectiveness of\ncaption-generator fine-tuning for caption diversity, and the additive\nimprovement from combining synthetic captions generated by multiple models. We\ntrain a new family of models called MobileCLIP2 and achieve state-of-the-art\nImageNet-1k zero-shot accuracies at low latencies. In particular, we observe\n2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with\nMobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot\naccuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\\times$ smaller and\nimproves on DFN ViT-L/14 at 2.5$\\times$ lower latency. We release our\npretrained models (https://github.com/apple/ml-mobileclip) and the data\ngeneration code (https://github.com/apple/ml-mobileclip-dr). The data\ngeneration code makes it easy to create new reinforced datasets with arbitrary\nteachers using distributed scalable processing.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20691v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20691v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.473,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on multi-modal reinforced training for image-text models via knowledge distillation from teacher models, not on aligning AI with human preferences. There is no mention of human feedback, reward models, or reinforcement learning based on human-ranked data, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses improvements to image-text models and training techniques for zero-shot accuracy, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning. It lacks any components related to treating reasoning paths as entities for correction.",
      "distributed_training_justification": "The paper includes the release of code for generating reinforced datasets using arbitrary teacher models with distributed scalable processing, which involves parallel computing for data generation. However, this is not the main focus; the primary contributions are on model training improvements, making it only moderately relevant to distributed training concepts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled MobileCLIP2: Improving Multi-Modal Reinforced Training, builds on the MobileCLIP framework by enhancing its multi-modal reinforced training through better CLIP teacher ensembles and improved captioner teachers fine-tuned on diverse datasets. The authors conduct ablations to uncover insights such as the benefits of temperature tuning in contrastive knowledge distillation and the value of caption diversity, resulting in a new family of models that achieve state-of-the-art zero-shot accuracies on ImageNet-1k with lower latency and fewer parameters compared to prior models like SigLIP and DFN, while also releasing pretrained models and scalable code for dataset generation.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents notable improvements and new insights, such as enhanced teacher ensembles and ablation studies on training techniques, which cleverly build on existing MobileCLIP methods to address efficiency in multi-modal training, rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in efficient image-text models for mobile applications by providing better performance at lower latencies and releasing accessible code, though its impact may be confined to specific subfields like computer vision and AI on resource-constrained devices.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers high-quality enhancements and practical contributions to multi-modal training that are valuable for researchers in machine learning and computer vision, making it a significant but not essential read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0ca0e37e2db85c97ed26d2d9177d182ecac153fa",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 46,
      "average_h_index": 13.0,
      "notable_authors_count": 5,
      "author_h_indexes": [
        {
          "name": "Fartash Faghri",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/2978170"
        },
        {
          "name": "Pavan Kumar Anasosalu Vasu",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2111681666"
        },
        {
          "name": "Cem Koc",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2302802949"
        },
        {
          "name": "V. Shankar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362259031"
        },
        {
          "name": "Alexander Toshev",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2238621175"
        },
        {
          "name": "Oncel Tuzel",
          "h_index": 46,
          "profile_url": "https://www.semanticscholar.org/author/2577513"
        },
        {
          "name": "Hadi Pouransari",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2239094185"
        }
      ]
    },
    {
      "id": "2508.20700",
      "title": "Generative Annotation for ASR Named Entity Correction",
      "authors": [
        "Yuanchang Luo",
        "Daimeng Wei",
        "Shaojun Li",
        "Hengchao Shang",
        "Jiaxin Guo",
        "Zongyao Li",
        "Zhanglin Wu",
        "Xiaoyu Chen",
        "Zhiqiang Rao",
        "Jinlong Yang",
        "Hao Yang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "End-to-end automatic speech recognition systems often fail to transcribe\ndomain-specific named entities, causing catastrophic failures in downstream\ntasks. Numerous fast and lightweight named entity correction (NEC) models have\nbeen proposed in recent years. These models, mainly leveraging phonetic-level\nedit distance algorithms, have shown impressive performances. However, when the\nforms of the wrongly-transcribed words(s) and the ground-truth entity are\nsignificantly different, these methods often fail to locate the wrongly\ntranscribed words in hypothesis, thus limiting their usage. We propose a novel\nNEC method that utilizes speech sound features to retrieve candidate entities.\nWith speech sound features and candidate entities, we inovatively design a\ngenerative method to annotate entity errors in ASR transcripts and replace the\ntext with correct entities. This method is effective in scenarios of word form\ndifference. We test our method using open-source and self-constructed test\nsets. The results demonstrate that our NEC method can bring significant\nimprovement to entity accuracy. We will open source our self-constructed test\nset and training data.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20700v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20700v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.277,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper mentions the use of weak supervised data in the introduction as a factor that improves ASR performance, but this is not the main focus or contribution. The primary contribution is a novel method for named entity correction in ASR transcripts using generative annotation, rather than exploring or advancing weak supervision techniques themselves. Thus, weak supervision is only briefly referenced as background context.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20701",
      "title": "Transparent Semantic Spaces: A Categorical Approach to Explainable Word\n  Embeddings",
      "authors": [
        "Ares Fabregat-Hernández",
        "Javier Palanca",
        "Vicent Botti"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "math.CT (Category Theory)"
      ],
      "abstract": "The paper introduces a novel framework based on category theory to enhance\nthe explainability of artificial intelligence systems, particularly focusing on\nword embeddings. Key topics include the construction of categories\n$\\mathcal{L}_T$ and $\\mathcal{P}_T$, providing schematic representations of the\nsemantics of a text $ T $, and reframing the selection of the element with\nmaximum probability as a categorical notion. Additionally, the monoidal\ncategory $\\mathcal{P}_T$ is constructed to visualize various methods of\nextracting semantic information from $T$, offering a dimension-agnostic\ndefinition of semantic spaces reliant solely on information within the text.\n  Furthermore, the paper defines the categories of configurations Conf and word\nembeddings $\\mathcal{Emb}$, accompanied by the concept of divergence as a\ndecoration on $\\mathcal{Emb}$. It establishes a mathematically precise method\nfor comparing word embeddings, demonstrating the equivalence between the GloVe\nand Word2Vec algorithms and the metric MDS algorithm, transitioning from neural\nnetwork algorithms (black box) to a transparent framework. Finally, the paper\npresents a mathematical approach to computing biases before embedding and\noffers insights on mitigating biases at the semantic space level, advancing the\nfield of explainable artificial intelligence.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20701v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20701v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.323,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on category theory for explainable word embeddings, comparing algorithms like GloVe and Word2Vec, and addressing biases in NLP. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are central to diffusion-based reasoning. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20705",
      "title": "EEGDM: Learning EEG Representation with Latent Diffusion Model",
      "authors": [
        "Shaocong Wang",
        "Tong Liu",
        "Ming Li",
        "Minjing Yu",
        "Yong-Jin Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While electroencephalography (EEG) signal analysis using deep learning has\nshown great promise, existing approaches still face significant challenges in\nlearning generalizable representations that perform well across diverse tasks,\nparticularly when training data is limited. Current EEG representation learning\nmethods including EEGPT and LaBraM typically rely on simple masked\nreconstruction objective, which may not fully capture the rich semantic\ninformation and complex patterns inherent in EEG signals. In this paper, we\npropose EEGDM, a novel self-supervised EEG representation learning method based\non the latent diffusion model, which leverages EEG signal generation as a\nself-supervised objective, turning the diffusion model into a strong\nrepresentation learner capable of capturing EEG semantics. EEGDM incorporates\nan EEG encoder that distills EEG signals and their channel augmentations into a\ncompact representation, acting as conditional information to guide the\ndiffusion model for generating EEG signals. This design endows EEGDM with a\ncompact latent space, which not only offers ample control over the generative\nprocess but also can be leveraged for downstream tasks. Experimental results\nshow that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectively\nlearns robust representations, and (3) achieves competitive performance with\nmodest pre-training data size across diverse downstream tasks, underscoring its\ngeneralizability and practical utility.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20705v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20705v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.556,
      "distributed_training_score": 0.357,
      "datasets_score": 0.38,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using latent diffusion models for EEG signal generation and representation learning, specifically for self-supervised tasks in signal processing. It does not involve adapting the diffusion process for solving complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity. Since there is no component for logical reasoning, the paper does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20709",
      "title": "Learned Rate Control for Frame-Level Adaptive Neural Video Compression\n  via Dynamic Neural Network",
      "authors": [
        "Chenhao Zhang",
        "Wei Gao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Neural Video Compression (NVC) has achieved remarkable performance in recent\nyears. However, precise rate control remains a challenge due to the inherent\nlimitations of learning-based codecs. To solve this issue, we propose a dynamic\nvideo compression framework designed for variable bitrate scenarios. First, to\nachieve variable bitrate implementation, we propose the Dynamic-Route\nAutoencoder with variable coding routes, each occupying partial computational\ncomplexity of the whole network and navigating to a distinct RD trade-off.\nSecond, to approach the target bitrate, the Rate Control Agent estimates the\nbitrate of each route and adjusts the coding route of DRA at run time. To\nencompass a broad spectrum of variable bitrates while preserving overall RD\nperformance, we employ the Joint-Routes Optimization strategy, achieving\ncollaborative training of various routes. Extensive experiments on the HEVC and\nUVG datasets show that the proposed method achieves an average BD-Rate\nreduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods\nwhile maintaining an average bitrate error of 1.66%, achieving\nRate-Distortion-Complexity Optimization (RDCO) for various bitrate and\nbitrate-constrained applications. Our code is available at\nhttps://git.openi.org.cn/OpenAICoding/DynamicDVC.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20709v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20709v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.288,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.389,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20729",
      "title": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and\n  Revision",
      "authors": [
        "Ao Cheng",
        "Lei Zhang",
        "Guowei He"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) serve as an active and promising field of\ngenerative artificial intelligence and have demonstrated abilities to perform\ncomplex tasks in multiple domains, including mathematical and scientific\nreasoning. In this work, we construct a novel agent framework for solving\nrepresentative problems in scientific computing. The proposed agent,\nincorporating a \"rewriting-resolution-review-revision\" logical chain via three\nreasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,\nrespectively), is integrated in a collaborative and interactive manner. The\nConsultant module endows the agent with knowledge transfer capabilities to link\nproblems to professional domain insights, thereby rewriting problem\ndescriptions through text augmentation. The Programmer module is responsible\nfor generating and executing well-structured code to deliver the problem\nresolution. The Reviewer module equips the agent with the capacity for\nself-debugging and self-refinement through interactive feedback with code\nruntime outputs. By leveraging the end-to-end review mechanism, the executable\ncode provided by the Programmer attains the iterative revision. A comprehensive\nevaluation is conducted on the performance of the proposed agent framework in\nsolving PDEs, ill-conditioned linear systems, and data-driven physical analysis\nproblems. Compared to single-model, this collaborative framework significantly\nimproves the bug-free code generation rate and reduces the occurrence of\nnon-physical solutions, thereby establishing a highly reliable framework for\nautonomous code generation based on natural language descriptions. The review\nmechanism improved the average execution success (bug-free code and non-NaN\nsolutions) rate of the latest reasoning models. In summary, our agent framework\nestablishes automatic code generation and review as a promising scientific\ncomputing paradigm.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20729v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20729v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.386,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a multi-agent framework using LLMs for scientific computing, featuring a \"rewriting-resolution-review-revision\" process for iterative refinement and code generation. However, it does not involve or adapt the iterative refinement process of diffusion models, nor does it treat the Chain-of-Thought as a single entity for holistic correction in the manner described. The framework relies on agent collaboration and feedback loops, with no reference to diffusion-based reasoning techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20734",
      "title": "CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian\n  Recurrent Deep Network",
      "authors": [
        "Reza Akbari Movahed",
        "Abuzar Rezaee",
        "Arezoo Zakeri",
        "Colin Berry",
        "Edmond S. L. Ho",
        "Ali Gooya"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)\nimages is vital for assessing cardiac function and detecting its abnormalities.\nExisting methods often struggle to capture heart motion accurately because they\nrely on intensity-based image registration similarity losses that may overlook\ncardiac anatomical regions. To address this, we propose CardioMorphNet, a\nrecurrent Bayesian deep learning framework for 3D cardiac shape-guided\ndeformable registration using short-axis (SAX) CMR images. It employs a\nrecurrent variational autoencoder to model spatio-temporal dependencies over\nthe cardiac cycle and two posterior models for bi-ventricular segmentation and\nmotion estimation. The derived loss function from the Bayesian formulation\nguides the framework to focus on anatomical regions by recursively registering\nsegmentation maps without using intensity-based image registration similarity\nloss, while leveraging sequential SAX volumes and spatio-temporal features. The\nBayesian modelling also enables computation of uncertainty maps for the\nestimated motion fields. Validated on the UK Biobank dataset by comparing\nwarped mask shapes with ground truth masks, CardioMorphNet demonstrates\nsuperior performance in cardiac motion estimation, outperforming\nstate-of-the-art methods. Uncertainty assessment shows that it also yields\nlower uncertainty values for estimated motion fields in the cardiac region\ncompared with other probabilistic-based cardiac registration methods,\nindicating higher confidence in its predictions.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20734v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20734v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.265,
      "weak_supervision_score": 0.266,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.277,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20737",
      "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges,\n  and a Lightweight Interaction Protocol",
      "authors": [
        "Wei Ma",
        "Yixiao Yang",
        "Qiang Hu",
        "Shi Ying",
        "Zhi Jin",
        "Bo Du",
        "Zhenchang Xing",
        "Tianlin Li",
        "Junjie Shi",
        "Yang Liu",
        "Linxiao Jiang"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20737v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20737v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.371,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on testing and quality assurance frameworks for LLM applications, including evaluation methods like LLM-as-judge, but does not discuss training AI models using human feedback or reinforcement learning techniques. There is no mention of aligning models with human preferences through a reward model or fine-tuning via RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses testing challenges for LLM applications and mentions concepts like chain-of-thought reasoning, but it does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. There is no component related to diffusion-based methods for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20745",
      "title": "Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis\n  Classification",
      "authors": [
        "Kaustubh Atey",
        "Sameer Anand Jha",
        "Gouranga Bala",
        "Amit Sethi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Atypical mitotic figures (AMFs) are important histopathological markers yet\nremain challenging to identify consistently, particularly under domain shift\nstemming from scanner, stain, and acquisition differences. We present a simple\ntraining-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.\nThe approach (i) increases feature diversity via style perturbations inserted\nat early and mid backbone stages, (ii) aligns attention-refined features across\nsites using weak domain labels (Scanner, Origin, Species, Tumor) through an\nauxiliary alignment loss, and (iii) stabilizes predictions by distilling from\nan exponential moving average (EMA) teacher with temperature-scaled KL\ndivergence. On the organizer-run preliminary leaderboard for atypical mitosis\nclassification, our submission attains balanced accuracy of 0.8762, sensitivity\nof 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs\nnegligible inference-time overhead, relies only on coarse domain metadata, and\ndelivers strong, balanced performance, positioning it as a competitive\nsubmission for the MIDOG 2025 challenge.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20745v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20745v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.371,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20751",
      "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable\n  Text-to-Image Reinforcement Learning",
      "authors": [
        "Yibin Wang",
        "Zhimin Li",
        "Yuhang Zang",
        "Yujie Zhou",
        "Jiazi Bu",
        "Chunyu Wang",
        "Qinglin Lu",
        "Cheng Jin",
        "Jiaqi Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advancements highlight the importance of GRPO-based reinforcement\nlearning methods and benchmarking in enhancing text-to-image (T2I) generation.\nHowever, current methods using pointwise reward models (RM) for scoring\ngenerated images are susceptible to reward hacking. We reveal that this happens\nwhen minimal score differences between images are amplified after\nnormalization, creating illusory advantages that drive the model to\nover-optimize for trivial gains, ultimately destabilizing the image generation\nprocess. To address this, we propose Pref-GRPO, a pairwise preference\nreward-based GRPO method that shifts the optimization objective from score\nmaximization to preference fitting, ensuring more stable training. In\nPref-GRPO, images are pairwise compared within each group using preference RM,\nand the win rate is used as the reward signal. Extensive experiments\ndemonstrate that PREF-GRPO differentiates subtle image quality differences,\nproviding more stable advantages and mitigating reward hacking. Additionally,\nexisting T2I benchmarks are limited by coarse evaluation criteria, hindering\ncomprehensive model assessment. To solve this, we introduce UniGenBench, a\nunified T2I benchmark comprising 600 prompts across 5 main themes and 20\nsubthemes. It evaluates semantic consistency through 10 primary and 27\nsub-criteria, leveraging MLLM for benchmark construction and evaluation. Our\nbenchmarks uncover the strengths and weaknesses of both open and closed-source\nT2I models and validate the effectiveness of Pref-GRPO.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20751v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20751v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.518,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.373,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution, Pref-GRPO, uses a pairwise preference reward model derived from human preferences to guide reinforcement learning in text-to-image generation, directly aligning with RLHF by training on human-ranked data and fine-tuning the model accordingly.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for text-to-image generation and does not involve adapting diffusion models for multi-step logical reasoning or iterative refinement of reasoning paths; it only mentions diffusion in the context of T2I models without innovating on reasoning aspects.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses limitations in text-to-image (T2I) generation by introducing Pref-GRPO, a novel reinforcement learning method that replaces pointwise reward models with pairwise preference rewards to mitigate reward hacking and ensure stable training, while also proposing UniGenBench, a comprehensive benchmark with 600 prompts across diverse themes for fine-grained evaluation. Through extensive experiments, the authors demonstrate that Pref-GRPO provides more stable advantages, better discerns subtle image quality differences, and aligns more closely with human preferences, whereas UniGenBench effectively uncovers the strengths and weaknesses of various T2I models, particularly in areas like logical reasoning.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by reformulating GRPO optimization from pointwise reward maximization to pairwise preference fitting, significantly advancing the state-of-the-art in stable T2I reinforcement learning and addressing reward hacking effectively.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in T2I generation by providing a more stable training method and a detailed benchmark, potentially leading to citations and improvements within the subfield of computer vision, though its broader commercial impact may be limited.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution with practical innovations for T2I research, making it valuable for experts in reinforcement learning and computer vision to understand and potentially apply in their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e7197f0ff2e60c94c8009e1c9b0885be6e2b1c2e",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 24,
      "average_h_index": 4.333333333333333,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yibin Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376550824"
        },
        {
          "name": "Zhimin Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359266160"
        },
        {
          "name": "Yuhang Zang",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/12862495"
        },
        {
          "name": "Yujie Zhou",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2110334261"
        },
        {
          "name": "Jiazi Bu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2281827319"
        },
        {
          "name": "Chunyu Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359400153"
        },
        {
          "name": "Qinglin Lu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365376135"
        },
        {
          "name": "Cheng Jin",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2334445793"
        },
        {
          "name": "Jiaqi Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377814159"
        }
      ]
    },
    {
      "id": "2508.20754",
      "title": "${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale\n  Feature for Generalizable Gaussian Splatting",
      "authors": [
        "Yuxi Hu",
        "Jun Zhang",
        "Kuangyi Chen",
        "Zhe Zhang",
        "Friedrich Fraundorfer"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generalizable Gaussian Splatting aims to synthesize novel views for unseen\nscenes without per-scene optimization. In particular, recent advancements\nutilize feed-forward networks to predict per-pixel Gaussian parameters,\nenabling high-quality synthesis from sparse input views. However, existing\napproaches fall short in encoding discriminative, multi-view consistent\nfeatures for Gaussian predictions, which struggle to construct accurate\ngeometry with sparse views. To address this, we propose $\\mathbf{C}^{3}$-GS, a\nframework that enhances feature learning by incorporating context-aware,\ncross-dimension, and cross-scale constraints. Our architecture integrates three\nlightweight modules into a unified rendering pipeline, improving feature fusion\nand enabling photorealistic synthesis without requiring additional supervision.\nExtensive experiments on benchmark datasets validate that $\\mathbf{C}^{3}$-GS\nachieves state-of-the-art rendering quality and generalization ability. Code is\navailable at: https://github.com/YuhsiHu/C3-GS.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20754v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20754v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.347,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving Gaussian Splatting for novel view synthesis in computer vision, emphasizing feature learning and rendering from sparse views. It does not involve diffusion models, iterative refinement processes, or any adaptation for solving complex logical tasks or Chain-of-Thought reasoning. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20755",
      "title": "Provable Benefits of In-Tool Learning for Large Language Models",
      "authors": [
        "Sam Houliston",
        "Ambroise Odonnat",
        "Charles Arnal",
        "Vivien Cabannes"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Tool-augmented language models, equipped with retrieval, memory, or external\nAPIs, are reshaping AI, yet their theoretical advantages remain underexplored.\nIn this paper, we address this question by demonstrating the benefits of\nin-tool learning (external retrieval) over in-weight learning (memorization)\nfor factual recall. We show that the number of facts a model can memorize\nsolely in its weights is fundamentally limited by its parameter count. In\ncontrast, we prove that tool-use enables unbounded factual recall via a simple\nand efficient circuit construction. These results are validated in controlled\nexperiments, where tool-using models consistently outperform memorizing ones.\nWe further show that for pretrained large language models, teaching tool-use\nand general rules is more effective than finetuning facts into memory. Our work\nprovides both a theoretical and empirical foundation, establishing why\ntool-augmented workflows are not just practical, but provably more scalable.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20755v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20755v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.449,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.469,
      "distributed_training_score": 0.409,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on comparing in-tool learning versus in-weight learning for factual recall in LLMs, with theoretical and empirical analysis, but does not involve human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "The paper discusses training models for tool-use or memorization but does not address generating labels from noisy or programmatic sources, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "The paper examines tool-augmented learning and retrieval for factual recall, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion techniques.",
      "distributed_training_justification": "The paper's main contribution is on the theoretical and empirical benefits of tool-use in LLMs, not on distributed training methods, parallel computing, or partitioning computations across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20758",
      "title": "SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for\n  Zero-Shot 3D Visual Grounding",
      "authors": [
        "Jiawen Lin",
        "Shiran Bian",
        "Yihang Zhu",
        "Wenbin Tan",
        "Yachao Zhang",
        "Yuan Xie",
        "Yanyun Qu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using\nnatural language descriptions. Although supervised methods achieve higher\naccuracy in constrained settings, zero-shot 3DVG holds greater promise for\nreal-world applications since eliminating scene-specific training requirements.\nHowever, existing zero-shot methods face challenges of spatial-limited\nreasoning due to reliance on single-view localization, and contextual omissions\nor detail degradation. To address these issues, we propose SeqVLM, a novel\nzero-shot 3DVG framework that leverages multi-view real-world scene images with\nspatial information for target object reasoning. Specifically, SeqVLM first\ngenerates 3D instance proposals via a 3D semantic segmentation network and\nrefines them through semantic filtering, retaining only semantic-relevant\ncandidates. A proposal-guided multi-view projection strategy then projects\nthese candidate proposals onto real scene image sequences, preserving spatial\nrelationships and contextual details in the conversion process of 3D point\ncloud to images. Furthermore, to mitigate VLM computational overload, we\nimplement a dynamic scheduling mechanism that iteratively processes\nsequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to\nidentify textually specified objects. Experiments on the ScanRefer and Nr3D\nbenchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores\nof 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,\nrespectively, which advance 3DVG toward greater generalization and real-world\napplicability. The code is available at https://github.com/JiawLin/SeqVLM.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20758v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20758v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.351,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces SeqVLM, which uses an iterative reasoning mechanism with Visual Language Models (VLMs) for zero-shot 3D Visual Grounding. However, this mechanism focuses on dynamic scheduling for processing sequences and handling computational limits, without any reference to diffusion models or adapting iterative refinement processes from diffusion for logical tasks. The core contributions involve 3D proposals, multi-view projections, and VLM-based reasoning, which do not align with diffusion-based reasoning concepts.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20760",
      "title": "Occlusion Robustness of CLIP for Military Vehicle Classification",
      "authors": [
        "Jan Erik van Woerden",
        "Gertjan Burghouts",
        "Lotte Nijskens",
        "Alma M. Liezenga",
        "Sabina van Rooij",
        "Frank Ruis",
        "Hugo J. Kuijf"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vision-language models (VLMs) like CLIP enable zero-shot classification by\naligning images and text in a shared embedding space, offering advantages for\ndefense applications with scarce labeled data. However, CLIP's robustness in\nchallenging military environments, with partial occlusion and degraded\nsignal-to-noise ratio (SNR), remains underexplored. We investigate CLIP\nvariants' robustness to occlusion using a custom dataset of 18 military vehicle\nclasses and evaluate using Normalized Area Under the Curve (NAUC) across\nocclusion percentages. Four key insights emerge: (1) Transformer-based CLIP\nmodels consistently outperform CNNs, (2) fine-grained, dispersed occlusions\ndegrade performance more than larger contiguous occlusions, (3) despite\nimproved accuracy, performance of linear-probed models sharply drops at around\n35% occlusion, (4) by finetuning the model's backbone, this performance drop\noccurs at more than 60% occlusion. These results underscore the importance of\nocclusion-specific augmentations during training and the need for further\nexploration into patch-level sensitivity and architectural resilience for\nreal-world deployment of CLIP.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20760v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20760v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.337,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20762",
      "title": "SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and\n  Navigation Using Skip Stage Swin Transformer",
      "authors": [
        "Fachri Najm Noer Kartiman",
        "Rasim",
        "Yaya Wihardi",
        "Nurul Hasanah",
        "Oskar Natan",
        "Bambang Wahono",
        "Taufik Ibnu Salim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Focusing on the development of an end-to-end autonomous vehicle model with\npixel-to-pixel context awareness, this research proposes the SKGE-Swin\narchitecture. This architecture utilizes the Swin Transformer with a skip-stage\nmechanism to broaden feature representation globally and at various network\nlevels. This approach enables the model to extract information from distant\npixels by leveraging the Swin Transformer's Shifted Window-based Multi-head\nSelf-Attention (SW-MSA) mechanism and to retain critical information from the\ninitial to the final stages of feature extraction, thereby enhancing its\ncapability to comprehend complex patterns in the vehicle's surroundings. The\nmodel is evaluated on the CARLA platform using adversarial scenarios to\nsimulate real-world conditions. Experimental results demonstrate that the\nSKGE-Swin architecture achieves a superior Driving Score compared to previous\nmethods. Furthermore, an ablation study will be conducted to evaluate the\ncontribution of each architectural component, including the influence of skip\nconnections and the use of the Swin Transformer, in improving model\nperformance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20762v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20762v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.374,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing an end-to-end autonomous vehicle model using Swin Transformer with skip connections for waypoint prediction and navigation, emphasizing computer vision techniques like self-attention for feature extraction. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20765",
      "title": "Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for\n  Video Understanding",
      "authors": [
        "Gowreesh Mago",
        "Pascal Mettes",
        "Stevan Rudinac"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The automatic understanding of video content is advancing rapidly. Empowered\nby deeper neural networks and large datasets, machines are increasingly capable\nof understanding what is concretely visible in video frames, whether it be\nobjects, actions, events, or scenes. In comparison, humans retain a unique\nability to also look beyond concrete entities and recognize abstract concepts\nlike justice, freedom, and togetherness. Abstract concept recognition forms a\ncrucial open challenge in video understanding, where reasoning on multiple\nsemantic levels based on contextual information is key. In this paper, we argue\nthat the recent advances in foundation models make for an ideal setting to\naddress abstract concept understanding in videos. Automated understanding of\nhigh-level abstract concepts is imperative as it enables models to be more\naligned with human reasoning and values. In this survey, we study different\ntasks and datasets used to understand abstract concepts in video content. We\nobserve that, periodically and over a long period, researchers have attempted\nto solve these tasks, making the best use of the tools available at their\ndisposal. We advocate that drawing on decades of community experience will help\nus shed light on this important open grand challenge and avoid ``re-inventing\nthe wheel'' as we start revisiting it in the era of multi-modal foundation\nmodels.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20765v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20765v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.376,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper surveys methods for abstract concept recognition in videos, noting challenges with subjective datasets that may involve noisy or imprecise labels, but it does not explicitly discuss or contribute to weak supervision techniques, such as programmatically generating training labels.",
      "diffusion_reasoning_justification": "The paper focuses on the evolution of methods for video understanding, including foundation models, but does not mention diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20766",
      "title": "Turning the Spell Around: Lightweight Alignment Amplification via\n  Rank-One Safety Injection",
      "authors": [
        "Harethah Abu Shairah",
        "Hasan Abed Al Kader Hammoud",
        "George Turkiyyah",
        "Bernard Ghanem"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Safety alignment in Large Language Models (LLMs) often involves mediating\ninternal representations to refuse harmful requests. Recent research has\ndemonstrated that these safety mechanisms can be bypassed by ablating or\nremoving specific representational directions within the model. In this paper,\nwe propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box\nmethod that amplifies a model's safety alignment by permanently steering its\nactivations toward the refusal-mediating subspace. ROSI operates as a simple,\nfine-tuning-free rank-one weight modification applied to all residual stream\nwrite matrices. The required safety direction can be computed from a small set\nof harmful and harmless instruction pairs. We show that ROSI consistently\nincreases safety refusal rates - as evaluated by Llama Guard 3 - while\npreserving the utility of the model on standard benchmarks such as MMLU,\nHellaSwag, and Arc. Furthermore, we show that ROSI can also re-align\n'uncensored' models by amplifying their own latent safety directions,\ndemonstrating its utility as an effective last-mile safety procedure. Our\nresults suggest that targeted, interpretable weight steering is a cheap and\npotent mechanism to improve LLM safety, complementing more resource-intensive\nfine-tuning paradigms.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20766v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20766v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.526,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.371,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions safety alignment techniques like RLHF in the introduction as existing methods, but its main contribution, Rank-One Safety Injection (ROSI), is a fine-tuning-free weight modification that does not involve human feedback, reward models, or reinforcement learning. It only indirectly relates through the broader context of alignment.",
      "weak_supervision_justification": "The paper uses a small set of harmful and harmless instruction pairs to derive a safety direction for weight editing, but this is not weak supervision, as it does not involve programmatically generating labels for model training. Instead, it is a direct computational method without relying on noisy or imprecise label sources for learning.",
      "diffusion_reasoning_justification": "The paper's focus is on amplifying safety in LLMs through rank-one weight modifications, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It deals solely with safety alignment mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20771",
      "title": "Signs of Struggle: Spotting Cognitive Distortions across Language and\n  Register",
      "authors": [
        "Abhishek Kuber",
        "Enrico Liscio",
        "Ruixuan Zhang",
        "Caroline Figueroa",
        "Pradeep K. Murukannaiah"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Rising mental health issues among youth have increased interest in automated\napproaches for detecting early signs of psychological distress in digital text.\nOne key focus is the identification of cognitive distortions, irrational\nthought patterns that have a role in aggravating mental distress. Early\ndetection of these distortions may enable timely, low-cost interventions. While\nprior work has focused on English clinical data, we present the first in-depth\nstudy of cross-lingual and cross-register generalization of cognitive\ndistortion detection, analyzing forum posts written by Dutch adolescents. Our\nfindings show that while changes in language and writing style can\nsignificantly affect model performance, domain adaptation methods show the most\npromise.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20771v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20771v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.34,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the detection of cognitive distortions in text data, focusing on cross-lingual and cross-register generalization using methods like prompting, supervised learning, and domain adaptation. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, there is no connection to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20773",
      "title": "Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI",
      "authors": [
        "Christoforos N. Spartalis",
        "Theodoros Semertzidis",
        "Petros Daras",
        "Efstratios Gavves"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce SAFEMax, a novel method for Machine Unlearning in diffusion\nmodels. Grounded in information-theoretic principles, SAFEMax maximizes the\nentropy in generated images, causing the model to generate Gaussian noise when\nconditioned on impermissible classes by ultimately halting its denoising\nprocess. Also, our method controls the balance between forgetting and retention\nby selectively focusing on the early diffusion steps, where class-specific\ninformation is prominent. Our results demonstrate the effectiveness of SAFEMax\nand highlight its substantial efficiency gains over state-of-the-art methods.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20773v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20773v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.445,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.364,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on machine unlearning in diffusion models, specifically introducing SAFEMax to forget impermissible classes by maximizing entropy. It does not involve training models with programmatically generated labels, noisy sources, or any form of weak supervision; instead, it deals with modifying pre-trained models, making it unrelated to weak supervision concepts.",
      "diffusion_reasoning_justification": "The paper applies diffusion models for machine unlearning, emphasizing entropy maximization to halt denoising processes and forget specific classes. However, it does not involve multi-step logical reasoning, iterative refinement of a chain-of-thought, or solving complex logical tasks; it is solely about generative image unlearning, lacking any components of diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20776",
      "title": "Safer Skin Lesion Classification with Global Class Activation\n  Probability Map Evaluation and SafeML",
      "authors": [
        "Kuniko Paxton",
        "Koorosh Aslansefat",
        "Amila Akagić",
        "Dhavalkumar Thakker",
        "Yiannis Papadopoulos"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advancements in skin lesion classification models have significantly\nimproved accuracy, with some models even surpassing dermatologists' diagnostic\nperformance. However, in medical practice, distrust in AI models remains a\nchallenge. Beyond high accuracy, trustworthy, explainable diagnoses are\nessential. Existing explainability methods have reliability issues, with\nLIME-based methods suffering from inconsistency, while CAM-based methods\nfailing to consider all classes. To address these limitations, we propose\nGlobal Class Activation Probabilistic Map Evaluation, a method that analyses\nall classes' activation probability maps probabilistically and at a pixel\nlevel. By visualizing the diagnostic process in a unified manner, it helps\nreduce the risk of misdiagnosis. Furthermore, the application of SafeML\nenhances the detection of false diagnoses and issues warnings to doctors and\npatients as needed, improving diagnostic reliability and ultimately patient\nsafety. We evaluated our method using the ISIC datasets with MobileNetV2 and\nVision Transformers.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20776v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20776v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.313,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20783",
      "title": "Evaluating Compositional Generalisation in VLMs and Diffusion Models",
      "authors": [
        "Beth Pearson",
        "Bilal Boulbarss",
        "Michael Wray",
        "Martha Lewis"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "A fundamental aspect of the semantics of natural language is that novel\nmeanings can be formed from the composition of previously known parts.\nVision-language models (VLMs) have made significant progress in recent years,\nhowever, there is evidence that they are unable to perform this kind of\ncomposition. For example, given an image of a red cube and a blue cylinder, a\nVLM such as CLIP is likely to incorrectly label the image as a red cylinder or\na blue cube, indicating it represents the image as a `bag-of-words' and fails\nto capture compositional semantics. Diffusion models have recently gained\nsignificant attention for their impressive generative abilities, and zero-shot\nclassifiers based on diffusion models have been shown to perform competitively\nwith CLIP in certain compositional tasks. In this work we explore whether the\ngenerative Diffusion Classifier has improved compositional generalisation\nabilities compared to discriminative models. We assess three models --\nDiffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with\nattributes and relations in both zero-shot learning (ZSL) and generalised\nzero-shot learning (GZSL) settings. Our results show that the Diffusion\nClassifier and ViLT perform well at concept binding tasks, but that all models\nstruggle significantly with the relational GZSL task, underscoring the broader\nchallenges VLMs face with relational reasoning. Analysis of CLIP embeddings\nsuggests that the difficulty may stem from overly similar representations of\nrelational concepts such as left and right. Code and dataset are available at:\nhttps://github.com/otmive/diffusion_classifier_clip",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20783v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20783v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.591,
      "distributed_training_score": 0.337,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates Diffusion Classifier, derived from Stable Diffusion, for compositional generalization in vision-language tasks, such as attribute-object binding and relational reasoning. However, it focuses on classification performance in zero-shot and generalized zero-shot settings, rather than adapting the iterative refinement process of diffusion models for multi-step logical reasoning or Chain-of-Thought processes. While diffusion models are involved, the paper does not explore their use in holistically correcting reasoning paths, making it only tangentially related to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20784",
      "title": "Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control",
      "authors": [
        "Yifan Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Bus bunching remains a challenge for urban transit due to stochastic traffic\nand passenger demand. Traditional solutions rely on multi-agent reinforcement\nlearning (MARL) in loop-line settings, which overlook realistic operations\ncharacterized by heterogeneous routes, timetables, fluctuating demand, and\nvarying fleet sizes. We propose a novel single-agent reinforcement learning\n(RL) framework for bus holding control that avoids the data imbalance and\nconvergence issues of MARL under near-realistic simulation. A bidirectional\ntimetabled network with dynamic passenger demand is constructed. The key\ninnovation is reformulating the multi-agent problem into a single-agent one by\naugmenting the state space with categorical identifiers (vehicle ID, station\nID, time period) in addition to numerical features (headway, occupancy,\nvelocity). This high-dimensional encoding enables single-agent policies to\ncapture inter-agent dependencies, analogous to projecting non-separable inputs\ninto a higher-dimensional space. We further design a structured reward function\naligned with operational goals: instead of exponential penalties on headway\ndeviations, a ridge-shaped reward balances uniform headways and schedule\nadherence. Experiments show that our modified soft actor-critic (SAC) achieves\nmore stable and superior performance than benchmarks, including MADDPG (e.g.,\n-430k vs. -530k under stochastic conditions). These results demonstrate that\nsingle-agent deep RL, when enhanced with categorical structuring and\nschedule-aware rewards, can effectively manage bus holding in non-loop,\nreal-world contexts. This paradigm offers a robust, scalable alternative to\nMARL frameworks, particularly where agent-specific experiences are imbalanced.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20784v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20784v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.341,
      "datasets_score": 0.262,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20789",
      "title": "Surfel-based 3D Registration with Equivariant SE(3) Features",
      "authors": [
        "Xueyang Kang",
        "Hang Zhao",
        "Kourosh Khoshelham",
        "Patrick Vandewalle"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Point cloud registration is crucial for ensuring 3D alignment consistency of\nmultiple local point clouds in 3D reconstruction for remote sensing or digital\nheritage. While various point cloud-based registration methods exist, both\nnon-learning and learning-based, they ignore point orientations and point\nuncertainties, making the model susceptible to noisy input and aggressive\nrotations of the input point cloud like orthogonal transformation; thus, it\nnecessitates extensive training point clouds with transformation augmentations.\nTo address these issues, we propose a novel surfel-based pose learning\nregression approach. Our method can initialize surfels from Lidar point cloud\nusing virtual perspective camera parameters, and learns explicit\n$\\mathbf{SE(3)}$ equivariant features, including both position and rotation\nthrough $\\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative\ntransformation between source and target scans. The model comprises an\nequivariant convolutional encoder, a cross-attention mechanism for similarity\ncomputation, a fully-connected decoder, and a non-linear Huber loss.\nExperimental results on indoor and outdoor datasets demonstrate our model\nsuperiority and robust performance on real point-cloud scans compared to\nstate-of-the-art methods.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20789v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20789v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.317,
      "distributed_training_score": 0.319,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20796",
      "title": "Speech Emotion Recognition via Entropy-Aware Score Selection",
      "authors": [
        "ChenYi Chua",
        "JunKai Wong",
        "Chengxin Chen",
        "Xiaoxiao Miao"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we propose a multimodal framework for speech emotion\nrecognition that leverages entropy-aware score selection to combine speech and\ntextual predictions. The proposed method integrates a primary pipeline that\nconsists of an acoustic model based on wav2vec2.0 and a secondary pipeline that\nconsists of a sentiment analysis model using RoBERTa-XLM, with transcriptions\ngenerated via Whisper-large-v3. We propose a late score fusion approach based\non entropy and varentropy thresholds to overcome the confidence constraints of\nprimary pipeline predictions. A sentiment mapping strategy translates three\nsentiment categories into four target emotion classes, enabling coherent\nintegration of multimodal predictions. The results on the IEMOCAP and\nMSP-IMPROV datasets show that the proposed method offers a practical and\nreliable enhancement over traditional single-modality systems.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20796v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20796v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.279,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a multimodal framework for speech emotion recognition using entropy-aware score selection, involving models like wav2vec2.0 and RoBERTa-XLM for fusing speech and text predictions. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20805",
      "title": "Exploring Machine Learning and Language Models for Multimodal Depression\n  Detection",
      "authors": [
        "Javier Si Zhao Hong",
        "Timothy Zoe Delaya",
        "Sherwyn Chan Yin Kit",
        "Pai Chet Ng",
        "Xiaoxiao Miao"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.SD (Sound)"
      ],
      "abstract": "This paper presents our approach to the first Multimodal Personality-Aware\nDepression Detection Challenge, focusing on multimodal depression detection\nusing machine learning and deep learning models. We explore and compare the\nperformance of XGBoost, transformer-based architectures, and large language\nmodels (LLMs) on audio, video, and text features. Our results highlight the\nstrengths and limitations of each type of model in capturing depression-related\nsignals across modalities, offering insights into effective multimodal\nrepresentation strategies for mental health prediction.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20805v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20805v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.382,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating XGBoost, transformer-based models, and LLMs for multimodal depression detection, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not adapt diffusion techniques for any tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces and analyzes the MPDD dataset, detailing its multimodal features (audio, video, text), annotations (PHQ-9, personality traits, demographics), and class distributions through a table. It also benchmarks models on this dataset, aligning with dataset creation, analysis, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper evaluates and compares the performance of XGBoost, transformer-based architectures, and large language models for detecting depression using multimodal data from the Multimodal Personality-Aware Depression Detection (MPDD) dataset, which includes audio, video, and text features. The study highlights the strengths and limitations of each model in capturing depression signals across modalities, providing insights into effective strategies for mental health prediction while addressing the challenges of traditional methods and leveraging a richly annotated dataset for more accurate and inclusive modeling.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by systematically comparing existing models on a new dataset for multimodal depression detection, offering a clever combination of techniques to address a known problem in mental health AI. However, it does not introduce a truly novel architecture or technique, making it an incremental advancement rather than a significant state-of-the-art breakthrough.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in multimodal AI for mental health by providing comparative insights and strategies, potentially leading to better detection systems in specific subfields. Nonetheless, its applicability may be limited to niche areas like depression detection challenges, reducing its broader commercial or widespread impact.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution through its comparative analysis and insights into multimodal depression detection, making it important for researchers in AI and mental health. While not essential for all, it provides useful knowledge that could inform ongoing work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/37b46f221f951c4a5f898928f4e13353548c5aaa",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Javier Si Zhao Hong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377899444"
        },
        {
          "name": "Timothy Zoe Delaya",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377796601"
        },
        {
          "name": "Sherwyn Chan Yin Kit",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377796205"
        },
        {
          "name": "Pai Chet Ng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377779333"
        },
        {
          "name": "Xiaoxiao Miao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377799252"
        }
      ]
    },
    {
      "id": "2508.20810",
      "title": "A Graph-Based Test-Harness for LLM Evaluation",
      "authors": [
        "Jessica Lundin",
        "Guillaume Chabot-Couture"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We present a first known prototype of a dynamic, systematic benchmark of\nmedical guidelines for 400+ questions, with 3.3+ trillion possible\ncombinations, covering 100\\% of guideline relationships. We transformed the WHO\nIMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,\ntreatments, follow-ups, severities) and 300+ edges, then used graph traversal\nto generate questions that incorporated age-specific scenarios and contextual\ndistractors to ensure clinical relevance. Our graph-based approach enables\nsystematic evaluation across clinical tasks (45-67\\% accuracy), and we find\nmodels excel at symptom recognition but struggle with triaging severity,\ntreatment protocols and follow-up care, demonstrating how customized benchmarks\ncan identify specific capability gaps that general-domain evaluations miss.\nBeyond evaluation, this dynamic MCQA methodology enhances LLM post-training\n(supervised finetuning, GRPO, DPO), where correct answers provide high-reward\nsamples without expensive human annotation. The graph-based approach\nsuccessfully addresses the coverage limitations of manually curated benchmarks.\nThis methodology is a step toward scalable, contamination-resistant solution\nfor creating comprehensive benchmarks that can be dynamically generated,\nincluding when the guidelines are updated. Code and datasets are available at\nhttps://github.com/jessicalundin/graph_testing_harness",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20810v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20810v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.323,
      "datasets_score": 0.412,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a graph-based approach for generating and evaluating MCQA benchmarks using graph traversal, without any mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for correction. It does not involve multi-step logical reasoning via diffusion techniques, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and evaluating a new benchmark dataset for LLM assessment, including a graph-based framework for auto-generating 400+ MCQA questions from medical guidelines, ensuring comprehensive coverage, and providing baseline evaluations. This directly aligns with research on dataset creation, curation methodologies, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper presents a graph-based test-harness for evaluating large language models (LLMs) on medical guidelines, specifically by transforming the WHO IMCI handbook into a directed graph with over 200 nodes and 300 edges to generate more than 400 multiple-choice questions covering all guideline relationships. The methodology uses graph traversal to create age-specific scenarios and distractors, revealing that LLMs achieve 45-67% accuracy, performing well in symptom recognition but struggling with severity triaging, treatment protocols, and follow-ups, while also enabling efficient post-training enhancements and addressing the limitations of static benchmarks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper introduces a clever combination of graph-based question generation with existing medical QA techniques, providing a notable improvement in ensuring comprehensive coverage of guidelines, though it builds on prior graph and synthetic dataset methods rather than introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in domain-specific LLM evaluation and benchmark creation, particularly in healthcare, by offering a scalable, dynamic approach that could be adopted or extended in subfields, though its impact may remain confined to specialized applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a valuable, practical contribution to AI evaluation in medical contexts with its innovative methodology and findings on LLM capabilities, making it essential for researchers in healthcare AI to be aware of, though not groundbreaking enough for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2d5c934b1bb522cf39670f035165cd510b620698",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jessica Lundin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377790025"
        },
        {
          "name": "Guillaume Chabot-Couture",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2311655215"
        }
      ]
    },
    {
      "id": "2508.20812",
      "title": "Uncertainty Aware-Predictive Control Barrier Functions: Safer Human\n  Robot Interaction through Probabilistic Motion Forecasting",
      "authors": [
        "Lorenzo Busellato",
        "Federico Cunico",
        "Diego Dall'Alba",
        "Marco Emporio",
        "Andrea Giachetti",
        "Riccardo Muradore",
        "Marco Cristani"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "To enable flexible, high-throughput automation in settings where people and\nrobots share workspaces, collaborative robotic cells must reconcile stringent\nsafety guarantees with the need for responsive and effective behavior. A\ndynamic obstacle is the stochastic, task-dependent variability of human motion:\nwhen robots fall back on purely reactive or worst-case envelopes, they brake\nunnecessarily, stall task progress, and tamper with the fluidity that true\nHuman-Robot Interaction demands. In recent years, learning-based human-motion\nprediction has rapidly advanced, although most approaches produce worst-case\nscenario forecasts that often do not treat prediction uncertainty in a\nwell-structured way, resulting in over-conservative planning algorithms,\nlimiting their flexibility. We introduce Uncertainty-Aware Predictive Control\nBarrier Functions (UA-PCBFs), a unified framework that fuses probabilistic\nhuman hand motion forecasting with the formal safety guarantees of Control\nBarrier Functions. In contrast to other variants, our framework allows for\ndynamic adjustment of the safety margin thanks to the human motion uncertainty\nestimation provided by a forecasting module. Thanks to uncertainty estimation,\nUA-PCBFs empower collaborative robots with a deeper understanding of future\nhuman states, facilitating more fluid and intelligent interactions through\ninformed motion planning. We validate UA-PCBFs through comprehensive real-world\nexperiments with an increasing level of realism, including automated setups (to\nperform exactly repeatable motions) with a robotic hand and direct human-robot\ninteractions (to validate promptness, usability, and human confidence).\nRelative to state-of-the-art HRI architectures, UA-PCBFs show better\nperformance in task-critical metrics, significantly reducing the number of\nviolations of the robot's safe space during interaction with respect to the\nstate-of-the-art.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20812v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20812v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.314,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of Uncertainty-Aware Predictive Control Barrier Functions (UA-PCBFs) for safer human-robot interaction, using probabilistic motion forecasting via a deep learning model. It does not involve reinforcement learning, human feedback for training models, reward models, or fine-tuning based on human-ranked data. Instead, it focuses on control strategies and motion prediction, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20813",
      "title": "Adapting Foundation Model for Dental Caries Detection with Dual-View\n  Co-Training",
      "authors": [
        "Tao Luo",
        "Han Wu",
        "Tong Yang",
        "Dinggang Shen",
        "Zhiming Cui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate dental caries detection from panoramic X-rays plays a pivotal role\nin preventing lesion progression. However, current detection methods often\nyield suboptimal accuracy due to subtle contrast variations and diverse lesion\nmorphology of dental caries. In this work, inspired by the clinical workflow\nwhere dentists systematically combine whole-image screening with detailed\ntooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training\nnetwork for accurate dental caries detection. Our DVCTNet starts with employing\nautomated tooth detection to establish two complementary views: a global view\nfrom panoramic X-ray images and a local view from cropped tooth images. We then\npretrain two vision foundation models separately on the two views. The\nglobal-view foundation model serves as the detection backbone, generating\nregion proposals and global features, while the local-view model extracts\ndetailed features from corresponding cropped tooth patches matched by the\nregion proposals. To effectively integrate information from both views, we\nintroduce a Gated Cross-View Attention (GCV-Atten) module that dynamically\nfuses dual-view features, enhancing the detection pipeline by integrating the\nfused features back into the detection model for final caries detection. To\nrigorously evaluate our DVCTNet, we test it on a public dataset and further\nvalidate its performance on a newly curated, high-precision dental caries\ndetection dataset, annotated using both intra-oral images and panoramic X-rays\nfor double verification. Experimental results demonstrate DVCTNet's superior\nperformance against existing state-of-the-art (SOTA) methods on both datasets,\nindicating the clinical applicability of our method. Our code and labeled\ndataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20813v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20813v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.366,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20816",
      "title": "Multi-Agent Penetration Testing AI for the Web",
      "authors": [
        "Isaac David",
        "Arthur Gervais"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "AI-powered development platforms are making software creation accessible to a\nbroader audience, but this democratization has triggered a scalability crisis\nin security auditing. With studies showing that up to 40% of AI-generated code\ncontains vulnerabilities, the pace of development now vastly outstrips the\ncapacity for thorough security assessment.\n  We present MAPTA, a multi-agent system for autonomous web application\nsecurity assessment that combines large language model orchestration with\ntool-grounded execution and end-to-end exploit validation. On the 104-challenge\nXBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance\non SSRF and misconfiguration vulnerabilities, 83% success on broken\nauthorization, and strong results on injection attacks including server-side\ntemplate injection (85%) and SQL injection (83%). Cross-site scripting (57%)\nand blind SQL injection (0%) remain challenging. Our comprehensive cost\nanalysis across all challenges totals $21.38 with a median cost of $0.073 for\nsuccessful attempts versus $0.357 for failures. Success correlates strongly\nwith resource efficiency, enabling practical early-stopping thresholds at\napproximately 40 tool calls or $0.30 per challenge.\n  MAPTA's real-world findings are impactful given both the popularity of the\nrespective scanned GitHub repositories (8K-70K stars) and MAPTA's low average\noperating cost of $3.67 per open-source assessment: MAPTA discovered critical\nvulnerabilities including RCEs, command injections, secret exposure, and\narbitrary file write vulnerabilities. Findings are responsibly disclosed, 10\nfindings are under CVE review.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20816v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20816v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.361,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20817",
      "title": "FusionCounting: Robust visible-infrared image fusion guided by crowd\n  counting via multi-task learning",
      "authors": [
        "He Li",
        "Xinyu Liu",
        "Weihang Kong",
        "Xingchen Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visible and infrared image fusion (VIF) is an important multimedia task in\ncomputer vision. Most VIF methods focus primarily on optimizing fused image\nquality. Recent studies have begun incorporating downstream tasks, such as\nsemantic segmentation and object detection, to provide semantic guidance for\nVIF. However, semantic segmentation requires extensive annotations, while\nobject detection, despite reducing annotation efforts compared with\nsegmentation, faces challenges in highly crowded scenes due to overlapping\nbounding boxes and occlusion. Moreover, although RGB-T crowd counting has\ngained increasing attention in recent years, no studies have integrated VIF and\ncrowd counting into a unified framework. To address these challenges, we\npropose FusionCounting, a novel multi-task learning framework that integrates\ncrowd counting into the VIF process. Crowd counting provides a direct\nquantitative measure of population density with minimal annotation, making it\nparticularly suitable for dense scenes. Our framework leverages both input\nimages and population density information in a mutually beneficial multi-task\ndesign. To accelerate convergence and balance tasks contributions, we introduce\na dynamic loss function weighting strategy. Furthermore, we incorporate\nadversarial training to enhance the robustness of both VIF and crowd counting,\nimproving the model's stability and resilience to adversarial attacks.\nExperimental results on public datasets demonstrate that FusionCounting not\nonly enhances image fusion quality but also achieves superior crowd counting\nperformance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20817v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20817v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.346,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20830",
      "title": "Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models\n  with Low-Rank Adaptation",
      "authors": [
        "Krit Duangprom",
        "Tryphon Lambrou",
        "Binod Bhattarai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper presents a novel pipeline for 2D keypoint estima- tion of surgical\ntools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank\nadjusting (LoRA) technique. Unlike traditional Convolutional Neural Network\n(CNN) or Transformer-based approaches, which often suffer from overfitting in\nsmall-scale medical datasets, our method harnesses the generalization\ncapabilities of pre-trained VLMs. We carefully design prompts to create an\ninstruction-tuning dataset and use them to align visual features with semantic\nkeypoint descriptions. Experimental results show that with only two epochs of\nfine tuning, the adapted VLM outperforms the baseline models, demonstrating the\nef- fectiveness of LoRA in low-resource scenarios. This approach not only\nimproves keypoint detection performance, but also paves the way for future work\nin 3D surgical hands and tools pose estimation.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20830v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20830v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.327,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20835",
      "title": "PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for\n  Point Cloud Classification",
      "authors": [
        "Hao Yang",
        "Qianyu Zhou",
        "Haijia Sun",
        "Xiangtai Li",
        "Xuequan Lu",
        "Lizhuang Ma",
        "Shuicheng Yan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Domain Generalization (DG) has been recently explored to enhance the\ngeneralizability of Point Cloud Classification (PCC) models toward unseen\ndomains. Prior works are based on convolutional networks, Transformer or Mamba\narchitectures, either suffering from limited receptive fields or high\ncomputational cost, or insufficient long-range dependency modeling. RWKV, as an\nemerging architecture, possesses superior linear complexity, global receptive\nfields, and long-range dependency. In this paper, we present the first work\nthat studies the generalizability of RWKV models in DG PCC. We find that\ndirectly applying RWKV to DG PCC encounters two significant challenges: RWKV's\nfixed direction token shift methods, like Q-Shift, introduce spatial\ndistortions when applied to unstructured point clouds, weakening local\ngeometric modeling and reducing robustness. In addition, the Bi-WKV attention\nin RWKV amplifies slight cross-domain differences in key distributions through\nexponential weighting, leading to attention shifts and degraded generalization.\nTo this end, we propose PointDGRWKV, the first RWKV-based framework tailored\nfor DG PCC. It introduces two key modules to enhance spatial modeling and\ncross-domain robustness, while maintaining RWKV's linear efficiency. In\nparticular, we present Adaptive Geometric Token Shift to model local\nneighborhood structures to improve geometric context awareness. In addition,\nCross-Domain key feature Distribution Alignment is designed to mitigate\nattention drift by aligning key feature distributions across domains. Extensive\nexperiments on multiple benchmarks demonstrate that PointDGRWKV achieves\nstate-of-the-art performance on DG PCC.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20835v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20835v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.426,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is developing an RWKV-based framework for domain generalization in point cloud classification, focusing on spatial modeling and cross-domain robustness. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "The paper emphasizes the linear computational complexity and efficiency of the RWKV architecture for point cloud tasks but does not discuss distributed training, parallel computing, data partitioning across nodes, or strategies for multi-node machine learning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20840",
      "title": "Learning Primitive Embodied World Models: Towards Scalable Robotic\n  Learning",
      "authors": [
        "Qiao Sun",
        "Liujia Yang",
        "Wei Tang",
        "Wei Huang",
        "Kaixin Xu",
        "Yongchao Chen",
        "Mingyu Liu",
        "Jiange Yang",
        "Haoyi Zhu",
        "Yating Wang",
        "Tong He",
        "Yilun Chen",
        "Xili Dai",
        "Nanyang Ye",
        "Qinying Gu"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "While video-generation-based embodied world models have gained increasing\nattention, their reliance on large-scale embodied interaction data remains a\nkey bottleneck. The scarcity, difficulty of collection, and high dimensionality\nof embodied data fundamentally limit the alignment granularity between language\nand actions and exacerbate the challenge of long-horizon video\ngeneration--hindering generative models from achieving a \"GPT moment\" in the\nembodied domain. There is a naive observation: the diversity of embodied data\nfar exceeds the relatively small space of possible primitive motions. Based on\nthis insight, we propose a novel paradigm for world modeling--Primitive\nEmbodied World Models (PEWM). By restricting video generation to fixed short\nhorizons, our approach 1) enables fine-grained alignment between linguistic\nconcepts and visual representations of robotic actions, 2) reduces learning\ncomplexity, 3) improves data efficiency in embodied data collection, and 4)\ndecreases inference latency. By equipping with a modular Vision-Language Model\n(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further\nenables flexible closed-loop control and supports compositional generalization\nof primitive-level policies over extended, complex tasks. Our framework\nleverages the spatiotemporal vision priors in video models and the semantic\nawareness of VLMs to bridge the gap between fine-grained physical interaction\nand high-level reasoning, paving the way toward scalable, interpretable, and\ngeneral-purpose embodied intelligence.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20840v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20840v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.347,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing Primitive Embodied World Models for robotic learning using video generation and embodied data, without any mention of human feedback, reward models, or fine-tuning based on human preferences. It relies on autonomous data collection and learning, which does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for generating short-horizon motion sequences in robotics, involving iterative refinement for action rollout, but it does not adapt this process for multi-step logical reasoning or treat a Chain-of-Thought as a single entity for holistic correction. It is more about physical simulation than complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20848",
      "title": "JADES: A Universal Framework for Jailbreak Assessment via\n  Decompositional Scoring",
      "authors": [
        "Junjie Chu",
        "Mingjie Li",
        "Ziqing Yang",
        "Ye Leng",
        "Chenhao Lin",
        "Chao Shen",
        "Michael Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurately determining whether a jailbreak attempt has succeeded is a\nfundamental yet unresolved challenge. Existing evaluation methods rely on\nmisaligned proxy indicators or naive holistic judgments. They frequently\nmisinterpret model responses, leading to inconsistent and subjective\nassessments that misalign with human perception. To address this gap, we\nintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal\njailbreak evaluation framework. Its key mechanism is to automatically decompose\nan input harmful question into a set of weighted sub-questions, score each\nsub-answer, and weight-aggregate the sub-scores into a final decision. JADES\nalso incorporates an optional fact-checking module to strengthen the detection\nof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a\nnewly introduced benchmark proposed in this work, consisting of 400 pairs of\njailbreak prompts and responses, each meticulously annotated by humans. In a\nbinary setting (success/failure), JADES achieves 98.5% agreement with human\nevaluators, outperforming strong baselines by over 9%. Re-evaluating five\npopular attacks on four LLMs reveals substantial overestimation (e.g., LAA's\nattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show\nthat JADES could deliver accurate, consistent, and interpretable evaluations,\nproviding a reliable basis for measuring future jailbreak attacks.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20848v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20848v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.296,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20851",
      "title": "PathMR: Multimodal Visual Reasoning for Interpretable Pathology\n  Diagnosis",
      "authors": [
        "Ye Zhang",
        "Yu Zhou",
        "Jingwen Qi",
        "Yongbing Zhang",
        "Simon Puettmann",
        "Finn Wichmann",
        "Larissa Pereira Ferreira",
        "Lara Sichward",
        "Julius Keyl",
        "Sylvia Hartmann",
        "Shuo Zhao",
        "Hongxiao Wang",
        "Xiaowei Xu",
        "Jianxu Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning based automated pathological diagnosis has markedly improved\ndiagnostic efficiency and reduced variability between observers, yet its\nclinical adoption remains limited by opaque model decisions and a lack of\ntraceable rationale. To address this, recent multimodal visual reasoning\narchitectures provide a unified framework that generates segmentation masks at\nthe pixel level alongside semantically aligned textual explanations. By\nlocalizing lesion regions and producing expert style diagnostic narratives,\nthese models deliver the transparent and interpretable insights necessary for\ndependable AI assisted pathology. Building on these advancements, we propose\nPathMR, a cell-level Multimodal visual Reasoning framework for Pathological\nimage analysis. Given a pathological image and a textual query, PathMR\ngenerates expert-level diagnostic explanations while simultaneously predicting\ncell distribution patterns. To benchmark its performance, we evaluated our\napproach on the publicly available PathGen dataset as well as on our newly\ndeveloped GADVR dataset. Extensive experiments on these two datasets\ndemonstrate that PathMR consistently outperforms state-of-the-art visual\nreasoning methods in text generation quality, segmentation accuracy, and\ncross-modal alignment. These results highlight the potential of PathMR for\nimproving interpretability in AI-driven pathological diagnosis. The code will\nbe publicly available in https://github.com/zhangye-zoe/PathMR.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20851v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20851v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.336,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents PathMR, a multimodal visual reasoning framework for pathology diagnosis, focusing on segmentation masks and textual explanations using techniques like large language models and visual perception modules. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. The core contributions are in interpretability for pathology, with no mention of adapting diffusion for reasoning, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20866",
      "title": "AI Agentic Vulnerability Injection And Transformation with Optimized\n  Reasoning",
      "authors": [
        "Amine Lbath",
        "Massih-Reza Amini",
        "Aurelien Delaitre",
        "Vadim Okun"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The increasing complexity of software systems and the sophistication of\ncyber-attacks have underscored the critical need for effective automated\nvulnerability detection and repair systems. Traditional methods, such as static\nprogram analysis, face significant challenges related to scalability,\nadaptability, and high false-positive and false-negative rates. AI-driven\napproaches, particularly those using machine learning and deep learning models,\nshow promise but are heavily reliant on the quality and quantity of training\ndata. This paper introduces a novel framework designed to automatically\nintroduce realistic, category-specific vulnerabilities into secure C/C++\ncodebases to generate datasets. The proposed approach coordinates multiple AI\nagents that simulate expert reasoning, along with function agents and\ntraditional code analysis tools. It leverages Retrieval-Augmented Generation\nfor contextual grounding and employs Low-Rank approximation of weights for\nefficient model fine-tuning. Our experimental study on 116 code samples from\nthree different benchmarks suggests that our approach outperforms other\ntechniques with regard to dataset accuracy, achieving between 89\\% and 95\\%\nsuccess rates in injecting vulnerabilities at function level.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20866v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20866v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.432,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.379,
      "datasets_score": 0.44,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on AI agents for vulnerability injection using techniques like RAG and LoRA, but does not involve reinforcement learning, human feedback, reward models, or aligning models with human preferences. There is no mention of training on human-ranked data.",
      "weak_supervision_justification": "The paper's approach to programmatically injecting vulnerabilities to generate labels aligns with weak supervision, as it creates noisy or automated labels rather than relying on hand-labeled data. However, the primary focus is on dataset creation for vulnerability detection, not on training models with weak supervision as the core methodology.",
      "diffusion_reasoning_justification": "The paper describes multi-agent reasoning for vulnerability injection but does not mention diffusion models, iterative refinement of reasoning paths, or adapting diffusion processes for logical tasks. There is no evidence of a diffusion-based component.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of high-quality datasets for AI-driven vulnerability detection, including methods for injecting vulnerabilities, benchmarking on code samples, and analyzing dataset accuracy. This directly aligns with research on dataset curation, generation, and evaluation for machine learning applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces AVIATOR, a novel framework that utilizes multiple AI agents to simulate expert reasoning and inject realistic, category-specific vulnerabilities into secure C/C++ codebases, aiming to generate high-quality datasets for training AI-based vulnerability detection models. The methodology integrates Retrieval-Augmented Generation for contextual grounding and Low-Rank Adaptation for efficient model fine-tuning, with experiments on 116 code samples demonstrating success rates of 89-95% in vulnerability injection, outperforming existing techniques.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new multi-agent pipeline that combines AI agents, Retrieval-Augmented Generation, and Low-Rank Adaptation for automated vulnerability injection, representing a significant advancement in generating high-quality datasets for cybersecurity.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to improve AI-driven vulnerability detection by providing more accurate training datasets, which could influence a wide range of future research and commercial applications in software security.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper presents a strong, innovative contribution to AI and cybersecurity that addresses a critical need for better datasets, making it valuable for researchers in these fields to review for potential applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4deadcbed8b7a08edc5d3f0d5abed87854381d8e",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 29,
      "average_h_index": 9.75,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Amine Lbath",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2312045934"
        },
        {
          "name": "Massih-Reza Amini",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/144444438"
        },
        {
          "name": "A. Delaitre",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2190614"
        },
        {
          "name": "Vadim Okun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377793946"
        }
      ]
    },
    {
      "id": "2508.20877",
      "title": "Deep Learning Framework for Early Detection of Pancreatic Cancer Using\n  Multi-Modal Medical Imaging Analysis",
      "authors": [
        "Dennis Slobodzian",
        "Amir Kordijazi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms\nof cancer, with a five-year survival rate below 10% primarily due to late\ndetection. This research develops and validates a deep learning framework for\nearly PDAC detection through analysis of dual-modality imaging:\nautofluorescence and second harmonic generation (SHG). We analyzed 40 unique\npatient samples to create a specialized neural network capable of\ndistinguishing between normal, fibrotic, and cancerous tissue. Our methodology\nevaluated six distinct deep learning architectures, comparing traditional\nConvolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).\nThrough systematic experimentation, we identified and overcome significant\nchallenges in medical image analysis, including limited dataset size and class\nimbalance. The final optimized framework, based on a modified ResNet\narchitecture with frozen pre-trained layers and class-weighted training,\nachieved over 90% accuracy in cancer detection. This represents a significant\nimprovement over current manual analysis methods an demonstrates potential for\nclinical deployment. This work establishes a robust pipeline for automated PDAC\ndetection that can augment pathologists' capabilities while providing a\nfoundation for future expansion to other cancer types. The developed\nmethodology also offers valuable insights for applying deep learning to\nlimited-size medical imaging datasets, a common challenge in clinical\napplications.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20877v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20877v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.28,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.355,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20881",
      "title": "Understanding and evaluating computer vision models through the lens of\n  counterfactuals",
      "authors": [
        "Pushkar Shukla"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Counterfactual reasoning -- the practice of asking ``what if'' by varying\ninputs and observing changes in model behavior -- has become central to\ninterpretable and fair AI. This thesis develops frameworks that use\ncounterfactuals to explain, audit, and mitigate bias in vision classifiers and\ngenerative models. By systematically altering semantically meaningful\nattributes while holding others fixed, these methods uncover spurious\ncorrelations, probe causal dependencies, and help build more robust systems.\n  The first part addresses vision classifiers. CAVLI integrates attribution\n(LIME) with concept-level analysis (TCAV) to quantify how strongly decisions\nrely on human-interpretable concepts. With localized heatmaps and a Concept\nDependency Score, CAVLI shows when models depend on irrelevant cues like\nbackgrounds. Extending this, ASAC introduces adversarial counterfactuals that\nperturb protected attributes while preserving semantics. Through curriculum\nlearning, ASAC fine-tunes biased models for improved fairness and accuracy\nwhile avoiding stereotype-laden artifacts.\n  The second part targets generative Text-to-Image (TTI) models. TIBET provides\na scalable pipeline for evaluating prompt-sensitive biases by varying\nidentity-related terms, enabling causal auditing of how race, gender, and age\naffect image generation. To capture interactions, BiasConnect builds causal\ngraphs diagnosing intersectional biases. Finally, InterMit offers a modular,\ntraining-free algorithm that mitigates intersectional bias via causal\nsensitivity scores and user-defined fairness goals.\n  Together, these contributions show counterfactuals as a unifying lens for\ninterpretability, fairness, and causality in both discriminative and generative\nmodels, establishing principled, scalable methods for socially responsible bias\nevaluation and mitigation.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20881v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20881v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.317,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily discusses counterfactual reasoning for interpretability, fairness, and bias mitigation in computer vision models, including methods like CAVLI, ASAC, TIBET, BiasConnect, and InterMit. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks such as Chain-of-Thought. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20892",
      "title": "To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle\n  Software",
      "authors": [
        "Loïc Stratil",
        "Felix Fent",
        "Esteban Rivera",
        "Markus Lienkamp"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Autonomous vehicle perception typically relies on modular pipelines that\ndecompose the task into detection, tracking, and prediction. While\ninterpretable, these pipelines suffer from error accumulation and limited\ninter-task synergy. Unified perception has emerged as a promising paradigm that\nintegrates these sub-tasks within a shared architecture, potentially improving\nrobustness, contextual reasoning, and efficiency while retaining interpretable\noutputs. In this survey, we provide a comprehensive overview of unified\nperception, introducing a holistic and systemic taxonomy that categorizes\nmethods along task integration, tracking formulation, and representation flow.\nWe define three paradigms -Early, Late, and Full Unified Perception- and\nsystematically review existing methods, their architectures, training\nstrategies, datasets used, and open-source availability, while highlighting\nfuture research directions. This work establishes the first comprehensive\nframework for understanding and advancing unified perception, consolidates\nfragmented efforts, and guides future research toward more robust,\ngeneralizable, and interpretable perception.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20892v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20892v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.374,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20907",
      "title": "Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant",
      "authors": [
        "Nicolas Dupuis",
        "Adarsh Tiwari",
        "Youssef Mroueh",
        "David Kremer",
        "Ismael Faro",
        "Juan Cruz-Benito"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Qiskit is an open-source quantum computing framework that allows users to\ndesign, simulate, and run quantum circuits on real quantum hardware. We explore\npost-training techniques for LLMs to assist in writing Qiskit code. We\nintroduce quantum verification as an effective method for ensuring code quality\nand executability on quantum hardware. To support this, we developed a\nsynthetic data pipeline that generates quantum problem-unit test pairs and used\nit to create preference data for aligning LLMs with DPO. Additionally, we\ntrained models using GRPO, leveraging quantum-verifiable rewards provided by\nthe quantum hardware. Our best-performing model, combining DPO and GRPO,\nsurpasses the strongest open-source baselines on the challenging\nQiskit-HumanEval-hard benchmark.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20907v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20907v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.491,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.432,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on post-training LLMs using quantum-verifiable rewards from hardware execution, not human-ranked data. While it mentions DPO, which is related to RLHF, the rewards are derived from synthetic data and quantum hardware, excluding human feedback as required for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses LLM training for quantum code generation using DPO and GRPO, but does not involve diffusion models, iterative refinement for logical reasoning, or treating a Chain-of-Thought as a holistic entity. There is no component for multi-step logical reasoning via diffusion processes.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node machine learning techniques. It focuses on optimization methods like DPO and GRPO for LLM alignment, without mentioning strategies for partitioning data, architecture, or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20909",
      "title": "Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation\n  Models for Medical Image Segmentation",
      "authors": [
        "Yifan Gao",
        "Haoyue Li",
        "Feng Yuan",
        "Xiaosong Wang",
        "Xin Gao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Foundation models pre-trained on large-scale natural image datasets offer a\npowerful paradigm for medical image segmentation. However, effectively\ntransferring their learned representations for precise clinical applications\nremains a challenge. In this work, we propose Dino U-Net, a novel\nencoder-decoder architecture designed to exploit the high-fidelity dense\nfeatures of the DINOv3 vision foundation model. Our architecture introduces an\nencoder built upon a frozen DINOv3 backbone, which employs a specialized\nadapter to fuse the model's rich semantic features with low-level spatial\ndetails. To preserve the quality of these representations during dimensionality\nreduction, we design a new fidelity-aware projection module (FAPM) that\neffectively refines and projects the features for the decoder. We conducted\nextensive experiments on seven diverse public medical image segmentation\ndatasets. Our results show that Dino U-Net achieves state-of-the-art\nperformance, consistently outperforming previous methods across various imaging\nmodalities. Our framework proves to be highly scalable, with segmentation\naccuracy consistently improving as the backbone model size increases up to the\n7-billion-parameter variant. The findings demonstrate that leveraging the\nsuperior, dense-pretrained features from a general-purpose foundation model\nprovides a highly effective and parameter-efficient approach to advance the\naccuracy of medical image segmentation. The code is available at\nhttps://github.com/yifangao112/DinoUNet.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20909v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20909v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.364,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20912",
      "title": "Research Challenges in Relational Database Management Systems for LLM\n  Queries",
      "authors": [
        "Kerem Akillioglu",
        "Anurag Chakraborty",
        "Sairaj Voruganti",
        "M. Tamer Özsu"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have become essential for applications such as\ntext summarization, sentiment analysis, and automated question-answering.\nRecently, LLMs have also been integrated into relational database management\nsystems to enhance querying and support advanced data processing. Companies\nsuch as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly\nwithin SQL, denoted as LLM queries, to boost data insights. However,\nopen-source solutions currently have limited functionality and poor\nperformance. In this work, we present an early exploration of two open-source\nsystems and one enterprise platform, using five representative queries to\nexpose functional, performance, and scalability limits in today's SQL-invoked\nLLM integrations. We identify three main issues: enforcing structured outputs,\noptimizing resource utilization, and improving query planning. We implemented\ninitial solutions and observed improvements in accommodating LLM powered SQL\nqueries. These early gains demonstrate that tighter integration of LLM+DBMS is\nthe key to scalable and efficient processing of LLM queries.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20912v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20912v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.419,
      "datasets_score": 0.408,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on integrating LLMs with relational DBMS for querying, including performance and scalability issues, but does not involve training AI models, human feedback, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper addresses challenges in LLM-DBMS integration and query optimization, without any discussion of machine learning training, label generation, or using noisy sources for supervision.",
      "diffusion_reasoning_justification": "The paper explores LLM queries in SQL and related optimizations, but lacks any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "The paper discusses scalability and performance issues with LLM queries on large datasets, such as processing times on a single GPU, which indirectly relates to distributed computing concepts, but it focuses on inference and querying rather than algorithms for distributed model training.",
      "datasets_justification": "The paper uses a set of representative queries to evaluate systems, which involves some benchmarking of performance, but it does not primarily focus on creating, analyzing, or curating datasets; instead, it centers on LLM-DBMS integration challenges.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20919",
      "title": "Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble\n  Learning and Rule Based Refinement",
      "authors": [
        "Sara Krauss",
        "Ellena Spieß",
        "Daniel Hieber",
        "Frank Kramer",
        "Johannes Schobel",
        "Dominik Müller"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Mitotic figures (MFs) are relevant biomarkers in tumor grading.\nDifferentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult,\nas manual annotation is time-consuming and subjective. In this work an ensemble\nof ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based\nrefinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble\nachieved a balanced accuracy of 84.02%. While the RBR increased specificity, it\nreduced sensitivity and overall performance. The results show that deep\nensembles perform well for AMF classification. RBR can increase specific\nmetrics but requires further research.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20919v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20919v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.337,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20920",
      "title": "COMETH: Convex Optimization for Multiview Estimation and Tracking of\n  Humans",
      "authors": [
        "Enrico Martini",
        "Ho Jin Choi",
        "Nadia Figueroa",
        "Nicola Bombieri"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "In the era of Industry 5.0, monitoring human activity is essential for\nensuring both ergonomic safety and overall well-being. While multi-camera\ncentralized setups improve pose estimation accuracy, they often suffer from\nhigh computational costs and bandwidth requirements, limiting scalability and\nreal-time applicability. Distributing processing across edge devices can reduce\nnetwork bandwidth and computational load. On the other hand, the constrained\nresources of edge devices lead to accuracy degradation, and the distribution of\ncomputation leads to temporal and spatial inconsistencies. We address this\nchallenge by proposing COMETH (Convex Optimization for Multiview Estimation and\nTracking of Humans), a lightweight algorithm for real-time multi-view human\npose fusion that relies on three concepts: it integrates kinematic and\nbiomechanical constraints to increase the joint positioning accuracy; it\nemploys convex optimization-based inverse kinematics for spatial fusion; and it\nimplements a state observer to improve temporal consistency. We evaluate COMETH\non both public and industrial datasets, where it outperforms state-of-the-art\nmethods in localization, detection, and tracking accuracy. The proposed fusion\npipeline enables accurate and scalable human motion tracking, making it\nwell-suited for industrial and safety-critical applications. The code is\npublicly available at https://github.com/PARCO-LAB/COMETH.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20920v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20920v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.31,
      "distributed_training_score": 0.38,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20953",
      "title": "A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling",
      "authors": [
        "Vipul Patel",
        "Anirudh Deodhar",
        "Dagnachew Birru"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.DM (Discrete Mathematics)"
      ],
      "abstract": "Workforce scheduling in the healthcare sector is a significant operational\nchallenge, characterized by fluctuating patient loads, diverse clinical skills,\nand the critical need to control labor costs while upholding high standards of\npatient care. This problem is inherently multi-objective, demanding a delicate\nbalance between competing goals: minimizing payroll, ensuring adequate staffing\nfor patient needs, and accommodating staff preferences to mitigate burnout. We\npropose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital\nunit workforce scheduling problem as a multi-objective optimization task. Our\nmodel incorporates real-world complexities, including hourly appointment-driven\ndemand and the use of modular shifts for a multi-skilled workforce. By defining\nobjective functions for cost, patient care coverage, and staff satisfaction,\nthe GA navigates the vast search space to identify a set of high-quality,\nnon-dominated solutions. Demonstrated on datasets representing a typical\nhospital unit, the results show that our MOO-GA generates robust and balanced\nschedules. On average, the schedules produced by our algorithm showed a 66\\%\nperformance improvement over a baseline that simulates a conventional, manual\nscheduling process. This approach effectively manages trade-offs between\ncritical operational and staff-centric objectives, providing a practical\ndecision support tool for nurse managers and hospital administrators.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20953v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20953v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.265,
      "diffusion_reasoning_score": 0.243,
      "distributed_training_score": 0.31,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20954",
      "title": "Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase\n  Refinement",
      "authors": [
        "Amir Jmal",
        "Chaima Chtourou",
        "Mahdi Louati",
        "Abdelaziz Kallel",
        "Houda Khmila"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In the context of proven climate change, maintaining olive biodiversity\nthrough early anomaly detection and treatment using remote sensing technology\nis crucial, offering effective management solutions. This paper presents an\ninnovative approach to olive tree segmentation from satellite images. By\nleveraging foundational models and advanced segmentation techniques, the study\nintegrates the Segment Anything Model (SAM) to accurately identify and segment\nolive trees in agricultural plots. The methodology includes SAM segmentation\nand corrections based on trees alignement in the field and a learanble\nconstraint about the shape and the size. Our approach achieved a 98\\% accuracy\nrate, significantly surpassing the initial SAM performance of 82\\%.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20954v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20954v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.257,
      "weak_supervision_score": 0.295,
      "diffusion_reasoning_score": 0.277,
      "distributed_training_score": 0.27,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20955",
      "title": "E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with\n  Cross-Stage Partial Connections",
      "authors": [
        "Fang Wang",
        "Huitao Li",
        "Wenhan Chao",
        "Zheng Zhuo",
        "Yiran Ji",
        "Chang Peng",
        "Yupeng Sun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Many high-performance networks were not designed with lightweight application\nscenarios in mind from the outset, which has greatly restricted their scope of\napplication. This paper takes ConvNeXt as the research object and significantly\nreduces the parameter scale and network complexity of ConvNeXt by integrating\nthe Cross Stage Partial Connections mechanism and a series of optimized\ndesigns. The new network is named E-ConvNeXt, which can maintain high accuracy\nperformance under different complexity configurations. The three core\ninnovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network\n(CSPNet) with ConvNeXt and adjusting the network structure, which reduces the\nmodel's network complexity by up to 80%; (2) Optimizing the Stem and Block\nstructures to enhance the model's feature expression capability and operational\nefficiency; (3) Replacing Layer Scale with channel attention. Experimental\nvalidation on ImageNet classification demonstrates E-ConvNeXt's superior\naccuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at\n0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer\nlearning tests on object detection tasks further confirm its generalization\ncapability.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20955v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20955v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.279,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.404,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on developing a lightweight variant of ConvNeXt by optimizing network architecture, reducing parameters, and improving efficiency for tasks like image classification and object detection. It does not discuss distributed training, parallel computing, multi-node machine learning, or strategies for partitioning data/computation across processors. Thus, the paper's contributions are unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20965",
      "title": "DrivingGaussian++: Towards Realistic Reconstruction and Editable\n  Simulation for Surrounding Dynamic Driving Scenes",
      "authors": [
        "Yajiao Xiong",
        "Xiaoyu Zhou",
        "Yongtao Wan",
        "Deqing Sun",
        "Ming-Hsuan Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present DrivingGaussian++, an efficient and effective framework for\nrealistic reconstructing and controllable editing of surrounding dynamic\nautonomous driving scenes. DrivingGaussian++ models the static background using\nincremental 3D Gaussians and reconstructs moving objects with a composite\ndynamic Gaussian graph, ensuring accurate positions and occlusions. By\nintegrating a LiDAR prior, it achieves detailed and consistent scene\nreconstruction, outperforming existing methods in dynamic scene reconstruction\nand photorealistic surround-view synthesis. DrivingGaussian++ supports\ntraining-free controllable editing for dynamic driving scenes, including\ntexture modification, weather simulation, and object manipulation, leveraging\nmulti-view images and depth priors. By integrating large language models (LLMs)\nand controllable editing, our method can automatically generate dynamic object\nmotion trajectories and enhance their realism during the optimization process.\nDrivingGaussian++ demonstrates consistent and realistic editing results and\ngenerates dynamic multi-view driving scenarios, while significantly enhancing\nscene diversity. More results and code can be found at the project site:\nhttps://xiong-creator.github.io/DrivingGaussian_plus.github.io",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20965v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20965v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.379,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on 3D Gaussian Splatting for reconstructing and editing dynamic driving scenes, incorporating LLMs for motion trajectories, but it does not involve diffusion models, iterative refinement for logical tasks, or any adaptation of diffusion processes for reasoning. There is no component related to multi-step logical reasoning or Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20973",
      "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue\n  Agents",
      "authors": [
        "Tianjian Liu",
        "Fanqi Wan",
        "Jiajian Guo",
        "Xiaojun Quan"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Proactive dialogue has emerged as a critical and challenging research problem\nin advancing large language models (LLMs). Existing works predominantly focus\non domain-specific or task-oriented scenarios, which leads to fragmented\nevaluations and limits the comprehensive exploration of models' proactive\nconversation abilities. In this work, we propose ProactiveEval, a unified\nframework designed for evaluating proactive dialogue capabilities of LLMs. This\nframework decomposes proactive dialogue into target planning and dialogue\nguidance, establishing evaluation metrics across various domains. Moreover, it\nalso enables the automatic generation of diverse and challenging evaluation\ndata. Based on the proposed framework, we develop 328 evaluation environments\nspanning 6 distinct domains. Through experiments with 22 different types of\nLLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional\nperformance on target planning and dialogue guidance tasks, respectively.\nFinally, we investigate how reasoning capabilities influence proactive\nbehaviors and discuss their implications for future model development.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20973v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20973v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.344,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes an evaluation framework for proactive dialogue in LLMs, focusing on assessment metrics and data generation, but it does not involve training models with human feedback or using reinforcement learning techniques for alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses reasoning in proactive dialogue, including chain-of-thought strategies, but it does not adapt diffusion models or iterative refinement processes for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20976",
      "title": "WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language\n  Models via Marine Mammal Vocalizations",
      "authors": [
        "Jaeyeon Kim",
        "Heeseung Yun",
        "Sang Hoon Woo",
        "Chao-Han Huck Yang",
        "Gunhee Kim"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Large audio language models (LALMs) extend language understanding into the\nauditory domain, yet their ability to perform low-level listening, such as\npitch and duration detection, remains underexplored. However, low-level\nlistening is critical for real-world, out-of-distribution tasks where models\nmust reason about unfamiliar sounds based on fine-grained acoustic cues. To\naddress this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to\nevaluate low-level auditory perception and cognition using marine mammal\nvocalizations. WoW-bench is composed of a Perception benchmark for categorizing\nnovel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess\nthe abilities to remember, understand, apply, and analyze sound events. For the\nCognition benchmark, we additionally introduce distractor questions to evaluate\nwhether models are truly solving problems through listening rather than relying\non other heuristics. Experiments with state-of-the-art LALMs show performance\nfar below human levels, indicating a need for stronger auditory grounding in\nLALMs.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20976v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20976v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.278,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20978",
      "title": "Efficient Neuro-Symbolic Learning of Constraints and Objective",
      "authors": [
        "Marianne Defresne",
        "Romain Gambardella",
        "Sophie Barbe",
        "Thomas Schiex"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)",
        "cs.SC (Symbolic Computation)"
      ],
      "abstract": "In the ongoing quest for hybridizing discrete reasoning with neural nets,\nthere is an increasing interest in neural architectures that can learn how to\nsolve discrete reasoning or optimization problems from natural inputs, a task\nthat Large Language Models seem to struggle with.\n  Objectives: We introduce a differentiable neuro-symbolic architecture and a\nloss function dedicated to learning how to solve NP-hard reasoning problems.\n  Methods: Our new probabilistic loss allows for learning both the constraints\nand the objective, thus delivering a complete model that can be scrutinized and\ncompleted with side constraints. By pushing the combinatorial solver out of the\ntraining loop, our architecture also offers scalable training while exact\ninference gives access to maximum accuracy.\n  Results: We empirically show that it can efficiently learn how to solve\nNP-hard reasoning problems from natural inputs. On three variants of the Sudoku\nbenchmark -- symbolic, visual, and many-solution --, our approach requires a\nfraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut\ntask, it optimizes the regret better than a Decision-Focused-Learning\nregret-dedicated loss. Finally, it efficiently learns the energy optimization\nformulation of the large real-world problem of designing proteins.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20978v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20978v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.393,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a differentiable neuro-symbolic architecture for learning to solve NP-hard problems using a probabilistic loss, without any involvement of human feedback, reward models, or reinforcement learning techniques. It relies on direct learning from data, not aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a neuro-symbolic approach using graphical models for reasoning and optimization, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning like a Chain-of-Thought. There is no evidence of adapting diffusion for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20981",
      "title": "ActLoc: Learning to Localize on the Move via Active Viewpoint Selection",
      "authors": [
        "Jiajie Li",
        "Boyang Sun",
        "Luca Di Giammarino",
        "Hermann Blum",
        "Marc Pollefeys"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reliable localization is critical for robot navigation, yet most existing\nsystems implicitly assume that all viewing directions at a location are equally\ninformative. In practice, localization becomes unreliable when the robot\nobserves unmapped, ambiguous, or uninformative regions. To address this, we\npresent ActLoc, an active viewpoint-aware planning framework for enhancing\nlocalization accuracy for general robot navigation tasks. At its core, ActLoc\nemploys a largescale trained attention-based model for viewpoint selection. The\nmodel encodes a metric map and the camera poses used during map construction,\nand predicts localization accuracy across yaw and pitch directions at arbitrary\n3D locations. These per-point accuracy distributions are incorporated into a\npath planner, enabling the robot to actively select camera orientations that\nmaximize localization robustness while respecting task and motion constraints.\nActLoc achieves stateof-the-art results on single-viewpoint selection and\ngeneralizes effectively to fulltrajectory planning. Its modular design makes it\nreadily applicable to diverse robot navigation and inspection tasks.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20981v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20981v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.316,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20987",
      "title": "Webly-Supervised Image Manipulation Localization via Category-Aware\n  Auto-Annotation",
      "authors": [
        "Chenfan Qu",
        "Yiwu Zhong",
        "Bin Li",
        "Lianwen Jin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Images manipulated using image editing tools can mislead viewers and pose\nsignificant risks to social security. However, accurately localizing the\nmanipulated regions within an image remains a challenging problem. One of the\nmain barriers in this area is the high cost of data acquisition and the severe\nlack of high-quality annotated datasets. To address this challenge, we\nintroduce novel methods that mitigate data scarcity by leveraging readily\navailable web data. We utilize a large collection of manually forged images\nfrom the web, as well as automatically generated annotations derived from a\nsimpler auxiliary task, constrained image manipulation localization.\nSpecifically, we introduce a new paradigm CAAAv2, which automatically and\naccurately annotates manipulated regions at the pixel level. To further improve\nannotation quality, we propose a novel metric, QES, which filters out\nunreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a\nlarge-scale, diverse, and high-quality dataset containing 246,212 manually\nforged images with pixel-level mask annotations. This is over 120x larger than\nexisting handcrafted datasets like IMD20. Additionally, we introduce Object\nJitter, a technique that further enhances model training by generating\nhigh-quality manipulation artifacts. Building on these advances, we develop a\nnew model, Web-IML, designed to effectively leverage web-scale supervision for\nthe image manipulation localization task. Extensive experiments demonstrate\nthat our approach substantially alleviates the data scarcity problem and\nsignificantly improves the performance of various models on multiple real-world\nforgery benchmarks. With the proposed web supervision, Web-IML achieves a\nstriking performance gain of 31% and surpasses previous SOTA TruFor by 24.1\naverage IoU points. The dataset and code will be made publicly available at\nhttps://github.com/qcf-568/MIML.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20987v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20987v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.473,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.365,
      "datasets_score": 0.391,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using web data to automatically generate pixel-level annotations for image manipulation localization, which aligns closely with weak supervision. It employs techniques like CAAAv2 for auto-annotation and QES for filtering unreliable labels, programmatically creating large-scale training data from noisy sources (e.g., web images) without relying on hand-labeled data. This directly addresses the core idea of weak supervision by mitigating data scarcity through high-level, imprecise sources, as seen in the construction of the MIMLv2 dataset and the use of Object Jitter for further data enhancement.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of data scarcity in image manipulation localization (IML) by proposing a novel approach that leverages web data to automatically annotate manipulated regions, introducing the Category-Aware Auto-Annotation v2 (CAAAv2) paradigm, a Quality Evaluation Score (QES) metric for filtering annotations, the MIMLv2 dataset with over 246,000 manually forged images, the Object Jitter technique for enhancing data diversity, and the Web-IML model for improved localization performance; their methodology demonstrates significant advancements, achieving up to a 31% performance gain and surpassing state-of-the-art methods by 24.1 average IoU points on real-world benchmarks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces innovative elements like CAAAv2 for auto-annotation, QES for quality filtering, MIMLv2 dataset, Object Jitter technique, and Web-IML model, which collectively represent a significant advancement in addressing data scarcity and improving IML tasks.",
      "impact_score": "High",
      "impact_justification": "The work provides a large-scale dataset and methods that could broadly influence research in computer vision, particularly in detecting image forgeries, and enhance real-world applications in social media security and misinformation detection.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable contributions to IML through novel techniques and a substantial dataset, making it a significant and practical read for researchers in computer vision, though it may not be essential for those outside the specific subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/995b3291e062d48f8d85932734f7bb3c4cc35b78",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 3,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Chenfan Qu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2224334719"
        },
        {
          "name": "Yiwu Zhong",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313037370"
        },
        {
          "name": "Bin Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377905414"
        },
        {
          "name": "Lianwen Jin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2332480134"
        }
      ]
    },
    {
      "id": "2508.20991",
      "title": "ExpertSim: Fast Particle Detector Simulation Using\n  Mixture-of-Generative-Experts",
      "authors": [
        "Patryk Będkowski",
        "Jan Dubiński",
        "Filip Szatkowski",
        "Kamil Deja",
        "Przemysław Rokita",
        "Tomasz Trzciński"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Simulating detector responses is a crucial part of understanding the inner\nworkings of particle collisions in the Large Hadron Collider at CERN. Such\nsimulations are currently performed with statistical Monte Carlo methods, which\nare computationally expensive and put a significant strain on CERN's\ncomputational grid. Therefore, recent proposals advocate for generative machine\nlearning methods to enable more efficient simulations. However, the\ndistribution of the data varies significantly across the simulations, which is\nhard to capture with out-of-the-box methods. In this study, we present\nExpertSim - a deep learning simulation approach tailored for the Zero Degree\nCalorimeter in the ALICE experiment. Our method utilizes a\nMixture-of-Generative-Experts architecture, where each expert specializes in\nsimulating a different subset of the data. This allows for a more precise and\nefficient generation process, as each expert focuses on a specific aspect of\nthe calorimeter response. ExpertSim not only improves accuracy, but also\nprovides a significant speedup compared to the traditional Monte-Carlo methods,\noffering a promising solution for high-efficiency detector simulations in\nparticle physics experiments at CERN. We make the code available at\nhttps://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20991v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20991v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.416,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a Mixture-of-Generative-Experts architecture for efficient particle detector simulation, focusing on generative models and simulation speed rather than distributed training methods. It mentions straightforward parallelization on high-performance GPUs for simulations, which indirectly relates to parallel computing, but does not discuss algorithms for partitioning data or computation across multiple nodes during model training. Thus, it is not a core focus.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20996",
      "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic\n  Support in Addiction Recovery",
      "authors": [
        "Junda Wang",
        "Zonghai Yao",
        "Zhichao Yang",
        "Lingxi Li",
        "Junhui Qian",
        "Hong Yu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet\nfew receive effective care due to stigma, motivational barriers, and limited\npersonalized support. Although large language models (LLMs) show promise for\nmental-health assistance, most systems lack tight integration with clinically\nvalidated strategies, reducing effectiveness in addiction recovery. We present\nChatThero, a multi-agent conversational framework that couples dynamic patient\nmodeling with context-sensitive therapeutic dialogue and adaptive persuasive\nstrategies grounded in cognitive behavioral therapy (CBT) and motivational\ninterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,\nMedium, and Hard resistance levels, and train ChatThero with a two-stage\npipeline comprising supervised fine-tuning (SFT) followed by direct preference\noptimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in\npatient motivation, a 0.49\\% increase in treatment confidence, and resolves\nhard cases with 26\\% fewer turns than GPT-4o, and both automated and human\nclinical assessments rate it higher in empathy, responsiveness, and behavioral\nrealism. The framework supports rigorous, privacy-preserving study of\ntherapeutic conversation and provides a robust, replicable basis for research\nand clinical translation.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.20996v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20996v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.334,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves a two-stage training pipeline that includes Direct Preference Optimization (DPO) guided by expert and AI feedback, which aligns closely with RLHF. DPO uses human-ranked data (from experts) to fine-tune the model, optimizing for preferences in therapeutic dialogues, thereby matching the core elements of RLHF as defined.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on LLM-based conversational frameworks, SFT, and DPO for therapeutic support, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described. There is no component involving diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "ChatThero is a novel AI-driven chatbot framework designed to support addiction recovery by integrating dynamic patient modeling, context-sensitive therapeutic dialogue, and adaptive strategies based on cognitive behavioral therapy (CBT) and motivational interviewing (MI) to enhance motivation and confidence. The system is developed using a two-stage training pipeline involving supervised fine-tuning (SFT) on a synthetic benchmark and direct preference optimization (DPO), resulting in significant improvements such as a 41.5% gain in patient motivation, higher treatment confidence, and superior performance in empathy and efficiency compared to baselines like GPT-4o.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a groundbreaking framework specifically tailored for addiction recovery using LLMs integrated with clinically validated strategies like CBT and MI, addressing a previously underexplored area in AI-driven mental health support. This represents a significant advancement by creating the first dedicated computational tool and benchmark for this domain.",
      "impact_score": "High",
      "impact_justification": "The work could substantially influence future AI applications in mental health and addiction treatment by providing a replicable, privacy-preserving framework that enhances access to personalized care, potentially affecting millions globally. Its open-sourcing of tools and methods may lead to broader adoption in research and clinical settings.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution to AI in healthcare with practical implications for addiction recovery, making it essential for researchers in AI ethics, mental health, and social impact fields to understand its methodologies and findings. While highly valuable, it may not be critical for those outside these specific areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/62cfafb4ccf847cb2f06fd0d5493203d5080480f",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 12,
      "average_h_index": 5.833333333333333,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Junda Wang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2261394407"
        },
        {
          "name": "Zonghai Yao",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/1576489304"
        },
        {
          "name": "Zhichao Yang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2322987613"
        },
        {
          "name": "Lingxi Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2232824313"
        },
        {
          "name": "Junhui Qian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378172900"
        },
        {
          "name": "Hong Yu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2261455807"
        }
      ]
    },
    {
      "id": "2508.21001",
      "title": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees",
      "authors": [
        "Yaniv Hassidof",
        "Tom Jurgenson",
        "Kiril Solovey"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Kinodynamic motion planning is concerned with computing collision-free\ntrajectories while abiding by the robot's dynamic constraints. This critical\nproblem is often tackled using sampling-based planners (SBPs) that explore the\nrobot's high-dimensional state space by constructing a search tree via action\npropagations. Although SBPs can offer global guarantees on completeness and\nsolution quality, their performance is often hindered by slow exploration due\nto uninformed action sampling. Learning-based approaches can yield\nsignificantly faster runtimes, yet they fail to generalize to\nout-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,\nthus limiting their deployment on physical robots. We present Diffusion Tree\n(DiTree): a provably-generalizable framework leveraging diffusion policies\n(DPs) as informed samplers to efficiently guide state-space search within SBPs.\nDiTree combines DP's ability to model complex distributions of expert\ntrajectories, conditioned on local observations, with the completeness of SBPs\nto yield provably-safe solutions within a few action propagation iterations for\ncomplex dynamical systems. We demonstrate DiTree's power with an implementation\ncombining the popular RRT planner with a DP action sampler trained on a single\nenvironment. In comprehensive evaluations on OOD scenarios, DiTree achieves on\naverage a 30% higher success rate compared to standalone DP or SBPs, on a\ndynamic car and Mujoco's ant robot settings (for the latter, SBPs fail\ncompletely). Beyond simulation, real-world car experiments confirm DiTree's\napplicability, demonstrating superior trajectory quality and robustness even\nunder severe sim-to-real gaps. Project webpage:\nhttps://sites.google.com/view/ditree.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21001v2",
      "pdf_url": "http://arxiv.org/pdf/2508.21001v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.503,
      "distributed_training_score": 0.369,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework called Diffusion Tree (DiTree) that integrates diffusion policies with sampling-based planners for kinodynamic motion planning in robotics. It uses diffusion models to guide trajectory sampling and search in state spaces for robot navigation, but this is focused on physical motion and environmental interactions, not on adapting diffusion for multi-step logical reasoning or Chain-of-Thought processes as defined in the topic. There is no component for solving complex logical tasks or holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21010",
      "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate\n  Representations for Improved and Explainable Causal Video Question Answering",
      "authors": [
        "Paritosh Parmar",
        "Eric Peh",
        "Basura Fernando"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.HC (Human-Computer Interaction)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle\nwith higher-order reasoning, relying on opaque, monolithic pipelines that\nentangle video understanding, causal inference, and answer generation. These\nblack-box approaches offer limited interpretability and tend to depend on\nshallow heuristics. We propose a novel, modular framework that explicitly\ndecouples causal reasoning from answer generation, introducing natural language\ncausal chains as interpretable intermediate representations. Inspired by human\ncognitive models, these structured cause-effect sequences bridge low-level\nvideo content with high-level causal reasoning, enabling transparent and\nlogically coherent inference. Our two-stage architecture comprises a Causal\nChain Extractor (CCE) that generates causal chains from video-question pairs,\nand a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in\nthese chains. To address the lack of annotated reasoning traces, we introduce a\nscalable method for generating high-quality causal chains from existing\ndatasets using large language models. We also propose CauCo, a new evaluation\nmetric for causality-oriented captioning. Experiments on three large-scale\nbenchmarks demonstrate that our approach not only outperforms state-of-the-art\nmodels, but also yields substantial gains in explainability, user trust, and\ngeneralization -- positioning the CCE as a reusable causal reasoning engine\nacross diverse domains. Project page:\nhttps://paritoshparmar.github.io/chainreaction/",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21010v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21010v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.517,
      "distributed_training_score": 0.285,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a structured framework using causal chains, Structural Causal Models (SCMs), and Chain-of-Thought (CoT) reasoning for Causal Video Question Answering, but it does not involve diffusion models or their iterative refinement processes. There is no component for multi-step logical reasoning via diffusion, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21016",
      "title": "Inference-Time Alignment Control for Diffusion Models with Reinforcement\n  Learning Guidance",
      "authors": [
        "Luozhijie Jin",
        "Zijie Qiu",
        "Jie Liu",
        "Zijie Diao",
        "Lifeng Qiao",
        "Ning Ding",
        "Alex Lamb",
        "Xipeng Qiu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Denoising-based generative models, particularly diffusion and flow matching\nalgorithms, have achieved remarkable success. However, aligning their output\ndistributions with complex downstream objectives, such as human preferences,\ncompositional accuracy, or data compressibility, remains challenging. While\nreinforcement learning (RL) fine-tuning methods, inspired by advances in RL\nfrom human feedback (RLHF) for large language models, have been adapted to\nthese generative frameworks, current RL approaches are suboptimal for diffusion\nmodels and offer limited flexibility in controlling alignment strength after\nfine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models\nthrough the lens of stochastic differential equations and implicit reward\nconditioning. We introduce Reinforcement Learning Guidance (RLG), an\ninference-time method that adapts Classifier-Free Guidance (CFG) by combining\nthe outputs of the base and RL fine-tuned models via a geometric average. Our\ntheoretical analysis shows that RLG's guidance scale is mathematically\nequivalent to adjusting the KL-regularization coefficient in standard RL\nobjectives, enabling dynamic control over the alignment-quality trade-off\nwithout further training. Extensive experiments demonstrate that RLG\nconsistently improves the performance of RL fine-tuned models across various\narchitectures, RL algorithms, and downstream tasks, including human\npreferences, compositional control, compressibility, and text rendering.\nFurthermore, RLG supports both interpolation and extrapolation, thereby\noffering unprecedented flexibility in controlling generative alignment. Our\napproach provides a practical and theoretically sound solution for enhancing\nand controlling diffusion model alignment at inference. The source code for RLG\nis publicly available at the Github:\nhttps://github.com/jinluo12345/Reinforcement-learning-guidance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21016v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21016v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.557,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.568,
      "distributed_training_score": 0.369,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution, RLG, builds directly on RL fine-tuning inspired by RLHF for LLMs, adapting it to diffusion models for alignment with human preferences and other objectives. It references RLHF techniques like REINFORCE and DPO, and experiments include tasks such as human preferences, making it a clear extension of RLHF principles.",
      "weak_supervision_justification": "The paper focuses on RL fine-tuning and inference-time guidance for diffusion models, with no mention of programmatically generating labels, noisy supervision, or training with imprecise sources, which are core to weak supervision.",
      "diffusion_reasoning_justification": "The paper deals with diffusion models for generative tasks like image alignment and control, but it does not involve multi-step logical reasoning, chain-of-thought processes, or adapting diffusion for complex reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Reinforcement Learning Guidance (RLG), a novel inference-time method for aligning diffusion models with complex objectives such as human preferences and compositional accuracy, by combining outputs from base and RL fine-tuned models using a geometric average inspired by stochastic differential equations. The methodology reinterprets RL fine-tuning as implicit reward conditioning, demonstrating that RLG allows dynamic control of alignment strength equivalent to adjusting KL-regularization without additional training, and extensive experiments show it enhances performance across various tasks like image aesthetics and text rendering while supporting flexible interpolation and extrapolation.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of reinforcement learning and Classifier-Free Guidance to enable dynamic alignment control in diffusion models, offering a notable improvement over existing methods by addressing inflexibility in post-fine-tuning adjustments. While it builds on established techniques, it innovatively applies them to solve a known problem in a new inference-time context.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in generative models by providing a flexible, training-free approach for alignment control, potentially leading to broader adoption in subfields like AI and machine learning for tasks such as image generation. However, its impact may be confined to specific applications rather than transforming the entire field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality, practical contribution that advances diffusion model alignment techniques, making it valuable for researchers in generative AI to understand and potentially build upon. While not essential for all audiences, it offers significant insights for those working on model fine-tuning and control.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0b9a4ec41a0d995f60b780fbb262764e97044213",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 1,
      "average_h_index": 0.125,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Luozhijie Jin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2346900719"
        },
        {
          "name": "Zi-Long Qiu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2338991730"
        },
        {
          "name": "Jie Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377976092"
        },
        {
          "name": "Zijie Diao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377797917"
        },
        {
          "name": "Lifeng Qiao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377881618"
        },
        {
          "name": "Ning Ding",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377798889"
        },
        {
          "name": "Alex Lamb",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377798133"
        },
        {
          "name": "Xipeng Qiu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377953082"
        }
      ]
    },
    {
      "id": "2508.21019",
      "title": "POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models",
      "authors": [
        "Jiaxiang Cheng",
        "Bing Ma",
        "Xuhua Ren",
        "Hongyi Jin",
        "Kai Yu",
        "Peng Zhang",
        "Wenyue Li",
        "Yuan Zhou",
        "Tianxiang Zheng",
        "Qinglin Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The field of video diffusion generation faces critical bottlenecks in\nsampling efficiency, especially for large-scale models and long sequences.\nExisting video acceleration methods adopt image-based techniques but suffer\nfrom fundamental limitations: they neither model the temporal coherence of\nvideo frames nor provide single-step distillation for large-scale video models.\nTo bridge this gap, we propose POSE (Phased One-Step Equilibrium), a\ndistillation framework that reduces the sampling steps of large-scale video\ndiffusion models, enabling the generation of high-quality videos in a single\nstep. POSE employs a carefully designed two-phase process to distill video\nmodels:(i) stability priming: a warm-up mechanism to stabilize adversarial\ndistillation that adapts the high-quality trajectory of the one-step generator\nfrom high to low signal-to-noise ratio regimes, optimizing the video quality of\nsingle-step mappings near the endpoints of flow trajectories. (ii) unified\nadversarial equilibrium: a flexible self-adversarial distillation mechanism\nthat promotes stable single-step adversarial training towards a Nash\nequilibrium within the Gaussian noise space, generating realistic single-step\nvideos close to real videos. For conditional video generation, we propose (iii)\nconditional adversarial consistency, a method to improve both semantic\nconsistency and frame consistency between conditional frames and generated\nframes. Comprehensive experiments demonstrate that POSE outperforms other\nacceleration methods on VBench-I2V by average 7.15% in semantic alignment,\ntemporal conference and frame quality, reducing the latency of the pre-trained\nmodel by 100$\\times$, from 1000 seconds to 10 seconds, while maintaining\ncompetitive performance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21019v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21019v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.516,
      "distributed_training_score": 0.386,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on accelerating video diffusion models for efficient video generation, focusing on techniques like adversarial distillation and single-step sampling. It does not involve adapting diffusion processes for complex logical tasks, chain-of-thought reasoning, or holistic correction of reasoning paths. The work is centered on generative synthesis, not reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21032",
      "title": "Reusing Computation in Text-to-Image Diffusion for Efficient Generation\n  of Image Sets",
      "authors": [
        "Dale Decatur",
        "Thibault Groueix",
        "Wang Yifan",
        "Rana Hanocka",
        "Vladimir Kim",
        "Matheus Gadelha"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-image diffusion models enable high-quality image generation but are\ncomputationally expensive. While prior work optimizes per-inference efficiency,\nwe explore an orthogonal approach: reducing redundancy across correlated\nprompts. Our method leverages the coarse-to-fine nature of diffusion models,\nwhere early denoising steps capture shared structures among similar prompts. We\npropose a training-free approach that clusters prompts based on semantic\nsimilarity and shares computation in early diffusion steps. Experiments show\nthat for models trained conditioned on image embeddings, our approach\nsignificantly reduces compute cost while improving image quality. By leveraging\nUnClip's text-to-image prior, we enhance diffusion step allocation for greater\nefficiency. Our method seamlessly integrates with existing pipelines, scales\nwith prompt sets, and reduces the environmental and financial burden of\nlarge-scale text-to-image generation. Project page:\nhttps://ddecatur.github.io/hierarchical-diffusion/",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21032v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21032v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.611,
      "distributed_training_score": 0.421,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on optimizing text-to-image diffusion models for efficient image generation by reusing computations across similar prompts, without any adaptation for multi-step logical reasoning or treating a chain-of-thought as an entity. It deals solely with visual generation, not complex logical tasks.",
      "distributed_training_justification": "The paper addresses efficiency in inference for text-to-image generation by sharing computations across prompts, but it does not involve distributed training, parallel computing for model training, or strategies for partitioning data/models across nodes. It is focused on inference optimization, not training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21033",
      "title": "Mitosis detection in domain shift scenarios: a Mamba-based approach",
      "authors": [
        "Gennaro Percannella",
        "Mattia Sarno",
        "Francesco Tortorella",
        "Mario Vento"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Mitosis detection in histopathology images plays a key role in tumor\nassessment. Although machine learning algorithms could be exploited for aiding\nphysicians in accurately performing such a task, these algorithms suffer from\nsignificative performance drop when evaluated on images coming from domains\nthat are different from the training ones. In this work, we propose a\nMamba-based approach for mitosis detection under domain shift, inspired by the\npromising performance demonstrated by Mamba in medical imaging segmentation\ntasks. Specifically, our approach exploits a VM-UNet architecture for carrying\nout the addressed task, as well as stain augmentation operations for further\nimproving model robustness against domain shift. Our approach has been\nsubmitted to the track 1 of the MItosis DOmain Generalization (MIDOG)\nchallenge. Preliminary experiments, conducted on the MIDOG++ dataset, show\nlarge room for improvement for the proposed method.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21033v2",
      "pdf_url": "http://arxiv.org/pdf/2508.21033v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.227,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.336,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21035",
      "title": "A multi-task neural network for atypical mitosis recognition under\n  domain shift",
      "authors": [
        "Gennaro Percannella",
        "Mattia Sarno",
        "Francesco Tortorella",
        "Mario Vento"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recognizing atypical mitotic figures in histopathology images allows\nphysicians to correctly assess tumor aggressiveness. Although machine learning\nmodels could be exploited for automatically performing such a task, under\ndomain shift these models suffer from significative performance drops. In this\nwork, an approach based on multi-task learning is proposed for addressing this\nproblem. By exploiting auxiliary tasks, correlated to the main classification\ntask, the proposed approach, submitted to the track 2 of the MItosis DOmain\nGeneralization (MIDOG) challenge, aims to aid the model to focus only on the\nobject to classify, ignoring the domain varying background of the image. The\nproposed approach shows promising performance in a preliminary evaluation\nconducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training\nSet, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25\nchallenge.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21035v3",
      "pdf_url": "http://arxiv.org/pdf/2508.21035v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.384,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21036",
      "title": "Understanding, Protecting, and Augmenting Human Cognition with\n  Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop",
      "authors": [
        "Lev Tankelevitch",
        "Elena L. Glassman",
        "Jessica He",
        "Aniket Kittur",
        "Mina Lee",
        "Srishti Palani",
        "Advait Sarkar",
        "Gonzalo Ramos",
        "Yvonne Rogers",
        "Hari Subramonyam"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative AI (GenAI) radically expands the scope and capability of\nautomation for work, education, and everyday tasks, a transformation posing\nboth risks and opportunities for human cognition. How will human cognition\nchange, and what opportunities are there for GenAI to augment it? Which\ntheories, metrics, and other tools are needed to address these questions? The\nCHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of\nhow the use of GenAI affects human thought, from metacognition to critical\nthinking, memory, and creativity, with an emerging design practice for building\nGenAI tools that both protect and augment human thought. Fifty-six researchers,\ndesigners, and thinkers from across disciplines as well as industry and\nacademia, along with 34 papers and portfolios, seeded a day of discussion,\nideation, and community-building. We synthesize this material here to begin\nmapping the space of research and design opportunities and to catalyze a\nmultidisciplinary community around this pressing area of research.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21036v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21036v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.338,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on synthesizing discussions from a workshop on Generative AI's impact on human cognition, including risks, opportunities, and design practices. It does not mention or involve techniques like training AI models with human-ranked data for reward modeling and fine-tuning via reinforcement learning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses broad themes of AI's effects on cognition, such as metacognition and creativity, and workshop outputs on AI tools. It does not address diffusion models, iterative refinement for logical reasoning, or treating reasoning paths as entities for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper references 34 accepted papers and portfolios from the workshop as part of synthesizing research opportunities, which might indirectly include dataset-related work in AI applications. However, its main contribution is not on creating, analyzing, benchmarking, or evaluating datasets, but rather on broader cognitive impacts and community building.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21040",
      "title": "FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP\n  Generator",
      "authors": [
        "Huynh Tong Dang Khoa",
        "Dang Hoai Nam",
        "Vo Nguyen Le Duy"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Labeled handwriting data is often scarce, limiting the effectiveness of\nrecognition systems that require diverse, style-consistent training samples.\nHandwriting synthesis offers a promising solution by generating artificial data\nto augment training. However, current methods face two major limitations.\nFirst, most are built on conventional convolutional architectures, which\nstruggle to model long-range dependencies and complex stroke patterns. Second,\nthey largely ignore the crucial role of frequency information, which is\nessential for capturing fine-grained stylistic and structural details in\nhandwriting. To address these challenges, we propose FW-GAN, a one-shot\nhandwriting synthesis framework that generates realistic, writer-consistent\ntext from a single example. Our generator integrates a phase-aware Wave-MLP to\nbetter capture spatial relationships while preserving subtle stylistic cues. We\nfurther introduce a frequency-guided discriminator that leverages\nhigh-frequency components to enhance the authenticity detection of generated\nsamples. Additionally, we introduce a novel Frequency Distribution Loss that\naligns the frequency characteristics of synthetic and real handwriting, thereby\nenhancing visual fidelity. Experiments on Vietnamese and English handwriting\ndatasets demonstrate that FW-GAN generates high-quality, style-consistent\nhandwriting, making it a valuable tool for augmenting data in low-resource\nhandwriting recognition (HTR) pipelines. Official implementation is available\nat https://github.com/DAIR-Group/FW-GAN",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21040v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21040v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.354,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21041",
      "title": "Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for\n  Atypical Mitotic Figure Classification in MIDOG 2025",
      "authors": [
        "Guillaume Balezo",
        "Raphaël Bourgade",
        "Thomas Walter"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Atypical mitotic figures (AMFs) are markers of abnormal cell division\nassociated with poor prognosis, yet their detection remains difficult due to\nlow prevalence, subtle morphology, and inter-observer variability. The MIDOG\n2025 challenge introduces a benchmark for AMF classification across multiple\ndomains. In this work, we evaluate the recently published DINOv3-H+ vision\ntransformer, pretrained on natural images, which we fine-tuned using low-rank\nadaptation (LoRA, 650k trainable parameters) and extensive augmentation.\nDespite the domain gap, DINOv3 transfers effectively to histopathology,\nachieving a balanced accuracy of 0.8871 on the preliminary test set. These\nresults highlight the robustness of DINOv3 pretraining and show that, when\ncombined with parameter-efficient fine-tuning, it provides a strong baseline\nfor atypical mitosis classification in MIDOG 2025.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21041v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21041v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.38,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21044",
      "title": "MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for\n  Efficient Video LLMs",
      "authors": [
        "Junpeng Ma",
        "Qizhe Zhang",
        "Ming Lu",
        "Zhibin Wang",
        "Qiang Zhou",
        "Jun Song",
        "Shanghang Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video Large Language Models (VLLMs) excel in video understanding, but their\nexcessive visual tokens pose a significant computational challenge for\nreal-world applications. Current methods aim to enhance inference efficiency by\nvisual token pruning. However, they do not consider the dynamic characteristics\nand temporal dependencies of video frames, as they perceive video understanding\nas a multi-frame task. To address these challenges, we propose MMG-Vid, a novel\ntraining-free visual token pruning framework that removes redundancy by\nMaximizing Marginal Gains at both segment-level and token-level. Specifically,\nwe first divide the video into segments based on frame similarity, and then\ndynamically allocate the token budget for each segment to maximize the marginal\ngain of each segment. Subsequently, we propose a temporal-guided DPC algorithm\nthat jointly models inter-frame uniqueness and intra-frame diversity, thereby\nmaximizing the marginal gain of each token. By combining both stages, MMG-Vid\ncan maximize the utilization of the limited token budget, significantly\nimproving efficiency while maintaining strong performance. Extensive\nexperiments demonstrate that MMG-Vid can maintain over 99.5% of the original\nperformance, while effectively reducing 75% visual tokens and accelerating the\nprefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21044v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21044v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.402,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on optimizing inference efficiency in Video Large Language Models (VLLMs) through visual token pruning, specifically addressing computational challenges during video understanding. It does not involve distributed training, parallel computing, or strategies for accelerating model training across multiple nodes or processors. The main contributions are centered on token-level and segment-level pruning for inference, with no discussion of training methodologies or distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21046",
      "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification",
      "authors": [
        "Wei Li",
        "Renshan Zhang",
        "Rui Shao",
        "Jie He",
        "Liqiang Nie"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21046v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21046v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.384,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces CogVLA, a framework for efficient Vision-Language-Action models using instruction-driven routing and sparsification, but it does not involve human feedback, reward models, or reinforcement learning for model alignment. The training is based on benchmarks like LIBERO, without any RLHF components.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on mechanisms like EFA-Routing, LFP-Routing, and coupled attention for multimodal coordination, but it does not incorporate diffusion models, iterative refinement, or multi-step logical reasoning processes as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21048",
      "title": "Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning",
      "authors": [
        "Hao Tan",
        "Jun Lan",
        "Zichang Tan",
        "Ajian Liu",
        "Chuanbiao Song",
        "Senyuan Shi",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Jun Wan",
        "Zhen Lei"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deepfake detection remains a formidable challenge due to the complex and\nevolving nature of fake content in real-world scenarios. However, existing\nacademic benchmarks suffer from severe discrepancies from industrial practice,\ntypically featuring homogeneous training sources and low-quality testing\nimages, which hinder the practical deployments of current detectors. To\nmitigate this gap, we introduce HydraFake, a dataset that simulates real-world\nchallenges with hierarchical generalization testing. Specifically, HydraFake\ninvolves diversified deepfake techniques and in-the-wild forgeries, along with\nrigorous training and evaluation protocol, covering unseen model architectures,\nemerging forgery techniques and novel data domains. Building on this resource,\nwe propose Veritas, a multi-modal large language model (MLLM) based deepfake\ndetector. Different from vanilla chain-of-thought (CoT), we introduce\npattern-aware reasoning that involves critical reasoning patterns such as\n\"planning\" and \"self-reflection\" to emulate human forensic process. We further\npropose a two-stage training pipeline to seamlessly internalize such deepfake\nreasoning capacities into current MLLMs. Experiments on HydraFake dataset\nreveal that although previous detectors show great generalization on\ncross-model scenarios, they fall short on unseen forgeries and data domains.\nOur Veritas achieves significant gains across different OOD scenarios, and is\ncapable of delivering transparent and faithful detection outputs.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21048v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21048v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.472,
      "distributed_training_score": 0.388,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a pattern-aware reasoning framework for deepfake detection using multi-modal large language models (MLLMs), focusing on enhancements like planning and self-reflection to Chain-of-Thought processes. However, it does not adapt the iterative refinement process of diffusion models for logical tasks, nor does it treat reasoning as a holistically corrected entity over multiple steps. There is no mention of diffusion models or their mechanisms, making the paper's contributions unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21051",
      "title": "Language Models and Logic Programs for Trustworthy Financial Reasoning",
      "authors": [
        "William Jurayj",
        "Nils Holzenberger",
        "Benjamin Van Durme"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "According to the United States Internal Revenue Service, ''the average\nAmerican spends $\\$270$ and 13 hours filing their taxes''. Even beyond the\nU.S., tax filing requires complex reasoning, combining application of\noverlapping rules with numerical calculations. Because errors can incur costly\npenalties, any automated system must deliver high accuracy and auditability,\nmaking modern large language models (LLMs) poorly suited for this task. We\npropose an approach that integrates LLMs with a symbolic solver to calculate\ntax obligations. We evaluate variants of this system on the challenging\nStAtutory Reasoning Assessment (SARA) dataset, and include a novel method for\nestimating the cost of deploying such a system based on real-world penalties\nfor tax errors. We further show how combining up-front translation of\nplain-text rules into formal logic programs, combined with intelligently\nretrieved exemplars for formal case representations, can dramatically improve\nperformance on this task and reduce costs to well below real-world averages.\nOur results demonstrate the promise and economic feasibility of neuro-symbolic\narchitectures for increasing equitable access to reliable tax assistance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21051v2",
      "pdf_url": "http://arxiv.org/pdf/2508.21051v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.323,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on integrating LLMs with symbolic solvers for tax reasoning, without any mention of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs LLMs and symbolic logic programs for reasoning tasks but does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21052",
      "title": "FakeParts: a New Family of AI-Generated DeepFakes",
      "authors": [
        "Gaetan Brison",
        "Soobash Daiboo",
        "Samy Aimeur",
        "Awais Hussain Sani",
        "Xi Wang",
        "Gianni Franchi",
        "Vicky Kalogeiton"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "We introduce FakeParts, a new class of deepfakes characterized by subtle,\nlocalized manipulations to specific spatial regions or temporal segments of\notherwise authentic videos. Unlike fully synthetic content, these partial\nmanipulations, ranging from altered facial expressions to object substitutions\nand background modifications, blend seamlessly with real elements, making them\nparticularly deceptive and difficult to detect. To address the critical gap in\ndetection capabilities, we present FakePartsBench, the first large-scale\nbenchmark dataset specifically designed to capture the full spectrum of partial\ndeepfakes. Comprising over 25K videos with pixel-level and frame-level\nmanipulation annotations, our dataset enables comprehensive evaluation of\ndetection methods. Our user studies demonstrate that FakeParts reduces human\ndetection accuracy by over 30% compared to traditional deepfakes, with similar\nperformance degradation observed in state-of-the-art detection models. This\nwork identifies an urgent vulnerability in current deepfake detection\napproaches and provides the necessary resources to develop more robust methods\nfor partial video manipulations.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21052v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21052v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.353,
      "datasets_score": 0.423,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the introduction of FakePartsBench, a new large-scale benchmark dataset for partial deepfakes, comprising over 25,000 videos with detailed annotations. This directly involves creating and curating a dataset for AI applications in deepfake detection, as well as benchmarking and evaluating detection methods, which aligns closely with research on datasets for machine learning and AI.",
      "llm_score_status": "completed",
      "summary": "The paper introduces FakeParts, a novel category of deepfakes involving subtle, localized manipulations in specific parts of otherwise authentic videos, which are harder to detect than traditional deepfakes. It presents FakePartsBench, a large-scale dataset with over 25,000 annotated videos, and conducts user studies and evaluations of state-of-the-art detection models to demonstrate significant reductions in detection accuracy—over 30% for humans and up to 43% for AI models—highlighting critical vulnerabilities and providing resources to advance more robust detection methods.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new problem with FakeParts as a distinct family of deepfakes focused on localized manipulations and provides the first large-scale benchmark dataset for this, significantly advancing the state-of-the-art in deepfake detection research.",
      "impact_score": "High",
      "impact_justification": "The work addresses an urgent vulnerability in deepfake detection by revealing detection gaps and supplying a comprehensive dataset, likely influencing future research in AI security, misinformation prevention, and commercial applications for video authentication.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by identifying a new deepfake threat and providing essential tools for improvement, making it valuable for researchers and practitioners in AI and computer vision fields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a4a8f4969180ab89fffd0d56e118ce3d11f37595",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 3,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Gaëtan Brison",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2182938993"
        },
        {
          "name": "Soobash Daiboo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2245712281"
        },
        {
          "name": "Samy Aimeur",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377795026"
        },
        {
          "name": "Awais Hussain Sani",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376536677"
        },
        {
          "name": "Xi Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377912395"
        },
        {
          "name": "Gianni Franchi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377794173"
        },
        {
          "name": "Vicky Kalogeiton",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2306786839"
        }
      ]
    },
    {
      "id": "2508.21058",
      "title": "Mixture of Contexts for Long Video Generation",
      "authors": [
        "Shengqu Cai",
        "Ceyuan Yang",
        "Lvmin Zhang",
        "Yuwei Guo",
        "Junfei Xiao",
        "Ziyan Yang",
        "Yinghao Xu",
        "Zhenheng Yang",
        "Alan Yuille",
        "Leonidas Guibas",
        "Maneesh Agrawala",
        "Lu Jiang",
        "Gordon Wetzstein"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Long video generation is fundamentally a long context memory problem: models\nmust retain and retrieve salient events across a long range without collapsing\nor drifting. However, scaling diffusion transformers to generate long-context\nvideos is fundamentally limited by the quadratic cost of self-attention, which\nmakes memory and computation intractable and difficult to optimize for long\nsequences. We recast long-context video generation as an internal information\nretrieval task and propose a simple, learnable sparse attention routing module,\nMixture of Contexts (MoC), as an effective long-term memory retrieval engine.\nIn MoC, each query dynamically selects a few informative chunks plus mandatory\nanchors (caption, local windows) to attend to, with causal routing that\nprevents loop closures. As we scale the data and gradually sparsify the\nrouting, the model allocates compute to salient history, preserving identities,\nactions, and scenes over minutes of content. Efficiency follows as a byproduct\nof retrieval (near-linear scaling), which enables practical training and\nsynthesis, and the emergence of memory and consistency at the scale of minutes.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21058v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21058v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.54,
      "distributed_training_score": 0.364,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving long video generation in diffusion transformers using a sparse attention mechanism, which involves iterative refinement for visual content. However, it does not adapt this process for complex logical tasks, Chain-of-Thought reasoning, or multi-step logical corrections. The relevance is tangential due to the use of diffusion models, but the core contribution is on video memory and retrieval, not reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21060",
      "title": "Multi-View 3D Point Tracking",
      "authors": [
        "Frano Rajič",
        "Haofei Xu",
        "Marko Mihajlovic",
        "Siyuan Li",
        "Irem Demir",
        "Emircan Gündoğdu",
        "Lei Ke",
        "Sergey Prokudin",
        "Marc Pollefeys",
        "Siyu Tang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce the first data-driven multi-view 3D point tracker, designed to\ntrack arbitrary points in dynamic scenes using multiple camera views. Unlike\nexisting monocular trackers, which struggle with depth ambiguities and\nocclusion, or prior multi-camera methods that require over 20 cameras and\ntedious per-sequence optimization, our feed-forward model directly predicts 3D\ncorrespondences using a practical number of cameras (e.g., four), enabling\nrobust and accurate online tracking. Given known camera poses and either\nsensor-based or estimated multi-view depth, our tracker fuses multi-view\nfeatures into a unified point cloud and applies k-nearest-neighbors correlation\nalongside a transformer-based update to reliably estimate long-range 3D\ncorrespondences, even under occlusion. We train on 5K synthetic multi-view\nKubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and\nDexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.\nOur method generalizes well to diverse camera setups of 1-8 views with varying\nvantage points and video lengths of 24-150 frames. By releasing our tracker\nalongside training and evaluation datasets, we aim to set a new standard for\nmulti-view 3D tracking research and provide a practical tool for real-world\napplications. Project page available at https://ethz-vlg.github.io/mvtracker.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21060v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21060v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.292,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.364,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21061",
      "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn\n  Dialogue with Large Language Models",
      "authors": [
        "Adam Coscia",
        "Shunan Guo",
        "Eunyee Koh",
        "Alex Endert"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "As multi-turn dialogues with large language models (LLMs) grow longer and\nmore complex, how can users better evaluate and review progress on their\nconversational goals? We present OnGoal, an LLM chat interface that helps users\nbetter manage goal progress. OnGoal provides real-time feedback on goal\nalignment through LLM-assisted evaluation, explanations for evaluation results\nwith examples, and overviews of goal progression over time, enabling users to\nnavigate complex dialogues more effectively. Through a study with 20\nparticipants on a writing task, we evaluate OnGoal against a baseline chat\ninterface without goal tracking. Using OnGoal, participants spent less time and\neffort to achieve their goals while exploring new prompting strategies to\novercome miscommunication, suggesting tracking and visualizing goals can\nenhance engagement and resilience in LLM dialogues. Our findings inspired\ndesign implications for future LLM chat interfaces that improve goal\ncommunication, reduce cognitive load, enhance interactivity, and enable\nfeedback to improve LLM performance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21061v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21061v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.332,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces OnGoal, a chat interface for tracking conversational goals in LLMs, and evaluates it through user studies. It does not involve training or fine-tuning AI models using human feedback, a reward model, or reinforcement learning. Instead, it focuses on user interface enhancements and goal management, with no mention of RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes OnGoal, which uses LLMs for goal evaluation and visualization in dialogues, but it does not adapt diffusion models for iterative refinement, multi-step logical reasoning, or treat Chain-of-Thought as a holistic entity. There is no component involving diffusion-based processes for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21063",
      "title": "Prompt-to-Product: Generative Assembly via Bimanual Manipulation",
      "authors": [
        "Ruixuan Liu",
        "Philip Huang",
        "Ava Pun",
        "Kangle Deng",
        "Shobhit Aggarwal",
        "Kevin Tang",
        "Michelle Liu",
        "Deva Ramanan",
        "Jun-Yan Zhu",
        "Jiaoyang Li",
        "Changliu Liu"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Creating assembly products demands significant manual effort and expert\nknowledge in 1) designing the assembly and 2) constructing the product. This\npaper introduces Prompt-to-Product, an automated pipeline that generates\nreal-world assembly products from natural language prompts. Specifically, we\nleverage LEGO bricks as the assembly platform and automate the process of\ncreating brick assembly structures. Given the user design requirements,\nPrompt-to-Product generates physically buildable brick designs, and then\nleverages a bimanual robotic system to construct the real assembly products,\nbringing user imaginations into the real world. We conduct a comprehensive user\nstudy, and the results demonstrate that Prompt-to-Product significantly lowers\nthe barrier and reduces manual effort in creating assembly products from\nimaginative ideas.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21063v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21063v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.262,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21066",
      "title": "OneReward: Unified Mask-Guided Image Generation via Multi-Task Human\n  Preference Learning",
      "authors": [
        "Yuan Gong",
        "Xionghui Wang",
        "Jie Wu",
        "Shiyin Wang",
        "Yitong Wang",
        "Xinglong Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we introduce OneReward, a unified reinforcement learning\nframework that enhances the model's generative capabilities across multiple\ntasks under different evaluation criteria using only \\textit{One Reward} model.\nBy employing a single vision-language model (VLM) as the generative reward\nmodel, which can distinguish the winner and loser for a given task and a given\nevaluation criterion, it can be effectively applied to multi-task generation\nmodels, particularly in contexts with varied data and diverse task objectives.\nWe utilize OneReward for mask-guided image generation, which can be further\ndivided into several sub-tasks such as image fill, image extend, object\nremoval, and text rendering, involving a binary mask as the edit area. Although\nthese domain-specific tasks share same conditioning paradigm, they differ\nsignificantly in underlying data distributions and evaluation metrics. Existing\nmethods often rely on task-specific supervised fine-tuning (SFT), which limits\ngeneralization and training efficiency. Building on OneReward, we develop\nSeedream 3.0 Fill, a mask-guided generation model trained via multi-task\nreinforcement learning directly on a pre-trained base model, eliminating the\nneed for task-specific SFT. Experimental results demonstrate that our unified\nedit model consistently outperforms both commercial and open-source\ncompetitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across\nmultiple evaluation dimensions. Code and model are available at:\nhttps://one-reward.github.io",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21066v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21066v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.508,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.363,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution, OneReward, directly employs a reward model (a VLM) to provide feedback for reinforcement learning, aligning the generative model with human preferences across tasks. This matches the RLHF definition, as the VLM evaluates preferences (e.g., via pairwise comparisons) and is used to fine-tune the policy model, even if human feedback is indirectly sourced through the VLM.",
      "weak_supervision_justification": "The paper focuses on reinforcement learning with a VLM for reward signals, rather than training models using programmatically generated labels from noisy sources. There is no evidence of weak supervision techniques, such as deriving labels from high-level or imprecise data, in the described methodology.",
      "diffusion_reasoning_justification": "The paper discusses diffusion models for image generation tasks like inpainting, but does not adapt them for multi-step logical reasoning or chain-of-thought processes. It lacks any component for holistic correction of reasoning paths, focusing instead on visual generation.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces OneReward, a unified reinforcement learning framework that utilizes a single vision-language model (VLM) as a reward model to enhance multi-task mask-guided image generation, covering tasks such as image filling, extending, object removal, and text rendering. By optimizing a pre-trained base model like Seedream 3.0 directly through reinforcement learning based on human preferences, without task-specific supervised fine-tuning, the authors develop Seedream 3.0 Fill, which outperforms leading commercial and open-source models across various evaluation metrics, demonstrating superior generalization and efficiency.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework, OneReward, which uses a single VLM for multi-task reinforcement learning in image generation, marking the first direct application of RL to multi-task image editing and significantly advancing beyond task-specific methods.",
      "impact_score": "High",
      "impact_justification": "The work could broadly influence future research in generative AI and commercial applications by providing a versatile, efficient framework for multi-task image editing, as evidenced by its superior performance and open-sourced resources.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, innovative contribution to computer vision that advances multi-task image generation, making it essential for researchers in the field to be aware of its methods and results.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/314b3077a2b954736e6d26fa65516696d8016bff",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 5,
      "average_h_index": 1.8333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yuan Gong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378147969"
        },
        {
          "name": "Xionghui Wang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/50141950"
        },
        {
          "name": "Jie Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378200085"
        },
        {
          "name": "Shiyin Wang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2268798428"
        },
        {
          "name": "Yitong Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2268627199"
        },
        {
          "name": "Xinglong Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375440559"
        }
      ]
    },
    {
      "id": "2508.21070",
      "title": "Dress&Dance: Dress up and Dance as You Like It - Technical Preview",
      "authors": [
        "Jun-Kun Chen",
        "Aayush Bansal",
        "Minh Phuoc Vo",
        "Yu-Xiong Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We present Dress&Dance, a video diffusion framework that generates high\nquality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a\nuser wearing desired garments while moving in accordance with a given reference\nvideo. Our approach requires a single user image and supports a range of tops,\nbottoms, and one-piece garments, as well as simultaneous tops and bottoms\ntry-on in a single pass. Key to our framework is CondNet, a novel conditioning\nnetwork that leverages attention to unify multi-modal inputs (text, images, and\nvideos), thereby enhancing garment registration and motion fidelity. CondNet is\ntrained on heterogeneous training data, combining limited video data and a\nlarger, more readily available image dataset, in a multistage progressive\nmanner. Dress&Dance outperforms existing open source and commercial solutions\nand enables a high quality and flexible try-on experience.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21070v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21070v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.322,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a video diffusion framework for generating virtual try-on videos, utilizing diffusion models for visual content creation with multi-modal inputs. However, it does not involve adapting diffusion for complex logical tasks, such as treating a Chain-of-Thought as a single entity for holistic reasoning. The core focus is on image and video generation, not multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21072",
      "title": "First-Place Solution to NeurIPS 2024 Invisible Watermark Removal\n  Challenge",
      "authors": [
        "Fahad Shamshad",
        "Tameem Bakr",
        "Yahia Shaaban",
        "Noor Hussein",
        "Karthik Nandakumar",
        "Nils Lukas"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Content watermarking is an important tool for the authentication and\ncopyright protection of digital media. However, it is unclear whether existing\nwatermarks are robust against adversarial attacks. We present the winning\nsolution to the NeurIPS 2024 Erasing the Invisible challenge, which\nstress-tests watermark robustness under varying degrees of adversary knowledge.\nThe challenge consisted of two tracks: a black-box and beige-box track,\ndepending on whether the adversary knows which watermarking method was used by\nthe provider. For the beige-box track, we leverage an adaptive VAE-based\nevasion attack, with a test-time optimization and color-contrast restoration in\nCIELAB space to preserve the image's quality. For the black-box track, we first\ncluster images based on their artifacts in the spatial or frequency-domain.\nThen, we apply image-to-image diffusion models with controlled noise injection\nand semantic priors from ChatGPT-generated captions to each cluster with\noptimized parameter settings. Empirical evaluations demonstrate that our method\nsuccessfully achieves near-perfect watermark removal (95.7%) with negligible\nimpact on the residual image's quality. We hope that our attacks inspire the\ndevelopment of more robust image watermarking methods.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21072v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21072v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.264,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.267,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21097",
      "title": "Model-Driven Quantum Code Generation Using Large Language Models and\n  Retrieval-Augmented Generation",
      "authors": [
        "Nazanin Siavash",
        "Armin Moin"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces a novel research direction for model-to-text/code\ntransformations by leveraging Large Language Models (LLMs) that can be enhanced\nwith Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum\nand hybrid quantum-classical software systems, where model-driven approaches\ncan help reduce the costs and mitigate the risks associated with the\nheterogeneous platform landscape and lack of developers' skills. We validate\none of the proposed ideas regarding generating code out of UML model instances\nof software systems. This Python code uses a well-established library, called\nQiskit, to execute on gate-based or circuit-based quantum computers. The RAG\npipeline that we deploy incorporates sample Qiskit code from public GitHub\nrepositories. Experimental results show that well-engineered prompts can\nimprove CodeBLEU scores by up to a factor of four, yielding more accurate and\nconsistent quantum code. However, the proposed research direction can go beyond\nthis through further investigation in the future by conducting experiments to\naddress our other research questions and ideas proposed here, such as deploying\nsoftware system model instances as the source of information in the RAG\npipelines, or deploying LLMs for code-to-code transformations, for instance,\nfor transpilation use cases.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21097v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21097v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.505,
      "distributed_training_score": 0.379,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using LLMs with RAG for quantum code generation from models, emphasizing prompt engineering and retrieval to improve accuracy. It does not mention human feedback, reward models, or reinforcement learning for fine-tuning, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes LLMs enhanced with RAG for code generation, which involves retrieving and incorporating external data, but it does not use diffusion models, iterative refinement processes, or multi-step logical reasoning as defined for diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21098",
      "title": "TrInk: Ink Generation with Transformer Network",
      "authors": [
        "Zezhong Jin",
        "Shubhang Desai",
        "Xu Chen",
        "Biyi Fang",
        "Zhuoyi Huang",
        "Zhe Li",
        "Chong-Xin Gan",
        "Xiao Tu",
        "Man-Wai Mak",
        "Yan Lu",
        "Shujie Liu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we propose TrInk, a Transformer-based model for ink\ngeneration, which effectively captures global dependencies. To better\nfacilitate the alignment between the input text and generated stroke points, we\nintroduce scaled positional embeddings and a Gaussian memory mask in the\ncross-attention module. Additionally, we design both subjective and objective\nevaluation pipelines to comprehensively assess the legibility and style\nconsistency of the generated handwriting. Experiments demonstrate that our\nTransformer-based model achieves a 35.56\\% reduction in character error rate\n(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset\ncompared to previous methods. We provide an demo page with handwriting samples\nfrom TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21098v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21098v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.331,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a Transformer-based model for ink generation in handwriting synthesis, focusing on attention mechanisms and stroke prediction. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21099",
      "title": "Safe-Control: A Safety Patch for Mitigating Unsafe Content in\n  Text-to-Image Generation Models",
      "authors": [
        "Xiangtao Meng",
        "Yingkai Dong",
        "Ning Yu",
        "Li Wang",
        "Zheng Li",
        "Shanqing Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Despite the advancements in Text-to-Image (T2I) generation models, their\npotential for misuse or even abuse raises serious safety concerns. Model\ndevelopers have made tremendous efforts to introduce safety mechanisms that can\naddress these concerns in T2I models. However, the existing safety mechanisms,\nwhether external or internal, either remain susceptible to evasion under\ndistribution shifts or require extensive model-specific adjustments. To address\nthese limitations, we introduce Safe-Control, an innovative plug-and-play\nsafety patch designed to mitigate unsafe content generation in T2I models.\nUsing data-driven strategies and safety-aware conditions, Safe-Control injects\nsafety control signals into the locked T2I model, acting as an update in a\npatch-like manner. Model developers can also construct various safety patches\nto meet the evolving safety requirements, which can be flexibly merged into a\nsingle, unified patch. Its plug-and-play design further ensures adaptability,\nmaking it compatible with other T2I models of similar denoising architecture.\nWe conduct extensive evaluations on six diverse and public T2I models.\nEmpirical results highlight that Safe-Control is effective in reducing unsafe\ncontent generation across six diverse T2I models with similar generative\narchitectures, yet it successfully maintains the quality and text alignment of\nbenign images. Compared to seven state-of-the-art safety mechanisms, including\nboth external and internal defenses, Safe-Control significantly outperforms all\nbaselines in reducing unsafe content generation. For example, it reduces the\nprobability of unsafe content generation to 7%, compared to approximately 20%\nfor most baseline methods, under both unsafe prompts and the latest adversarial\nattacks.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21099v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21099v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.356,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a safety patch for Text-to-Image models using data-driven strategies and safety conditions, with no mention of human feedback, reward models, or reinforcement learning techniques. It focuses solely on mitigating unsafe content generation, not on aligning models with human preferences through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper applies safety mechanisms to diffusion-based Text-to-Image models, which use iterative refinement processes similar to diffusion, but it does not adapt these for multi-step logical reasoning or Chain-of-Thought tasks. Instead, it focuses on enhancing safety, making the connection indirect through the underlying model architecture.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21101",
      "title": "Beyond Prediction: Reinforcement Learning as the Defining Leap in\n  Healthcare AI",
      "authors": [
        "Dilruk Perera",
        "Gousia Habib",
        "Qianyi Xu",
        "Daniel J. Tan",
        "Kai He",
        "Erik Cambria",
        "Mengling Feng"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reinforcement learning (RL) marks a fundamental shift in how artificial\nintelligence is applied in healthcare. Instead of merely predicting outcomes,\nRL actively decides interventions with long term goals. Unlike traditional\nmodels that operate on fixed associations, RL systems learn through trial,\nfeedback, and long-term reward optimization, introducing transformative\npossibilities and new risks. From an information fusion lens, healthcare RL\ntypically integrates multi-source signals such as vitals, labs clinical notes,\nimaging and device telemetry using temporal and decision-level mechanisms.\nThese systems can operate within centralized, federated, or edge architectures\nto meet real-time clinical constraints, and naturally span data, features and\ndecision fusion levels. This survey explore RL's rise in healthcare as more\nthan a set of tools, rather a shift toward agentive intelligence in clinical\nenvironments. We first structure the landscape of RL techniques including\nmodel-based and model-free methods, offline and batch-constrained approaches,\nand emerging strategies for reward specification and uncertainty calibration\nthrough the lens of healthcare constraints. We then comprehensively analyze RL\napplications spanning critical care, chronic disease, mental health,\ndiagnostics, and robotic assistance, identifying their trends, gaps, and\ntranslational bottlenecks. In contrast to prior reviews, we critically analyze\nRL's ethical, deployment, and reward design challenges, and synthesize lessons\nfor safe, human-aligned policy learning. This paper serves as both a a\ntechnical roadmap and a critical reflection of RL's emerging transformative\nrole in healthcare AI not as prediction machinery, but as agentive clinical\nintelligence.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21101v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21101v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.547,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.388,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper surveys reinforcement learning (RL) in healthcare, emphasizing its applications, challenges, and human-aligned policy learning, such as in reward design and ethical considerations. While it mentions \"human-aligned policy learning\" and emerging trends like human-in-the-loop systems, it does not specifically address RLHF techniques, such as training a reward model on human-ranked data for fine-tuning. Thus, the connection is indirect and not a core focus.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21102",
      "title": "GENNAV: Polygon Mask Generation for Generalized Referring Navigable\n  Regions",
      "authors": [
        "Kei Katsumata",
        "Yui Iioka",
        "Naoki Hosomi",
        "Teruhisa Misu",
        "Kentaro Yamada",
        "Komei Sugiura"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "We focus on the task of identifying the location of target regions from a\nnatural language instruction and a front camera image captured by a mobility.\nThis task is challenging because it requires both existence prediction and\nsegmentation, particularly for stuff-type target regions with ambiguous\nboundaries. Existing methods often underperform in handling stuff-type target\nregions, in addition to absent or multiple targets. To overcome these\nlimitations, we propose GENNAV, which predicts target existence and generates\nsegmentation masks for multiple stuff-type target regions. To evaluate GENNAV,\nwe constructed a novel benchmark called GRiN-Drive, which includes three\ndistinct types of samples: no-target, single-target, and multi-target. GENNAV\nachieved superior performance over baseline methods on standard evaluation\nmetrics. Furthermore, we conducted real-world experiments with four automobiles\noperated in five geographically distinct urban areas to validate its zero-shot\ntransfer performance. In these experiments, GENNAV outperformed baseline\nmethods and demonstrated its robustness across diverse real-world environments.\nThe project page is available at https://gennav.vercel.app/.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21102v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21102v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.337,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21103",
      "title": "Spatiotemporal EEG-Based Emotion Recognition Using SAM Ratings from\n  Serious Games with Hybrid Deep Learning",
      "authors": [
        "Abdul Rehman",
        "Ilona Heldal",
        "Jerry Chun-Wei Lin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advancements in EEG-based emotion recognition have shown promising\noutcomes using both deep learning and classical machine learning approaches;\nhowever, most existing studies focus narrowly on binary valence prediction or\nsubject-specific classification, which limits generalizability and deployment\nin real-world affective computing systems. To address this gap, this paper\npresents a unified, multigranularity EEG emotion classification framework built\non the GAMEEMO dataset, which consists of 14-channel EEG recordings and\ncontinuous self-reported emotion ratings (boring, horrible, calm, and funny)\nfrom 28 subjects across four emotion-inducing gameplay scenarios. Our pipeline\nemploys a structured preprocessing strategy that comprises temporal window\nsegmentation, hybrid statistical and frequency-domain feature extraction, and\nz-score normalization to convert raw EEG signals into robust, discriminative\ninput vectors. Emotion labels are derived and encoded across three\ncomplementary axes: (i) binary valence classification based on the averaged\npolarity of positive and negative emotion ratings, and (ii) Multi-class emotion\nclassification, where the presence of the most affective state is predicted.\n(iii) Fine-grained multi-label representation via binning each emotion into 10\nordinal classes. We evaluate a broad spectrum of models, including Random\nForest, XGBoost, and SVM, alongside deep neural architectures such as LSTM,\nLSTM-GRU, and CNN-LSTM. Among these, the LSTM-GRU model consistently\noutperforms the others, achieving an F1-score of 0.932 in the binary valence\ntask and 94.5% and 90.6% in both multi-class and Multi-Label emotion\nclassification.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21103v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21103v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.311,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on EEG-based emotion recognition using supervised machine learning and deep learning techniques, such as LSTM and LSTM-GRU, trained on self-reported emotion ratings for classification tasks. While it uses human feedback in the form of ratings for labeling data, this is for standard supervised learning, not for aligning models via a reward model or reinforcement learning as defined in RLHF. There is no mention of reinforcement learning, fine-tuning with human-ranked data, or reward mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21104",
      "title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic\n  Reasoning",
      "authors": [
        "Wenfeng Feng",
        "Penghong Zhao",
        "Guochao Jiang",
        "Chuzhan Hao",
        "Yuewei Zhang",
        "Hao Wang",
        "Guohua Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Critic-free reinforcement learning methods, particularly group policies, have\nattracted considerable attention for their efficiency in complex tasks.\nHowever, these methods rely heavily on multiple sampling and comparisons within\nthe policy to estimate advantage, which may cause the policy to fall into local\noptimum and increase computational cost. To address these issues, we propose\nPVPO, an efficient reinforcement learning method enhanced by an advantage\nreference anchor and data pre-sampling. Specifically, we use the reference\nmodel to rollout in advance and employ the calculated reward score as a\nreference anchor. Our approach effectively corrects the cumulative bias\nintroduced by intra-group comparisons and significantly reduces reliance on the\nnumber of rollouts during training. Meanwhile, the reference model can assess\nsample difficulty during data pre-sampling, enabling effective selection of\nhigh-gain data to improve training efficiency. Moreover, PVPO is orthogonal to\nother advanced critic-free RL algorithms, making it compatible with and\ncomplementary to these methods. Experiments conducted on nine datasets across\ntwo domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance.\nOur approach not only demonstrates robust generalization across multiple tasks,\nbut also exhibits scalable performance across models of varying scales.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21104v2",
      "pdf_url": "http://arxiv.org/pdf/2508.21104v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.495,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.387,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces PVPO, a critic-free RL method that uses a reference model for efficiency in policy optimization, but it does not involve human feedback, human-ranked data, or a reward model trained on human preferences. Instead, it relies on automated processes like data pre-sampling and rollouts, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on RL techniques for policy optimization, such as using a reference model and grouping policies, without any mention of diffusion models, iterative refinement processes, or adapting diffusion for multi-step logical reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21106",
      "title": "Dynamic Low-rank Approximation of Full-Matrix Preconditioner for\n  Training Generalized Linear Models",
      "authors": [
        "Tatyana Matveeva",
        "Aleksandr Katrutsa",
        "Evgeny Frolov"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Adaptive gradient methods like Adagrad and its variants are widespread in\nlarge-scale optimization. However, their use of diagonal preconditioning\nmatrices limits the ability to capture parameter correlations. Full-matrix\nadaptive methods, approximating the exact Hessian, can model these correlations\nand may enable faster convergence. At the same time, their computational and\nmemory costs are often prohibitive for large-scale models. To address this\nlimitation, we propose AdaGram, an optimizer that enables efficient full-matrix\nadaptive gradient updates. To reduce memory and computational overhead, we\nutilize fast symmetric factorization for computing the preconditioned update\ndirection at each iteration. Additionally, we maintain the low-rank structure\nof a preconditioner along the optimization trajectory using matrix integrator\nmethods. Numerical experiments on standard machine learning tasks show that\nAdaGram converges faster or matches the performance of diagonal adaptive\noptimizers when using rank five and smaller rank approximations. This\ndemonstrates AdaGram's potential as a scalable solution for adaptive\noptimization in large models.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21106v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21106v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.397,
      "datasets_score": 0.254,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21107",
      "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning",
      "authors": [
        "Dongjun Lee",
        "Changho Hwang",
        "Kimin Lee"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Unit testing is a core practice in programming, enabling systematic\nevaluation of programs produced by human developers or large language models\n(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have\nbeen employed to automate test generation, yet methods for training LLMs to\nproduce high-quality tests remain underexplored. In this work, we propose UTRL,\na novel reinforcement learning framework that trains an LLM to generate\nhigh-quality unit tests given a programming instruction. Our key idea is to\niteratively train two LLMs, the unit test generator and the code generator, in\nan adversarial manner via reinforcement learning. The unit test generator is\ntrained to maximize a discrimination reward, which reflects its ability to\nproduce tests that expose faults in the code generator's solutions, and the\ncode generator is trained to maximize a code reward, which reflects its ability\nto produce solutions that pass the unit tests generated by the test generator.\nIn our experiments, we demonstrate that unit tests generated by Qwen3-4B\ntrained via UTRL show higher quality compared to unit tests generated by the\nsame model trained via supervised fine-tuning on human-written ground-truth\nunit tests, yielding code evaluations that more closely align with those\ninduced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL\noutperforms frontier models such as GPT-4.1 in generating high-quality unit\ntests, highlighting the effectiveness of UTRL in training LLMs for this task.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21107v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21107v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.341,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes an adversarial reinforcement learning framework for training LLMs to generate unit tests, using rewards based on code discrimination and passing tests, without any involvement of human feedback, rankings, or a separate reward model trained on human data. Since RLHF specifically requires human feedback to align models with preferences, this paper does not qualify.",
      "weak_supervision_justification": "The paper's main contribution involves training LLMs for unit test generation using programmatically derived rewards from ground-truth code and generated code, eliminating the need for hand-labeled unit tests. This aligns directly with weak supervision, as it relies on high-level, noisy sources (e.g., code discrimination rewards) to generate training signals without perfect annotations.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces UTRL, an adversarial reinforcement learning framework designed to train large language models (LLMs) for generating high-quality unit tests without relying on ground-truth annotations. The methodology involves iteratively training two LLMs—a unit test generator that aims to create tests exposing faults in code generated by the second LLM, and a code generator that produces code passing those tests—resulting in improved test quality; experiments demonstrate that Qwen3-4B trained via UTRL outperforms models trained with supervised fine-tuning and even surpasses GPT-4.1, leading to higher code accuracy and more effective evaluation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly novel adversarial RL framework for training LLMs to generate unit tests without ground-truth annotations, significantly advancing the state-of-the-art in automated test generation by addressing scalability issues in existing methods.",
      "impact_score": "High",
      "impact_justification": "This work has the potential to influence a wide range of future research and commercial applications in AI-driven software engineering, such as scalable code generation and testing, by providing a more efficient way to produce high-quality unit tests.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a strong and innovative contribution to LLM applications in software testing, making it valuable for researchers and practitioners in AI and software engineering to understand and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4a8e6bbe2e23e1901675e1de57f14449075f894e",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 0.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Dongjun Lee",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2350520536"
        },
        {
          "name": "Changho Hwang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378129983"
        },
        {
          "name": "Kimin Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378396591"
        }
      ]
    },
    {
      "id": "2508.21109",
      "title": "An Explainable, Attention-Enhanced, Bidirectional Long Short-Term Memory\n  Neural Network for Joint 48-Hour Forecasting of Temperature, Irradiance, and\n  Relative Humidity",
      "authors": [
        "Georgios Vamvouras",
        "Konstantinos Braimakis",
        "Christos Tzivanidis"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents a Deep Learning (DL) framework for 48-hour forecasting of\ntemperature, solar irradiance, and relative humidity to support Model\nPredictive Control (MPC) in smart HVAC systems. The approach employs a stacked\nBidirectional Long Short-Term Memory (BiLSTM) network with attention, capturing\ntemporal and cross-feature dependencies by jointly predicting all three\nvariables. Historical meteorological data (2019-2022) with encoded cyclical\ntime features were used for training, while 2023 data evaluated generalization.\nThe model achieved Mean Absolute Errors of 1.3 degrees Celsius (temperature),\n31 W/m2 (irradiance), and 6.7 percentage points (humidity), outperforming\nstate-of-the-art numerical weather prediction and machine learning benchmarks.\nIntegrated Gradients quantified feature contributions, and attention weights\nrevealed temporal patterns, enhancing interpretability. By combining\nmultivariate forecasting, attention-based DL, and explainability, this work\nadvances data-driven weather prediction. The demonstrated accuracy and\ntransparency highlight the framework's potential for energy-efficient building\ncontrol through reliable short-term meteorological forecasting.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21109v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21109v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.321,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21111",
      "title": "Automating the Deep Space Network Data Systems; A Case Study in Adaptive\n  Anomaly Detection through Agentic AI",
      "authors": [
        "Evan J. Chou",
        "Lisa S. Locke",
        "Harvey M. Soldan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The Deep Space Network (DSN) is NASA's largest network of antenna facilities\nthat generate a large volume of multivariate time-series data. These facilities\ncontain DSN antennas and transmitters that undergo degradation over long\nperiods of time, which may cause costly disruptions to the data flow and\nthreaten the earth-connection of dozens of spacecraft that rely on the Deep\nSpace Network for their lifeline. The purpose of this study was to experiment\nwith different methods that would be able to assist JPL engineers with directly\npinpointing anomalies and equipment degradation through collected data, and\ncontinue conducting maintenance and operations of the DSN for future space\nmissions around our universe. As such, we have researched various machine\nlearning techniques that can fully reconstruct data through predictive\nanalysis, and determine anomalous data entries within real-time datasets\nthrough statistical computations and thresholds. On top of the fully trained\nand tested machine learning models, we have also integrated the use of a\nreinforcement learning subsystem that classifies identified anomalies based on\nseverity level and a Large Language Model that labels an explanation for each\nanomalous data entry, all of which can be improved and fine-tuned over time\nthrough human feedback/input. Specifically, for the DSN transmitters, we have\nalso implemented a full data pipeline system that connects the data extraction,\nparsing, and processing workflow all together as there was no coherent program\nor script for performing these tasks before. Using this data pipeline system,\nwe were able to then also connect the models trained from DSN antenna data,\ncompleting the data workflow for DSN anomaly detection. This was all wrapped\naround and further connected by an agentic AI system, where complex reasoning\nwas utilized to determine the classifications and predictions of anomalous\ndata.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21111v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21111v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.385,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21112",
      "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control",
      "authors": [
        "Delin Qu",
        "Haoming Song",
        "Qizhi Chen",
        "Zhaoqing Chen",
        "Xianqiang Gao",
        "Xinyi Ye",
        "Qi Lv",
        "Modi Shi",
        "Guanghui Ren",
        "Cheng Ruan",
        "Maoqing Yao",
        "Haoran Yang",
        "Jiacheng Bao",
        "Bin Zhao",
        "Dong Wang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The human ability to seamlessly perform multimodal reasoning and physical\ninteraction in the open world is a core goal for general-purpose embodied\nintelligent systems. Recent vision-language-action (VLA) models, which are\nco-trained on large-scale robot and visual-text data, have demonstrated notable\nprogress in general robot control. However, they still fail to achieve\nhuman-level flexibility in interleaved reasoning and interaction. In this work,\nintroduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is\na unified embodied foundation model that achieves superior performance in\nmultimodal embodied reasoning and robot control through interleaved\nvision-text-action pre-training. The development of EO-1 is based on two key\npillars: (i) a unified architecture that processes multimodal inputs\nindiscriminately (image, text, video, and action), and (ii) a massive,\nhigh-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains\nover 1.5 million samples with emphasis on interleaved vision-text-action\ncomprehension. EO-1 is trained through synergies between auto-regressive\ndecoding and flow matching denoising on EO-Data1.5M, enabling seamless robot\naction generation and multimodal embodied reasoning. Extensive experiments\ndemonstrate the effectiveness of interleaved vision-text-action learning for\nopen-world understanding and generalization, validated through a variety of\nlong-horizon, dexterous manipulation tasks across multiple embodiments. This\npaper details the architecture of EO-1, the data construction strategy of\nEO-Data1.5M, and the training methodology, offering valuable insights for\ndeveloping advanced embodied foundation models.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21112v3",
      "pdf_url": "http://arxiv.org/pdf/2508.21112v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.381,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses human annotations for dataset creation (e.g., QA pairs for robot episodes), but there is no evidence of training a reward model or using reinforcement learning to fine-tune the model based on human preferences. The training involves auto-regressive decoding and flow matching denoising on curated data, which is supervised learning, not RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs flow matching denoising as part of training for robot action generation and multimodal reasoning, which is related to diffusion models. However, it does not adapt diffusion for multi-step logical reasoning or treat a 'Chain-of-Thought' as a holistically refined entity, focusing instead on action sequences and embodied tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21113",
      "title": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning",
      "authors": [
        "Qi Yang",
        "Bolin Ni",
        "Shiming Xiang",
        "Han Hu",
        "Houwen Peng",
        "Jie Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking\ncapabilities have demonstrated remarkable performance on complex reasoning\nproblems. However, this thinking process is redundant for simple problems\nsolvable without complex reasoning. To address this inefficiency, we propose\nR-4B, an auto-thinking MLLM, which can adaptively decide when to think based on\nproblem complexity. The central idea of R-4B is to empower the model with both\nthinking and non-thinking capabilities using bi-mode annealing, and apply\nBi-mode Policy Optimization (BPO) to improve the model's accuracy in\ndetermining whether to activate the thinking process. Specifically, we first\ntrain the model on a carefully curated dataset spanning various topics, which\ncontains samples from both thinking and non-thinking modes. Then it undergoes a\nsecond phase of training under an improved GRPO framework, where the policy\nmodel is forced to generate responses from both modes for each input query.\nExperimental results show that R-4B achieves state-of-the-art performance\nacross 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks\nand achieves performance comparable to larger models such as\nKimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower\ncomputational cost.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21113v2",
      "pdf_url": "http://arxiv.org/pdf/2508.21113v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.454,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.55,
      "distributed_training_score": 0.401,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning via Bi-mode Policy Optimization (BPO) with a rule-based reward to fine-tune the model's decision-making for thinking modes, which relates to RL concepts. However, it does not involve human feedback, a reward model trained on human-ranked data, or alignment with human preferences, making it only tangentially relevant to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adaptive thinking in MLLMs using bi-mode annealing and reinforcement learning, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for logical tasks. Thus, it lacks any components related to diffusion-based reasoning.",
      "distributed_training_justification": "The paper discusses training paradigms like bi-mode annealing and BPO for MLLMs but does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors. It is solely focused on model capabilities and optimization, not distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21135",
      "title": "HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object\n  Detection",
      "authors": [
        "Harris Song",
        "Tuan-Anh Vu",
        "Sanjith Menon",
        "Sriram Narasimhan",
        "M. Khalid Jawed"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Detecting hidden or partially concealed objects remains a fundamental\nchallenge in multimodal environments, where factors like occlusion, camouflage,\nand lighting variations significantly hinder performance. Traditional RGB-based\ndetection methods often fail under such adverse conditions, motivating the need\nfor more robust, modality-agnostic approaches. In this work, we present\nHiddenObject, a fusion framework that integrates RGB, thermal, and depth data\nusing a Mamba-based fusion mechanism. Our method captures complementary signals\nacross modalities, enabling enhanced detection of obscured or camouflaged\ntargets. Specifically, the proposed approach identifies modality-specific\nfeatures and fuses them in a unified representation that generalizes well\nacross challenging scenarios. We validate HiddenObject across multiple\nbenchmark datasets, demonstrating state-of-the-art or competitive performance\ncompared to existing methods. These results highlight the efficacy of our\nfusion design and expose key limitations in current unimodal and na\\\"ive fusion\nstrategies. More broadly, our findings suggest that Mamba-based fusion\narchitectures can significantly advance the field of multimodal object\ndetection, especially under visually degraded or complex conditions.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21135v2",
      "pdf_url": "http://arxiv.org/pdf/2508.21135v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.325,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a Mamba-based fusion mechanism for multimodal object detection, focusing on integrating RGB, thermal, and depth data to handle occlusions and camouflage. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21143",
      "title": "Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?",
      "authors": [
        "Samrajnee Ghosh",
        "Naman Agarwal",
        "Hemanshu Garg",
        "Chinmay Mittal",
        "Mausam",
        "Parag Singla"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The reasoning abilities of Multimodal Large Language Models (MLLMs) have\ngarnered a lot of attention in recent times, with advances made in frontiers\nlike coding, mathematics, and science. However, very limited experiments have\nbeen done to assess their performance in simple perception tasks performed over\nuncontaminated, generated images containing basic shapes and structures. To\naddress this issue, the paper introduces a dataset, Percept-V, containing a\ntotal of 7200 program-generated images equally divided into 30 categories, each\ntesting a combination of visual perception skills. Unlike previously proposed\ndatasets, Percept-V comprises very basic tasks of varying complexity that test\nthe perception abilities of MLLMs. This dataset is then tested on\nstate-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large\nReasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their\nperformance. Contrary to the evidence that MLLMs excel in many complex tasks,\nour experiments show a significant drop in the models' performance with\nincreasing problem complexity across all categories. An analysis of the\nperformances also reveals that the tested MLLMs exhibit a similar trend in\naccuracy across categories, testing a particular cognitive skill and find some\nskills to be more difficult than others.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21143v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21143v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.497,
      "distributed_training_score": 0.365,
      "datasets_score": 0.433,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing and evaluating the Percept-V dataset for testing visual perception skills in MLLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It does not involve adapting diffusion techniques for reasoning tasks, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation, description, and evaluation of the Percept-V dataset, which includes program-generated images for benchmarking MLLMs on visual perception tasks. This directly aligns with research on dataset introduction, curation, and analysis for AI applications, as it addresses dataset design based on the TVPS-4 framework and assesses model performance on it.",
      "llm_score_status": "completed",
      "summary": "This paper introduces the Percept-V dataset, consisting of 7200 program-generated images designed to evaluate basic visual perception skills in Multimodal Large Language Models (MLLMs) based on the TVPS-4 framework, addressing the gap in uncontaminated testing for these models. The authors assess state-of-the-art MLLMs like GPT-4o, Gemini, and Claude on 30 categories of tasks, revealing that while these models perform well on complex reasoning tasks, their accuracy significantly declines with increasing problem complexity in basic perception domains, highlighting limitations in their perceptual abilities.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the Percept-V dataset, a clever combination of the TVPS-4 framework with program-generated images to test basic perception in MLLMs, addressing a known gap in uncontaminated evaluation methods. However, it does not introduce a entirely new problem or technique, as it builds on existing cognitive science frameworks.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI model evaluation, particularly for improving MLLMs' perception capabilities through the reusable Percept-V dataset. Nonetheless, its influence may remain confined to specific areas like computer vision and language model testing, limiting broader applicability.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers valuable insights into the limitations of MLLMs in basic perception tasks, making it essential for researchers in AI development and evaluation to understand these gaps. It represents a strong contribution that advances the discussion on model capabilities, though it is not groundbreaking enough to be a must-read for all.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a320446d0dfe7e9f05943c3c8275749c504bcb7d",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 1,
      "average_h_index": 0.16666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Samrajnee Ghosh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378192020"
        },
        {
          "name": "Naman Agarwal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378132224"
        },
        {
          "name": "Hemanshu Garg",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378131700"
        },
        {
          "name": "Mausam Chinmay Mittal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378132602"
        },
        {
          "name": "Parag Singla",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378131387"
        },
        {
          "name": "Iit Delhi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2085631449"
        }
      ]
    },
    {
      "id": "2508.21148",
      "title": "A Survey of Scientific Large Language Models: From Data Foundations to\n  Agent Frontiers",
      "authors": [
        "Ming Hu",
        "Chenglong Ma",
        "Wei Li",
        "Wanghan Xu",
        "Jiamin Wu",
        "Jucheng Hu",
        "Tianbin Li",
        "Guohang Zhuang",
        "Jiaqi Liu",
        "Yingzhou Lu",
        "Ying Chen",
        "Chaoyang Zhang",
        "Cheng Tan",
        "Jie Ying",
        "Guocheng Wu",
        "Shujian Gao",
        "Pengcheng Chen",
        "Jiashi Lin",
        "Haitao Wu",
        "Lulu Chen",
        "Fengxiang Wang",
        "Yuanyuan Zhang",
        "Xiangyu Zhao",
        "Feilong Tang",
        "Encheng Su",
        "Junzhi Ning",
        "Xinyao Liu",
        "Ye Du",
        "Changkai Ji",
        "Cheng Tang",
        "Huihui Xu",
        "Ziyang Chen",
        "Ziyan Huang",
        "Jiyao Liu",
        "Pengfei Jiang",
        "Yizhou Wang",
        "Chen Tang",
        "Jianyu Wu",
        "Yuchen Ren",
        "Siyuan Yan",
        "Zhonghua Wang",
        "Zhongxing Xu",
        "Shiyan Su",
        "Shangquan Sun",
        "Runkai Zhao",
        "Zhisheng Zhang",
        "Yu Liu",
        "Fudi Wang",
        "Yuanfeng Ji",
        "Yanzhou Su",
        "Hongming Shan",
        "Chunmei Feng",
        "Jiahao Xu",
        "Jiangtao Yan",
        "Wenhao Tang",
        "Diping Song",
        "Lihao Liu",
        "Yanyan Huang",
        "Lequan Yu",
        "Bin Fu",
        "Shujun Wang",
        "Xiaomeng Li",
        "Xiaowei Hu",
        "Yun Gu",
        "Ben Fei",
        "Zhongying Deng",
        "Benyou Wang",
        "Yuewen Cao",
        "Minjie Shen",
        "Haodong Duan",
        "Jie Xu",
        "Yirong Chen",
        "Fang Yan",
        "Hongxia Hao",
        "Jielan Li",
        "Jiajun Du",
        "Yanbo Wang",
        "Imran Razzak",
        "Chi Zhang",
        "Lijun Wu",
        "Conghui He",
        "Zhaohui Lu",
        "Jinhai Huang",
        "Yihao Liu",
        "Fenghua Ling",
        "Yuqiang Li",
        "Aoran Wang",
        "Qihao Zheng",
        "Nanqing Dong",
        "Tianfan Fu",
        "Dongzhan Zhou",
        "Yan Lu",
        "Wenlong Zhang",
        "Jin Ye",
        "Jianfei Cai",
        "Wanli Ouyang",
        "Yu Qiao",
        "Zongyuan Ge",
        "Shixiang Tang",
        "Junjun He",
        "Chunfeng Song",
        "Lei Bai",
        "Bowen Zhou"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is\nrepresented, integrated, and applied in scientific research, yet their progress\nis shaped by the complex nature of scientific data. This survey presents a\ncomprehensive, data-centric synthesis that reframes the development of Sci-LLMs\nas a co-evolution between models and their underlying data substrate. We\nformulate a unified taxonomy of scientific data and a hierarchical model of\nscientific knowledge, emphasizing the multimodal, cross-scale, and\ndomain-specific challenges that differentiate scientific corpora from general\nnatural language processing datasets. We systematically review recent Sci-LLMs,\nfrom general-purpose foundations to specialized models across diverse\nscientific disciplines, alongside an extensive analysis of over 270\npre-/post-training datasets, showing why Sci-LLMs pose distinct demands --\nheterogeneous, multi-scale, uncertainty-laden corpora that require\nrepresentations preserving domain invariance and enabling cross-modal\nreasoning. On evaluation, we examine over 190 benchmark datasets and trace a\nshift from static exams toward process- and discovery-oriented assessments with\nadvanced evaluation protocols. These data-centric analyses highlight persistent\nissues in scientific data development and discuss emerging solutions involving\nsemi-automated annotation pipelines and expert validation. Finally, we outline\na paradigm shift toward closed-loop systems where autonomous agents based on\nSci-LLMs actively experiment, validate, and contribute to a living, evolving\nknowledge base. Collectively, this work provides a roadmap for building\ntrustworthy, continually evolving artificial intelligence (AI) systems that\nfunction as a true partner in accelerating scientific discovery.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21148v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21148v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.421,
      "datasets_score": 0.454,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper is a survey on Scientific Large Language Models (Sci-LLMs), focusing on data foundations, model evolution, and agent capabilities. It does not mention or discuss diffusion-based models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. The content centers on LLMs, transfer learning, scaling, and instruction-following, with no reference to treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "The paper discusses the scaling of LLMs, such as GPT-3 with 175 billion parameters and models like Galactica trained on massive datasets, which implicitly involves large-scale computing. However, it does not delve into specific algorithms, systems, or methodologies for distributed training, parallel computing, or multi-node partitioning. The focus is on model development phases rather than detailed technical aspects of distributed systems.",
      "datasets_justification": "The paper's core contribution includes a unified taxonomy of scientific data, analysis of over 270 pre-/post-training datasets, and evaluation of over 190 benchmark datasets. It discusses dataset challenges, curation methods (e.g., semi-automated annotation), and their role in Sci-LLM development, directly aligning with creating, analyzing, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This survey paper examines the development of Scientific Large Language Models (Sci-LLMs) by framing their evolution as a co-evolution with scientific data, introducing a unified taxonomy of scientific data and a hierarchical model of scientific knowledge to address challenges in multimodal, cross-scale, and domain-specific datasets. It systematically reviews over 270 pre-/post-training datasets and 190 benchmark datasets across six scientific domains, traces the progression of Sci-LLMs through four phases from 2018 to 2025, highlights persistent issues like data heterogeneity and evaluation gaps, and proposes a roadmap toward autonomous agents and closed-loop systems for accelerating scientific discovery.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel epistemological framework with a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, significantly advancing how Sci-LLMs are conceptualized and developed beyond existing surveys. This represents a truly new approach to synthesizing the field, addressing gaps in representing heterogeneous scientific corpora.",
      "impact_score": "High",
      "impact_justification": "The work provides a comprehensive roadmap for building trustworthy AI systems in science, likely influencing a wide range of future research and applications in Sci-LLMs across disciplines by emphasizing data-centric solutions and autonomous agents. Its extensive analysis of datasets and benchmarks positions it as a foundational reference that could drive innovations in scientific AI development.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, significant synthesis of Sci-LLMs that is valuable for researchers in AI and scientific computing, providing essential insights into data foundations and future directions. While not groundbreaking original research, its comprehensive overview makes it a strong contribution worth engaging with for those in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cbb510aef3f016f37c2689c8fddaf823364eefe9",
      "total_authors": 103,
      "authors_found": 98,
      "highest_h_index": 26,
      "average_h_index": 2.36734693877551,
      "notable_authors_count": 13,
      "author_h_indexes": [
        {
          "name": "Mingxue Hu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2319940719"
        },
        {
          "name": "Chenglong Ma",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2332089083"
        },
        {
          "name": "Wei Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378198189"
        },
        {
          "name": "Wanghan Xu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2282543731"
        },
        {
          "name": "Jiamin Wu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349198660"
        },
        {
          "name": "Jucheng Hu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356707647"
        },
        {
          "name": "Tian-Xin Li",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2118911039"
        },
        {
          "name": "Guohang Zhuang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2274271773"
        },
        {
          "name": "Jiaqi Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2296737776"
        },
        {
          "name": "Yingzhou Lu",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2276324714"
        },
        {
          "name": "Ying Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2377245226"
        },
        {
          "name": "Chaoyang Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2280285507"
        },
        {
          "name": "Cheng Tan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379603985"
        },
        {
          "name": "Jie Ying",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2362399912"
        },
        {
          "name": "Guocheng Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378197704"
        },
        {
          "name": "Shujian Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378722156"
        },
        {
          "name": "Pengcheng Chen",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2271493720"
        },
        {
          "name": "Jiashi Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375399137"
        },
        {
          "name": "Haitao Wu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2321920847"
        },
        {
          "name": "Lulu Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2348580143"
        },
        {
          "name": "Fengxiang Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2356068143"
        },
        {
          "name": "Yuanyuan Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2366643675"
        },
        {
          "name": "Xiangyu Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2361873415"
        },
        {
          "name": "Feilong Tang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2290911367"
        },
        {
          "name": "Encheng Su",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352108190"
        },
        {
          "name": "Junzhi Ning",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2353285720"
        },
        {
          "name": "Xinyao Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2361910826"
        },
        {
          "name": "Ye Du",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2293356806"
        },
        {
          "name": "Changkai Ji",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376105272"
        },
        {
          "name": "Cheng Tang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2356376560"
        },
        {
          "name": "Huihui Xu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2373743782"
        },
        {
          "name": "Ziyang Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2297912213"
        },
        {
          "name": "Ziyang Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378564520"
        },
        {
          "name": "Jiyao Liu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2353316550"
        },
        {
          "name": "Pengfei Jiang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2304389572"
        },
        {
          "name": "Yizhou Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2202233953"
        },
        {
          "name": "Cheng Tang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2356376560"
        },
        {
          "name": "Jianyu Wu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2308548523"
        },
        {
          "name": "Yuchen Ren",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2258184764"
        },
        {
          "name": "Siyuan Yan",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2261122876"
        },
        {
          "name": "Zhonghua Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2368019251"
        },
        {
          "name": "Zhongxing Xu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2290974030"
        },
        {
          "name": "Shiyan Su",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374366460"
        },
        {
          "name": "Shangquan Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378196108"
        },
        {
          "name": "Runkai Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378187477"
        },
        {
          "name": "Zhisheng Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2376271458"
        },
        {
          "name": "Yu Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2273360925"
        },
        {
          "name": "Fudi Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378683414"
        },
        {
          "name": "Yu Ji",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375252325"
        },
        {
          "name": "Yan-Cheng Su",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/121873665"
        },
        {
          "name": "Hongming Shan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2352941033"
        },
        {
          "name": "Chunmei Feng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2346909464"
        },
        {
          "name": "Jiahao Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378191988"
        },
        {
          "name": "Jiang-Peng Yan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2269117847"
        },
        {
          "name": "Wenhao Tang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Diping Song",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/27062648"
        },
        {
          "name": "Lihao Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2365994221"
        },
        {
          "name": "Yanyan Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2367745347"
        },
        {
          "name": "Lequan Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378284259"
        },
        {
          "name": "Bin Fu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375824874"
        },
        {
          "name": "Shujun Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2293316934"
        },
        {
          "name": "Xiaomeng Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2303237937"
        },
        {
          "name": "Xiaowei Hu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2324676436"
        },
        {
          "name": "Yun Gu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2238545614"
        },
        {
          "name": "Ben Fei",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2330082502"
        },
        {
          "name": "Zhongying Deng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Benyou Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2311641524"
        },
        {
          "name": "Yuewen Cao",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2345185039"
        },
        {
          "name": "Minjie Shen",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2153277187"
        },
        {
          "name": "Haodong Duan",
          "h_index": 26,
          "profile_url": "https://www.semanticscholar.org/author/31463937"
        },
        {
          "name": "Jie Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2341333907"
        },
        {
          "name": "Yirong Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378188066"
        },
        {
          "name": "Fang Yan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2333205819"
        },
        {
          "name": "Hongxia Hao",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2266466627"
        },
        {
          "name": "Jielan Li",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2266802121"
        },
        {
          "name": "Jiajun Du",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378727793"
        },
        {
          "name": "Yanbo Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378185367"
        },
        {
          "name": "Imran Razzak",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2240180680"
        },
        {
          "name": "Chi Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2298723422"
        },
        {
          "name": "Lijun Wu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356573147"
        },
        {
          "name": "Conghui He",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2351236188"
        },
        {
          "name": "Zhaohui Lu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378192302"
        },
        {
          "name": "Jinhai Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378185125"
        },
        {
          "name": "Yihao Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2361197049"
        },
        {
          "name": "Fenghua Ling",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2006398365"
        },
        {
          "name": "Yuqiang Li",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2248091979"
        },
        {
          "name": "Aoran Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2370955980"
        },
        {
          "name": "Qihao Zheng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Nanqing Dong",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349368298"
        },
        {
          "name": "Tianfan Fu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2362307752"
        },
        {
          "name": "Dongzhan Zhou",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2116324147"
        },
        {
          "name": "Yan Lu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2220668751"
        },
        {
          "name": "Wenlong Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2200041892"
        },
        {
          "name": "Jin Ye",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378047679"
        },
        {
          "name": "Jianfei Cai",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2354295763"
        },
        {
          "name": "Wanli Ouyang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2338266436"
        },
        {
          "name": "Yu Qiao",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2261364205"
        },
        {
          "name": "Zongyuan Ge",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2274358979"
        },
        {
          "name": "Shixiang Tang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Junjun He",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359820353"
        },
        {
          "name": "Chunfeng Song",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2321174558"
        },
        {
          "name": "Lei Bai",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2283849531"
        },
        {
          "name": "Bowen Zhou",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2508.21153",
      "title": "WaveLLDM: Design and Development of a Lightweight Latent Diffusion Model\n  for Speech Enhancement and Restoration",
      "authors": [
        "Kevin Putra Santoso",
        "Rizka Wakhidatus Sholikah",
        "Raden Venantius Hari Ginardi"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "High-quality audio is essential in a wide range of applications, including\nonline communication, virtual assistants, and the multimedia industry. However,\ndegradation caused by noise, compression, and transmission artifacts remains a\nmajor challenge. While diffusion models have proven effective for audio\nrestoration, they typically require significant computational resources and\nstruggle to handle longer missing segments. This study introduces WaveLLDM\n(Wave Lightweight Latent Diffusion Model), an architecture that integrates an\nefficient neural audio codec with latent diffusion for audio restoration and\ndenoising. Unlike conventional approaches that operate in the time or spectral\ndomain, WaveLLDM processes audio in a compressed latent space, reducing\ncomputational complexity while preserving reconstruction quality. Empirical\nevaluations on the Voicebank+DEMAND test set demonstrate that WaveLLDM achieves\naccurate spectral reconstruction with low Log-Spectral Distance (LSD) scores\n(0.48 to 0.60) and good adaptability to unseen data. However, it still\nunderperforms compared to state-of-the-art methods in terms of perceptual\nquality and speech clarity, with WB-PESQ scores ranging from 1.62 to 1.71 and\nSTOI scores between 0.76 and 0.78. These limitations are attributed to\nsuboptimal architectural tuning, the absence of fine-tuning, and insufficient\ntraining duration. Nevertheless, the flexible architecture that combines a\nneural audio codec and latent diffusion model provides a strong foundation for\nfuture development.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21153v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21153v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.481,
      "distributed_training_score": 0.32,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using a latent diffusion model for speech enhancement and restoration, which involves iterative refinement for audio processing tasks like denoising. However, it does not adapt diffusion models for multi-step logical reasoning or chain-of-thought processes as defined in the topic. There is no component for solving complex logical tasks, making the paper unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21154",
      "title": "RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D\n  Radiative Gaussians Reconstruction and 3D/3D Registration",
      "authors": [
        "Ao Shen",
        "Xueming Fu",
        "Junfeng Jiang",
        "Qiang Zeng",
        "Ye Tang",
        "Zhengming Chen",
        "Luming Nong",
        "Feng Wang",
        "S. Kevin Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Computed Tomography (CT)/X-ray registration in image-guided navigation\nremains challenging because of its stringent requirements for high accuracy and\nreal-time performance. Traditional \"render and compare\" methods, relying on\niterative projection and comparison, suffer from spatial information loss and\ndomain gap. 3D reconstruction from biplanar X-rays supplements spatial and\nshape information for 2D/3D registration, but current methods are limited by\ndense-view requirements and struggles with noisy X-rays. To address these\nlimitations, we introduce RadGS-Reg, a novel framework for vertebral-level\nCT/X-ray registration through joint 3D Radiative Gaussians (RadGS)\nreconstruction and 3D/3D registration. Specifically, our biplanar X-rays\nvertebral RadGS reconstruction module explores learning-based RadGS\nreconstruction method with a Counterfactual Attention Learning (CAL) mechanism,\nfocusing on vertebral regions in noisy X-rays. Additionally, a patient-specific\npre-training strategy progressively adapts the RadGS-Reg from simulated to real\ndata while simultaneously learning vertebral shape prior knowledge. Experiments\non in-house datasets demonstrate the state-of-the-art performance for both\ntasks, surpassing existing methods. The code is available at:\nhttps://github.com/shenao1995/RadGS_Reg.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21154v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21154v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.289,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.314,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21164",
      "title": "Quantifying Label-Induced Bias in Large Language Model Self- and\n  Cross-Evaluations",
      "authors": [
        "Muskan Saraf",
        "Sajjad Rezvani Boroujeni",
        "Justin Beaudry",
        "Hossein Abedi",
        "Tom Bush"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) are increasingly used to evaluate outputs, yet\ntheir judgments may be influenced. This study examines bias in self- and\ncross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:\nno labels, true labels, and two false-label scenarios. Blog posts authored by\neach model were evaluated by all three using both overall preference voting and\nquality ratings for Coherence, Informativeness, and Conciseness, with all\nscores expressed as percentages for direct comparison. Results reveal striking\nasymmetries: the \"Claude\" label consistently boosts scores, while the \"Gemini\"\nlabel consistently depresses them, regardless of actual content. False labels\nfrequently reversed rankings, producing shifts of up to 50 percentage points in\npreference votes and up to 12 percentage points in converted quality ratings.\nGemini's self-scores collapsed under true labels, while Claude's\nself-preference intensified. These findings show that perceived model identity\ncan heavily distort high-level judgments and subtly influence detailed quality\nratings, underscoring the need for blind or multimodel evaluation protocols to\nensure fairness in LLM benchmarking.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21164v2",
      "pdf_url": "http://arxiv.org/pdf/2508.21164v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.449,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.349,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on biases in LLM evaluations based on labels, not on training models with human feedback or using reinforcement learning for alignment. It does not involve creating a reward model or fine-tuning via human-ranked data, so it has no connection to RLHF.",
      "weak_supervision_justification": "The paper examines label-induced biases in LLM evaluations, but it does not address training models with programmatically generated or noisy labels. It is not about weak supervision techniques for machine learning, making it unrelated.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves generating and evaluating a set of blog posts as experimental data to analyze biases in LLM assessments, which relates to benchmarking and evaluating datasets for AI applications. However, it does not primarily focus on dataset creation, curation, or analysis methodologies, limiting its relevance.",
      "llm_score_status": "completed",
      "summary": "This study examines how labels influence the self- and cross-evaluations of large language models (LLMs) like ChatGPT, Gemini, and Claude by assessing blog posts under four conditions: no labels, true labels, and two false-label scenarios. Using preference voting and quality ratings for Coherence, Informativeness, and Conciseness, the researchers found significant biases, such as the \"Claude\" label consistently boosting scores and the \"Gemini\" label depressing them, with false labels causing up to 50 percentage point shifts in preferences, ultimately recommending blind or multi-model evaluation protocols to mitigate such distortions.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by systematically quantifying label-induced bias across multiple LLMs and conditions, building on prior work to offer a clever combination of experimental designs for evaluating self-preference and cross-model effects. While it advances the field, it does not introduce an entirely new problem, as biases in evaluations have been previously studied.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future LLM benchmarking practices by providing evidence for the need for blind evaluations, potentially leading to citations and adaptations within the AI evaluation subfield. However, its applicability may be limited to specific areas of AI research rather than broader commercial or interdisciplinary impacts.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights into evaluation biases in LLMs, which are essential for researchers working on AI assessment and benchmarking, making it a significant contribution worth engaging with. While not groundbreaking enough to be essential for all, it provides practical recommendations that could improve future studies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8cd23f2d2d670fc7dbcbd5f25a028173a27abf9d",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 1,
      "average_h_index": 0.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Muskan Saraf",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313970015"
        },
        {
          "name": "Sajjad Rezvani Boroujeni",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2367600344"
        },
        {
          "name": "Justin Beaudry",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378132122"
        },
        {
          "name": "Hossein Abedi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2366546773"
        },
        {
          "name": "Tom Bush",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2366547324"
        }
      ]
    },
    {
      "id": "2508.21169",
      "title": "SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic\n  dataset of 3D building models at Level of Detail 4",
      "authors": [
        "Kevin Mayer",
        "Alex Vesel",
        "Xinyi Zhao",
        "Martin Fischer"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D building models are critical for applications in architecture, energy\nsimulation, and navigation. Yet, generating accurate and semantically rich 3D\nbuildings automatically remains a major challenge due to the lack of\nlarge-scale annotated datasets in the public domain. Inspired by the success of\nsynthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse,\nand multi-modal dataset of over 6.2 million synthetic 3D residential buildings\nat Level of Detail (LoD) 4. In the dataset, each building is represented\nthrough three distinct modalities: a semantically enriched 3D wireframe graph\nat LoD 4 (Modality I), the corresponding floor plan images (Modality II), and a\nLiDAR-like roof point cloud (Modality III). The semantic annotations for each\nbuilding wireframe are derived from the corresponding floor plan images and\ninclude information on rooms, doors, and windows. Through its tri-modal nature,\nfuture work can use SYNBUILD-3D to develop novel generative AI algorithms that\nautomate the creation of 3D building models at LoD 4, subject to predefined\nfloor plan layouts and roof geometries, while enforcing semantic-geometric\nconsistency. Dataset and code samples are publicly available at\nhttps://github.com/kdmayer/SYNBUILD-3D.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21169v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21169v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.276,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.319,
      "datasets_score": 0.415,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of SYNBUILD-3D, a new large-scale synthetic dataset for 3D building models, which directly aligns with research on creating datasets for machine learning and AI applications. It details the dataset's creation, including its multi-modal nature (e.g., 3D wireframe graphs, floor plans, and point clouds), semantic annotations, and potential uses for generative AI, fitting the criteria for new dataset introduction and curation methodologies.",
      "llm_score_status": "completed",
      "summary": "The paper introduces SYNBUILD-3D, a large-scale synthetic dataset comprising over 6.2 million 3D residential building models at Level of Detail 4, designed to address the scarcity of annotated data for automated 3D building generation. It employs a multi-modal approach, including semantically enriched 3D wireframe graphs, corresponding floor plan images, and LiDAR-like roof point clouds, with annotations derived from floor plans to ensure semantic-geometric consistency, aiming to facilitate the development of generative AI algorithms for creating accurate 3D models.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new, large-scale multi-modal synthetic dataset for 3D building models at LoD 4, which significantly advances the state-of-the-art by addressing the lack of annotated data and enabling new AI applications. This represents a substantial innovation in computer vision for architectural modeling.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in computer vision, AI-driven architecture, and energy simulation by providing a rich resource for training generative models. Its public availability and tri-modal design could lead to broader commercial applications in 3D modeling and navigation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to computer vision and 3D modeling, making it essential for researchers in these areas to be aware of the dataset for advancing their work. While not universally groundbreaking, its practical utility and innovation warrant attention from relevant subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3da2245503c381e57d4652cd1818ec7e060bf822",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 1,
      "average_h_index": 0.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kevin Mayer",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2243421741"
        },
        {
          "name": "Alex Vesel",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378131129"
        },
        {
          "name": "Xinyi Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379491445"
        },
        {
          "name": "Martin Fischer",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378161272"
        }
      ]
    },
    {
      "id": "2508.21172",
      "title": "Deep Residual Echo State Networks: exploring residual orthogonal\n  connections in untrained Recurrent Neural Networks",
      "authors": [
        "Matteo Pinna",
        "Andrea Ceni",
        "Claudio Gallicchio"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Echo State Networks (ESNs) are a particular type of untrained Recurrent\nNeural Networks (RNNs) within the Reservoir Computing (RC) framework, popular\nfor their fast and efficient learning. However, traditional ESNs often struggle\nwith long-term information processing. In this paper, we introduce a novel\nclass of deep untrained RNNs based on temporal residual connections, called\nDeep Residual Echo State Networks (DeepResESNs). We show that leveraging a\nhierarchy of untrained residual recurrent layers significantly boosts memory\ncapacity and long-term temporal modeling. For the temporal residual\nconnections, we consider different orthogonal configurations, including\nrandomly generated and fixed-structure configurations, and we study their\neffect on network dynamics. A thorough mathematical analysis outlines necessary\nand sufficient conditions to ensure stable dynamics within DeepResESN. Our\nexperiments on a variety of time series tasks showcase the advantages of the\nproposed approach over traditional shallow and deep RC.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21172v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21172v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.382,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21181",
      "title": "FUTURE: Flexible Unlearning for Tree Ensemble",
      "authors": [
        "Ziheng Chen",
        "Jin Huang",
        "Jiali Cheng",
        "Yuchan Guo",
        "Mengjie Wang",
        "Lalitesh Morishetti",
        "Kaushiki Nag",
        "Hadi Amiri"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Tree ensembles are widely recognized for their effectiveness in\nclassification tasks, achieving state-of-the-art performance across diverse\ndomains, including bioinformatics, finance, and medical diagnosis. With\nincreasing emphasis on data privacy and the \\textit{right to be forgotten},\nseveral unlearning algorithms have been proposed to enable tree ensembles to\nforget sensitive information. However, existing methods are often tailored to a\nparticular model or rely on the discrete tree structure, making them difficult\nto generalize to complex ensembles and inefficient for large-scale datasets. To\naddress these limitations, we propose FUTURE, a novel unlearning algorithm for\ntree ensembles. Specifically, we formulate the problem of forgetting samples as\na gradient-based optimization task. In order to accommodate\nnon-differentiability of tree ensembles, we adopt the probabilistic model\napproximations within the optimization framework. This enables end-to-end\nunlearning in an effective and efficient manner. Extensive experiments on\nreal-world datasets show that FUTURE yields significant and successful\nunlearning performance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21181v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21181v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.361,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on machine unlearning for tree ensembles, specifically developing an algorithm to forget specific data points for privacy reasons. It does not involve training models with programmatically generated labels from noisy sources, which is the core of weak supervision. Therefore, there is no direct or indirect connection to weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21184",
      "title": "BED-LLM: Intelligent Information Gathering with LLMs and Bayesian\n  Experimental Design",
      "authors": [
        "Deepro Choudhury",
        "Sinead Williamson",
        "Adam Goliński",
        "Ning Miao",
        "Freddie Bickford Smith",
        "Michael Kirchhof",
        "Yizhe Zhang",
        "Tom Rainforth"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "We propose a general-purpose approach for improving the ability of Large\nLanguage Models (LLMs) to intelligently and adaptively gather information from\na user or other external source using the framework of sequential Bayesian\nexperimental design (BED). This enables LLMs to act as effective multi-turn\nconversational agents and interactively interface with external environments.\nOur approach, which we call BED-LLM (Bayesian Experimental Design with Large\nLanguage Models), is based on iteratively choosing questions or queries that\nmaximize the expected information gain (EIG) about the task of interest given\nthe responses gathered previously. We show how this EIG can be formulated in a\nprincipled way using a probabilistic model derived from the LLM's belief\ndistribution and provide detailed insights into key decisions in its\nconstruction. Further key to the success of BED-LLM are a number of specific\ninnovations, such as a carefully designed estimator for the EIG, not solely\nrelying on in-context updates for conditioning on previous responses, and a\ntargeted strategy for proposing candidate queries. We find that BED-LLM\nachieves substantial gains in performance across a wide range of tests based on\nthe 20-questions game and using the LLM to actively infer user preferences,\ncompared to direct prompting of the LLM and other adaptive design strategies.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21184v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21184v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.468,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.482,
      "distributed_training_score": 0.346,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Bayesian Experimental Design (BED) to enable LLMs to adaptively gather information through iterative questioning, such as in multi-turn conversations or games like 20-questions. It does not involve training or fine-tuning an AI model using human feedback, a reward model, or reinforcement learning techniques. Therefore, it lacks the core elements of RLHF, which centers on aligning models with human preferences via iterative training.",
      "weak_supervision_justification": "The paper's main contribution is a method for adaptive information gathering with LLMs using Bayesian principles, without any discussion of training models through programmatically generated labels from noisy sources. Weak supervision typically involves creating training data from imprecise sources, but this paper deals with runtime decision-making and querying, not label generation or model training.",
      "diffusion_reasoning_justification": "The paper employs sequential Bayesian Experimental Design for iterative information gathering and query selection, but it does not incorporate diffusion models or adapt iterative refinement processes for multi-step logical reasoning. There is no mention of treating reasoning paths as entities for holistic correction, as in diffusion-based approaches; instead, it relies on probabilistic models and expected information gain.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21186",
      "title": "Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics\n  to Softmax Equilibrium",
      "authors": [
        "Christopher R. Lee-Jenkins"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.DS (Dynamical Systems)"
      ],
      "abstract": "Decoding in large language models is often described as scoring tokens and\nnormalizing with softmax. We give a minimal, self-contained account of this\nstep as a constrained variational principle on the probability simplex. The\ndiscrete, normalization-respecting ascent is the classical\nmultiplicative-weights (entropic mirror) update; its continuous-time limit is\nthe replicator flow. From these ingredients we prove that, for a fixed context\nand temperature, the next-token distribution follows a smooth trajectory inside\nthe simplex and converges to the softmax equilibrium. This formalizes the\ncommon ``manifold traversal'' intuition at the output-distribution level. The\nanalysis yields precise, practice-facing consequences: temperature acts as an\nexact rescaling of time along the same trajectory, while top-k and nucleus\nsampling restrict the flow to a face with identical guarantees. We also outline\na controlled account of path-dependent score adjustments and their connection\nto loop-like, hallucination-style behavior. We make no claims about training\ndynamics or internal representations; those are deferred to future work.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21186v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21186v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.26,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.322,
      "datasets_score": 0.218,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution focuses on modeling the next-token prediction process in large language models using concepts like replicator dynamics, softmax equilibrium, and trajectories in the probability simplex. It does not involve diffusion models, iterative refinement for logical tasks, or treating a Chain-of-Thought as a single entity for holistic correction. There is no component related to multi-step logical reasoning via diffusion processes, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21190",
      "title": "Radially Distorted Homographies, Revisited",
      "authors": [
        "Mårten Wadenbäck",
        "Marcus Valtonen Örnhag",
        "Johan Edstedt"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Homographies are among the most prevalent transformations occurring in\ngeometric computer vision and projective geometry, and homography estimation is\nconsequently a crucial step in a wide assortment of computer vision tasks. When\nworking with real images, which are often afflicted with geometric distortions\ncaused by the camera lens, it may be necessary to determine both the homography\nand the lens distortion-particularly the radial component, called radial\ndistortion-simultaneously to obtain anything resembling useful estimates. When\nconsidering a homography with radial distortion between two images, there are\nthree conceptually distinct configurations for the radial distortion; (i)\ndistortion in only one image, (ii) identical distortion in the two images, and\n(iii) independent distortion in the two images. While these cases have been\naddressed separately in the past, the present paper provides a novel and\nunified approach to solve all three cases. We demonstrate how the proposed\napproach can be used to construct new fast, stable, and accurate minimal\nsolvers for radially distorted homographies. In all three cases, our proposed\nsolvers are faster than the existing state-of-the-art solvers while maintaining\nsimilar accuracy. The solvers are tested on well-established benchmarks\nincluding images taken with fisheye cameras. The source code for our solvers\nwill be made available in the event our paper is accepted for publication.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21190v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21190v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.267,
      "weak_supervision_score": 0.241,
      "diffusion_reasoning_score": 0.296,
      "distributed_training_score": 0.253,
      "datasets_score": 0.22,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21197",
      "title": "GCAV: A Global Concept Activation Vector Framework for Cross-Layer\n  Consistency in Interpretability",
      "authors": [
        "Zhenghao He",
        "Sanchit Sinha",
        "Guangzhi Xiong",
        "Aidong Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Concept Activation Vectors (CAVs) provide a powerful approach for\ninterpreting deep neural networks by quantifying their sensitivity to\nhuman-defined concepts. However, when computed independently at different\nlayers, CAVs often exhibit inconsistencies, making cross-layer comparisons\nunreliable. To address this issue, we propose the Global Concept Activation\nVector (GCAV), a novel framework that unifies CAVs into a single, semantically\nconsistent representation. Our method leverages contrastive learning to align\nconcept representations across layers and employs an attention-based fusion\nmechanism to construct a globally integrated CAV. By doing so, our method\nsignificantly reduces the variance in TCAV scores while preserving concept\nrelevance, ensuring more stable and reliable concept attributions. To evaluate\nthe effectiveness of GCAV, we introduce Testing with Global Concept Activation\nVectors (TGCAV) as a method to apply TCAV to GCAV-based representations. We\nconduct extensive experiments on multiple deep neural networks, demonstrating\nthat our method effectively mitigates concept inconsistency across layers,\nenhances concept localization, and improves robustness against adversarial\nperturbations. By integrating cross-layer information into a coherent\nframework, our method offers a more comprehensive and interpretable\nunderstanding of how deep learning models encode human-defined concepts. Code\nand models are available at https://github.com/Zhenghao-He/GCAV.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21197v2",
      "pdf_url": "http://arxiv.org/pdf/2508.21197v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.353,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving concept activation vectors (CAVs) for neural network interpretability using contrastive learning and attention-based fusion, addressing cross-layer consistency in deep learning models. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21201",
      "title": "Improving Aviation Safety Analysis: Automated HFACS Classification Using\n  Reinforcement Learning with Group Relative Policy Optimization",
      "authors": [
        "Arash Ahmadi",
        "Sarah Sharif",
        "Yaser Banad"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Analyzing the human factors behind aviation accidents is crucial for\npreventing future incidents, yet traditional methods using the Human Factors\nAnalysis and Classification System (HFACS) are limited by scalability and\nconsistency. To address this, we introduce an automated HFACS classification\nframework for aviation safety analysis that utilizes Reinforcement Learning\nwith Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B\nlanguage model. Our approach incorporates a multi-component reward system\ntailored for aviation safety analysis and integrates synthetic data generation\nto overcome class imbalance in accident datasets. The resulting GRPO-optimized\nmodel achieved noticeable performance gains, including a 350% increase in exact\nmatch accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy\nof 0.8800. Significantly, our specialized model outperforms state-of-the-art\nLLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key\nmetrics. This research also proposes exact match accuracy in multi-label HFACS\nclassification problem as a new benchmarking methodology to evaluate the\nadvanced reasoning capabilities of language models. Ultimately, our work\nvalidates that smaller, domain-optimized models can provide a computationally\nefficient and better solution for critical safety analysis. This approach makes\npowerful, low-latency deployment on resource-constrained edge devices feasible.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21201v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21201v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.587,
      "weak_supervision_score": 0.423,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.408,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs Reinforcement Learning with Group Relative Policy Optimization (GRPO) and a multi-component reward system, but this system relies on automated evaluations (e.g., correctness, partial match, and an LLM judge) rather than human-ranked data. RLHF specifically requires training with a reward model based on human preferences, which is not present here, making the paper's approach distinct from RLHF.",
      "weak_supervision_justification": "The paper integrates synthetic data generation to address class imbalance in aviation accident datasets, which aligns directly with weak supervision. This technique programmatically generates labels from noisy or imprecise sources, reducing reliance on hand-labeled data, and is a core part of the paper's methodology for improving HFACS classification.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on GRPO for efficient training of a single language model and discusses computational advantages like reduced memory use, but it does not involve distributed training, parallel computing, or partitioning across multiple nodes or processors. There is no mention of strategies for accelerating training via multi-node setups.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper presents an automated framework for classifying Human Factors Analysis and Classification System (HFACS) in aviation safety using Reinforcement Learning with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B language model, addressing scalability and consistency issues in traditional methods. The methodology incorporates a multi-component reward system and synthetic data generation to handle class imbalance, resulting in significant performance improvements, such as a 350% increase in exact match accuracy and superior results compared to larger state-of-the-art LLMs, while proposing exact match accuracy as a new benchmarking methodology for evaluating LLMs in multi-label classification tasks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper introduces a notable improvement by applying GRPO to aviation safety analysis and proposing a new benchmarking methodology, effectively combining existing RL techniques in a new domain to enhance HFACS classification.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in AI applications for aviation safety and potentially other safety domains due to its efficiency gains and performance improvements, though its applicability is primarily within specialized subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a strong, practical contribution to automated safety analysis with RL, making it valuable for researchers in AI and aviation, though not essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3053397b3e646632332d0a332cf1371f3982b8b8",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 1.6666666666666667,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Arash Ahmadi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2304552858"
        },
        {
          "name": "Sarah Sharif",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2334629082"
        },
        {
          "name": "Y. Banad",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2329308043"
        }
      ]
    },
    {
      "id": "2508.21204",
      "title": "Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive\n  Scaffolding",
      "authors": [
        "Vanessa Figueiredo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We study how architectural inductive biases influence the cognitive behavior\nof large language models (LLMs) in instructional dialogue. We introduce a\nsymbolic scaffolding mechanism paired with a short-term memory schema designed\nto promote adaptive, structured reasoning in Socratic tutoring. Using\ncontrolled ablation across five system variants, we evaluate model outputs via\nexpert-designed rubrics covering scaffolding, responsiveness, symbolic\nreasoning, and conversational memory. We present preliminary results using an\nLLM-based evaluation framework aligned to a cognitively grounded rubric. This\nenables scalable, systematic comparisons across architectural variants in\nearly-stage experimentation. The preliminary results show that our full system\nconsistently outperforms baseline variants. Analysis reveals that removing\nmemory or symbolic structure degrades key cognitive behaviors, including\nabstraction, adaptive probing, and conceptual continuity. These findings\nsupport a processing-level account in which architectural scaffolds can\nreliably shape emergent instructional strategies in LLMs.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21204v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21204v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.453,
      "weak_supervision_score": 0.427,
      "diffusion_reasoning_score": 0.506,
      "distributed_training_score": 0.333,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on enhancing LLMs through symbolic scaffolding and memory schemas via prompt engineering, without any mention of training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper does not involve generating training labels from noisy or programmatic sources; instead, it explores architectural modifications for LLMs in instructional dialogues, with no reference to weak supervision methods.",
      "diffusion_reasoning_justification": "The paper introduces symbolic scaffolding and memory for reasoning in LLMs but does not adapt diffusion processes or involve iterative refinement for multi-step logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21206",
      "title": "Enhancing Robustness of Autoregressive Language Models against\n  Orthographic Attacks via Pixel-based Approach",
      "authors": [
        "Han Yang",
        "Jian Lan",
        "Yihong Liu",
        "Hinrich Schütze",
        "Thomas Seidl"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autoregressive language models are vulnerable to orthographic attacks, where\ninput text is perturbed with characters from multilingual alphabets, leading to\nsubstantial performance degradation. This vulnerability primarily stems from\nthe out-of-vocabulary issue inherent in subword tokenizers and their\nembeddings. To address this limitation, we propose a pixel-based generative\nlanguage model that replaces the text-based embeddings with pixel-based\nrepresentations by rendering words as individual images. This design provides\nstronger robustness to noisy inputs, while an extension of compatibility to\nmultilingual text across diverse writing systems. We evaluate the proposed\nmethod on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2\nbenchmark, demonstrating both its resilience to orthographic noise and its\neffectiveness in multilingual settings.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21206v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21206v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.342,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a pixel-based approach for enhancing robustness in autoregressive language models against orthographic attacks, focusing on rendering words as images for better handling of noisy and multilingual text. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as described in the topic definition.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21222",
      "title": "Generalizable Object Re-Identification via Visual In-Context Prompting",
      "authors": [
        "Zhizhong Huang",
        "Xiaoming Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Current object re-identification (ReID) methods train domain-specific models\n(e.g., for persons or vehicles), which lack generalization and demand costly\nlabeled data for new categories. While self-supervised learning reduces\nannotation needs by learning instance-wise invariance, it struggles to capture\n\\textit{identity-sensitive} features critical for ReID. This paper proposes\nVisual In-Context Prompting~(VICP), a novel framework where models trained on\nseen categories can directly generalize to unseen novel categories using only\n\\textit{in-context examples} as prompts, without requiring parameter\nadaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer\nsemantic identity rules from few-shot positive/negative pairs through\ntask-specific prompting, which then guides a VFM (\\eg, DINO) to extract\nID-discriminative features via \\textit{dynamic visual prompts}. By aligning\nLLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables\ngeneralization to novel categories, eliminating the need for dataset-specific\nretraining. To support evaluation, we introduce ShopID10K, a dataset of 10K\nobject instances from e-commerce platforms, featuring multi-view images and\ncross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks\ndemonstrate that VICP outperforms baselines by a clear margin on unseen\ncategories. Code is available at https://github.com/Hzzone/VICP.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21222v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21222v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.365,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a framework for object re-identification using visual in-context prompting with LLMs and vision models, focusing on few-shot learning and generalization without parameter adaptation. It does not involve human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes LLMs for inferring rules and vision models for feature extraction in re-identification tasks, but it does not employ diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21225",
      "title": "Can Layer-wise SSL Features Improve Zero-Shot ASR Performance for\n  Children's Speech?",
      "authors": [
        "Abhijit Sinha",
        "Hemant Kumar Kathania",
        "Sudarsana Reddy Kadiri",
        "Shrikanth Narayanan"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SD (Sound)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Automatic Speech Recognition (ASR) systems often struggle to accurately\nprocess children's speech due to its distinct and highly variable acoustic and\nlinguistic characteristics. While recent advancements in self-supervised\nlearning (SSL) models have greatly enhanced the transcription of adult speech,\naccurately transcribing children's speech remains a significant challenge. This\nstudy investigates the effectiveness of layer-wise features extracted from\nstate-of-the-art SSL pre-trained models - specifically, Wav2Vec2, HuBERT,\nData2Vec, and WavLM in improving the performance of ASR for children's speech\nin zero-shot scenarios. A detailed analysis of features extracted from these\nmodels was conducted, integrating them into a simplified DNN-based ASR system\nusing the Kaldi toolkit. The analysis identified the most effective layers for\nenhancing ASR performance on children's speech in a zero-shot scenario, where\nWSJCAM0 adult speech was used for training and PFSTAR children speech for\ntesting. Experimental results indicated that Layer 22 of the Wav2Vec2 model\nachieved the lowest Word Error Rate (WER) of 5.15%, representing a 51.64%\nrelative improvement over the direct zero-shot decoding using Wav2Vec2 (WER of\n10.65%). Additionally, age group-wise analysis demonstrated consistent\nperformance improvements with increasing age, along with significant gains\nobserved even in younger age groups using the SSL features. Further experiments\non the CMU Kids dataset confirmed similar trends, highlighting the\ngeneralizability of the proposed approach.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21225v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21225v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.333,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper utilizes self-supervised learning (SSL) models, which involve learning from unlabeled data—a concept related to weak supervision as it uses indirect or noisy signals for representation learning. However, the main contribution focuses on extracting and analyzing layer-wise features from pre-trained SSL models for zero-shot ASR on children's speech, rather than directly employing weak supervision techniques for label generation or model training. Thus, while SSL indirectly connects to weak supervision, the paper's core work is on feature adaptation, not weak supervision itself.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21227",
      "title": "Lightweight MRI-Based Automated Segmentation of Pancreatic Cancer with\n  Auto3DSeg",
      "authors": [
        "Keshav Jha",
        "William Sharp",
        "Dominic LaBella"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate delineation of pancreatic tumors is critical for diagnosis,\ntreatment planning, and outcome assessment, yet automated segmentation remains\nchallenging due to anatomical variability and limited dataset availability. In\nthis study, SegResNet models, as part of the Auto3DSeg architecture, were\ntrained and evaluated on two MRI-based pancreatic tumor segmentation tasks as\npart of the 2025 PANTHER Challenge. Algorithm methodology included 5-fold\ncross-validation with STAPLE ensembling after focusing on an anatomically\nrelevant region-of-interest. The Pancreatic Tumor Segmentation on Diagnostic\nMRI task 1 training set included 91 T1-weighted arterial contrast-enhanced MRI\nwith expert annotated pancreas and tumor labels. The Pancreatic Tumor\nSegmentation on MR-Linac task 2 training set used 50 T2-weighted MR-Linac cases\nwith expert annotated pancreas and tumor labels. Algorithm-automated\nsegmentation performance of pancreatic tumor was assessed using Dice Similarity\nCoefficient (DSC), 5 mm DSC, 95th percentile Hausdorff Distance (HD95), Mean\nAverage Surface Distance (MASD), and Root Mean Square Error (RMSE). For Task 1,\nthe algorithm achieved a DSC of 0.56, 5 mm DSC of 0.73, HD95 of 41.1 mm, MASD\nof 26.0 mm, and RMSE of 5164 mm. For Task 2, performance decreased, with a DSC\nof 0.33, 5 mm DSC of 0.50, HD95 of 20.1 mm, MASD of 7.2 mm, and RMSE of 17,203\nmm. These findings illustrate the challenges of MRI-based pancreatic tumor\nsegmentation with small datasets, highlighting variability introduced by\ndifferent MRI sequences. Despite modest performance, the results demonstrate\npotential for automated delineation and emphasize the need for larger,\nstandardized MRI datasets to improve model robustness and clinical utility.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21227v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21227v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.228,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.281,
      "distributed_training_score": 0.31,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21228",
      "title": "Decoding Memories: An Efficient Pipeline for Self-Consistency\n  Hallucination Detection",
      "authors": [
        "Weizhi Gao",
        "Xiaorui Liu",
        "Feiyi Wang",
        "Dan Lu",
        "Junqi Yin"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive performance in both\nresearch and real-world applications, but they still struggle with\nhallucination. Existing hallucination detection methods often perform poorly on\nsentence-level generation or rely heavily on domain-specific knowledge. While\nself-consistency approaches help address these limitations, they incur high\ncomputational costs due to repeated generation. In this paper, we conduct the\nfirst study on identifying redundancy in self-consistency methods, manifested\nas shared prefix tokens across generations, and observe that non-exact-answer\ntokens contribute minimally to the semantic content. Based on these insights,\nwe propose a novel Decoding Memory Pipeline (DMP) that accelerates generation\nthrough selective inference and annealed decoding. Being orthogonal to the\nmodel, dataset, decoding strategy, and self-consistency baseline, our DMP\nconsistently improves the efficiency of multi-response generation and holds\npromise for extension to alignment and reasoning tasks. Extensive experiments\nshow that our method achieves up to a 3x speedup without sacrificing AUROC\nperformance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21228v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21228v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.506,
      "distributed_training_score": 0.427,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving efficiency in hallucination detection for LLMs using self-consistency methods, with no mention of human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses efficiency in generation for hallucination detection, but it does not involve diffusion models, iterative refinement for logical reasoning, or treating a Chain-of-Thought as a holistic entity.",
      "distributed_training_justification": "The paper is about optimizing inference during generation for self-consistency, such as reusing computations, and does not discuss distributed training, parallel computing across nodes, or accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21238",
      "title": "Addressing accuracy and hallucination of LLMs in Alzheimer's disease\n  research through knowledge graphs",
      "authors": [
        "Tingxuan Xu",
        "Jiarui Feng",
        "Justin Melendez",
        "Kaleigh Roberts",
        "Donghong Cai",
        "Mingfang Zhu",
        "Donald Elbert",
        "Yixin Chen",
        "Randall J. Bateman"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In the past two years, large language model (LLM)-based chatbots, such as\nChatGPT, have revolutionized various domains by enabling diverse task\ncompletion and question-answering capabilities. However, their application in\nscientific research remains constrained by challenges such as hallucinations,\nlimited domain-specific knowledge, and lack of explainability or traceability\nfor the response. Graph-based Retrieval-Augmented Generation (GraphRAG) has\nemerged as a promising approach to improving chatbot reliability by integrating\ndomain-specific contextual information before response generation, addressing\nsome limitations of standard LLMs. Despite its potential, there are only\nlimited studies that evaluate GraphRAG on specific domains that require\nintensive knowledge, like Alzheimer's disease or other biomedical domains. In\nthis paper, we assess the quality and traceability of two popular GraphRAG\nsystems. We compile a database of 50 papers and 70 expert questions related to\nAlzheimer's disease, construct a GraphRAG knowledge base, and employ GPT-4o as\nthe LLM for answering queries. We then compare the quality of responses\ngenerated by GraphRAG with those from a standard GPT-4o model. Additionally, we\ndiscuss and evaluate the traceability of several Retrieval-Augmented Generation\n(RAG) and GraphRAG systems. Finally, we provide an easy-to-use interface with a\npre-built Alzheimer's disease database for researchers to test the performance\nof both standard RAG and GraphRAG.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21238v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21238v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.47,
      "distributed_training_score": 0.348,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the use of GraphRAG to enhance LLM accuracy and reduce hallucinations in Alzheimer's disease research by integrating knowledge graphs. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21243",
      "title": "Full-Frequency Temporal Patching and Structured Masking for Enhanced\n  Audio Classification",
      "authors": [
        "Aditya Makineni",
        "Baocheng Geng",
        "Qing Tian"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Transformers and State-Space Models (SSMs) have advanced audio classification\nby modeling spectrograms as sequences of patches. However, existing models such\nas the Audio Spectrogram Transformer (AST) and Audio Mamba (AuM) adopt square\npatching from computer vision, which disrupts continuous frequency patterns and\nproduces an excessive number of patches, slowing training, and increasing\ncomputation. We propose Full-Frequency Temporal Patching (FFTP), a patching\nstrategy that better matches the time-frequency asymmetry of spectrograms by\nspanning full frequency bands with localized temporal context, preserving\nharmonic structure, and significantly reducing patch count and computation. We\nalso introduce SpecMask, a patch-aligned spectrogram augmentation that combines\nfull-frequency and localized time-frequency masks under a fixed masking budget,\nenhancing temporal robustness while preserving spectral continuity. When\napplied on both AST and AuM, our patching method with SpecMask improves mAP by\nup to +6.76 on AudioSet-18k and accuracy by up to +8.46 on SpeechCommandsV2,\nwhile reducing computation by up to 83.26%, demonstrating both performance and\nefficiency gains.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21243v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21243v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.34,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21246",
      "title": "HCQA: Hybrid Classical-Quantum Agent for Generating Optimal Quantum\n  Sensor Circuits",
      "authors": [
        "Ahmad Alomari",
        "Sathish A. P. Kumar"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study proposes an HCQA for designing optimal Quantum Sensor Circuits\n(QSCs) to address complex quantum physics problems. The HCQA integrates\ncomputational intelligence techniques by leveraging a Deep Q-Network (DQN) for\nlearning and policy optimization, enhanced by a quantum-based action selection\nmechanism based on the Q-values. A quantum circuit encodes the agent current\nstate using Ry gates, and then creates a superposition of possible actions.\nMeasurement of the circuit results in probabilistic action outcomes, allowing\nthe agent to generate optimal QSCs by selecting sequences of gates that\nmaximize the Quantum Fisher Information (QFI) while minimizing the number of\ngates. This computational intelligence-driven HCQA enables the automated\ngeneration of entangled quantum states, specifically the squeezed states, with\nhigh QFI sensitivity for quantum state estimation and control. Evaluation of\nthe HCQA on a QSC that consists of two qubits and a sequence of Rx, Ry, and S\ngates demonstrates its efficiency in generating optimal QSCs with a QFI of 1.\nThis work highlights the synergy between AI-driven learning and quantum\ncomputation, illustrating how intelligent agents can autonomously discover\noptimal quantum circuit designs for enhanced sensing and estimation tasks.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21246v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21246v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.296,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21248",
      "title": "Zero-Shot KWS for Children's Speech using Layer-Wise Features from SSL\n  Models",
      "authors": [
        "Subham Kutum",
        "Abhijit Sinha",
        "Hemant Kumar Kathania",
        "Sudarsana Reddy Kadiri",
        "Mahesh Chandra Govil"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)",
        "cs.SD (Sound)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Numerous methods have been proposed to enhance Keyword Spotting (KWS) in\nadult speech, but children's speech presents unique challenges for KWS systems\ndue to its distinct acoustic and linguistic characteristics. This paper\nintroduces a zero-shot KWS approach that leverages state-of-the-art\nself-supervised learning (SSL) models, including Wav2Vec2, HuBERT and Data2Vec.\nFeatures are extracted layer-wise from these SSL models and used to train a\nKaldi-based DNN KWS system. The WSJCAM0 adult speech dataset was used for\ntraining, while the PFSTAR children's speech dataset was used for testing,\ndemonstrating the zero-shot capability of our method. Our approach achieved\nstate-of-the-art results across all keyword sets for children's speech.\nNotably, the Wav2Vec2 model, particularly layer 22, performed the best,\ndelivering an ATWV score of 0.691, a MTWV score of 0.7003 and probability of\nfalse alarm and probability of miss of 0.0164 and 0.0547 respectively, for a\nset of 30 keywords. Furthermore, age-specific performance evaluation confirmed\nthe system's effectiveness across different age groups of children. To assess\nthe system's robustness against noise, additional experiments were conducted\nusing the best-performing layer of the best-performing Wav2Vec2 model. The\nresults demonstrated a significant improvement over traditional MFCC-based\nbaseline, emphasizing the potential of SSL embeddings even in noisy conditions.\nTo further generalize the KWS framework, the experiments were repeated for an\nadditional CMU dataset. Overall the results highlight the significant\ncontribution of SSL features in enhancing Zero-Shot KWS performance for\nchildren's speech, effectively addressing the challenges associated with the\ndistinct characteristics of child speakers.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21248v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21248v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.331,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper utilizes self-supervised learning (SSL) models like Wav2Vec2, which involve generating pseudo-targets from unlabeled data, aligning with weak supervision by relying on noisy or programmatically derived labels rather than fully hand-labeled data. However, the main contribution focuses on applying these pre-trained SSL features to a zero-shot Keyword Spotting (KWS) system for children's speech, with the KWS model itself trained on labeled adult speech data (WSJCAM0). Thus, while weak supervision is integral to the SSL pre-training phase, it is not the primary innovation or central theme of the paper, making it moderately relevant rather than highly focused.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a zero-shot Keyword Spotting (KWS) system for children's speech, leveraging layer-wise features from self-supervised learning models like Wav2Vec2, HuBERT, and Data2Vec, trained on adult speech from the WSJCAM0 dataset and tested on the PFSTAR children's dataset to avoid privacy issues. The methodology involves extracting features from various model layers, achieving state-of-the-art results with Wav2Vec2's layer 22 providing the best performance in terms of ATWV, MTWV, and error rates, while also demonstrating robustness to noise across different conditions and generalizability to the CMU Kids dataset.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing SSL models for zero-shot KWS on children's speech, which is a notable improvement in an underexplored area, but it does not introduce a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in child speech processing and SSL applications by addressing privacy concerns and noise robustness, though its applicability may remain confined to specific subfields like audio processing for children.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to KWS for children's speech with innovative use of SSL features, making it valuable for researchers in speech processing, though it is not essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/162aca2e89272ff31f6cac2c2c9b2dfb1e0612db",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 21,
      "average_h_index": 8.4,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Subham Kutum",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376618116"
        },
        {
          "name": "Abhijit Sinha",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2316768113"
        },
        {
          "name": "H. Kathania",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/9231636"
        },
        {
          "name": "Sudarsana Reddy Kadiri",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/1982304"
        },
        {
          "name": "M. C. Govil",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2229423724"
        }
      ]
    },
    {
      "id": "2508.21249",
      "title": "A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in\n  External Aerodynamics",
      "authors": [
        "Mohammad Amin Nabian",
        "Sanjay Choudhry"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)",
        "math.NA (Numerical Analysis)"
      ],
      "abstract": "The computational cost associated with high-fidelity CFD simulations remains\na significant bottleneck in the automotive design and optimization cycle. While\nML-based surrogate models have emerged as a promising alternative to accelerate\naerodynamic predictions, the field is characterized by a diverse and rapidly\nevolving landscape of specialized neural network architectures, with no single\nmodel demonstrating universal superiority. This paper introduces a novel\nmeta-learning framework that leverages this architectural diversity as a\nstrength. We propose a Mixture of Experts (MoE) model that employs a dedicated\ngating network to dynamically and optimally combine the predictions from three\nheterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable\nmulti-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph\nneural network; and FigConvNet, a factorized implicit global convolution\nnetwork. The gating network learns a spatially-variant weighting strategy,\nassigning credibility to each expert based on its localized performance in\npredicting surface pressure and wall shear stress fields. To prevent model\ncollapse and encourage balanced expert contributions, we integrate an entropy\nregularization term into the training loss function. The entire system is\ntrained and validated on the DrivAerML dataset, a large-scale, public benchmark\nof high-fidelity CFD simulations for automotive aerodynamics. Quantitative\nresults demonstrate that the MoE model achieves a significant reduction in L-2\nprediction error, outperforming not only the ensemble average but also the most\naccurate individual expert model across all evaluated physical quantities. This\nwork establishes the MoE framework as a powerful and effective strategy for\ncreating more robust and accurate composite surrogate models by synergistically\ncombining the complementary strengths of specialized architectures.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21249v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21249v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.406,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Mixture of Experts model for surrogate modeling in aerodynamics, combining predictions from neural networks for CFD simulations. It does not involve diffusion-based processes, iterative refinement for logical tasks, or any form of Chain-of-Thought reasoning. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "The paper discusses the computational challenges of CFD simulations and proposes an ML-based surrogate model, but it does not address distributed training techniques, parallel computing for ML training, or strategies for partitioning data/computation across multiple nodes. The focus is on model architecture and prediction, not on training acceleration methods.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21252",
      "title": "Quantum Machine Learning for Optimizing Entanglement Distribution in\n  Quantum Sensor Circuits",
      "authors": [
        "Laxmisha Ashok Attisara",
        "Sathish Kumar"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In the rapidly evolving field of quantum computing, optimizing quantum\ncircuits for specific tasks is crucial for enhancing performance and\nefficiency. More recently, quantum sensing has become a distinct and rapidly\ngrowing branch of research within the area of quantum science and technology.\nThe field is expected to provide new opportunities, especially regarding high\nsensitivity and precision. Entanglement is one of the key factors in achieving\nhigh sensitivity and measurement precision [3]. This paper presents a novel\napproach utilizing quantum machine learning techniques to optimize entanglement\ndistribution in quantum sensor circuits. By leveraging reinforcement learning\nwithin a quantum environment, we aim to optimize the entanglement layout to\nmaximize Quantum Fisher Information (QFI) and entanglement entropy, which are\nkey indicators of a quantum system's sensitivity and coherence, while\nminimizing circuit depth and gate counts. Our implementation, based on Qiskit,\nintegrates noise models and error mitigation strategies to simulate realistic\nquantum environments. The results demonstrate significant improvements in\ncircuit performance and sensitivity, highlighting the potential of machine\nlearning in quantum circuit optimization by measuring high QFI and entropy in\nthe range of 0.84-1.0 with depth and gate count reduction by 20-86%.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21252v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21252v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.401,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the application of quantum machine learning, specifically reinforcement learning, to optimize entanglement in quantum sensor circuits. It does not address distributed training, parallel computing, or multi-node machine learning strategies, such as partitioning data or computation across processors. The focus is solely on quantum circuit optimization using tools like Qiskit, with no mention of accelerating model training in a distributed manner.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21253",
      "title": "Reinforcement Learning for Optimizing Large Qubit Array based Quantum\n  Sensor Circuits",
      "authors": [
        "Laxmisha Ashok Attisara",
        "Sathish Kumar"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As the number of qubits in a sensor increases, the complexity of designing\nand controlling the quantum circuits grows exponentially. Manually optimizing\nthese circuits becomes infeasible. Optimizing entanglement distribution in\nlarge-scale quantum circuits is critical for enhancing the sensitivity and\nefficiency of quantum sensors [5], [6]. This paper presents an engineering\nintegration of reinforcement learning with tensor-network-based simulation\n(MPS) for scalable circuit optimization for optimizing quantum sensor circuits\nwith up to 60 qubits. To enable efficient simulation and scalability, we adopt\ntensor network methods, specifically the Matrix Product State (MPS)\nrepresentation, instead of traditional state vector or density matrix\napproaches. Our reinforcement learning agent learns to restructure circuits to\nmaximize Quantum Fisher Information (QFI) and entanglement entropy while\nreducing gate counts and circuit depth. Experimental results show consistent\nimprovements, with QFI values approaching 1, entanglement entropy in the\n0.8-1.0 range, and up to 90% reduction in depth and gate count. These results\nhighlight the potential of combining quantum machine learning and tensor\nnetworks to optimize complex quantum circuits under realistic constraints.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21253v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21253v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.375,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using reinforcement learning to optimize quantum sensor circuits via tensor-network simulations and automated metrics like Quantum Fisher Information, without any mention of human feedback, reward models trained on human-ranked data, or alignment with human preferences. This makes it unrelated to Reinforcement Learning from Human Feedback (RLHF).",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21254",
      "title": "Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI\n  Segmentation",
      "authors": [
        "Yidong Zhao",
        "Peter Kellman",
        "Hui Xue",
        "Tongyun Yang",
        "Yi Zhang",
        "Yuchi Han",
        "Orlando Simonetti",
        "Qian Tao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Pretrained segmentation models for cardiac magnetic resonance imaging (MRI)\nstruggle to generalize across different imaging sequences due to significant\nvariations in image contrast. These variations arise from changes in imaging\nprotocols, yet the same fundamental spin properties, including proton density,\nT1, and T2 values, govern all acquired images. With this core principle, we\nintroduce Reverse Imaging, a novel physics-driven method for cardiac MRI data\naugmentation and domain adaptation to fundamentally solve the generalization\nproblem. Our method reversely infers the underlying spin properties from\nobserved cardiac MRI images, by solving ill-posed nonlinear inverse problems\nregularized by the prior distribution of spin properties. We acquire this \"spin\nprior\" by learning a generative diffusion model from the multiparametric\nSAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which\noffers joint cardiac T1 and T2 maps. Our method enables approximate but\nmeaningful spin-property estimates from MR images, which provide an\ninterpretable \"latent variable\" that lead to highly flexible image synthesis of\narbitrary novel sequences. We show that Reverse Imaging enables highly accurate\nsegmentation across vastly different image contrasts and imaging protocols,\nrealizing wide-spectrum generalization of cardiac MRI segmentation.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21254v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21254v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.388,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs a diffusion model as a generative prior to regularize the inverse problem of estimating spin properties from MRI images, involving iterative refinement processes. However, this application is focused on physical and image reconstruction tasks in medical imaging, not on multi-step logical reasoning or treating a 'Chain-of-Thought' as an entity for holistic correction in complex logical tasks. Thus, it shares some conceptual overlap with diffusion-based iteration but does not align with the topic's emphasis on logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21257",
      "title": "PHD: Personalized 3D Human Body Fitting with Point Diffusion",
      "authors": [
        "Hsuan-I Ho",
        "Chen Guo",
        "Po-Chen Wu",
        "Ivan Shugurov",
        "Chengcheng Tang",
        "Abhay Mittal",
        "Sizhe An",
        "Manuel Kaufmann",
        "Linguang Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce PHD, a novel approach for personalized 3D human mesh recovery\n(HMR) and body fitting that leverages user-specific shape information to\nimprove pose estimation accuracy from videos. Traditional HMR methods are\ndesigned to be user-agnostic and optimized for generalization. While these\nmethods often refine poses using constraints derived from the 2D image to\nimprove alignment, this process compromises 3D accuracy by failing to jointly\naccount for person-specific body shapes and the plausibility of 3D poses. In\ncontrast, our pipeline decouples this process by first calibrating the user's\nbody shape and then employing a personalized pose fitting process conditioned\non that shape. To achieve this, we develop a body shape-conditioned 3D pose\nprior, implemented as a Point Diffusion Transformer, which iteratively guides\nthe pose fitting via a Point Distillation Sampling loss. This learned 3D pose\nprior effectively mitigates errors arising from an over-reliance on 2D\nconstraints. Consequently, our approach improves not only pelvis-aligned pose\naccuracy but also absolute pose accuracy -- an important metric often\noverlooked by prior work. Furthermore, our method is highly data-efficient,\nrequiring only synthetic data for training, and serves as a versatile\nplug-and-play module that can be seamlessly integrated with existing 3D pose\nestimators to enhance their performance. Project page:\nhttps://phd-pose.github.io/",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21257v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21257v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.342,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on personalized 3D human body fitting and pose estimation using diffusion models, emphasizing user-specific shape information and synthetic data for training. It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI models with human preferences, which are core to RLHF. Thus, there is no overlap with the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21259",
      "title": "Breaking the Cold-Start Barrier: Reinforcement Learning with Double and\n  Dueling DQNs",
      "authors": [
        "Minda Zhao"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recommender systems struggle to provide accurate suggestions to new users\nwith limited interaction history, a challenge known as the cold-user problem.\nThis paper proposes a reinforcement learning approach using Double and Dueling\nDeep Q-Networks (DQN) to dynamically learn user preferences from sparse\nfeedback, enhancing recommendation accuracy without relying on sensitive\ndemographic data. By integrating these advanced DQN variants with a matrix\nfactorization model, we achieve superior performance on a large e-commerce\ndataset compared to traditional methods like popularity-based and active\nlearning strategies. Experimental results show that our method, particularly\nDueling DQN, reduces Root Mean Square Error (RMSE) for cold users, offering an\neffective solution for privacy-constrained environments.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21259v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21259v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.472,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.384,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with user interactions (e.g., clicks or purchases) as rewards to learn preferences in recommender systems, which involves human feedback indirectly. However, it does not align with RLHF's specific definition, as it lacks a separate reward model trained on human-ranked data for fine-tuning the main model. Instead, it focuses on standard RL techniques like Double and Dueling DQNs for cold-start recommendations, making it only loosely related.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21263",
      "title": "Deep Active Learning for Lung Disease Severity Classification from Chest\n  X-rays: Learning with Less Data in the Presence of Class Imbalance",
      "authors": [
        "Roy M. Gabriel",
        "Mohammadreza Zandehshahvar",
        "Marly van Assen",
        "Nattakorn Kittisut",
        "Kyle Peters",
        "Carlo N. De Cecco",
        "Ali Adibi"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "To reduce the amount of required labeled data for lung disease severity\nclassification from chest X-rays (CXRs) under class imbalance, this study\napplied deep active learning with a Bayesian Neural Network (BNN) approximation\nand weighted loss function. This retrospective study collected 2,319 CXRs from\n963 patients (mean age, 59.2 $\\pm$ 16.6 years; 481 female) at Emory Healthcare\naffiliated hospitals between January and November 2020. All patients had\nclinically confirmed COVID-19. Each CXR was independently labeled by 3 to 6\nboard-certified radiologists as normal, moderate, or severe. A deep neural\nnetwork with Monte Carlo Dropout was trained using active learning to classify\ndisease severity. Various acquisition functions were used to iteratively select\nthe most informative samples from an unlabeled pool. Performance was evaluated\nusing accuracy, area under the receiver operating characteristic curve (AU\nROC), and area under the precision-recall curve (AU PRC). Training time and\nacquisition time were recorded. Statistical analysis included descriptive\nmetrics and performance comparisons across acquisition strategies. Entropy\nSampling achieved 93.7% accuracy (AU ROC, 0.91) in binary classification\n(normal vs. diseased) using 15.4% of the training data. In the multi-class\nsetting, Mean STD sampling achieved 70.3% accuracy (AU ROC, 0.86) using 23.1%\nof the labeled data. These methods outperformed more complex and\ncomputationally expensive acquisition functions and significantly reduced\nlabeling needs. Deep active learning with BNN approximation and weighted loss\neffectively reduces labeled data requirements while addressing class imbalance,\nmaintaining or exceeding diagnostic performance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2508.21263v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21263v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.446,
      "diffusion_reasoning_score": 0.316,
      "distributed_training_score": 0.347,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper uses active learning to selectively label data for lung disease classification, aiming to reduce labeled data requirements under class imbalance. While this shares the goal of minimizing reliance on extensive hand-labeled data, it does not align with weak supervision's core method of programmatically generating large quantities of noisy or imprecise labels from high-level sources. Instead, the paper involves querying for accurate labels from radiologists, making it only indirectly related.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00108",
      "title": "Dual-Stage Global and Local Feature Framework for Image Dehazing",
      "authors": [
        "Anas M. Ali",
        "Anis Koubaa",
        "Bilel Benjdira"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Addressing the challenge of removing atmospheric fog or haze from digital\nimages, known as image dehazing, has recently gained significant traction in\nthe computer vision community. Although contemporary dehazing models have\ndemonstrated promising performance, few have thoroughly investigated\nhigh-resolution imagery. In such scenarios, practitioners often resort to\ndownsampling the input image or processing it in smaller patches, which leads\nto a notable performance degradation. This drop is primarily linked to the\ndifficulty of effectively combining global contextual information with\nlocalized, fine-grained details as the spatial resolution grows. In this\nchapter, we propose a novel framework, termed the Streamlined Global and Local\nFeatures Combinator (SGLC), to bridge this gap and enable robust dehazing for\nhigh-resolution inputs. Our approach is composed of two principal components:\nthe Global Features Generator (GFG) and the Local Features Enhancer (LFE). The\nGFG produces an initial dehazed output by focusing on broad contextual\nunderstanding of the scene. Subsequently, the LFE refines this preliminary\noutput by enhancing localized details and pixel-level features, thereby\ncapturing the interplay between global appearance and local structure. To\nevaluate the effectiveness of SGLC, we integrated it with the Uformer\narchitecture, a state-of-the-art dehazing model. Experimental results on\nhigh-resolution datasets reveal a considerable improvement in peak\nsignal-to-noise ratio (PSNR) when employing SGLC, indicating its potency in\naddressing haze in large-scale imagery. Moreover, the SGLC design is\nmodel-agnostic, allowing any dehazing network to be augmented with the proposed\nglobal-and-local feature fusion mechanism. Through this strategy, practitioners\ncan harness both scene-level cues and granular details, significantly improving\nvisual fidelity in high-resolution environments.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.00108v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00108v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.28,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.311,
      "datasets_score": 0.279,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00110",
      "title": "The Application of Virtual Environments and Artificial Intelligence in\n  Higher Education: Experimental Findings in Philosophy Teaching",
      "authors": [
        "Adel Vehrer",
        "Zsolt Palfalusi"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study explores how virtual environments and artificial intelligence can\nenhance university students' learning experiences, with particular attention to\nthe digital preferences of Generation Z. An experiment was conducted at the\nFaculty of Pedagogy, Humanities, and Social Sciences at University of Gyor,\nwhere Walter's Cube technology and a trained AI mediator were integrated into\nthe instruction of ten philosophical topics. The curriculum was aligned with\nthe official syllabus and enriched with visual content, quotations, and\nexplanatory texts related to iconic figures in philosophy. A total of 77\nfirst-year undergraduate students from full-time humanities and social sciences\nprograms participated in the study. Following their end-of-semester offline\nwritten examination, students voluntarily completed a paper-based, anonymous\nten-question test and provided feedback on the method's effectiveness. No\nsensitive personal data were collected, and the research was conducted with\nformal approval from the Faculty Dean. Descriptive statistics and inferential\ntests were applied to evaluate the impact of the virtual environment and AI\nmediation on learning outcomes. Results indicate that 80 percent of\nparticipants achieved good or excellent final exam grades, and the majority\nrated the virtual material as highly effective. Qualitative feedback emphasized\nincreased motivation and deeper engagement, attributed to the immersive 3D\npresentation and interactive AI support. This research contributes to the\nadvancement of digital pedagogy and suggests new directions for applying\nvirtual and AI-based methods in higher education, particularly in disciplines\nwhere abstract reasoning and conceptual understanding are central.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.00110v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00110v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.278,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.232,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00115",
      "title": "Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems",
      "authors": [
        "Manish Shukla"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Agentic artificial intelligence (AI) -- multi-agent systems that combine\nlarge language models with external tools and autonomous planning -- are\nrapidly transitioning from research laboratories into high-stakes domains. Our\nearlier \"Basic\" paper introduced a five-axis framework and proposed preliminary\nmetrics such as goal drift and harm reduction but did not provide an\nalgorithmic instantiation or empirical evidence. This \"Advanced\" sequel fills\nthat gap. First, we revisit recent benchmarks and industrial deployments to\nshow that technical metrics still dominate evaluations: a systematic review of\n84 papers from 2023--2025 found that 83% report capability metrics while only\n30% consider human-centred or economic axes [2]. Second, we formalise an\nAdaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises\nheterogeneous metrics, applies per-axis exponentially weighted moving-average\nthresholds and performs joint anomaly detection via the Mahalanobis distance\n[7]. Third, we conduct simulations and real-world experiments. AMDM cuts\nanomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and\nreduces false-positive rates from 4.5% to 0.9% compared with static thresholds.\nWe present a comparison table and ROC/PR curves, and we reanalyse case studies\nto surface missing metrics. Code, data and a reproducibility checklist\naccompany this paper to facilitate replication. The code supporting this work\nis available at\nhttps://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.00115v3",
      "pdf_url": "http://arxiv.org/pdf/2509.00115v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.497,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.375,
      "datasets_score": 0.427,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on adaptive monitoring and evaluation of agentic AI systems, including metrics and anomaly detection, but does not involve training AI models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper analyzes existing benchmarks from literature reviews and releases code and data for reproducibility, but its main contribution is the development of a monitoring algorithm, not the creation, analysis, or benchmarking of datasets as a primary focus.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00116",
      "title": "Meta-learning ecological priors from large language models explains\n  human learning and decision making",
      "authors": [
        "Akshay K. Jagadish",
        "Mirko Thalmann",
        "Julian Coda-Forno",
        "Marcel Binz",
        "Eric Schulz"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Human cognition is profoundly shaped by the environments in which it unfolds.\nYet, it remains an open question whether learning and decision making can be\nexplained as a principled adaptation to the statistical structure of real-world\ntasks. We introduce ecologically rational analysis, a computational framework\nthat unifies the normative foundations of rational analysis with ecological\ngrounding. Leveraging large language models to generate ecologically valid\ncognitive tasks at scale, and using meta-learning to derive rational models\noptimized for these environments, we develop a new class of learning\nalgorithms: Ecologically Rational Meta-learned Inference (ERMI). ERMI\ninternalizes the statistical regularities of naturalistic problem spaces and\nadapts flexibly to novel situations, without requiring hand-crafted heuristics\nor explicit parameter updates. We show that ERMI captures human behavior across\n15 experiments spanning function learning, category learning, and decision\nmaking, outperforming several established cognitive models in trial-by-trial\nprediction. Our results suggest that much of human cognition may reflect\nadaptive alignment to the ecological structure of the problems we encounter in\neveryday life.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.00116v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00116v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.481,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.343,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using large language models for task generation and meta-learning to derive models like ERMI, without any involvement of human feedback, reward models, or reinforcement learning for model alignment. It does not describe training on human-ranked data or fine-tuning via RL.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses meta-learning and in-context learning with LLMs for cognitive tasks, but it does not mention diffusion models, iterative refinement processes, or treating reasoning paths as entities for multi-step correction. There is no component for diffusion-based logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00117",
      "title": "Embodied AI: Emerging Risks and Opportunities for Policy Action",
      "authors": [
        "Jared Perlo",
        "Alexander Robey",
        "Fazl Barez",
        "Luciano Floridi",
        "Jakob Mökander"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI\nsystems can exist in, learn from, reason about, and act in the physical world.\nWith recent advances in AI models and hardware, EAI systems are becoming\nincreasingly capable across wider operational domains. While EAI systems can\noffer many benefits, they also pose significant risks, including physical harm\nfrom malicious use, mass surveillance, as well as economic and societal\ndisruption. These risks require urgent attention from policymakers, as existing\npolicies governing industrial robots and autonomous vehicles are insufficient\nto address the full range of concerns EAI systems present. To help address this\nissue, this paper makes three contributions. First, we provide a taxonomy of\nthe physical, informational, economic, and social risks EAI systems pose.\nSecond, we analyze policies in the US, EU, and UK to assess how existing\nframeworks address these risks and to identify critical gaps. We conclude by\noffering policy recommendations for the safe and beneficial deployment of EAI\nsystems, such as mandatory testing and certification schemes, clarified\nliability frameworks, and strategies to manage EAI's potentially transformative\neconomic and societal impacts.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.00117v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00117v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.454,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.333,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is to provide a taxonomy of risks for Embodied AI, analyze policies, and offer recommendations, focusing on societal, economic, and physical implications. It discusses general AI advancements like LLMs and VLAs but does not mention or involve Reinforcement Learning from Human Feedback (RLHF) as a training method or core element.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02582",
      "title": "Application of Quantum Convolutional Neural Networks for MRI-Based Brain\n  Tumor Detection and Classification",
      "authors": [
        "Sugih Pratama Nugraha",
        "Ariiq Islam Alfajri",
        "Tony Sumaryada",
        "Duong Thanh Tai",
        "Nissren Tamam",
        "Abdelmoneim Sulieman",
        "Sitti Yani"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This study explores the application of Quantum Convolutional Neural Networks\n(QCNNs) for brain tumor classification using MRI images, leveraging quantum\ncomputing for enhanced computational efficiency. A dataset of 3,264 MRI images,\nincluding glioma, meningioma, pituitary tumors, and non-tumor cases, was\nutilized. The data was split into 80% training and 20% testing, with an\noversampling technique applied to address class imbalance. The QCNN model\nconsists of quantum convolution layers, flatten layers, and dense layers, with\na filter size of 2, depth of 4, and 4 qubits, trained over 10 epochs. Two\nmodels were developed: a binary classification model distinguishing tumor\npresence and a multiclass classification model categorizing tumor types. The\nbinary model achieved 88% accuracy, improving to 89% after data balancing,\nwhile the multiclass model achieved 52% accuracy, increasing to 62% after\noversampling. Despite strong binary classification performance, the multiclass\nmodel faced challenges due to dataset complexity and quantum circuit\nlimitations. These findings suggest that QCNNs hold promise for medical imaging\napplications, particularly in binary classification. However, further\nrefinements, including optimized quantum circuit architectures and hybrid\nclassical-quantum approaches, are necessary to enhance multiclass\nclassification accuracy and improve QCNN applicability in clinical settings.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.02582v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02582v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.22,
      "weak_supervision_score": 0.271,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.299,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02585",
      "title": "Pan-Cancer mitotic figures detection and domain generalization: MIDOG\n  2025 Challenge",
      "authors": [
        "Zhuoyan Shen",
        "Esther Bär",
        "Maria Hawkins",
        "Konstantin Bräutigam",
        "Charles-Antoine Collins-Fekete"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This report details our submission to the Mitotic Domain Generalization\n(MIDOG) 2025 challenge, which addresses the critical task of mitotic figure\ndetection in histopathology for cancer prognostication. Following the \"Bitter\nLesson\"\\cite{sutton2019bitterlesson} principle that emphasizes data scale over\nalgorithmic novelty, we have publicly released two new datasets to bolster\ntraining data for both conventional \\cite{Shen2024framework} and atypical\nmitoses \\cite{shen_2025_16780587}. Besides, we implement up-to-date training\nmethodologies for both track and reach a Track-1 F1-Score of 0.8407 on our test\nset, as well as a Track-2 balanced accuracy of 0.9107 for atypical mitotic cell\nclassification.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.02585v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02585v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.226,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.31,
      "distributed_training_score": 0.349,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02586",
      "title": "MitoDetect++: A Domain-Robust Pipeline for Mitosis Detection and\n  Atypical Subtyping",
      "authors": [
        "Esha Sadia Nasir",
        "Jiaqi Lv",
        "Mostafa Jahanifar",
        "Shan E Ahmed Raza"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Automated detection and classification of mitotic figures especially\ndistinguishing atypical from normal remain critical challenges in computational\npathology. We present MitoDetect++, a unified deep learning pipeline designed\nfor the MIDOG 2025 challenge, addressing both mitosis detection and atypical\nmitosis classification. For detection (Track 1), we employ a U-Net-based\nencoder-decoder architecture with EfficientNetV2-L as the backbone, enhanced\nwith attention modules, and trained via combined segmentation losses. For\nclassification (Track 2), we leverage the Virchow2 vision transformer,\nfine-tuned efficiently using Low-Rank Adaptation (LoRA) to minimize resource\nconsumption. To improve generalization and mitigate domain shifts, we integrate\nstrong augmentations, focal loss, and group-aware stratified 5-fold\ncross-validation. At inference, we deploy test-time augmentation (TTA) to boost\nrobustness. Our method achieves a balanced accuracy of 0.892 across validation\ndomains, highlighting its clinical applicability and scalability across tasks.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.02586v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02586v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.26,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.377,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02588",
      "title": "Sequential Hard Mining: a data-centric approach for Mitosis Detection",
      "authors": [
        "Maxime W. Lafarge",
        "Viktor H. Koelzer"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With a continuously growing availability of annotated datasets of mitotic\nfigures in histology images, finding the best way to optimally use with this\nunprecedented amount of data to optimally train deep learning models has become\na new challenge. Here, we build upon previously proposed approaches with a\nfocus on efficient sampling of training data inspired by boosting techniques\nand present our candidate solutions for the two tracks of the MIDOG 2025\nchallenge.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.02588v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02588v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.411,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on a data-centric approach for mitosis detection, emphasizing efficient data sampling, augmentation techniques, and sequential hard mining for training a single neural network. It does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across multiple processors or nodes, making it unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02589",
      "title": "Normal and Atypical Mitosis Image Classifier using Efficient Vision\n  Transformer",
      "authors": [
        "Xuan Qi",
        "Dominic Labella",
        "Thomas Sanford",
        "Maxwell Lee"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We tackle atypical versus normal mitosis classification in the MIDOG 2025\nchallenge using EfficientViT-L2, a hybrid CNN--ViT architecture optimized for\naccuracy and efficiency. A unified dataset of 13,938 nuclei from seven cancer\ntypes (MIDOG++ and AMi-Br) was used, with atypical mitoses comprising ~15. To\nassess domain generalization, we applied leave-one-cancer-type-out\ncross-validation with 5-fold ensembles, using stain-deconvolution for image\naugmentation. For challenge submissions, we trained an ensemble with the same\n5-fold split but on all cancer types. In the preliminary evaluation phase, this\nmodel achieved balanced accuracy of 0.859, ROC AUC of 0.942, and raw accuracy\nof 0.85, demonstrating competitive and well-balanced performance across\nmetrics.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.02589v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02589v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.226,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.324,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04460",
      "title": "CoCoNUTS: Concentrating on Content while Neglecting Uninformative\n  Textual Styles for AI-Generated Peer Review Detection",
      "authors": [
        "Yihan Chen",
        "Jiawei Chen",
        "Guozhao Mo",
        "Xuanang Chen",
        "Ben He",
        "Xianpei Han",
        "Le Sun"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The growing integration of large language models (LLMs) into the peer review\nprocess presents potential risks to the fairness and reliability of scholarly\nevaluation. While LLMs offer valuable assistance for reviewers with language\nrefinement, there is growing concern over their use to generate substantive\nreview content. Existing general AI-generated text detectors are vulnerable to\nparaphrasing attacks and struggle to distinguish between surface language\nrefinement and substantial content generation, suggesting that they primarily\nrely on stylistic cues. When applied to peer review, this limitation can result\nin unfairly suspecting reviews with permissible AI-assisted language\nenhancement, while failing to catch deceptively humanized AI-generated reviews.\nTo address this, we propose a paradigm shift from style-based to content-based\ndetection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark\nbuilt upon a fine-grained dataset of AI-generated peer reviews, covering six\ndistinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an\nAI review detector via a multi-task learning framework, designed to achieve\nmore accurate and robust detection of AI involvement in review content. Our\nwork offers a practical foundation for evaluating the use of LLMs in peer\nreview, and contributes to the development of more precise, equitable, and\nreliable detection methods for real-world scholarly applications. Our code and\ndata will be publicly available at https://github.com/Y1hanChen/COCONUTS.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.04460v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04460v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.36,
      "datasets_score": 0.457,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on detecting AI-generated content in peer reviews using benchmarks and multi-task learning, with no mention of training models via human-ranked data or reinforcement learning for alignment. It does not involve RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a benchmark and detector for AI-generated text but does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes as defined. There is no component related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating and introducing CoCoNUTS, a new benchmark dataset for AI-generated peer review detection, along with its curation, categorization, and evaluation, which directly aligns with research on datasets for machine learning applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of existing AI-generated text detectors, which focus on stylistic cues and fail to accurately identify AI involvement in peer reviews, by proposing a content-based detection paradigm. It introduces the CoCoNUTS benchmark, a fine-grained dataset encompassing six modes of human-AI collaboration categorized into Human, Mix, and AI, and develops CoCoDet, a multi-task learning framework that emphasizes content composition to detect AI use more accurately. Evaluations demonstrate that CoCoDet achieves over 98% macro F1-score, outperforms general detectors and LLMs, and reveals an increasing trend of AI usage in real-world conference reviews, providing a foundation for more reliable and equitable detection methods in scholarly applications.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel content-based detection paradigm, a new benchmark (CoCoNUTS), and a specialized detector (CoCoDet) that significantly advance the state-of-the-art in AI-generated text detection for peer reviews by addressing the shortcomings of style-dependent methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and practices in AI ethics and peer review processes within computational linguistics and artificial intelligence subfields, as it provides tools for detecting AI misuse that could be adopted by academic conferences.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution to a timely issue in academic integrity, offering practical innovations that are valuable for researchers in AI and scholarly communication to understand and address AI's role in peer reviews.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/512cf8e64799c7340c3d4f7de168d18e8392c2a9",
      "total_authors": 7,
      "authors_found": 6,
      "highest_h_index": 24,
      "average_h_index": 7.166666666666667,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Yihan Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372306506"
        },
        {
          "name": "Jiawei Chen",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2115448879"
        },
        {
          "name": "Guozhao Mo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2239885325"
        },
        {
          "name": "Xuanang Chen",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/49794910"
        },
        {
          "name": "Ben He",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xianpei Han",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/2118233348"
        },
        {
          "name": "Le Sun",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2372156137"
        }
      ]
    },
    {
      "id": "2509.04462",
      "title": "Benchmarking GPT-5 for biomedical natural language processing",
      "authors": [
        "Yu Hou",
        "Zaifu Zhan",
        "Rui Zhang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid expansion of biomedical literature has heightened the need for\nscalable natural language processing (NLP) solutions. While GPT-4 substantially\nnarrowed the gap with task-specific systems, especially in question answering,\nits performance across other domains remained uneven. We updated a standardized\nBioNLP benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot\nprompting across 12 datasets spanning six task families: named entity\nrecognition, relation extraction, multi-label document classification, question\nanswering, text summarization, and text simplification. Using fixed prompt\ntemplates, identical decoding parameters, and batch inference, we report\nprimary metrics per dataset and include prior results for GPT-4, GPT-3.5, and\nLLaMA-2-13B for comparison. GPT-5 achieved the strongest overall benchmark\nperformance, with macro-average scores rising to 0.557 under five-shot\nprompting versus 0.506 for GPT-4 and 0.508 for GPT-4o. On MedQA, GPT-5 reached\n94.1% accuracy, exceeding the previous supervised state of the art by over\nfifty points, and attained parity with supervised systems on PubMedQA (0.734).\nIn extraction tasks, GPT-5 delivered major gains in chemical NER (0.886 F1) and\nChemProt relation extraction (0.616 F1), outperforming GPT-4 and GPT-4o, though\nsummarization and disease NER still lagged behind domain-specific baselines.\nThese results establish GPT-5 as a general-purpose model now offering\ndeployment-ready performance for reasoning-oriented biomedical QA, while\nprecision-critical extraction and evidence-dense summarization continue to\nfavor fine-tuned or hybrid approaches. The benchmark delineates where simple\nprompting suffices and where retrieval-augmented or planning-based scaffolds\nare likely required, providing actionable guidance for BioNLP system design as\nfrontier models advance.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.04462v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04462v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.362,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04463",
      "title": "Multiscale Graph Neural Network for Turbulent Flow-Thermal Prediction\n  Around a Complex-Shaped Pin-Fin",
      "authors": [
        "Riddhiman Raut",
        "Evan M. Mihalko",
        "Amrita Basak"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.NA (Numerical Analysis)",
        "math.NA (Numerical Analysis)"
      ],
      "abstract": "This study presents the development of a domain-responsive edge-aware\nmultiscale Graph Neural Network for predicting steady, turbulent flow and\nthermal behavior in a two-dimensional channel containing arbitrarily shaped\ncomplex pin-fin geometries. The training dataset was constructed through an\nautomated framework that integrated geometry generation, meshing, and\nflow-field solutions in ANSYS Fluent. The pin-fin geometry was parameterized\nusing piecewise cubic splines, producing 1,000 diverse configurations through\nLatin Hypercube Sampling. Each simulation was converted into a graph structure,\nwhere nodes carried a feature vector containing spatial coordinates, a\nnormalized streamwise position, one-hot boundary indicators, and a signed\ndistance to the nearest boundary such as wall. This graph structure served as\ninput to the newly developed Graph Neural Network, which was trained to predict\ntemperature, velocity magnitude, and pressure at each node using data from\nANSYS. The network predicted fields with outstanding accuracy, capturing\nboundary layers, recirculation, and the stagnation region upstream of the\npin-fins while reducing wall time by 2-3 orders of magnitude. In conclusion,\nthe novel graph neural network offered a fast and reliable surrogate for\nsimulations in complex flow configurations.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.04463v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04463v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.273,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.374,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04464",
      "title": "Can Multiple Responses from an LLM Reveal the Sources of Its\n  Uncertainty?",
      "authors": [
        "Yang Nan",
        "Pengfei He",
        "Ravi Tandon",
        "Han Xu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have delivered significant breakthroughs across\ndiverse domains but can still produce unreliable or misleading outputs, posing\ncritical challenges for real-world applications. While many recent studies\nfocus on quantifying model uncertainty, relatively little work has been devoted\nto \\textit{diagnosing the source of uncertainty}. In this study, we show that,\nwhen an LLM is uncertain, the patterns of disagreement among its multiple\ngenerated responses contain rich clues about the underlying cause of\nuncertainty. To illustrate this point, we collect multiple responses from a\ntarget LLM and employ an auxiliary LLM to analyze their patterns of\ndisagreement. The auxiliary model is tasked to reason about the likely source\nof uncertainty, such as whether it stems from ambiguity in the input question,\na lack of relevant knowledge, or both. In cases involving knowledge gaps, the\nauxiliary model also identifies the specific missing facts or concepts\ncontributing to the uncertainty. In our experiment, we validate our framework\non AmbigQA, OpenBookQA, and MMLU-Pro, confirming its generality in diagnosing\ndistinct uncertainty sources. Such diagnosis shows the potential for relevant\nmanual interventions that improve LLM performance and reliability.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.04464v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04464v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.445,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.359,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is diagnosing sources of uncertainty in LLMs by analyzing multiple responses, without any mention of training models using human feedback, reward models, or reinforcement learning techniques. It does not involve aligning AI with human preferences through RLHF.",
      "weak_supervision_justification": "The paper does not address training models with programmatically generated or noisy labels, as in weak supervision. Instead, it focuses on generating and analyzing multiple LLM responses to diagnose uncertainty, without relying on or discussing weak supervision methods for label creation or model training.",
      "diffusion_reasoning_justification": "The paper's framework involves generating multiple responses and using an auxiliary LLM for analysis, but it does not employ diffusion models, iterative refinement processes, or multi-step logical reasoning in the context of diffusion. There is no adaptation of diffusion techniques for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04465",
      "title": "Emotionally-Aware Agents for Dispute Resolution",
      "authors": [
        "Sushrita Rakshit",
        "James Hale",
        "Kushal Chawla",
        "Jeanne M. Brett",
        "Jonathan Gratch"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In conflict, people use emotional expressions to shape their counterparts'\nthoughts, feelings, and actions. This paper explores whether automatic text\nemotion recognition offers insight into this influence in the context of\ndispute resolution. Prior work has shown the promise of such methods in\nnegotiations; however, disputes evoke stronger emotions and different social\nprocesses. We use a large corpus of buyer-seller dispute dialogues to\ninvestigate how emotional expressions shape subjective and objective outcomes.\nWe further demonstrate that large-language models yield considerably greater\nexplanatory power than previous methods for emotion intensity annotation and\nbetter match the decisions of human annotators. Findings support existing\ntheoretical models for how emotional expressions contribute to conflict\nescalation and resolution and suggest that agent-based systems could be useful\nin managing disputes by recognizing and potentially mitigating emotional\nescalation.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.04465v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04465v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.274,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the use of large language models for emotion recognition in dispute resolution dialogues, focusing on predicting outcomes and analyzing emotional dynamics. It does not involve training AI models with human-ranked data to create a reward model or fine-tuning via reinforcement learning, which are core elements of RLHF. Therefore, there is no direct or indirect connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04466",
      "title": "Just-in-time and distributed task representations in language models",
      "authors": [
        "Yuxuan Li",
        "Declan Campbell",
        "Stephanie C. Y. Chan",
        "Andrew Kyle Lampinen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Many of language models' impressive capabilities originate from their\nin-context learning: based on instructions or examples, they can infer and\nperform new tasks without weight updates. In this work, we investigate\n\\emph{when} representations for new tasks are formed in language models, and\n\\emph{how} these representations change over the course of context. We focus on\n''transferrable'' task representations -- vector representations that can\nrestore task context in another instance of the model, even without the full\nprompt. We show that these representations evolve in non-monotonic and sporadic\nways, and are distinct from a more inert representation of high-level task\ncategories that persists throughout the context. Specifically, models often\ncondense multiple evidence into these transferrable task representations, which\nalign well with the performance improvement based on more examples in the\ncontext. However, this accrual process exhibits strong locality along the\nsequence dimension, coming online only at certain tokens -- despite task\nidentity being reliably decodable throughout the context. Moreover, these local\nbut transferrable task representations tend to capture minimal ''task scopes'',\nsuch as a semantically-independent subtask, and models rely on more\ntemporally-distributed representations to support longer and composite tasks.\nThis two-fold locality (temporal and semantic) underscores a kind of\njust-in-time computational process underlying language models' ability to adapt\nto new evidence and learn new tasks on the fly.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.04466v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04466v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.498,
      "distributed_training_score": 0.435,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper investigates task representations and in-context learning in language models, focusing on how representations form and evolve during processing. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "The paper analyzes internal dynamics of language models for task adaptation, with no mention of distributed training techniques, parallel computing, multi-node systems, or methods for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05311",
      "title": "Large Language Model Integration with Reinforcement Learning to Augment\n  Decision-Making in Autonomous Cyber Operations",
      "authors": [
        "Konur Tholl",
        "François Rivest",
        "Mariam El Mezouar",
        "Ranwa Al Mallah"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reinforcement Learning (RL) has shown great potential for autonomous\ndecision-making in the cybersecurity domain, enabling agents to learn through\ndirect environment interaction. However, RL agents in Autonomous Cyber\nOperations (ACO) typically learn from scratch, requiring them to execute\nundesirable actions to learn their consequences. In this study, we integrate\nexternal knowledge in the form of a Large Language Model (LLM) pretrained on\ncybersecurity data that our RL agent can directly leverage to make informed\ndecisions. By guiding initial training with an LLM, we improve baseline\nperformance and reduce the need for exploratory actions with obviously negative\noutcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity\nenvironment, and demonstrate that our guided agent achieves over 2x higher\nrewards during early training and converges to a favorable policy approximately\n4,500 episodes faster than the baseline.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.05311v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05311v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.515,
      "weak_supervision_score": 0.428,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.391,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper integrates a pretrained Large Language Model (LLM) with Reinforcement Learning (RL) to guide decision-making, but it does not involve human feedback, such as ranking data to train a reward model. Instead, the LLM provides external knowledge directly, with no mention of human preferences or fine-tuning based on human-ranked data, which is core to RLHF.",
      "weak_supervision_justification": "The paper uses a pretrained LLM to provide guidance (e.g., action masking and auxiliary loss signals) in RL training, which could indirectly relate to weak supervision by leveraging noisy or imprecise external knowledge sources. However, the core focus is on augmenting RL for cyber operations, not on programmatically generating labels for model training, making it only a loose connection.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches. It focuses solely on integrating an LLM with RL for decision-making in cybersecurity, with no components related to diffusion or chain-of-thought refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.06974",
      "title": "Individualized and Interpretable Sleep Forecasting via a Two-Stage\n  Adaptive Spatial-Temporal Model",
      "authors": [
        "Xueyi Wang",
        "Elisabeth Wilhelm"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Sleep quality significantly impacts well-being. Therefore, healthcare\nproviders and individuals need accessible and reliable forecasting tools for\npreventive interventions. This paper introduces an interpretable,\nindividualized two-stage adaptive spatial-temporal model for predicting sleep\nquality scores. Our proposed framework combines multi-scale convolutional\nlayers to model spatial interactions across multiple input variables, recurrent\nlayers and attention mechanisms to capture long-term temporal dependencies, and\na two-stage domain adaptation strategy to enhance generalization. The first\nadaptation stage is applied during training to mitigate overfitting on the\ntraining set. In the second stage, a source-free test-time adaptation mechanism\nis employed to adapt the model to new users without requiring labels. We\nconducted various experiments with five input window sizes (3, 5, 7, 9, and 11\ndays) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model\nconsistently outperformed time series forecasting baseline approaches,\nincluding Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The\nbest performance was achieved with a three-day input window and a one-day\nprediction window, yielding a root mean square error (RMSE) of 0.216.\nFurthermore, the model demonstrated good predictive performance even for longer\nforecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction\nwindow), highlighting its practical utility for real-world applications. We\nalso conducted an explainability analysis to examine how different features\ninfluence sleep quality. These findings proved that the proposed framework\noffers a robust, adaptive, and explainable solution for personalized sleep\nforecasting using sparse data from commercial wearable devices.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.06974v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06974v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.327,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.06975",
      "title": "GSTBench: A Benchmark Study on the Transferability of Graph\n  Self-Supervised Learning",
      "authors": [
        "Yu Song",
        "Zhigang Hua",
        "Yan Xie",
        "Jingzhe Liu",
        "Bo Long",
        "Hui Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Self-supervised learning (SSL) has shown great promise in graph\nrepresentation learning. However, most existing graph SSL methods are developed\nand evaluated under a single-dataset setting, leaving their cross-dataset\ntransferability largely unexplored and limiting their ability to leverage\nknowledge transfer and large-scale pretraining, factors that are critical for\ndeveloping generalized intelligence beyond fitting training data. To address\nthis gap and advance foundation model research for graphs, we present GSTBench,\nthe first systematic benchmark for evaluating the transferability of graph SSL\nmethods. We conduct large-scale pretraining on ogbn-papers100M and evaluate\nfive representative SSL methods across a diverse set of target graphs. Our\nstandardized experimental setup decouples confounding factors such as model\narchitecture, dataset characteristics, and adaptation protocols, enabling\nrigorous comparisons focused solely on pretraining objectives. Surprisingly, we\nobserve that most graph SSL methods struggle to generalize, with some\nperforming worse than random initialization. In contrast, GraphMAE, a masked\nautoencoder approach, consistently improves transfer performance. We analyze\nthe underlying factors that drive these differences and offer insights to guide\nfuture research on transferable graph SSL, laying a solid foundation for the\n\"pretrain-then-transfer\" paradigm in graph learning. Our code is available at\nhttps://github.com/SongYYYY/GSTBench.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.06975v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06975v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.395,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12211",
      "title": "TinyServe: Query-Aware Cache Selection for Efficient LLM Serving",
      "authors": [
        "Dong Liu",
        "Yanxuan Yu"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Serving large language models (LLMs) efficiently remains challenging due to\nthe high memory and latency overhead of key-value (KV) cache access during\nautoregressive decoding. We present \\textbf{TinyServe}, a lightweight and\nextensible serving system for deploying tiny LLMs (e.g., TinyLLaMA, GPT2-345M)\nwith support for structured KV sparsity, plugin-based token selection, and\nhardware-efficient attention kernels. Unlike prior simulation frameworks,\nTinyServe executes real-time decoding with configurable sparsity strategies and\nfine-grained instrumentation.\n  To reduce decoding cost, we introduce a \\textit{query-aware page selection}\nmechanism that leverages bounding-box metadata to estimate attention relevance\nbetween the query and KV cache blocks. This enables selective KV loading with\nminimal overhead and no model modifications. Our fused CUDA kernel integrates\npage scoring, sparse memory access, and masked attention in a single pass.\n  Experiments show that TinyServe achieves up to \\textbf{3.4x} speedup and over\n\\textbf{2x} memory savings with negligible accuracy drop. Additional analysis\nof cache reuse, page hit rate, and multi-GPU scaling confirms its practicality\nas an efficient system-level design for LLM training and inference research on\nresource-constrained hardware.",
      "published_date": "2025-08-28",
      "arxiv_url": "http://arxiv.org/abs/2509.12211v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12211v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.463,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper introduces TinyServe, a system focused on efficient serving of LLMs, including aspects like query-aware cache selection and hardware-efficient kernels for inference. While it mentions multi-GPU scaling and training acceleration features (e.g., gradient checkpointing for fine-tuning), these are secondary to the main emphasis on inference efficiency. Distributed training typically involves core techniques for parallelizing model training across nodes, which this paper only touches on peripherally through hardware optimizations, not as its primary contribution.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 229,
  "date": "2025-08-28"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
