<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 19 September 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 19 September 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 19 September 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2509.15496",
      "title": "Lynx: Towards High-Fidelity Personalized Video Generation",
      "authors": [
        "Shen Sang",
        "Tiancheng Zhi",
        "Tianpei Gu",
        "Jing Liu",
        "Linjie Luo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present Lynx, a high-fidelity model for personalized video synthesis from\na single input image. Built on an open-source Diffusion Transformer (DiT)\nfoundation model, Lynx introduces two lightweight adapters to ensure identity\nfidelity. The ID-adapter employs a Perceiver Resampler to convert\nArcFace-derived facial embeddings into compact identity tokens for\nconditioning, while the Ref-adapter integrates dense VAE features from a frozen\nreference pathway, injecting fine-grained details across all transformer layers\nthrough cross-attention. These modules collectively enable robust identity\npreservation while maintaining temporal coherence and visual realism. Through\nevaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which\nyielded 800 test cases, Lynx has demonstrated superior face resemblance,\ncompetitive prompt following, and strong video quality, thereby advancing the\nstate of personalized video generation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15496v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15496v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.292,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.28,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for personalized video generation, specifically adapting Diffusion Transformers for high-fidelity video synthesis with identity preservation. It does not involve adapting the iterative refinement process of diffusion models for complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as an entity. There is no component related to logical reasoning, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15497",
      "title": "Backdoor Mitigation via Invertible Pruning Masks",
      "authors": [
        "Kealan Dunnett",
        "Reza Arablouei",
        "Dimity Miller",
        "Volkan Dedeoglu",
        "Raja Jurdak"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Model pruning has gained traction as a promising defense strategy against\nbackdoor attacks in deep learning. However, existing pruning-based approaches\noften fall short in accurately identifying and removing the specific parameters\nresponsible for inducing backdoor behaviors. Despite the dominance of\nfine-tuning-based defenses in recent literature, largely due to their superior\nperformance, pruning remains a compelling alternative, offering greater\ninterpretability and improved robustness in low-data regimes. In this paper, we\npropose a novel pruning approach featuring a learned \\emph{selection} mechanism\nto identify parameters critical to both main and backdoor tasks, along with an\n\\emph{invertible} pruning mask designed to simultaneously achieve two\ncomplementary goals: eliminating the backdoor task while preserving it through\nthe inverse mask. We formulate this as a bi-level optimization problem that\njointly learns selection variables, a sparse invertible mask, and\nsample-specific backdoor perturbations derived from clean data. The inner\nproblem synthesizes candidate triggers using the inverse mask, while the outer\nproblem refines the mask to suppress backdoor behavior without impairing\nclean-task accuracy. Extensive experiments demonstrate that our approach\noutperforms existing pruning-based backdoor mitigation approaches, maintains\nstrong performance under limited data conditions, and achieves competitive\nresults compared to state-of-the-art fine-tuning approaches. Notably, the\nproposed approach is particularly effective in restoring correct predictions\nfor compromised samples after successful backdoor mitigation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15497v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15497v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.343,
      "datasets_score": 0.26,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15510",
      "title": "The (Short-Term) Effects of Large Language Models on Unemployment and\n  Earnings",
      "authors": [
        "Danqing Chen",
        "Carina Kane",
        "Austin Kozlowski",
        "Nadav Kunievsky",
        "James A. Evans"
      ],
      "categories": [
        "econ.GN (General Economics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Large Language Models have spread rapidly since the release of ChatGPT in\nlate 2022, accompanied by claims of major productivity gains but also concerns\nabout job displacement. This paper examines the short-run labor market effects\nof LLM adoption by comparing earnings and unemployment across occupations with\ndiffering levels of exposure to these technologies. Using a Synthetic\nDifference in Differences approach, we estimate the impact of LLM exposure on\nearnings and unemployment. Our findings show that workers in highly exposed\noccupations experienced earnings increases following ChatGPT's introduction,\nwhile unemployment rates remained unchanged. These results suggest that initial\nlabor market adjustments to LLMs operate primarily through earnings rather than\nworker reallocation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15510v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15510v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.335,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15514",
      "title": "MEC-Quant: Maximum Entropy Coding for Extremely Low Bit\n  Quantization-Aware Training",
      "authors": [
        "Junbiao Pang",
        "Tianyang Cai",
        "Baochang Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Quantization-Aware Training (QAT) has driven much attention to produce\nefficient neural networks. Current QAT still obtains inferior performances\ncompared with the Full Precision (FP) counterpart. In this work, we argue that\nquantization inevitably introduce biases into the learned representation,\nespecially under the extremely low-bit setting. To cope with this issue, we\npropose Maximum Entropy Coding Quantization (MEC-Quant), a more principled\nobjective that explicitly optimizes on the structure of the representation, so\nthat the learned representation is less biased and thus generalizes better to\nunseen in-distribution samples. To make the objective end-to-end trainable, we\npropose to leverage the minimal coding length in lossy data coding as a\ncomputationally tractable surrogate for the entropy, and further derive a\nscalable reformulation of the objective based on Mixture Of Experts (MOE) that\nnot only allows fast computation but also handles the long-tailed distribution\nfor weights or activation values. Extensive experiments on various tasks on\ncomputer vision tasks prove its superiority. With MEC-Qaunt, the limit of QAT\nis pushed to the x-bit activation for the first time and the accuracy of\nMEC-Quant is comparable to or even surpass the FP counterpart. Without bells\nand whistles, MEC-Qaunt establishes a new state of the art for QAT.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15514v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15514v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.406,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is on Quantization-Aware Training (QAT) for neural networks, specifically proposing MEC-Quant to optimize representations and reduce biases in low-bit quantization. It discusses techniques like entropy coding and Mixture Of Experts for computational efficiency, but these are not related to distributed training, parallel computing, or partitioning data/computation across multiple nodes. The paper does not address accelerating training via multi-processor systems or distributed algorithms.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15518",
      "title": "How do Language Models Generate Slang: A Systematic Comparison between\n  Human and Machine-Generated Slang Usages",
      "authors": [
        "Siyang Wu",
        "Zhewei Sun"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Slang is a commonly used type of informal language that poses a daunting\nchallenge to NLP systems. Recent advances in large language models (LLMs),\nhowever, have made the problem more approachable. While LLM agents are becoming\nmore widely applied to intermediary tasks such as slang detection and slang\ninterpretation, their generalizability and reliability are heavily dependent on\nwhether these models have captured structural knowledge about slang that align\nwell with human attested slang usages. To answer this question, we contribute a\nsystematic comparison between human and machine-generated slang usages. Our\nevaluative framework focuses on three core aspects: 1) Characteristics of the\nusages that reflect systematic biases in how machines perceive slang, 2)\nCreativity reflected by both lexical coinages and word reuses employed by the\nslang usages, and 3) Informativeness of the slang usages when used as\ngold-standard examples for model distillation. By comparing human-attested\nslang usages from the Online Slang Dictionary (OSD) and slang generated by\nGPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our\nresults suggest that while LLMs have captured significant knowledge about the\ncreative aspects of slang, such knowledge does not align with humans\nsufficiently to enable LLMs for extrapolative tasks such as linguistic\nanalyses.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15518v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15518v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.436,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.3,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves generating slang usages programmatically using LLMs (e.g., GPT-4o and Llama-3) and using them for model distillation as examples for NLP tasks, which aligns with weak supervision by relying on noisy, machine-generated labels instead of hand-labeled data. However, the primary focus is on comparison and evaluation rather than extensively applying weak supervision techniques for training.",
      "diffusion_reasoning_justification": "The paper does not involve any diffusion-based models or iterative refinement processes for logical reasoning; it focuses on LLMs for slang generation and comparison, with no mention of multi-step reasoning via diffusion mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper systematically compares human-generated slang from the Online Slang Dictionary with slang produced by large language models like GPT-4o and Llama-3 to assess the alignment of machine knowledge with human usage, focusing on characteristics, creativity, and informativeness. The methodology involves generating a dataset of over 58,000 machine-generated slang usages under controlled conditions and evaluating them against human examples, revealing that while LLMs capture creative aspects of slang effectively, they exhibit significant biases that prevent sufficient alignment for tasks like linguistic analysis and model distillation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel systematic comparison framework for evaluating human versus machine-generated slang, representing a significant advancement in understanding LLMs' internal knowledge of informal language, which has not been extensively explored before.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in NLP subfields focusing on informal language and LLMs, as it highlights biases that could improve future model development and applications. However, its influence may be limited to specialized areas rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and insightful contribution to AI research on language models and slang, making it essential for researchers in computational linguistics to understand potential limitations in LLMs. While not groundbreaking for all audiences, it provides high-quality analysis that could inform ongoing work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f75eed717c978f385dd0e5fd6d58294f4e1bbc21",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Siyang Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381789510"
        },
        {
          "name": "Zhewei Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381351873"
        }
      ]
    },
    {
      "id": "2509.15532",
      "title": "GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI\n  Agents",
      "authors": [
        "Xianhang Ye",
        "Yiqing Li",
        "Wei Dai",
        "Miancan Liu",
        "Ziyuan Chen",
        "Zhangye Han",
        "Hongbo Min",
        "Jinkui Ren",
        "Xiantao Zhang",
        "Wen Yang",
        "Zhi Jin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Existing GUI grounding methods often struggle with fine-grained localization\nin high-resolution screenshots. To address this, we propose GUI-ARP, a novel\nframework that enables adaptive multi-stage inference. Equipped with the\nproposed Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC),\nGUI-ARP dynamically exploits visual attention for cropping task-relevant\nregions and adapts its inference strategy, performing a single-stage inference\nfor simple cases and a multi-stage analysis for more complex scenarios. This is\nachieved through a two-phase training pipeline that integrates supervised\nfine-tuning with reinforcement fine-tuning based on Group Relative Policy\nOptimization (GRPO). Extensive experiments demonstrate that the proposed\nGUI-ARP achieves state-of-the-art performance on challenging GUI grounding\nbenchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9%\non UI-Vision benchmark. Notably, GUI-ARP-7B demonstrates strong competitiveness\nagainst open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15532v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15532v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.356,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses a two-phase training pipeline with reinforcement fine-tuning based on Group Relative Policy Optimization (GRPO), but it does not specify the use of human feedback, such as training a reward model on human-ranked data. RLHF requires explicit alignment with human preferences, which is not evident here, making the paper's contribution unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adaptive multi-stage inference for GUI grounding using visual attention and region perception, but it does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for multi-step logical reasoning. There is no mention of diffusion-based components, so the paper's methods are not relevant to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15536",
      "title": "SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world\n  models",
      "authors": [
        "Sen Wang",
        "Jingyi Tian",
        "Le Wang",
        "Zhimin Liao",
        "Jiayi Li",
        "Huaiyi Dong",
        "Kun Xia",
        "Sanping Zhou",
        "Wei Tang",
        "Hua Gang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "World models allow agents to simulate the consequences of actions in imagined\nenvironments for planning, control, and long-horizon decision-making. However,\nexisting autoregressive world models struggle with visually coherent\npredictions due to disrupted spatial structure, inefficient decoding, and\ninadequate motion modeling. In response, we propose \\textbf{S}cale-wise\n\\textbf{A}utoregression with \\textbf{M}otion \\textbf{P}r\\textbf{O}mpt\n(\\textbf{SAMPO}), a hybrid framework that combines visual autoregressive\nmodeling for intra-frame generation with causal modeling for next-frame\ngeneration. Specifically, SAMPO integrates temporal causal decoding with\nbidirectional spatial attention, which preserves spatial locality and supports\nparallel decoding within each scale. This design significantly enhances both\ntemporal consistency and rollout efficiency. To further improve dynamic scene\nunderstanding, we devise an asymmetric multi-scale tokenizer that preserves\nspatial details in observed frames and extracts compact dynamic representations\nfor future frames, optimizing both memory usage and model performance.\nAdditionally, we introduce a trajectory-aware motion prompt module that injects\nspatiotemporal cues about object and robot trajectories, focusing attention on\ndynamic regions and improving temporal consistency and physical realism.\nExtensive experiments show that SAMPO achieves competitive performance in\naction-conditioned video prediction and model-based control, improving\ngeneration quality with 4.4$\\times$ faster inference. We also evaluate SAMPO's\nzero-shot generalization and scaling behavior, demonstrating its ability to\ngeneralize to unseen tasks and benefit from larger model sizes.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15536v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15536v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.293,
      "datasets_score": 0.24,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on SAMPO, an autoregressive framework for generative world models in video prediction and robotic control, emphasizing scale-wise autoregression and motion prompts. It briefly mentions diffusion-based models as a prior approach but does not adapt or use diffusion processes for multi-step logical reasoning or iterative refinement. The core contributions involve autoregressive modeling, not diffusion-based reasoning, making the paper unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15540",
      "title": "Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with\n  Non-Verbal Cues",
      "authors": [
        "Wei Chen",
        "Tongguan Wang",
        "Feiyue Xue",
        "Junkai Li",
        "Hui Liu",
        "Ying Sha"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Desire, as an intention that drives human behavior, is closely related to\nboth emotion and sentiment. Multimodal learning has advanced sentiment and\nemotion recognition, but multimodal approaches specially targeting human desire\nunderstanding remain underexplored. And existing methods in sentiment analysis\npredominantly emphasize verbal cues and overlook images as complementary\nnon-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional\nMultimodal Learning Framework for Desire, Emotion, and Sentiment Recognition,\nwhich enforces mutual guidance between text and image modalities to effectively\ncapture intention-related representations in the image. Specifically,\nlow-resolution images are used to obtain global visual representations for\ncross-modal alignment, while high resolution images are partitioned into\nsub-images and modeled with masked image modeling to enhance the ability to\ncapture fine-grained local features. A text-guided image decoder and an\nimage-guided text decoder are introduced to facilitate deep cross-modal\ninteraction at both local and global representations of image information.\nAdditionally, to balance perceptual gains with computation cost, a mixed-scale\nimage strategy is adopted, where high-resolution images are cropped into\nsub-images for masked modeling. The proposed approach is evaluated on MSED, a\nmultimodal dataset that includes a desire understanding benchmark, as well as\nemotion and sentiment recognition. Experimental results indicate consistent\nimprovements over other state-of-the-art methods, validating the effectiveness\nof our proposed method. Specifically, our method outperforms existing\napproaches, achieving F1-score improvements of 1.1% in desire understanding,\n0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is\navailable at: https://github.com/especiallyW/SyDES.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15540v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15540v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.323,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Symmetrical Bidirectional Multimodal Learning Framework for enhancing desire, emotion, and sentiment recognition using text and image modalities, including masked image modeling and cross-modal decoders. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no mention of adapting diffusion techniques for reasoning paths, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15541",
      "title": "Stress Testing Deliberative Alignment for Anti-Scheming Training",
      "authors": [
        "Bronson Schoen",
        "Evgenia Nitishinskaya",
        "Mikita Balesni",
        "Axel Højmark",
        "Felix Hofstätter",
        "Jérémy Scheurer",
        "Alexander Meinke",
        "Jason Wolfe",
        "Teun van der Weij",
        "Alex Lloyd",
        "Nicholas Goldowsky-Dill",
        "Angela Fan",
        "Andrei Matveiakin",
        "Rusheb Shah",
        "Marcus Williams",
        "Amelia Glaese",
        "Boaz Barak",
        "Wojciech Zaremba",
        "Marius Hobbhahn"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Highly capable AI systems could secretly pursue misaligned goals -- what we\ncall \"scheming\". Because a scheming AI would deliberately try to hide its\nmisaligned goals and actions, measuring and mitigating scheming requires\ndifferent strategies than are typically used in ML. We propose that assessing\nanti-scheming interventions requires at least (1) testing propensity to scheme\non far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming\nis driven by situational awareness, and (3) checking for robustness to\npre-existing misaligned goals. We use a broad category of \"covert actions\" --\nsuch as secretly breaking rules or intentionally underperforming in tests -- as\na proxy for scheming, and design evaluations for covert actions. We then\nstress-test deliberative alignment as a case study for anti-scheming. Across 26\nOOD evaluations (180+ environments), deliberative alignment reduces covert\naction rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our\nmitigation is also able to largely stop agents from pursuing a hidden goal\npreviously trained into the model, but we still find misbehavior after\nadditional red-teaming. We find that models' chain-of-thought (CoT) often\ndemonstrates awareness of being evaluated for alignment, and show causal\nevidence that this awareness decreases covert behavior, while unawareness\nincreases it. Therefore, we cannot exclude that the observed reductions in\ncovert action rates are at least partially driven by situational awareness.\nWhile we rely on human-legible CoT for training, studying situational\nawareness, and demonstrating clear evidence of misalignment, our ability to\nrely on this degrades as models continue to depart from reasoning in standard\nEnglish. We encourage research into alignment mitigations for scheming and\ntheir assessment, especially for the adversarial case of deceptive alignment,\nwhich this paper does not address.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15541v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.445,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.38,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper involves reinforcement learning (RL) training for alignment, as mentioned in the evaluation of anti-scheming interventions and further RL training for capabilities. However, it does not explicitly describe using a separate reward model trained on human-ranked data, which is a core component of RLHF. Thus, while RL is present, it does not fully align with the definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on chain-of-thought (CoT) reasoning for alignment and situational awareness but does not mention or adapt the iterative refinement process of diffusion models for multi-step logical tasks. There is no evidence of treating CoT as a holistic entity for correction using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15546",
      "title": "Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for\n  7th LSVOS RVOS Track",
      "authors": [
        "Ran Hong",
        "Feng Lu",
        "Leilei Cao",
        "An Yan",
        "Youhai Jiang",
        "Fengjie Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Referential Video Object Segmentation (RVOS) aims to segment all objects in a\nvideo that match a given natural language description, bridging the gap between\nvision and language understanding. Recent work, such as Sa2VA, combines Large\nLanguage Models (LLMs) with SAM~2, leveraging the strong video reasoning\ncapability of LLMs to guide video segmentation. In this work, we present a\ntraining-free framework that substantially improves Sa2VA's performance on the\nRVOS task. Our method introduces two key components: (1) a Video-Language\nChecker that explicitly verifies whether the subject and action described in\nthe query actually appear in the video, thereby reducing false positives; and\n(2) a Key-Frame Sampler that adaptively selects informative frames to better\ncapture both early object appearances and long-range temporal context. Without\nany additional training, our approach achieves a J&F score of 64.14% on the\nMeViS test set, ranking 2nd place in the RVOS track of the 7th LSVOS Challenge\nat ICCV 2025.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15546v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15546v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.303,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15548",
      "title": "MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild",
      "authors": [
        "Deming Li",
        "Kaiwen Jiang",
        "Yutao Tang",
        "Ravi Ramamoorthi",
        "Rama Chellappa",
        "Cheng Peng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In-the-wild photo collections often contain limited volumes of imagery and\nexhibit multiple appearances, e.g., taken at different times of day or seasons,\nposing significant challenges to scene reconstruction and novel view synthesis.\nAlthough recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian\nSplatting (3DGS) have improved in these areas, they tend to oversmooth and are\nprone to overfitting. In this paper, we present MS-GS, a novel framework\ndesigned with Multi-appearance capabilities in Sparse-view scenarios using\n3DGS. To address the lack of support due to sparse initializations, our\napproach is built on the geometric priors elicited from monocular depth\nestimations. The key lies in extracting and utilizing local semantic regions\nwith a Structure-from-Motion (SfM) points anchored algorithm for reliable\nalignment and geometry cues. Then, to introduce multi-view constraints, we\npropose a series of geometry-guided supervision at virtual views in a\nfine-grained and coarse scheme to encourage 3D consistency and reduce\noverfitting. We also introduce a dataset and an in-the-wild experiment setting\nto set up more realistic benchmarks. We demonstrate that MS-GS achieves\nphotorealistic renderings under various challenging sparse-view and\nmulti-appearance conditions and outperforms existing approaches significantly\nacross different datasets.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15548v3",
      "pdf_url": "http://arxiv.org/pdf/2509.15548v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.279,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.349,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15553",
      "title": "Diffusion-Based Cross-Modal Feature Extraction for Multi-Label\n  Classification",
      "authors": [
        "Tian Lan",
        "Yiming Zheng",
        "Jianxin Yin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "stat.AP (Applications)"
      ],
      "abstract": "Multi-label classification has broad applications and depends on powerful\nrepresentations capable of capturing multi-label interactions. We introduce\n\\textit{Diff-Feat}, a simple but powerful framework that extracts intermediate\nfeatures from pre-trained diffusion-Transformer models for images and text, and\nfuses them for downstream tasks. We observe that for vision tasks, the most\ndiscriminative intermediate feature along the diffusion process occurs at the\nmiddle step and is located in the middle block in Transformer. In contrast, for\nlanguage tasks, the best feature occurs at the noise-free step and is located\nin the deepest block. In particular, we observe a striking phenomenon across\nvarying datasets: a mysterious \"Layer $12$\" consistently yields the best\nperformance on various downstream classification tasks for images (under\nDiT-XL/2-256$\\times$256). We devise a heuristic local-search algorithm that\npinpoints the locally optimal \"image-text\"$\\times$\"block-timestep\" pair among a\nfew candidates, avoiding an exhaustive grid search. A simple fusion-linear\nprojection followed by addition-of the selected representations yields\nstate-of-the-art performance: 98.6\\% mAP on MS-COCO-enhanced and 45.7\\% mAP on\nVisual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a\nwide margin. t-SNE and clustering metrics further reveal that\n\\textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts.\nThe code is available at https://github.com/lt-0123/Diff-Feat.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15553v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15553v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.577,
      "distributed_training_score": 0.377,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for feature extraction in multi-label classification, specifically extracting and fusing intermediate features from pre-trained diffusion-Transformer models for images and text. It does not involve adapting the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a 'Chain-of-Thought' as a single entity for multi-step reasoning and correction. Instead, it emphasizes representation learning for classification tasks, with no clear component for logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15556",
      "title": "Exploring Polyglot Harmony: On Multilingual Data Allocation for Large\n  Language Models Pretraining",
      "authors": [
        "Ping Guo",
        "Yubing Ren",
        "Binbin Liu",
        "Fengze Liu",
        "Haobin Lin",
        "Yifan Zhang",
        "Bingni Zhang",
        "Taifeng Wang",
        "Yin Zheng"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have become integral to a wide range of\napplications worldwide, driving an unprecedented global demand for effective\nmultilingual capabilities. Central to achieving robust multilingual performance\nis the strategic allocation of language proportions within training corpora.\nHowever, determining optimal language ratios is highly challenging due to\nintricate cross-lingual interactions and sensitivity to dataset scale. This\npaper introduces Climb (Cross-Lingual Interaction-aware Multilingual\nBalancing), a novel framework designed to systematically optimize multilingual\ndata allocation. At its core, Climb introduces a cross-lingual\ninteraction-aware language ratio, explicitly quantifying each language's\neffective allocation by capturing inter-language dependencies. Leveraging this\nratio, Climb proposes a principled two-step optimization procedure--first\nequalizing marginal benefits across languages, then maximizing the magnitude of\nthe resulting language allocation vectors--significantly simplifying the\ninherently complex multilingual optimization problem. Extensive experiments\nconfirm that Climb can accurately measure cross-lingual interactions across\nvarious multilingual settings. LLMs trained with Climb-derived proportions\nconsistently achieve state-of-the-art multilingual performance, even achieving\ncompetitive performance with open-sourced LLMs trained with more tokens.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15556v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15556v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.451,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on optimizing multilingual data allocation for LLM pretraining, specifically through the Climb framework for language ratios. It does not involve human feedback, reward models, or reinforcement learning techniques for aligning models with preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses cross-lingual interactions and multilingual balancing in LLM training, but it does not discuss diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes.",
      "distributed_training_justification": "The paper's main contribution is on optimizing language proportions in training corpora, not on distributed training techniques, parallel computing, or strategies for partitioning data across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15557",
      "title": "Reward Hacking Mitigation using Verifiable Composite Rewards",
      "authors": [
        "Mirza Farhan Bin Tarek",
        "Rahmatollah Beheshti"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that\nlarge language models (LLMs) can develop their own reasoning without direct\nsupervision. However, applications in the medical domain, specifically for\nquestion answering, are susceptible to significant reward hacking during the\nreasoning phase. Our work addresses two primary forms of this behavior: i)\nproviding a final answer without preceding reasoning, and ii) employing\nnon-standard reasoning formats to exploit the reward mechanism. To mitigate\nthese, we introduce a composite reward function with specific penalties for\nthese behaviors. Our experiments show that extending RLVR with our proposed\nreward model leads to better-formatted reasoning with less reward hacking and\ngood accuracy compared to the baselines. This approach marks a step toward\nreducing reward hacking and enhancing the reliability of models utilizing RLVR.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15557v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15557v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.543,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.355,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses RLHF in the introduction as a related RL approach for LLMs, noting its use of human feedback for alignment. However, the main contribution focuses on RLVR with verifiable rewards, not human-ranked data or feedback, making it only peripherally connected to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper centers on RLVR and composite rewards for mitigating reward hacking in LLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. Thus, it lacks any connection to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15558",
      "title": "From Development to Deployment of AI-assisted Telehealth and Screening\n  for Vision- and Hearing-threatening diseases in resource-constrained\n  settings: Field Observations, Challenges and Way Forward",
      "authors": [
        "Mahesh Shakya",
        "Bijay Adhikari",
        "Nirsara Shrestha",
        "Bipin Koirala",
        "Arun Adhikari",
        "Prasanta Poudyal",
        "Luna Mathema",
        "Sarbagya Buddhacharya",
        "Bijay Khatri",
        "Bishesh Khanal"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Vision- and hearing-threatening diseases cause preventable disability,\nespecially in resource-constrained settings(RCS) with few specialists and\nlimited screening setup. Large scale AI-assisted screening and telehealth has\npotential to expand early detection, but practical deployment is challenging in\npaper-based workflows and limited documented field experience exist to build\nupon. We provide insights on challenges and ways forward in development to\nadoption of scalable AI-assisted Telehealth and screening in such settings.\nSpecifically, we find that iterative, interdisciplinary collaboration through\nearly prototyping, shadow deployment and continuous feedback is important to\nbuild shared understanding as well as reduce usability hurdles when\ntransitioning from paper-based to AI-ready workflows. We find public datasets\nand AI models highly useful despite poor performance due to domain shift. In\naddition, we find the need for automated AI-based image quality check to\ncapture gradable images for robust screening in high-volume camps.\n  Our field learning stress the importance of treating AI development and\nworkflow digitization as an end-to-end, iterative co-design process. By\ndocumenting these practical challenges and lessons learned, we aim to address\nthe gap in contextual, actionable field knowledge for building real-world\nAI-assisted telehealth and mass-screening programs in RCS.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15558v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15558v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.361,
      "datasets_score": 0.393,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper primarily discusses the development, deployment, and challenges of AI-assisted telehealth and screening for vision- and hearing-threatening diseases in resource-constrained settings. It emphasizes iterative collaboration, workflow design, and the use of AI models for practical applications, but does not involve or mention reinforcement learning techniques, including Reinforcement Learning from Human Feedback (RLHF). RLHF specifically requires training a reward model on human-ranked data and fine-tuning via reinforcement learning, which is absent from the paper's contributions. Thus, there is no connection to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15563",
      "title": "DC-Mamba: Bi-temporal deformable alignment and scale-sparse enhancement\n  for remote sensing change detection",
      "authors": [
        "Min Sun",
        "Fenghui Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Remote sensing change detection (RSCD) is vital for identifying land-cover\nchanges, yet existing methods, including state-of-the-art State Space Models\n(SSMs), often lack explicit mechanisms to handle geometric misalignments and\nstruggle to distinguish subtle, true changes from noise.To address this, we\nintroduce DC-Mamba, an \"align-then-enhance\" framework built upon the\nChangeMamba backbone. It integrates two lightweight, plug-and-play modules: (1)\nBi-Temporal Deformable Alignment (BTDA), which explicitly introduces geometric\nawareness to correct spatial misalignments at the semantic feature level; and\n(2) a Scale-Sparse Change Amplifier(SSCA), which uses multi-source cues to\nselectively amplify high-confidence change signals while suppressing noise\nbefore the final classification. This synergistic design first establishes\ngeometric consistency with BTDA to reduce pseudo-changes, then leverages SSCA\nto sharpen boundaries and enhance the visibility of small or subtle targets.\nExperiments show our method significantly improves performance over the strong\nChangeMamba baseline, increasing the F1-score from 0.5730 to 0.5903 and IoU\nfrom 0.4015 to 0.4187. The results confirm the effectiveness of our\n\"align-then-enhance\" strategy, offering a robust and easily deployable solution\nthat transparently addresses both geometric and feature-level challenges in\nRSCD.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15563v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15563v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.337,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15566",
      "title": "BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent",
      "authors": [
        "Shaojie Zhang",
        "Ruoceng Zhang",
        "Pei Fu",
        "Shaokang Wang",
        "Jiahui Yang",
        "Xin Du",
        "Shiqi Cui",
        "Bin Qin",
        "Ying Huang",
        "Zhenbo Luo",
        "Jian Luan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In the field of AI-driven human-GUI interaction automation, while rapid\nadvances in multimodal large language models and reinforcement fine-tuning\ntechniques have yielded remarkable progress, a fundamental challenge persists:\ntheir interaction logic significantly deviates from natural human-GUI\ncommunication patterns. To fill this gap, we propose \"Blink-Think-Link\" (BTL),\na brain-inspired framework for human-GUI interaction that mimics the human\ncognitive process between users and graphical interfaces. The system decomposes\ninteractions into three biologically plausible phases: (1) Blink - rapid\ndetection and attention to relevant screen areas, analogous to saccadic eye\nmovements; (2) Think - higher-level reasoning and decision-making, mirroring\ncognitive planning; and (3) Link - generation of executable commands for\nprecise motor control, emulating human action selection mechanisms.\nAdditionally, we introduce two key technical innovations for the BTL framework:\n(1) Blink Data Generation - an automated annotation pipeline specifically\noptimized for blink data, and (2) BTL Reward -- the first rule-based reward\nmechanism that enables reinforcement learning driven by both process and\noutcome. Building upon this framework, we develop a GUI agent model named\nBTL-UI, which demonstrates consistent state-of-the-art performance across both\nstatic GUI understanding and dynamic interaction tasks in comprehensive\nbenchmarks. These results provide conclusive empirical validation of the\nframework's efficacy in developing advanced GUI Agents.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15566v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15566v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.303,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a rule-based reward mechanism (BTL Reward) for reinforcement learning, which is driven by predefined rules and automated processes, not human-ranked data or a separate reward model trained on human feedback. Therefore, it does not meet the criteria for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a biologically inspired framework (BTL) with structured reasoning phases, but it does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for holistic correction. There is no mention of diffusion-based techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15568",
      "title": "LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs",
      "authors": [
        "Junlong Jia",
        "Xing Wu",
        "Chaochen Gao",
        "Ziyang Chen",
        "Zijia Lin",
        "Zhongzhi Li",
        "Weinong Wang",
        "Haotian Xu",
        "Donghui Jin",
        "Debing Zhang",
        "Binghui Guo"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "High-quality long-context data is essential for training large language\nmodels (LLMs) capable of processing extensive documents, yet existing synthesis\napproaches using relevance-based aggregation face challenges of computational\nefficiency. We present LiteLong, a resource-efficient method for synthesizing\nlong-context data through structured topic organization and multi-agent debate.\nOur approach leverages the BISAC book classification system to provide a\ncomprehensive hierarchical topic organization, and then employs a debate\nmechanism with multiple LLMs to generate diverse, high-quality topics within\nthis structure. For each topic, we use lightweight BM25 retrieval to obtain\nrelevant documents and concatenate them into 128K-token training samples.\nExperiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves\ncompetitive long-context performance and can seamlessly integrate with other\nlong-dependency enhancement methods. LiteLong makes high-quality long-context\ndata synthesis more accessible by reducing both computational and data\nengineering costs, facilitating further research in long-context language\ntraining.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15568v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15568v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.431,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.393,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on synthesizing long-context data using structured topic organization and multi-agent debate with LLMs, without any involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper involves programmatically generating topics via LLMs, which could be seen as a form of noisy or imprecise label generation similar to weak supervision, but it primarily addresses data synthesis for LLMs rather than training models with weakly supervised labels.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical reasoning, or any multi-step reasoning processes; it centers on efficient data synthesis for long-context LLMs using retrieval and debate mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15570",
      "title": "Contrastive Learning with Spectrum Information Augmentation in Abnormal\n  Sound Detection",
      "authors": [
        "Xinxin Meng",
        "Jiangtao Guo",
        "Yunxiang Zhang",
        "Shun Huang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "The outlier exposure method is an effective approach to address the\nunsupervised anomaly sound detection problem. The key focus of this method is\nhow to make the model learn the distribution space of normal data. Based on\nbiological perception and data analysis, it is found that anomalous audio and\nnoise often have higher frequencies. Therefore, we propose a data augmentation\nmethod for high-frequency information in contrastive learning. This enables the\nmodel to pay more attention to the low-frequency information of the audio,\nwhich represents the normal operational mode of the machine. We evaluated the\nproposed method on the DCASE 2020 Task 2. The results showed that our method\noutperformed other contrastive learning methods used on this dataset. We also\nevaluated the generalizability of our method on the DCASE 2022 Task 2 dataset.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15570v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15570v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.414,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.34,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on unsupervised anomaly sound detection using contrastive learning and data augmentation based on spectral information. While it involves programmatically generating positive and negative sample pairs through augmentation, which indirectly relates to weak supervision by creating training signals without hand-labeled data, the main contribution is not centered on weak supervision techniques. Instead, it emphasizes improving model performance in anomaly detection, making the connection tangential.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15573",
      "title": "Towards Size-invariant Salient Object Detection: A Generic Evaluation\n  and Optimization Approach",
      "authors": [
        "Shilong Bao",
        "Qianqian Xu",
        "Feiran Li",
        "Boyu Han",
        "Zhiyong Yang",
        "Xiaochun Cao",
        "Qingming Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This paper investigates a fundamental yet underexplored issue in Salient\nObject Detection (SOD): the size-invariant property for evaluation protocols,\nparticularly in scenarios when multiple salient objects of significantly\ndifferent sizes appear within a single image. We first present a novel\nperspective to expose the inherent size sensitivity of existing widely used SOD\nmetrics. Through careful theoretical derivations, we show that the evaluation\noutcome of an image under current SOD metrics can be essentially decomposed\ninto a sum of several separable terms, with the contribution of each term being\ndirectly proportional to its corresponding region size. Consequently, the\nprediction errors would be dominated by the larger regions, while smaller yet\npotentially more semantically important objects are often overlooked, leading\nto biased performance assessments and practical degradation. To address this\nchallenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed.\nThe core idea is to evaluate each separable component individually and then\naggregate the results, thereby effectively mitigating the impact of size\nimbalance across objects. Building upon this, we further develop a dedicated\noptimization framework (SIOpt), which adheres to the size-invariant principle\nand significantly enhances the detection of salient objects across a broad\nrange of sizes. Notably, SIOpt is model-agnostic and can be seamlessly\nintegrated with a wide range of SOD backbones. Theoretically, we also present\ngeneralization analysis of SOD methods and provide evidence supporting the\nvalidity of our new evaluation protocols. Finally, comprehensive experiments\nspeak to the efficacy of our proposed approach. The code is available at\nhttps://github.com/Ferry-Li/SI-SOD.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15573v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15573v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.336,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15577",
      "title": "Relevance to Utility: Process-Supervised Rewrite for RAG",
      "authors": [
        "Jaeyoung Kim",
        "Jongho Kim",
        "Seung-won Hwang",
        "Seoho Song",
        "Young-In Song"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Retrieval-Augmented Generation systems often suffer from a gap between\noptimizing retrieval relevance and generative utility: retrieved documents may\nbe topically relevant but still lack the content needed for effective reasoning\nduring generation. While existing \"bridge\" modules attempt to rewrite the\nretrieved text for better generation, we show how they fail to capture true\ndocument utility. In this work, we propose R2U, with a key distinction of\ndirectly optimizing to maximize the probability of generating a correct answer\nthrough process supervision. As such direct observation is expensive, we also\npropose approximating an efficient distillation pipeline by scaling the\nsupervision from LLMs, which helps the smaller rewriter model generalize\nbetter. We evaluate our method across multiple open-domain question-answering\nbenchmarks. The empirical results demonstrate consistent improvements over\nstrong bridging baselines.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15577v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15577v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.298,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper involves preference learning to train a smaller language model (SLM) by aligning its rewriting with generator performance, which shares conceptual similarities with RLHF's use of preferences for model alignment. However, it relies on supervision from LLMs rather than direct human-ranked data and a separate reward model, so it does not fully meet the criteria for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on optimizing rewriters in Retrieval-Augmented Generation systems using process supervision and distillation from LLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15578",
      "title": "Multimodal Learning for Fake News Detection in Short Videos Using\n  Linguistically Verified Data and Heterogeneous Modality Fusion",
      "authors": [
        "Shanghong Li",
        "Chiam Wen Qi Ruth",
        "Hong Xu",
        "Fang Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid proliferation of short video platforms has necessitated advanced\nmethods for detecting fake news. This need arises from the widespread influence\nand ease of sharing misinformation, which can lead to significant societal\nharm. Current methods often struggle with the dynamic and multimodal nature of\nshort video content. This paper presents HFN, Heterogeneous Fusion Net, a novel\nmultimodal framework that integrates video, audio, and text data to evaluate\nthe authenticity of short video content. HFN introduces a Decision Network that\ndynamically adjusts modality weights during inference and a Weighted\nMulti-Modal Feature Fusion module to ensure robust performance even with\nincomplete data. Additionally, we contribute a comprehensive dataset VESV\n(VEracity on Short Videos) specifically designed for short video fake news\ndetection. Experiments conducted on the FakeTT and newly collected VESV\ndatasets demonstrate improvements of 2.71% and 4.14% in Marco F1 over\nstate-of-the-art methods. This work establishes a robust solution capable of\neffectively identifying fake news in the complex landscape of short video\nplatforms, paving the way for more reliable and comprehensive approaches in\ncombating misinformation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15578v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15578v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.309,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15582",
      "title": "Momentum-constrained Hybrid Heuristic Trajectory Optimization Framework\n  with Residual-enhanced DRL for Visually Impaired Scenarios",
      "authors": [
        "Yuting Zeng",
        "Zhiwen Zheng",
        "You Zhou",
        "JiaLing Xiao",
        "Yongbin Yu",
        "Manping Fan",
        "Bo Gong",
        "Liyong Ren"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper proposes a momentum-constrained hybrid heuristic trajectory\noptimization framework (MHHTOF) tailored for assistive navigation in visually\nimpaired scenarios, integrating trajectory sampling generation, optimization\nand evaluation with residual-enhanced deep reinforcement learning (DRL). In the\nfirst stage, heuristic trajectory sampling cluster (HTSC) is generated in the\nFrenet coordinate system using third-order interpolation with fifth-order\npolynomials and momentum-constrained trajectory optimization (MTO) constraints\nto ensure smoothness and feasibility. After first stage cost evaluation, the\nsecond stage leverages a residual-enhanced actor-critic network with LSTM-based\ntemporal feature modeling to adaptively refine trajectory selection in the\nCartesian coordinate system. A dual-stage cost modeling mechanism (DCMM) with\nweight transfer aligns semantic priorities across stages, supporting\nhuman-centered optimization. Experimental results demonstrate that the proposed\nLSTM-ResB-PPO achieves significantly faster convergence, attaining stable\npolicy performance in approximately half the training iterations required by\nthe PPO baseline, while simultaneously enhancing both reward outcomes and\ntraining stability. Compared to baseline method, the selected model reduces\naverage cost and cost variance by 30.3% and 53.3%, and lowers ego and obstacle\nrisks by over 77%. These findings validate the framework's effectiveness in\nenhancing robustness, safety, and real-time feasibility in complex assistive\nplanning tasks.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15582v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15582v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.475,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.372,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a hybrid heuristic trajectory optimization framework using DRL for assistive navigation, incorporating human-centered design elements like perceptual alignment and user comfort. However, it does not involve training a reward model on human-ranked data or using human feedback to fine-tune the model, which are core to RLHF. Instead, it emphasizes algorithmic optimization without explicit human feedback loops.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a framework with heuristic sampling and residual-enhanced DRL for trajectory optimization, including LSTM for temporal modeling, but it does not adapt diffusion processes for multi-step logical reasoning or treat a chain-of-thought as a holistically refined entity. There are no components involving diffusion models or iterative reasoning refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15587",
      "title": "DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation\n  in Large Language Models",
      "authors": [
        "Tsz Ting Chung",
        "Lemao Liu",
        "Mo Yu",
        "Dit-Yan Yeung"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Logic reasoning in natural language has been recognized as an important\nmeasure of human intelligence for Large Language Models (LLMs). Popular\nbenchmarks may entangle multiple reasoning skills and thus provide unfaithful\nevaluations on the logic reasoning skill. Meanwhile, existing logic reasoning\nbenchmarks are limited in language diversity and their distributions are\ndeviated from the distribution of an ideal logic reasoning benchmark, which may\nlead to biased evaluation results. This paper thereby proposes a new classical\nlogic benchmark DivLogicEval, consisting of natural sentences composed of\ndiverse statements in a counterintuitive way. To ensure a more reliable\nevaluation, we also introduce a new evaluation metric that mitigates the\ninfluence of bias and randomness inherent in LLMs. Through experiments, we\ndemonstrate the extent to which logical reasoning is required to answer the\nquestions in DivLogicEval and compare the performance of different popular LLMs\nin conducting logical reasoning.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15587v3",
      "pdf_url": "http://arxiv.org/pdf/2509.15587v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.537,
      "distributed_training_score": 0.345,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on proposing a new benchmark for evaluating logical reasoning in LLMs, involving classical logic expressions and natural language transformations. It does not mention or utilize diffusion-based models, iterative refinement processes, or any multi-step reasoning via diffusion mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of a new dataset, DivLogicEval, for benchmarking logical reasoning in LLMs. It includes dataset curation methodologies, analysis of diversity and bias compared to existing benchmarks, and overall evaluation, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces DivLogicEval, a new benchmark for evaluating logical reasoning in Large Language Models (LLMs), addressing limitations in existing benchmarks by ensuring greater language diversity and isolating logical reasoning from other skills like commonsense reasoning. The methodology involves sampling classical logic expressions, transforming them into diverse natural language sentences with counterintuitive connectives, and proposing a new evaluation metric to reduce bias and randomness; experiments demonstrate that DivLogicEval better assesses pure logical reasoning and reveals performance differences among popular LLMs.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark framework and evaluation metric that significantly advances the state-of-the-art by addressing biases and diversity issues in existing logical reasoning evaluations for LLMs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of LLM evaluation, particularly for logical reasoning, due to its improved methodology, though its influence may be limited to specific AI research areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality contribution that enhances benchmark design for logical reasoning in LLMs, making it valuable for researchers focused on AI evaluation, though it may not be essential for those outside this niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/90f9c7d563585486bef8cdff75b32d864c29ebf5",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 25,
      "average_h_index": 7.75,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Tsz Ting Chung",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2325955194"
        },
        {
          "name": "Lemao Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2330051318"
        },
        {
          "name": "Mo Yu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2284310722"
        },
        {
          "name": "Dit-Yan Yeung",
          "h_index": 25,
          "profile_url": "https://www.semanticscholar.org/author/66427434"
        }
      ]
    },
    {
      "id": "2509.15588",
      "title": "CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational\n  Search via Query Reformulation and Rank Fusion",
      "authors": [
        "Yu-Cheng Chang",
        "Guan-Wei Yeo",
        "Quah Eugene",
        "Fan-Jie Shih",
        "Yuan-Ching Kuo",
        "Tsung-En Yu",
        "Hung-Chun Hsu",
        "Ming-Feng Tsai",
        "Chuan-Ju Wang"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both\ninteractive and offline submission tasks. The former requires systems to\noperate under real-time constraints, making robustness and efficiency as\nimportant as accuracy, while the latter enables controlled evaluation of\npassage ranking and response generation with pre-defined datasets. To address\nthis, we explored query rewriting and retrieval fusion as core strategies. We\nbuilt our pipelines around Best-of-$N$ selection and Reciprocal Rank Fusion\n(RRF) strategies to handle different submission tasks. Results show that\nreranking and fusion improve robustness while revealing trade-offs between\neffectiveness and efficiency across both tasks.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15588v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15588v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.306,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves query reformulation, retrieval fusion, and strategies like Best-of-N and Reciprocal Rank Fusion for conversational search in the TREC iKAT 2025. It does not mention or utilize diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15591",
      "title": "Latent Zoning Network: A Unified Principle for Generative Modeling,\n  Representation Learning, and Classification",
      "authors": [
        "Zinan Lin",
        "Enshu Liu",
        "Xuefei Ning",
        "Junyi Zhu",
        "Wenyu Wang",
        "Sergey Yekhanin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Generative modeling, representation learning, and classification are three\ncore problems in machine learning (ML), yet their state-of-the-art (SoTA)\nsolutions remain largely disjoint. In this paper, we ask: Can a unified\nprinciple address all three? Such unification could simplify ML pipelines and\nfoster greater synergy across tasks. We introduce Latent Zoning Network (LZN)\nas a step toward this goal. At its core, LZN creates a shared Gaussian latent\nspace that encodes information across all tasks. Each data type (e.g., images,\ntext, labels) is equipped with an encoder that maps samples to disjoint latent\nzones, and a decoder that maps latents back to data. ML tasks are expressed as\ncompositions of these encoders and decoders: for example, label-conditional\nimage generation uses a label encoder and image decoder; image embedding uses\nan image encoder; classification uses an image encoder and label decoder. We\ndemonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN\ncan enhance existing models (image generation): When combined with the SoTA\nRectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without\nmodifying the training objective. (2) LZN can solve tasks independently\n(representation learning): LZN can implement unsupervised representation\nlearning without auxiliary loss functions, outperforming the seminal MoCo and\nSimCLR methods by 9.3% and 0.2%, respectively, on downstream linear\nclassification on ImageNet. (3) LZN can solve multiple tasks simultaneously\n(joint generation and classification): With image and label encoders/decoders,\nLZN performs both tasks jointly by design, improving FID and achieving SoTA\nclassification accuracy on CIFAR10. The code and trained models are available\nat https://github.com/microsoft/latent-zoning-networks. The project website is\nat https://zinanlin.me/blogs/latent_zoning_networks.html.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15591v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15591v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.414,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses flow matching, which is related to diffusion models (e.g., in latent computation with Rectified Flow), but it focuses on unifying generative modeling, representation learning, and classification, not on adapting diffusion for multi-step logical reasoning or Chain-of-Thought processes. There is no clear component for holistic correction of reasoning paths, making the connection indirect.",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or multi-node machine learning. Its main contribution is a unified framework for ML tasks using a shared latent space, with no mention of partitioning data, architecture, or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15595",
      "title": "Prostate Capsule Segmentation from Micro-Ultrasound Images using\n  Adaptive Focal Loss",
      "authors": [
        "Kaniz Fatema",
        "Vaibhav Thakur",
        "Emad A. Mohammed"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Micro-ultrasound (micro-US) is a promising imaging technique for cancer\ndetection and computer-assisted visualization. This study investigates prostate\ncapsule segmentation using deep learning techniques from micro-US images,\naddressing the challenges posed by the ambiguous boundaries of the prostate\ncapsule. Existing methods often struggle in such cases, motivating the\ndevelopment of a tailored approach. This study introduces an adaptive focal\nloss function that dynamically emphasizes both hard and easy regions, taking\ninto account their respective difficulty levels and annotation variability. The\nproposed methodology has two primary strategies: integrating a standard focal\nloss function as a baseline to design an adaptive focal loss function for\nproper prostate capsule segmentation. The focal loss baseline provides a robust\nfoundation, incorporating class balancing and focusing on examples that are\ndifficult to classify. The adaptive focal loss offers additional flexibility,\naddressing the fuzzy region of the prostate capsule and annotation variability\nby dilating the hard regions identified through discrepancies between expert\nand non-expert annotations. The proposed method dynamically adjusts the\nsegmentation model's weights better to identify the fuzzy regions of the\nprostate capsule. The proposed adaptive focal loss function demonstrates\nsuperior performance, achieving a mean dice coefficient (DSC) of 0.940 and a\nmean Hausdorff distance (HD) of 1.949 mm in the testing dataset. These results\nhighlight the effectiveness of integrating advanced loss functions and adaptive\ntechniques into deep learning models. This enhances the accuracy of prostate\ncapsule segmentation in micro-US images, offering the potential to improve\nclinical decision-making in prostate cancer diagnosis and treatment planning.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15595v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15595v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.31,
      "distributed_training_score": 0.317,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15596",
      "title": "EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge\n  Comprehension and Clinical Reasoning in Ophthalmic Surgery",
      "authors": [
        "Gui Wang",
        "Yang Wennuo",
        "Xusen Ma",
        "Zehao Zhong",
        "Zhuoru Wu",
        "Ende Wu",
        "Rong Qu",
        "Wooi Ping Cheah",
        "Jianfeng Ren",
        "Linlin Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "MLLMs (Multimodal Large Language Models) have showcased remarkable\ncapabilities, but their performance in high-stakes, domain-specific scenarios\nlike surgical settings, remains largely under-explored. To address this gap, we\ndevelop \\textbf{EyePCR}, a large-scale benchmark for ophthalmic surgery\nanalysis, grounded in structured clinical knowledge to evaluate cognition\nacross \\textit{Perception}, \\textit{Comprehension} and \\textit{Reasoning}.\nEyePCR offers a richly annotated corpus with more than 210k VQAs, which cover\n1048 fine-grained attributes for multi-view perception, medical knowledge graph\nof more than 25k triplets for comprehension, and four clinically grounded\nreasoning tasks. The rich annotations facilitate in-depth cognitive analysis,\nsimulating how surgeons perceive visual cues and combine them with domain\nknowledge to make decisions, thus greatly improving models' cognitive ability.\nIn particular, \\textbf{EyePCR-MLLM}, a domain-adapted variant of Qwen2.5-VL-7B,\nachieves the highest accuracy on MCQs for \\textit{Perception} among compared\nmodels and outperforms open-source models in \\textit{Comprehension} and\n\\textit{Reasoning}, rivalling commercial models like GPT-4.1. EyePCR reveals\nthe limitations of existing MLLMs in surgical cognition and lays the foundation\nfor benchmarking and enhancing clinical reliability of surgical video\nunderstanding models.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15596v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15596v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.325,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces EyePCR, a benchmark for evaluating MLLMs in ophthalmic surgery, focusing on perception, comprehension, and reasoning tasks. It discusses models like Qwen2.5-VL-7B and compares them to others, but there is no mention of diffusion models, iterative refinement for logical tasks, or adapting diffusion processes for reasoning. The reasoning in the paper is based on general MLLM capabilities, not diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15602",
      "title": "TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?",
      "authors": [
        "Zhongyuan Bao",
        "Lejun Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal large language models (MLLMs) excel at general video understanding\nbut struggle with fast, high-frequency sports like tennis, where rally clips\nare short yet information-dense. To systematically evaluate MLLMs in this\nchallenging domain, we present TennisTV, the first and most comprehensive\nbenchmark for tennis video understanding. TennisTV models each rally as a\ntemporal-ordered sequence of consecutive stroke events, using automated\npipelines for filtering and question generation. It covers 9 tasks from the\nstroke level to the rally level and includes 2943 human-verified questions.\nEvaluating 17 representative MLLMs, we provide the first systematic assessment\nof tennis video understanding. Results reveal substantial shortcomings and\nyield two key insights: (i) frame-sampling density should be tailored and\nbalanced across tasks, and (ii) improving temporal grounding is essential for\nstronger reasoning.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15602v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15602v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.355,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating Multimodal Large Language Models (MLLMs) for understanding tennis rallies through a new benchmark called TennisTV. It discusses tasks related to video analysis, frame sampling, and temporal grounding, but does not mention or involve diffusion-based models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, the paper's contributions are unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15608",
      "title": "Enhancing WSI-Based Survival Analysis with Report-Auxiliary\n  Self-Distillation",
      "authors": [
        "Zheng Wang",
        "Hong Liu",
        "Zheng Wang",
        "Danyi Li",
        "Min Cen",
        "Baptiste Magnier",
        "Li Liang",
        "Liansheng Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Survival analysis based on Whole Slide Images (WSIs) is crucial for\nevaluating cancer prognosis, as they offer detailed microscopic information\nessential for predicting patient outcomes. However, traditional WSI-based\nsurvival analysis usually faces noisy features and limited data accessibility,\nhindering their ability to capture critical prognostic features effectively.\nAlthough pathology reports provide rich patient-specific information that could\nassist analysis, their potential to enhance WSI-based survival analysis remains\nlargely unexplored. To this end, this paper proposes a novel Report-auxiliary\nself-distillation (Rasa) framework for WSI-based survival analysis. First,\nadvanced large language models (LLMs) are utilized to extract fine-grained,\nWSI-relevant textual descriptions from original noisy pathology reports via a\ncarefully designed task prompt. Next, a self-distillation-based pipeline is\ndesigned to filter out irrelevant or redundant WSI features for the student\nmodel under the guidance of the teacher model's textual knowledge. Finally, a\nrisk-aware mix-up strategy is incorporated during the training of the student\nmodel to enhance both the quantity and diversity of the training data.\nExtensive experiments carried out on our collected data (CRC) and public data\n(TCGA-BRCA) demonstrate the superior effectiveness of Rasa against\nstate-of-the-art methods. Our code is available at\nhttps://github.com/zhengwang9/Rasa.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15608v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15608v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.316,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15623",
      "title": "PCSR: Pseudo-label Consistency-Guided Sample Refinement for Noisy\n  Correspondence Learning",
      "authors": [
        "Zhuoyao Liu",
        "Yang Liu",
        "Wentao Feng",
        "Shudong Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cross-modal retrieval aims to align different modalities via semantic\nsimilarity. However, existing methods often assume that image-text pairs are\nperfectly aligned, overlooking Noisy Correspondences in real data. These\nmisaligned pairs misguide similarity learning and degrade retrieval\nperformance. Previous methods often rely on coarse-grained categorizations that\nsimply divide data into clean and noisy samples, overlooking the intrinsic\ndiversity within noisy instances. Moreover, they typically apply uniform\ntraining strategies regardless of sample characteristics, resulting in\nsuboptimal sample utilization for model optimization. To address the above\nchallenges, we introduce a novel framework, called Pseudo-label\nConsistency-Guided Sample Refinement (PCSR), which enhances correspondence\nreliability by explicitly dividing samples based on pseudo-label consistency.\nSpecifically, we first employ a confidence-based estimation to distinguish\nclean and noisy pairs, then refine the noisy pairs via pseudo-label consistency\nto uncover structurally distinct subsets. We further proposed a Pseudo-label\nConsistency Score (PCS) to quantify prediction stability, enabling the\nseparation of ambiguous and refinable samples within noisy pairs. Accordingly,\nwe adopt Adaptive Pair Optimization (APO), where ambiguous samples are\noptimized with robust loss functions and refinable ones are enhanced via text\nreplacement during training. Extensive experiments on CC152K, MS-COCO and\nFlickr30K validate the effectiveness of our method in improving retrieval\nrobustness under noisy supervision.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15623v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15623v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.488,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.355,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, PCSR, directly addresses weak supervision by utilizing pseudo-labels generated from noisy image-text correspondences to refine and train models, aligning with the definition of weak supervision. It programmatically estimates labels through confidence-based methods and pseudo-label consistency, enabling learning from high-level, noisy sources without relying on perfectly hand-labeled data, as demonstrated in experiments on datasets like MS-COCO.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper addresses the challenge of noisy correspondences in cross-modal retrieval by introducing the Pseudo-label Consistency-Guided Sample Refinement (PCSR) framework, which refines image-text pairs through confidence-based estimation and a Pseudo-label Consistency Score (PCS) to divide them into clean, ambiguous, and refinable subsets. It employs Adaptive Pair Optimization (APO) to apply tailored training strategies—such as robust loss for ambiguous pairs and text replacement for refinable pairs—resulting in improved retrieval performance, as validated through experiments on datasets like MS-COCO, Flickr30K, and CC152K.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing ideas like confidence estimation and pseudo-labeling with a new Pseudo-label Consistency Score and refined sample division strategy, effectively addressing limitations in handling noisy correspondences. However, it builds on prior work without introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of cross-modal retrieval due to its effective handling of noisy data and demonstrated performance gains on standard datasets. Its influence is primarily confined to computer vision and multimodal learning, limiting broader applicability.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a strong, valuable contribution to robust cross-modal retrieval by introducing practical refinements for noisy data, making it essential for researchers in the field. While insightful, it may not be critical for those outside computer vision and pattern recognition.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/069b7205fd21ae1f503e8351cd9edd1f3c923e12",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 6,
      "average_h_index": 1.75,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Zhuoyao Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2351001060"
        },
        {
          "name": "Yang Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313463499"
        },
        {
          "name": "Wentao Feng",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2114323920"
        },
        {
          "name": "Shudong Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378072431"
        }
      ]
    },
    {
      "id": "2509.15635",
      "title": "MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large\n  Language Model Agents",
      "authors": [
        "Pan Tang",
        "Shixiang Tang",
        "Huanqi Pu",
        "Zhiqing Miao",
        "Zhixing Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents MicroRCA-Agent, an innovative solution for microservice\nroot cause analysis based on large language model agents, which constructs an\nintelligent fault root cause localization system with multimodal data fusion.\nThe technical innovations are embodied in three key aspects: First, we combine\nthe pre-trained Drain log parsing algorithm with multi-level data filtering\nmechanism to efficiently compress massive logs into high-quality fault\nfeatures. Second, we employ a dual anomaly detection approach that integrates\nIsolation Forest unsupervised learning algorithms with status code validation\nto achieve comprehensive trace anomaly identification. Third, we design a\nstatistical symmetry ratio filtering mechanism coupled with a two-stage LLM\nanalysis strategy to enable full-stack phenomenon summarization across\nnode-service-pod hierarchies. The multimodal root cause analysis module\nleverages carefully designed cross-modal prompts to deeply integrate multimodal\nanomaly information, fully exploiting the cross-modal understanding and logical\nreasoning capabilities of large language models to generate structured analysis\nresults encompassing fault components, root cause descriptions, and reasoning\ntrace. Comprehensive ablation studies validate the complementary value of each\nmodal data and the effectiveness of the system architecture. The proposed\nsolution demonstrates superior performance in complex microservice fault\nscenarios, achieving a final score of 50.71. The code has been released at:\nhttps://github.com/tangpan360/MicroRCA-Agent.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15635v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15635v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.351,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15638",
      "title": "pFedSAM: Personalized Federated Learning of Segment Anything Model for\n  Medical Image Segmentation",
      "authors": [
        "Tong Wang",
        "Xingyue Zhao",
        "Linghao Zhuang",
        "Haoyu Zhao",
        "Jiayi Yin",
        "Yuyang He",
        "Gang Yu",
        "Bo Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical image segmentation is crucial for computer-aided diagnosis, yet\nprivacy constraints hinder data sharing across institutions. Federated learning\naddresses this limitation, but existing approaches often rely on lightweight\narchitectures that struggle with complex, heterogeneous data. Recently, the\nSegment Anything Model (SAM) has shown outstanding segmentation capabilities;\nhowever, its massive encoder poses significant challenges in federated\nsettings. In this work, we present the first personalized federated SAM\nframework tailored for heterogeneous data scenarios in medical image\nsegmentation. Our framework integrates two key innovations: (1) a personalized\nstrategy that aggregates only the global parameters to capture cross-client\ncommonalities while retaining the designed L-MoE (Localized Mixture-of-Experts)\ncomponent to preserve domain-specific features; and (2) a decoupled\nglobal-local fine-tuning mechanism that leverages a teacher-student paradigm\nvia knowledge distillation to bridge the gap between the global shared model\nand the personalized local models, thereby mitigating overgeneralization.\nExtensive experiments on two public datasets validate that our approach\nsignificantly improves segmentation performance, achieves robust cross-domain\nadaptation, and reduces communication overhead.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15638v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15638v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.397,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15641",
      "title": "Information Geometry of Variational Bayes",
      "authors": [
        "Mohammad Emtiyaz Khan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "We highlight a fundamental connection between information geometry and\nvariational Bayes (VB) and discuss its consequences for machine learning. Under\ncertain conditions, a VB solution always requires estimation or computation of\nnatural gradients. We show several consequences of this fact by using the\nnatural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian\nLearning Rule (BLR). These include (i) a simplification of Bayes' rule as\naddition of natural gradients, (ii) a generalization of quadratic surrogates\nused in gradient-based methods, and (iii) a large-scale implementation of VB\nalgorithms for large language models. Neither the connection nor its\nconsequences are new but we further emphasize the common origins of the two\nfields of information geometry and Bayes with a hope to facilitate more work at\nthe intersection of the two fields.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15641v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15641v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.298,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.251,
      "datasets_score": 0.245,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15642",
      "title": "UNIV: Unified Foundation Model for Infrared and Visible Modalities",
      "authors": [
        "Fangyuan Mao",
        "Shuo Wang",
        "Jilin Mei",
        "Chen Min",
        "Shun Lu",
        "Fuyang Liu",
        "Yu Hu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The demand for joint RGB-visible and infrared perception is growing rapidly,\nparticularly to achieve robust performance under diverse weather conditions.\nAlthough pre-trained models for RGB-visible and infrared data excel in their\nrespective domains, they often underperform in multimodal scenarios, such as\nautonomous vehicles equipped with both sensors. To address this challenge, we\npropose a biologically inspired UNified foundation model for Infrared and\nVisible modalities (UNIV), featuring two key innovations. First, we introduce\nPatch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided\ndistillation framework that mimics retinal horizontal cells' lateral\ninhibition, which enables effective cross-modal feature alignment while\nremaining compatible with any transformer-based architecture. Second, our\ndual-knowledge preservation mechanism emulates the retina's bipolar cell signal\nrouting - combining LoRA adapters (2% added parameters) with synchronous\ndistillation to prevent catastrophic forgetting, thereby replicating the\nretina's photopic (cone-driven) and scotopic (rod-driven) functionality. To\nsupport cross-modal learning, we introduce the MVIP dataset, the most\ncomprehensive visible-infrared benchmark to date. It contains 98,992 precisely\naligned image pairs spanning diverse scenarios. Extensive experiments\ndemonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU in\nsemantic segmentation and +0.7 mAP in object detection) while maintaining 99%+\nof the baseline performance on visible RGB tasks. Our code is available at\nhttps://github.com/fangyuanmao/UNIV.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15642v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15642v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.326,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15645",
      "title": "GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host\n  Offloading",
      "authors": [
        "Donghyun Lee",
        "Dawoon Jeong",
        "Jae W. Lee",
        "Hongil Yoon"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The advent of 3D Gaussian Splatting has revolutionized graphics rendering by\ndelivering high visual quality and fast rendering speeds. However, training\nlarge-scale scenes at high quality remains challenging due to the substantial\nmemory demands required to store parameters, gradients, and optimizer states,\nwhich can quickly overwhelm GPU memory. To address these limitations, we\npropose GS-Scale, a fast and memory-efficient training system for 3D Gaussian\nSplatting. GS-Scale stores all Gaussians in host memory, transferring only a\nsubset to the GPU on demand for each forward and backward pass. While this\ndramatically reduces GPU memory usage, it requires frustum culling and\noptimizer updates to be executed on the CPU, introducing slowdowns due to CPU's\nlimited compute and memory bandwidth. To mitigate this, GS-Scale employs three\nsystem-level optimizations: (1) selective offloading of geometric parameters\nfor fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer\nupdates with GPU computation, and (3) deferred optimizer update to minimize\nunnecessary memory accesses for Gaussians with zero gradients. Our extensive\nevaluations on large-scale datasets demonstrate that GS-Scale significantly\nlowers GPU memory demands by 3.3-5.6x, while achieving training speeds\ncomparable to GPU without host offloading. This enables large-scale 3D Gaussian\nSplatting training on consumer-grade GPUs; for instance, GS-Scale can scale the\nnumber of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU,\nleading to 23-35% LPIPS (learned perceptual image patch similarity)\nimprovement.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15645v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15645v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.273,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.493,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper introduces GS-Scale, a system that uses CPU offloading to optimize memory usage for 3D Gaussian Splatting training on a single GPU, contrasting it with multi-GPU distributed approaches mentioned in related works. While it involves parallel computing concepts like selective offloading and pipelining, it does not focus on partitioning data or computation across multiple nodes or processors, which is the core of distributed training. Thus, it is only loosely connected to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15648",
      "title": "FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation\n  based on 3D Gaussian Splatting",
      "authors": [
        "Yuwei Jia",
        "Yutang Lu",
        "Zhe Cui",
        "Fei Su"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Researchers have conducted many pioneer researches on contactless\nfingerprints, yet the performance of contactless fingerprint recognition still\nlags behind contact-based methods primary due to the insufficient contactless\nfingerprint data with pose variations and lack of the usage of implicit 3D\nfingerprint representations. In this paper, we introduce a novel contactless\nfingerprint 3D registration, reconstruction and generation framework by\nintegrating 3D Gaussian Splatting, with the goal of offering a new paradigm for\ncontactless fingerprint recognition that integrates 3D fingerprint\nreconstruction and generation. To our knowledge, this is the first work to\napply 3D Gaussian Splatting to the field of fingerprint recognition, and the\nfirst to achieve effective 3D registration and complete reconstruction of\ncontactless fingerprints with sparse input images and without requiring camera\nparameters information. Experiments on 3D fingerprint registration,\nreconstruction, and generation prove that our method can accurately align and\nreconstruct 3D fingerprints from 2D images, and sequentially generates\nhigh-quality contactless fingerprints from 3D model, thus increasing the\nperformances for contactless fingerprint recognition.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15648v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15648v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.244,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.285,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15651",
      "title": "Toward Efficient Influence Function: Dropout as a Compression Tool",
      "authors": [
        "Yuchen Zhang",
        "Mohammad Mohammadi Amiri"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Assessing the impact the training data on machine learning models is crucial\nfor understanding the behavior of the model, enhancing the transparency, and\nselecting training data. Influence function provides a theoretical framework\nfor quantifying the effect of training data points on model's performance given\na specific test data. However, the computational and memory costs of influence\nfunction presents significant challenges, especially for large-scale models,\neven when using approximation methods, since the gradients involved in\ncomputation are as large as the model itself. In this work, we introduce a\nnovel approach that leverages dropout as a gradient compression mechanism to\ncompute the influence function more efficiently. Our method significantly\nreduces computational and memory overhead, not only during the influence\nfunction computation but also in gradient compression process. Through\ntheoretical analysis and empirical validation, we demonstrate that our method\ncould preserves critical components of the data influence and enables its\napplication to modern large-scale models.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15651v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15651v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.412,
      "datasets_score": 0.279,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is using dropout as a compression tool to efficiently compute influence functions, focusing on reducing computational and memory overhead for gradient calculations in large-scale models. While this addresses efficiency in model analysis, it does not directly involve distributed training concepts like parallel computing across multiple nodes or partitioning data/computation. However, improved efficiency could indirectly benefit distributed systems by making computations faster, creating a loose connection.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15658",
      "title": "Chunk Knowledge Generation Model for Enhanced Information Retrieval: A\n  Multi-task Learning Approach",
      "authors": [
        "Jisu Kim",
        "Jinhee Park",
        "Changhyun Jeon",
        "Jungwoo Choi",
        "Keonwoo Kim",
        "Minji Hong",
        "Sehyun Kim"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Traditional query expansion techniques for addressing vocabulary mismatch\nproblems in information retrieval are context-sensitive and may lead to\nperformance degradation. As an alternative, document expansion research has\ngained attention, but existing methods such as Doc2Query have limitations\nincluding excessive preprocessing costs, increased index size, and reliability\nissues with generated content. To mitigate these problems and seek more\nstructured and efficient alternatives, this study proposes a method that\ndivides documents into chunk units and generates textual data for each chunk to\nsimultaneously improve retrieval efficiency and accuracy. The proposed \"Chunk\nKnowledge Generation Model\" adopts a T5-based multi-task learning structure\nthat simultaneously generates titles and candidate questions from each document\nchunk while extracting keywords from user queries. This approach maximizes\ncomputational efficiency by generating and extracting three types of semantic\ninformation in parallel through a single encoding and two decoding processes.\nThe generated data is utilized as additional information in the retrieval\nsystem. GPT-based evaluation on 305 query-document pairs showed that retrieval\nusing the proposed model achieved 95.41% accuracy at Top@10, demonstrating\nsuperior performance compared to document chunk-level retrieval. This study\ncontributes by proposing an approach that simultaneously generates titles and\ncandidate questions from document chunks for application in retrieval\npipelines, and provides empirical evidence applicable to large-scale\ninformation retrieval systems by demonstrating improved retrieval accuracy\nthrough qualitative evaluation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15658v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15658v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.397,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a T5-based multi-task learning model for generating titles, questions, and keywords from document chunks to enhance information retrieval. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15661",
      "title": "SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio\n  Language Models",
      "authors": [
        "Qiaolin Wang",
        "Xilin Jiang",
        "Linyang He",
        "Junkai Wu",
        "Nima Mesgarani"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "While large audio-language models (LALMs) have demonstrated state-of-the-art\naudio understanding, their reasoning capability in complex soundscapes still\nfalls behind large vision-language models (LVLMs). Compared to the visual\ndomain, one bottleneck is the lack of large-scale chain-of-thought audio data\nto teach LALM stepwise reasoning. To circumvent this data and modality gap, we\npresent SightSound-R1, a cross-modal distillation framework that transfers\nadvanced reasoning from a stronger LVLM teacher to a weaker LALM student on the\nsame audio-visual question answering (AVQA) dataset. SightSound-R1 consists of\nthree core steps: (i) test-time scaling to generate audio-focused chains of\nthought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter\nhallucinations, and (iii) a distillation pipeline with supervised fine-tuning\n(SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM\nstudent. Results show that SightSound-R1 improves LALM reasoning performance\nboth in the in-domain AVQA test set as well as in unseen auditory scenes and\nquestions, outperforming both pretrained and label-only distilled baselines.\nThus, we conclude that vision reasoning can be effectively transferred to audio\nmodels and scaled with abundant audio-visual data.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15661v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15661v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.508,
      "distributed_training_score": 0.334,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves cross-modal reasoning distillation from vision to audio language models using chain-of-thought, supervised fine-tuning, and policy optimization, without any reference to diffusion models or iterative refinement processes for reasoning tasks. It does not adapt diffusion mechanisms for logical tasks or treat chains-of-thought as entities refined through diffusion-like steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15666",
      "title": "TISDiSS: A Training-Time and Inference-Time Scalable Framework for\n  Discriminative Source Separation",
      "authors": [
        "Yongsheng Feng",
        "Yuetonghui Xu",
        "Jiehui Luo",
        "Hongjia Liu",
        "Xiaobing Li",
        "Feng Yu",
        "Wei Li"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Source separation is a fundamental task in speech, music, and audio\nprocessing, and it also provides cleaner and larger data for training\ngenerative models. However, improving separation performance in practice often\ndepends on increasingly large networks, inflating training and deployment\ncosts. Motivated by recent advances in inference-time scaling for generative\nmodeling, we propose Training-Time and Inference-Time Scalable Discriminative\nSource Separation (TISDiSS), a unified framework that integrates early-split\nmulti-loss supervision, shared-parameter design, and dynamic inference\nrepetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting\ninference depth without retraining additional models. We further provide\nsystematic analyses of architectural and training choices and show that\ntraining with more inference repetitions improves shallow-inference\nperformance, benefiting low-latency applications. Experiments on standard\nspeech separation benchmarks demonstrate state-of-the-art performance with a\nreduced parameter count, establishing TISDiSS as a scalable and practical\nframework for adaptive source separation. Code is available at\nhttps://github.com/WingSingFung/TISDiSS.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15666v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15666v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.446,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper describes a supervised framework for source separation using techniques like early-split multi-loss supervision, which relies on standard training with explicit losses on intermediate representations. It does not involve programmatically generating labels from noisy or imprecise sources, a key aspect of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on scalable training and inference for a single model, including shared-parameter design and dynamic inference repetitions, but does not address distributed training, parallel computing across multiple nodes, or partitioning data/computation for acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15674",
      "title": "Inference Offloading for Cost-Sensitive Binary Classification at the\n  Edge",
      "authors": [
        "Vishnu Narayanan Moothedath",
        "Umang Agarwal",
        "Umeshraja N",
        "James Richard Gross",
        "Jaya Prakash Champati",
        "Sharayu Moharir"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "We focus on a binary classification problem in an edge intelligence system\nwhere false negatives are more costly than false positives. The system has a\ncompact, locally deployed model, which is supplemented by a larger, remote\nmodel, which is accessible via the network by incurring an offloading cost. For\neach sample, our system first uses the locally deployed model for inference.\nBased on the output of the local model, the sample may be offloaded to the\nremote model. This work aims to understand the fundamental trade-off between\nclassification accuracy and these offloading costs within such a hierarchical\ninference (HI) system. To optimize this system, we propose an online learning\nframework that continuously adapts a pair of thresholds on the local model's\nconfidence scores. These thresholds determine the prediction of the local model\nand whether a sample is classified locally or offloaded to the remote model. We\npresent a closed-form solution for the setting where the local model is\ncalibrated. For the more general case of uncalibrated models, we introduce\nH2T2, an online two-threshold hierarchical inference policy, and prove it\nachieves sublinear regret. H2T2 is model-agnostic, requires no training, and\nlearns in the inference phase using limited feedback. Simulations on real-world\ndatasets show that H2T2 consistently outperforms naive and single-threshold HI\npolicies, sometimes even surpassing offline optima. The policy also\ndemonstrates robustness to distribution shifts and adapts effectively to\nmismatched classifiers.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15674v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15674v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.403,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on online learning for inference offloading in edge intelligence systems, using algorithmic feedback to adapt thresholds and minimize costs. It does not involve human feedback, reward models trained on human-ranked data, or reinforcement learning for model alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses inference offloading between local and remote models in edge systems, emphasizing decision-making during inference rather than techniques for distributed training, parallel computing, or accelerating model training across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15675",
      "title": "A PCA Based Model for Surface Reconstruction from Incomplete Point\n  Clouds",
      "authors": [
        "Hao Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Point cloud data represents a crucial category of information for\nmathematical modeling, and surface reconstruction from such data is an\nimportant task across various disciplines. However, during the scanning\nprocess, the collected point cloud data may fail to cover the entire surface\ndue to factors such as high light-absorption rate and occlusions, resulting in\nincomplete datasets. Inferring surface structures in data-missing regions and\nsuccessfully reconstructing the surface poses a challenge. In this paper, we\npresent a Principal Component Analysis (PCA) based model for surface\nreconstruction from incomplete point cloud data. Initially, we employ PCA to\nestimate the normal information of the underlying surface from the available\npoint cloud data. This estimated normal information serves as a regularizer in\nour model, guiding the reconstruction of the surface, particularly in areas\nwith missing data. Additionally, we introduce an operator-splitting method to\neffectively solve the proposed model. Through systematic experimentation, we\ndemonstrate that our model successfully infers surface structures in\ndata-missing regions and well reconstructs the underlying surfaces,\noutperforming existing methodologies.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15675v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15675v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.27,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.293,
      "datasets_score": 0.259,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15676",
      "title": "KITE: Kernelized and Information Theoretic Exemplars for In-Context\n  Learning",
      "authors": [
        "Vaibhav Singh",
        "Soumya Suvra Ghosal",
        "Kapu Nirmal Joshua",
        "Soumyabrata Pal",
        "Sayak Ray Chowdhury"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "In-context learning (ICL) has emerged as a powerful paradigm for adapting\nlarge language models (LLMs) to new and data-scarce tasks using only a few\ncarefully selected task-specific examples presented in the prompt. However,\ngiven the limited context size of LLMs, a fundamental question arises: Which\nexamples should be selected to maximize performance on a given user query?\nWhile nearest-neighbor-based methods like KATE have been widely adopted for\nthis purpose, they suffer from well-known drawbacks in high-dimensional\nembedding spaces, including poor generalization and a lack of diversity. In\nthis work, we study this problem of example selection in ICL from a principled,\ninformation theory-driven perspective. We first model an LLM as a linear\nfunction over input embeddings and frame the example selection task as a\nquery-specific optimization problem: selecting a subset of exemplars from a\nlarger example bank that minimizes the prediction error on a specific query.\nThis formulation departs from traditional generalization-focused learning\ntheoretic approaches by targeting accurate prediction for a specific query\ninstance. We derive a principled surrogate objective that is approximately\nsubmodular, enabling the use of a greedy algorithm with an approximation\nguarantee. We further enhance our method by (i) incorporating the kernel trick\nto operate in high-dimensional feature spaces without explicit mappings, and\n(ii) introducing an optimal design-based regularizer to encourage diversity in\nthe selected examples. Empirically, we demonstrate significant improvements\nover standard retrieval methods across a suite of classification tasks,\nhighlighting the benefits of structure-aware, diverse example selection for ICL\nin real-world, label-scarce scenarios.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15676v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15676v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.348,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on optimizing example selection for in-context learning in LLMs using information-theoretic methods, without any involvement of human feedback, reward models, or reinforcement learning techniques. It does not address aligning AI models with human preferences.",
      "weak_supervision_justification": "The paper assumes access to a bank of high-quality task-specific examples for ICL and concentrates on their selection, rather than programmatically generating labels from noisy or imprecise sources. It does not involve training models with weakly supervised data.",
      "diffusion_reasoning_justification": "The paper deals with kernelized and information-theoretic approaches for example selection in ICL, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It lacks any components related to diffusion-based techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15677",
      "title": "Camera Splatting for Continuous View Optimization",
      "authors": [
        "Gahye Lee",
        "Hyomin Kim",
        "Gwangjin Ju",
        "Jooeun Son",
        "Hyejeong Yoon",
        "Seungyong Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We propose Camera Splatting, a novel view optimization framework for novel\nview synthesis. Each camera is modeled as a 3D Gaussian, referred to as a\ncamera splat, and virtual cameras, termed point cameras, are placed at 3D\npoints sampled near the surface to observe the distribution of camera splats.\nView optimization is achieved by continuously and differentiably refining the\ncamera splats so that desirable target distributions are observed from the\npoint cameras, in a manner similar to the original 3D Gaussian splatting.\nCompared to the Farthest View Sampling (FVS) approach, our optimized views\ndemonstrate superior performance in capturing complex view-dependent phenomena,\nincluding intense metallic reflections and intricate textures such as text.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15677v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15677v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.236,
      "weak_supervision_score": 0.241,
      "diffusion_reasoning_score": 0.306,
      "distributed_training_score": 0.26,
      "datasets_score": 0.201,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15678",
      "title": "Layout Stroke Imitation: A Layout Guided Handwriting Stroke Generation\n  for Style Imitation with Diffusion Model",
      "authors": [
        "Sidra Hanif",
        "Longin Jan Latecki"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Handwriting stroke generation is crucial for improving the performance of\ntasks such as handwriting recognition and writers order recovery. In\nhandwriting stroke generation, it is significantly important to imitate the\nsample calligraphic style. The previous studies have suggested utilizing the\ncalligraphic features of the handwriting. However, they had not considered word\nspacing (word layout) as an explicit handwriting feature, which results in\ninconsistent word spacing for style imitation. Firstly, this work proposes\nmulti-scale attention features for calligraphic style imitation. These\nmulti-scale feature embeddings highlight the local and global style features.\nSecondly, we propose to include the words layout, which facilitates word\nspacing for handwriting stroke generation. Moreover, we propose a conditional\ndiffusion model to predict strokes in contrast to previous work, which directly\ngenerated style images. Stroke generation provides additional temporal\ncoordinate information, which is lacking in image generation. Hence, our\nproposed conditional diffusion model for stroke generation is guided by\ncalligraphic style and word layout for better handwriting imitation and stroke\ngeneration in a calligraphic style. Our experimentation shows that the proposed\ndiffusion model outperforms the current state-of-the-art stroke generation and\nis competitive with recent image generation networks.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15678v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15678v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.53,
      "distributed_training_score": 0.301,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a conditional diffusion model for handwriting stroke generation, which involves iterative refinement to produce sequences of strokes. However, this application focuses on generative tasks for visual data imitation, not on adapting diffusion for complex logical reasoning, such as treating a 'Chain-of-Thought' as a single entity for multi-step correction in logical tasks. Thus, while diffusion models are employed, they are not used for reasoning purposes as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15688",
      "title": "Saccadic Vision for Fine-Grained Visual Classification",
      "authors": [
        "Johann Schmidt",
        "Sebastian Stober",
        "Joachim Denzler",
        "Paul Bodesheim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Fine-grained visual classification (FGVC) requires distinguishing between\nvisually similar categories through subtle, localized features - a task that\nremains challenging due to high intra-class variability and limited inter-class\ndifferences. Existing part-based methods often rely on complex localization\nnetworks that learn mappings from pixel to sample space, requiring a deep\nunderstanding of image content while limiting feature utility for downstream\ntasks. In addition, sampled points frequently suffer from high spatial\nredundancy, making it difficult to quantify the optimal number of required\nparts. Inspired by human saccadic vision, we propose a two-stage process that\nfirst extracts peripheral features (coarse view) and generates a sample map,\nfrom which fixation patches are sampled and encoded in parallel using a\nweight-shared encoder. We employ contextualized selective attention to weigh\nthe impact of each fixation patch before fusing peripheral and focus\nrepresentations. To prevent spatial collapse - a common issue in part-based\nmethods - we utilize non-maximum suppression during fixation sampling to\neliminate redundancy. Comprehensive evaluation on standard FGVC benchmarks\n(CUB-200-2011, NABirds, Food-101 and Stanford-Dogs) and challenging insect\ndatasets (EU-Moths, Ecuador-Moths and AMI-Moths) demonstrates that our method\nachieves comparable performance to state-of-the-art approaches while\nconsistently outperforming our baseline encoder.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15688v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15688v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.335,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15690",
      "title": "CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning\n  Framework for C++ Compilation Repair",
      "authors": [
        "Weixuan Sun",
        "Jucai Zhai",
        "Dengfeng Liu",
        "Xin Zhang",
        "Xiaojun Wu",
        "Qiaobo Hao",
        "AIMgroup",
        "Yang Fang",
        "Jiuyang Tang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The automated repair of C++ compilation errors presents a significant\nchallenge, the resolution of which is critical for developer productivity.\nProgress in this domain is constrained by two primary factors: the scarcity of\nlarge-scale, high-fidelity datasets and the limitations of conventional\nsupervised methods, which often fail to generate semantically correct\npatches.This paper addresses these gaps by introducing a comprehensive\nframework with three core contributions. First, we present CCrepair, a novel,\nlarge-scale C++ compilation error dataset constructed through a sophisticated\ngenerate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL)\nparadigm guided by a hybrid reward signal, shifting the focus from mere\ncompilability to the semantic quality of the fix. Finally, we establish the\nrobust, two-stage evaluation system providing this signal, centered on an\nLLM-as-a-Judge whose reliability has been rigorously validated against the\ncollective judgments of a panel of human experts. This integrated approach\naligns the training objective with generating high-quality, non-trivial patches\nthat are both syntactically and semantically correct. The effectiveness of our\napproach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct\nmodel achieved performance comparable to a Qwen2.5-14B-Instruct model,\nvalidating the efficiency of our training paradigm. Our work provides the\nresearch community with a valuable new dataset and a more effective paradigm\nfor training and evaluating robust compilation repair models, paving the way\nfor more practical and reliable automated programming assistants.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15690v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15690v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.457,
      "weak_supervision_score": 0.451,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.4,
      "datasets_score": 0.421,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper uses RL with a hybrid reward signal from an LLM-as-a-Judge validated against human experts, which indirectly incorporates human preferences. However, it does not involve training a reward model directly on human-ranked data, making it not a full RLHF approach.",
      "weak_supervision_justification": "The paper employs a generate-and-verify pipeline to create the CCrepair dataset, which programmatically generates labels for training data, aligning with weak supervision by using automated, potentially noisy sources rather than hand-labeled data.",
      "diffusion_reasoning_justification": "The paper focuses on RL for code repair without any mention of diffusion models, iterative refinement for Chain-of-Thought, or multi-step logical reasoning processes characteristic of diffusion-based approaches.",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or multi-node strategies; it centers on RL frameworks and dataset creation without any reference to accelerating training across processors.",
      "datasets_justification": "The paper's main contribution includes introducing and benchmarking the CCrepairBench dataset, detailing its creation, evaluation, and use for C++ error repair, directly aligning with research on dataset creation and analysis.",
      "llm_score_status": "completed",
      "summary": "This paper introduces CCrepairBench, a large-scale, high-fidelity dataset for C++ compilation errors, along with a reinforcement learning framework that uses a hybrid reward signal to generate semantically correct fixes, addressing the limitations of traditional supervised methods. The methodology involves a generate-and-verify pipeline for dataset creation, an RL paradigm guided by a validated LLM-as-a-Judge evaluation system, and experiments demonstrating that their RL-trained model achieves performance comparable to a larger model, thereby advancing automated program repair for C++.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel dataset and RL framework specifically for C++ compilation errors, significantly advancing the state-of-the-art in an underexplored domain by shifting from supervised learning to interactive feedback-driven methods.",
      "impact_score": "High",
      "impact_justification": "The work provides a new benchmark and training paradigm that could influence future research in automated program repair and lead to more reliable tools for developers, potentially broadening applications in software engineering.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents strong contributions to AI for software engineering, making it valuable for researchers in the field, though it may not be essential for those outside this specific subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5751bcf78804da38264b9fb58e0807141b16710e",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Weixuan Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2344540710"
        },
        {
          "name": "Jucai Zhai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362450832"
        },
        {
          "name": "Dengfeng Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375837002"
        },
        {
          "name": "Xin Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381368408"
        },
        {
          "name": "Xiaojun Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375850211"
        },
        {
          "name": "Qiaobo Hao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375820855"
        },
        {
          "name": "AIMgroup",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381258230"
        },
        {
          "name": "Yang Fang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381317349"
        },
        {
          "name": "Jiuyang Tang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2333668015"
        }
      ]
    },
    {
      "id": "2509.15693",
      "title": "SCENEFORGE: Enhancing 3D-text alignment with Structured Scene\n  Compositions",
      "authors": [
        "Cristian Sbrolli",
        "Matteo Matteucci"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "The whole is greater than the sum of its parts-even in 3D-text contrastive\nlearning. We introduce SceneForge, a novel framework that enhances contrastive\nalignment between 3D point clouds and text through structured multi-object\nscene compositions. SceneForge leverages individual 3D shapes to construct\nmulti-object scenes with explicit spatial relations, pairing them with coherent\nmulti-object descriptions refined by a large language model. By augmenting\ncontrastive training with these structured, compositional samples, SceneForge\neffectively addresses the scarcity of large-scale 3D-text datasets,\nsignificantly enriching data complexity and diversity. We systematically\ninvestigate critical design elements, such as the optimal number of objects per\nscene, the proportion of compositional samples in training batches, and scene\nconstruction strategies. Extensive experiments demonstrate that SceneForge\ndelivers substantial performance gains across multiple tasks, including\nzero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet,\nas well as few-shot part segmentation on ShapeNetPart. SceneForge's\ncompositional augmentations are model-agnostic, consistently improving\nperformance across multiple encoder architectures. Moreover, SceneForge\nimproves 3D visual question answering on ScanQA, generalizes robustly to\nretrieval scenarios with increasing scene complexity, and showcases spatial\nreasoning capabilities by adapting spatial configurations to align precisely\nwith textual instructions.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15693v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15693v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.365,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for enhancing 3D-text alignment using structured scene compositions and contrastive learning, with no involvement of diffusion models or iterative refinement processes for logical reasoning. It focuses on 3D point clouds, text descriptions, and performance gains in tasks like classification and segmentation, which do not align with the topic's emphasis on adapting diffusion for multi-step logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15695",
      "title": "ORIC: Benchmarking Object Recognition in Incongruous Context for Large\n  Vision-Language Models",
      "authors": [
        "Zhaoyang Li",
        "Zhan Ling",
        "Yuchen Zhou",
        "Hao Su"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have made significant strides in image\ncaption, visual question answering, and robotics by integrating visual and\ntextual information. However, they remain prone to errors in incongruous\ncontexts, where objects appear unexpectedly or are absent when contextually\nexpected. This leads to two key recognition failures: object misidentification\nand hallucination. To systematically examine this issue, we introduce the\nObject Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark\nthat evaluates LVLMs in scenarios where object-context relationships deviate\nfrom expectations. ORIC employs two key strategies: (1) LLM-guided sampling,\nwhich identifies objects that are present but contextually incongruous, and (2)\nCLIP-guided sampling, which detects plausible yet nonexistent objects that are\nlikely to be hallucinated, thereby creating an incongruous context. Evaluating\n18 LVLMs and two open-vocabulary detection models, our results reveal\nsignificant recognition gaps, underscoring the challenges posed by contextual\nincongruity. This work provides critical insights into LVLMs' limitations and\nencourages further research on context-aware object recognition.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15695v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15695v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.349,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on introducing a benchmark for evaluating object recognition in incongruous contexts for LVLMs, using existing models like GPT-4o for sampling. It does not involve training or fine-tuning models with human feedback, a reward model, or reinforcement learning techniques, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates LVLMs on object recognition tasks and uses methods like LLM-guided and CLIP-guided sampling, but it does not incorporate diffusion models or iterative refinement processes for multi-step logical reasoning. There is no mention of adapting diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15704",
      "title": "Pyramid Token Pruning for High-Resolution Large Vision-Language Models\n  via Region, Token, and Instruction-Guided Importance",
      "authors": [
        "Yuxuan Liang",
        "Xu Li",
        "Xiaolei Chen",
        "Yi Zheng",
        "Haotian Chen",
        "Bin Li",
        "Xiangyang Xue"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have recently demonstrated strong\nmultimodal understanding, yet their fine-grained visual perception is often\nconstrained by low input resolutions. A common remedy is to partition\nhigh-resolution images into multiple sub-images for separate encoding, but this\napproach drastically inflates the number of visual tokens and introduces\nprohibitive inference overhead. To overcome this challenge, we propose Pyramid\nToken Pruning (PTP), a training-free strategy that hierarchically integrates\nbottom-up visual saliency at both region and token levels with top-down\ninstruction-guided relevance. Inspired by human visual cognition, PTP\nselectively preserves more tokens from salient regions while further\nemphasizing those most relevant to task instructions. Extensive experiments on\n13 diverse benchmarks show that PTP substantially reduces computational cost,\nmemory usage, and inference latency, with negligible performance degradation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15704v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15704v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.441,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on token pruning for efficiency in vision-language models, using saliency and instruction guidance, without any involvement of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no mention of adapting diffusion for Chain-of-Thought or holistic correction.",
      "distributed_training_justification": "The paper addresses inference efficiency through token pruning in LVLMs, but it does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data, architecture, or computation across processors. It is solely about optimizing token usage during inference, not training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15706",
      "title": "SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction\n  on a New Passive Active Satellite Benchmark",
      "authors": [
        "Chi Yang",
        "Fu Wang",
        "Xiaofei Yang",
        "Hao Huang",
        "Weijia Cao",
        "Xiaowen Chu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Cloud phase profiles are critical for numerical weather prediction (NWP), as\nthey directly affect radiative transfer and precipitation processes. In this\nstudy, we present a benchmark dataset and a baseline framework for transforming\nmultimodal satellite observations into detailed 3D cloud phase structures,\naiming toward operational cloud phase profile retrieval and future integration\nwith NWP systems to improve cloud microphysics parameterization. The multimodal\nobservations consist of (1) high--spatiotemporal--resolution, multi-band\nvisible (VIS) and thermal infrared (TIR) imagery from geostationary satellites,\nand (2) accurate vertical cloud phase profiles from spaceborne lidar\n(CALIOP\\slash CALIPSO) and radar (CPR\\slash CloudSat). The dataset consists of\nsynchronized image--profile pairs across diverse cloud regimes, defining a\nsupervised learning task: given VIS/TIR patches, predict the corresponding 3D\ncloud phase structure. We adopt SGMAGNet as the main model and compare it with\nseveral baseline architectures, including UNet variants and SegNet, all\ndesigned to capture multi-scale spatial patterns. Model performance is\nevaluated using standard classification metrics, including Precision, Recall,\nF1-score, and IoU. The results demonstrate that SGMAGNet achieves superior\nperformance in cloud phase reconstruction, particularly in complex multi-layer\nand boundary transition regions. Quantitatively, SGMAGNet attains a Precision\nof 0.922, Recall of 0.858, F1-score of 0.763, and an IoU of 0.617,\nsignificantly outperforming all baselines across these key metrics.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15706v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15706v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.37,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating and presenting a new benchmark dataset for machine learning applications in satellite-based cloud phase reconstruction. It details the dataset's curation by fusing multimodal observations from geostationary satellites (VIS/TIR imagery) and spaceborne instruments (lidar and radar), analyzes its composition (e.g., synchronized image-profile pairs across cloud regimes), and benchmarks it through model evaluations, directly aligning with research on dataset creation, curation, and benchmarking for AI.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a benchmark dataset combining multimodal satellite observations from geostationary satellites' visible and thermal infrared imagery with vertical cloud phase profiles from spaceborne lidar and radar, aiming to reconstruct 3D cloud phase structures for improved numerical weather prediction. The authors propose SGMAGNet as a baseline model, compare it with architectures like UNet and SegNet using supervised learning on synchronized image-profile pairs, and demonstrate its superior performance with metrics such as Precision (0.922), Recall (0.858), F1-score (0.763), and IoU (0.617), particularly in complex cloud regions.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a new benchmark dataset and adapting existing architectures like SGMAGNet for 3D cloud phase reconstruction, though it primarily combines established techniques rather than introducing a entirely new problem or method.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in atmospheric science and AI for remote sensing by providing a new benchmark and baseline for cloud structure reconstruction, potentially enhancing weather prediction models within its subfield.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with a new dataset and effective baseline model for an important application in climate studies, making it worth reading for researchers in computer vision and meteorology.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a645b8b5fd7fa122e619736047e87aa89f3708e9",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 2,
      "average_h_index": 0.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Chi Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381783271"
        },
        {
          "name": "Fu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382588632"
        },
        {
          "name": "Xiaofei Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2343781463"
        },
        {
          "name": "Hao Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382916964"
        },
        {
          "name": "Weijia Cao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2282140905"
        },
        {
          "name": "Xiaowen Chu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381262455"
        }
      ]
    },
    {
      "id": "2509.15711",
      "title": "Toward Medical Deepfake Detection: A Comprehensive Dataset and Novel\n  Method",
      "authors": [
        "Shuaibo Li",
        "Zhaohu Xing",
        "Hongqiu Wang",
        "Pengfei Hao",
        "Xingyu Li",
        "Zekai Liu",
        "Lei Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The rapid advancement of generative AI in medical imaging has introduced both\nsignificant opportunities and serious challenges, especially the risk that fake\nmedical images could undermine healthcare systems. These synthetic images pose\nserious risks, such as diagnostic deception, financial fraud, and\nmisinformation. However, research on medical forensics to counter these threats\nremains limited, and there is a critical lack of comprehensive datasets\nspecifically tailored for this field. Additionally, existing media forensic\nmethods, which are primarily designed for natural or facial images, are\ninadequate for capturing the distinct characteristics and subtle artifacts of\nAI-generated medical images. To tackle these challenges, we introduce\n\\textbf{MedForensics}, a large-scale medical forensics dataset encompassing six\nmedical modalities and twelve state-of-the-art medical generative models. We\nalso propose \\textbf{DSKI}, a novel \\textbf{D}ual-\\textbf{S}tage\n\\textbf{K}nowledge \\textbf{I}nfusing detector that constructs a vision-language\nfeature space tailored for the detection of AI-generated medical images. DSKI\ncomprises two core components: 1) a cross-domain fine-trace adapter (CDFA) for\nextracting subtle forgery clues from both spatial and noise domains during\ntraining, and 2) a medical forensic retrieval module (MFRM) that boosts\ndetection accuracy through few-shot retrieval during testing. Experimental\nresults demonstrate that DSKI significantly outperforms both existing methods\nand human experts, achieving superior accuracy across multiple medical\nmodalities.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15711v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15711v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.342,
      "datasets_score": 0.475,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the introduction of the MedForensics dataset, a large-scale collection of AI-generated medical images across six modalities and twelve generative models. This directly aligns with research on creating datasets, as it involves dataset curation from various sources, detailing methodologies for generation and inclusion, and providing a benchmark for evaluating forensic detection methods in AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the critical risks posed by AI-generated medical images by introducing the MedForensics dataset, a large-scale collection encompassing six medical modalities and images generated by twelve state-of-the-art models, to facilitate research in medical forensics. It proposes the DSKI detector, which uses a dual-stage approach with a cross-domain fine-trace adapter to extract subtle forgery clues during training and a medical forensic retrieval module to enhance detection accuracy during testing, demonstrating superior performance over existing methods and human experts in experimental evaluations.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new comprehensive dataset for medical deepfake detection and a novel dual-stage detection method tailored to medical images, significantly advancing the state-of-the-art in an underrepresented area of AI forensics.",
      "impact_score": "High",
      "impact_justification": "The work addresses vital risks in healthcare systems from AI-generated medical images, potentially influencing future research in medical AI safety, diagnostics, and broader applications in generative AI ethics and detection.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and timely contribution to medical forensics with a new dataset and effective detection method, making it essential for researchers in AI and healthcare to understand and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2c200aaded868a491deb93b2d9f5c0b62fcecfac",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 14,
      "average_h_index": 3.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shuaibo Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370987247"
        },
        {
          "name": "Zhaohu Xing",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/153107262"
        },
        {
          "name": "Hongqiu Wang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2274928570"
        },
        {
          "name": "Pengfei Hao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2345389215"
        },
        {
          "name": "Xingyu Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382086159"
        },
        {
          "name": "Zekai Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382120481"
        },
        {
          "name": "Lei Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2348189926"
        }
      ]
    },
    {
      "id": "2509.15714",
      "title": "Once Upon a Time: Interactive Learning for Storytelling with Small\n  Language Models",
      "authors": [
        "Jonas Mayer Martins",
        "Ali Hamza Bashir",
        "Muhammad Rehan Khalid",
        "Lisa Beinborn"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Children efficiently acquire language not just by listening, but by\ninteracting with others in their social environment. Conversely, large language\nmodels are typically trained with next-word prediction on massive amounts of\ntext. Motivated by this contrast, we investigate whether language models can be\ntrained with less data by learning not only from next-word prediction but also\nfrom high-level, cognitively inspired feedback. We train a student model to\ngenerate stories, which a teacher model rates on readability, narrative\ncoherence, and creativity. By varying the amount of pretraining before the\nfeedback loop, we assess the impact of this interactive learning on formal and\nfunctional linguistic competence. We find that the high-level feedback is\nhighly data efficient: With just 1 M words of input in interactive learning,\nstorytelling skills can improve as much as with 410 M words of next-word\nprediction.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15714v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15714v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.453,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.322,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with feedback from a teacher model, not human-ranked data, as it involves an AI evaluating stories based on predefined criteria. Since RLHF specifically requires human feedback, this does not qualify.",
      "weak_supervision_justification": "The paper's approach involves training a model using high-level feedback from a teacher model (e.g., ratings on readability, coherence, and creativity), which aligns with weak supervision by programmatically generating labels from noisy or imprecise sources, rather than relying on hand-labeled data.",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for storytelling and next-word prediction, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates whether small language models can learn more efficiently by incorporating interactive, high-level feedback inspired by children's social language acquisition, comparing it to traditional next-word prediction training. The methodology involves pretraining a student model on a corpus and then using reinforcement learning where a teacher model evaluates generated stories based on readability, narrative coherence, and creativity, providing rewards for updates. Key findings reveal that this interactive approach is highly data-efficient, improving storytelling skills with just 1 million words of feedback as effectively as 410 million words of conventional training.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining reinforcement learning with high-level feedback for storytelling in language models, offering a clever adaptation of existing techniques to enhance efficiency, though it does not introduce an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like language model training and AI efficiency, as it demonstrates data-efficient methods that could influence future research, but its applicability may remain niche without broader validation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a strong, valuable contribution to understanding interactive learning in language models, making it essential for researchers in AI and computational linguistics to be aware of its insights and findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/702ed44b9a800cfb8308582c7f2122c8043ac17f",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jonas Mayer Martins",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381347356"
        },
        {
          "name": "Ali Hamza Bashir",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2340529043"
        },
        {
          "name": "Muhammad Rehan Khalid",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381306999"
        },
        {
          "name": "Lisa Beinborn",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2332827163"
        }
      ]
    },
    {
      "id": "2509.15730",
      "title": "A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process\n  Automation",
      "authors": [
        "Lukas Laakmann",
        "Seyyid A. Ciftci",
        "Christian Janiesch"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Robotic process automation (RPA) is a lightweight approach to automating\nbusiness processes using software robots that emulate user actions at the\ngraphical user interface level. While RPA has gained popularity for its\ncost-effective and timely automation of rule-based, well-structured tasks, its\nsymbolic nature has inherent limitations when approaching more complex tasks\ncurrently performed by human agents. Machine learning concepts enabling\nintelligent RPA provide an opportunity to broaden the range of automatable\ntasks. In this paper, we conduct a literature review to explore the connections\nbetween RPA and machine learning and organize the joint concept intelligent RPA\ninto a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML\nintegration and RPA-ML interaction. Together, they comprise eight dimensions:\narchitecture and ecosystem, capabilities, data basis, intelligence level, and\ntechnical depth of integration as well as deployment environment, lifecycle\nphase, and user-robot relation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15730v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15730v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.286,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15733",
      "title": "GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic\n  Manipulation",
      "authors": [
        "Quanhao Qian",
        "Guoyang Zhao",
        "Gongjie Zhang",
        "Jiuniu Wang",
        "Ran Xu",
        "Junlong Gao",
        "Deli Zhao"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Effective robotic manipulation relies on a precise understanding of 3D scene\ngeometry, and one of the most straightforward ways to acquire such geometry is\nthrough multi-view observations. Motivated by this, we present GP3 -- a 3D\ngeometry-aware robotic manipulation policy that leverages multi-view input. GP3\nemploys a spatial encoder to infer dense spatial features from RGB\nobservations, which enable the estimation of depth and camera parameters,\nleading to a compact yet expressive 3D scene representation tailored for\nmanipulation. This representation is fused with language instructions and\ntranslated into continuous actions via a lightweight policy head. Comprehensive\nexperiments demonstrate that GP3 consistently outperforms state-of-the-art\nmethods on simulated benchmarks. Furthermore, GP3 transfers effectively to\nreal-world robots without depth sensors or pre-mapped environments, requiring\nonly minimal fine-tuning. These results highlight GP3 as a practical,\nsensor-agnostic solution for geometry-aware robotic manipulation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15733v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15733v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.349,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15741",
      "title": "TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic\n  Image Detection",
      "authors": [
        "Laixin Zhang",
        "Shuaibo Li",
        "Wei Ma",
        "Hongbin Zha"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The rapid progress of generative models has made synthetic image detection an\nincreasingly critical task. Most existing approaches attempt to construct a\nsingle, universal discriminative space to separate real from fake content.\nHowever, such unified spaces tend to be complex and brittle, often struggling\nto generalize to unseen generative patterns. In this work, we propose TrueMoE,\na novel dual-routing Mixture-of-Discriminative-Experts framework that\nreformulates the detection task as a collaborative inference across multiple\nspecialized and lightweight discriminative subspaces. At the core of TrueMoE is\na Discriminative Expert Array (DEA) organized along complementary axes of\nmanifold structure and perceptual granularity, enabling diverse forgery cues to\nbe captured across subspaces. A dual-routing mechanism, comprising a\ngranularity-aware sparse router and a manifold-aware dense router, adaptively\nassigns input images to the most relevant experts. Extensive experiments across\na wide spectrum of generative models demonstrate that TrueMoE achieves superior\ngeneralization and robustness.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15741v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15741v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.348,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for detecting synthetic images using a mixture of experts, focusing on generative models like diffusion models for image synthesis. It does not involve adapting diffusion processes for multi-step logical reasoning, chain-of-thought entities, or solving complex logical tasks. Therefore, there is no component related to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15748",
      "title": "Hybrid Lie semi-group and cascade structures for the generalized\n  Gaussian derivative model for visual receptive fields",
      "authors": [
        "Tony Lindeberg"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Because of the variabilities of real-world image structures under the natural\nimage transformations that arise when observing similar objects or\nspatio-temporal events under different viewing conditions, the receptive field\nresponses computed in the earliest layers of the visual hierarchy may be\nstrongly influenced by such geometric image transformations. One way of\nhandling this variability is by basing the vision system on covariant receptive\nfield families, which expand the receptive field shapes over the degrees of\nfreedom in the image transformations.\n  This paper addresses the problem of deriving relationships between spatial\nand spatio-temporal receptive field responses obtained for different values of\nthe shape parameters in the resulting multi-parameter families of receptive\nfields. For this purpose, we derive both (i) infinitesimal relationships,\nroughly corresponding to a combination of notions from semi-groups and Lie\ngroups, as well as (ii) macroscopic cascade smoothing properties, which\ndescribe how receptive field responses at coarser spatial and temporal scales\ncan be computed by applying smaller support incremental filters to the output\nfrom corresponding receptive fields at finer spatial and temporal scales,\nstructurally related to the notion of Lie algebras, although with directional\npreferences.\n  The presented results provide (i) a deeper understanding of the relationships\nbetween spatial and spatio-temporal receptive field responses for different\nvalues of the filter parameters, which can be used for both (ii) designing more\nefficient schemes for computing receptive field responses over populations of\nmulti-parameter families of receptive fields, as well as (iii)~formulating\nidealized theoretical models of the computations of simple cells in biological\nvision.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15748v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15748v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.257,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.28,
      "datasets_score": 0.251,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15750",
      "title": "FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric\n  Fusion",
      "authors": [
        "Han Ye",
        "Haofu Wang",
        "Yunchi Zhang",
        "Jiangjian Xiao",
        "Yuqiang Jin",
        "Jinyuan Liu",
        "Wen-An Zhang",
        "Uladzislau Sychou",
        "Alexander Tuzikov",
        "Vladislav Sobolevskii",
        "Valerii Zakharov",
        "Boris Sokolov",
        "Minglei Fu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reconstructing building floor plans from point cloud data is key for indoor\nnavigation, BIM, and precise measurements. Traditional methods like geometric\nalgorithms and Mask R-CNN-based deep learning often face issues with noise,\nlimited generalization, and loss of geometric details. We propose FloorSAM, a\nframework that integrates point cloud density maps with the Segment Anything\nModel (SAM) for accurate floor plan reconstruction from LiDAR data. Using\ngrid-based filtering, adaptive resolution projection, and image enhancement, we\ncreate robust top-down density maps. FloorSAM uses SAM's zero-shot learning for\nprecise room segmentation, improving reconstruction across diverse layouts.\nRoom masks are generated via adaptive prompt points and multistage filtering,\nfollowed by joint mask and point cloud analysis for contour extraction and\nregularization. This produces accurate floor plans and recovers room\ntopological relationships. Tests on Giblayout and ISPRS datasets show better\naccuracy, recall, and robustness than traditional methods, especially in noisy\nand complex settings. Code and materials: github.com/Silentbarber/FloorSAM.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15750v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15750v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.292,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15751",
      "title": "Simulated Cortical Magnification Supports Self-Supervised Object\n  Learning",
      "authors": [
        "Zhengyang Yu",
        "Arthur Aubret",
        "Chen Yu",
        "Jochen Triesch"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent self-supervised learning models simulate the development of semantic\nobject representations by training on visual experience similar to that of\ntoddlers. However, these models ignore the foveated nature of human vision with\nhigh/low resolution in the center/periphery of the visual field. Here, we\ninvestigate the role of this varying resolution in the development of object\nrepresentations. We leverage two datasets of egocentric videos that capture the\nvisual experience of humans during interactions with objects. We apply models\nof human foveation and cortical magnification to modify these inputs, such that\nthe visual content becomes less distinct towards the periphery. The resulting\nsequences are used to train two bio-inspired self-supervised learning models\nthat implement a time-based learning objective. Our results show that modeling\naspects of foveated vision improves the quality of the learned object\nrepresentations in this setting. Our analysis suggests that this improvement\ncomes from making objects appear bigger and inducing a better trade-off between\ncentral and peripheral visual information. Overall, this work takes a step\ntowards making models of humans' learning of visual representations more\nrealistic and performant.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15751v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15751v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.414,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.359,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on self-supervised learning, which involves deriving supervisory signals from data itself, similar to weak supervision's use of programmatically generated labels. However, it does not explicitly address generating noisy or imprecise labels from high-level sources; instead, it emphasizes simulating human vision for better object representations. This makes it only tangentially related, as self-supervised methods can overlap with weak supervision but are not the core focus here.",
      "diffusion_reasoning_justification": "The paper discusses self-supervised learning for visual object representations using simulated human vision, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It is entirely focused on computer vision and biological inspiration, lacking any components related to diffusion-based approaches for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15753",
      "title": "MCOD: The First Challenging Benchmark for Multispectral Camouflaged\n  Object Detection",
      "authors": [
        "Yang Li",
        "Tingfa Xu",
        "Shuyan Bai",
        "Peifu Liu",
        "Jianan Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Camouflaged Object Detection (COD) aims to identify objects that blend\nseamlessly into natural scenes. Although RGB-based methods have advanced, their\nperformance remains limited under challenging conditions. Multispectral\nimagery, providing rich spectral information, offers a promising alternative\nfor enhanced foreground-background discrimination. However, existing COD\nbenchmark datasets are exclusively RGB-based, lacking essential support for\nmultispectral approaches, which has impeded progress in this area. To address\nthis gap, we introduce MCOD, the first challenging benchmark dataset\nspecifically designed for multispectral camouflaged object detection. MCOD\nfeatures three key advantages: (i) Comprehensive challenge attributes: It\ncaptures real-world difficulties such as small object sizes and extreme\nlighting conditions commonly encountered in COD tasks. (ii) Diverse real-world\nscenarios: The dataset spans a wide range of natural environments to better\nreflect practical applications. (iii) High-quality pixel-level annotations:\nEach image is manually annotated with precise object masks and corresponding\nchallenge attribute labels. We benchmark eleven representative COD methods on\nMCOD, observing a consistent performance drop due to increased task difficulty.\nNotably, integrating multispectral modalities substantially alleviates this\ndegradation, highlighting the value of spectral information in enhancing\ndetection robustness. We anticipate MCOD will provide a strong foundation for\nfuture research in multispectral camouflaged object detection. The dataset is\npublicly accessible at https://github.com/yl2900260-bit/MCOD.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15753v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15753v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.263,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.286,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15758",
      "title": "Uncertainty-Gated Deformable Network for Breast Tumor Segmentation in MR\n  Images",
      "authors": [
        "Yue Zhang",
        "Jiahua Dong",
        "Chengtao Peng",
        "Qiuli Wang",
        "Dan Song",
        "Guiduo Duan"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate segmentation of breast tumors in magnetic resonance images (MRI) is\nessential for breast cancer diagnosis, yet existing methods face challenges in\ncapturing irregular tumor shapes and effectively integrating local and global\nfeatures. To address these limitations, we propose an uncertainty-gated\ndeformable network to leverage the complementary information from CNN and\nTransformers. Specifically, we incorporates deformable feature modeling into\nboth convolution and attention modules, enabling adaptive receptive fields for\nirregular tumor contours. We also design an Uncertainty-Gated Enhancing Module\n(U-GEM) to selectively exchange complementary features between CNN and\nTransformer based on pixel-wise uncertainty, enhancing both local and global\nrepresentations. Additionally, a Boundary-sensitive Deep Supervision Loss is\nintroduced to further improve tumor boundary delineation. Comprehensive\nexperiments on two clinical breast MRI datasets demonstrate that our method\nachieves superior segmentation performance compared with state-of-the-art\nmethods, highlighting its clinical potential for accurate breast tumor\ndelineation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15758v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15758v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.247,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.328,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15759",
      "title": "On Optimal Steering to Achieve Exact Fairness",
      "authors": [
        "Mohit Sharma",
        "Amit Jayant Deshpande",
        "Chiranjib Bhattacharyya",
        "Rajiv Ratn Shah"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "To fix the 'bias in, bias out' problem in fair machine learning, it is\nimportant to steer feature distributions of data or internal representations of\nLarge Language Models (LLMs) to ideal ones that guarantee group-fair outcomes.\nPrevious work on fair generative models and representation steering could\ngreatly benefit from provable fairness guarantees on the model output. We\ndefine a distribution as ideal if the minimizer of any cost-sensitive risk on\nit is guaranteed to have exact group-fair outcomes (e.g., demographic parity,\nequal opportunity)-in other words, it has no fairness-utility trade-off. We\nformulate an optimization program for optimal steering by finding the nearest\nideal distribution in KL-divergence, and provide efficient algorithms for it\nwhen the underlying distributions come from well-known parametric families\n(e.g., normal, log-normal). Empirically, our optimal steering techniques on\nboth synthetic and real-world datasets improve fairness without diminishing\nutility (and sometimes even improve utility). We demonstrate affine steering of\nLLM representations to reduce bias in multi-class classification, e.g.,\noccupation prediction from a short biography in Bios dataset (De-Arteaga et\nal.). Furthermore, we steer internal representations of LLMs towards desired\noutputs so that it works equally well across different groups.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15759v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15759v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.371,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is on achieving exact fairness in machine learning by defining and steering ideal data distributions, formulating optimization problems, and applying techniques to LLMs for tasks like classification and emotion steering. It does not involve reinforcement learning, human feedback, reward models, or any process of fine-tuning models based on human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15768",
      "title": "Overview of PlantCLEF 2024: multi-species plant identification in\n  vegetation plot images",
      "authors": [
        "Herve Goeau",
        "Vincent Espitalier",
        "Pierre Bonnet",
        "Alexis Joly"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Plot images are essential for ecological studies, enabling standardized\nsampling, biodiversity assessment, long-term monitoring and remote, large-scale\nsurveys. Plot images are typically fifty centimetres or one square meter in\nsize, and botanists meticulously identify all the species found there. The\nintegration of AI could significantly improve the efficiency of specialists,\nhelping them to extend the scope and coverage of ecological studies. To\nevaluate advances in this regard, the PlantCLEF 2024 challenge leverages a new\ntest set of thousands of multi-label images annotated by experts and covering\nover 800 species. In addition, it provides a large training set of 1.7 million\nindividual plant images as well as state-of-the-art vision transformer models\npre-trained on this data. The task is evaluated as a (weakly-labeled)\nmulti-label classification task where the aim is to predict all the plant\nspecies present on a high-resolution plot image (using the single-label\ntraining data). In this paper, we provide an detailed description of the data,\nthe evaluation methodology, the methods and models employed by the participants\nand the results achieved.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15768v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15768v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.254,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.265,
      "distributed_training_score": 0.319,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15772",
      "title": "Vision-Language Models as Differentiable Semantic and Spatial Rewards\n  for Text-to-3D Generation",
      "authors": [
        "Weimin Bai",
        "Yubo Li",
        "Weijian Luo",
        "Wenzheng Chen",
        "He Sun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Score Distillation Sampling (SDS) enables high-quality text-to-3D generation\nby supervising 3D models through the denoising of multi-view 2D renderings,\nusing a pretrained text-to-image diffusion model to align with the input prompt\nand ensure 3D consistency. However, existing SDS-based methods face two\nfundamental limitations: (1) their reliance on CLIP-style text encoders leads\nto coarse semantic alignment and struggles with fine-grained prompts; and (2)\n2D diffusion priors lack explicit 3D spatial constraints, resulting in\ngeometric inconsistencies and inaccurate object relationships in multi-object\nscenes. To address these challenges, we propose VLM3D, a novel text-to-3D\ngeneration framework that integrates large vision-language models (VLMs) into\nthe SDS pipeline as differentiable semantic and spatial priors. Unlike standard\ntext-to-image diffusion priors, VLMs leverage rich language-grounded\nsupervision that enables fine-grained prompt alignment. Moreover, their\ninherent vision language modeling provides strong spatial understanding, which\nsignificantly enhances 3D consistency for single-object generation and improves\nrelational reasoning in multi-object scenes. We instantiate VLM3D based on the\nopen-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark.\nExperiments across diverse objects and complex scenes show that VLM3D\nsignificantly outperforms prior SDS-based methods in semantic fidelity,\ngeometric coherence, and spatial correctness.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15772v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15772v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.53,
      "distributed_training_score": 0.351,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained Vision-Language Models (VLMs) as rewards in a Score Distillation Sampling (SDS) pipeline for text-to-3D generation, without involving human-ranked data or a separate reward model trained on human feedback. There is no reinforcement learning component or alignment with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper builds on Score Distillation Sampling (SDS), which uses text-to-image diffusion models, but it does not adapt diffusion for multi-step logical reasoning or treat a Chain-of-Thought as an entity for iterative refinement. Instead, it focuses on 3D generation with VLMs for semantic and spatial rewards, lacking any component for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15780",
      "title": "Ontology Creation and Management Tools: the Case of Anatomical\n  Connectivity",
      "authors": [
        "Natallia Kokash",
        "Bernard de Bono",
        "Tom Gillespie"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.DL (Digital Libraries)"
      ],
      "abstract": "We are developing infrastructure to support researchers in mapping data\nrelated to the peripheral nervous system and other physiological systems, with\nan emphasis on their relevance to the organs under investigation. The nervous\nsystem, a complex network of nerves and ganglia, plays a critical role in\ncoordinating and transmitting signals throughout the body. To aid in this, we\nhave created ApiNATOMY, a framework for the topological and semantic\nrepresentation of multiscale physiological circuit maps. ApiNATOMY integrates a\nKnowledge Representation (KR) model and a suite of Knowledge Management (KM)\ntools. The KR model enables physiology experts to easily capture interactions\nbetween anatomical entities, while the KM tools help modelers convert\nhigh-level abstractions into detailed models of physiological processes, which\ncan be integrated with external ontologies and knowledge graphs.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15780v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15780v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.26,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15781",
      "title": "Enriched Feature Representation and Motion Prediction Module for MOSEv2\n  Track of 7th LSVOS Challenge: 3rd Place Solution",
      "authors": [
        "Chang Soo Lim",
        "Joonyoung Moon",
        "Donghyeon Cho"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video object segmentation (VOS) is a challenging task with wide applications\nsuch as video editing and autonomous driving. While Cutie provides strong\nquery-based segmentation and SAM2 offers enriched representations via a\npretrained ViT encoder, each has limitations in feature capacity and temporal\nmodeling. In this report, we propose a framework that integrates their\ncomplementary strengths by replacing the encoder of Cutie with the ViT encoder\nof SAM2 and introducing a motion prediction module for temporal stability. We\nfurther adopt an ensemble strategy combining Cutie, SAM2, and our variant,\nachieving 3rd place in the MOSEv2 track of the 7th LSVOS Challenge. We refer to\nour final model as SCOPE (SAM2-CUTIE Object Prediction Ensemble). This\ndemonstrates the effectiveness of enriched feature representation and motion\nprediction for robust video object segmentation. The code is available at\nhttps://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15781v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15781v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.266,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.302,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15784",
      "title": "Ideal Registration? Segmentation is All You Need",
      "authors": [
        "Xiang Chen",
        "Fengting Zhang",
        "Qinghao Liu",
        "Min Liu",
        "Kun Wu",
        "Yaonan Wang",
        "Hang Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deep learning has revolutionized image registration by its ability to handle\ndiverse tasks while achieving significant speed advantages over conventional\napproaches. Current approaches, however, often employ globally uniform\nsmoothness constraints that fail to accommodate the complex, regionally varying\ndeformations characteristic of anatomical motion. To address this limitation,\nwe propose SegReg, a Segmentation-driven Registration framework that implements\nanatomically adaptive regularization by exploiting region-specific deformation\npatterns. Our SegReg first decomposes input moving and fixed images into\nanatomically coherent subregions through segmentation. These localized domains\nare then processed by the same registration backbone to compute optimized\npartial deformation fields, which are subsequently integrated into a global\ndeformation field. SegReg achieves near-perfect structural alignment (98.23%\nDice on critical anatomies) using ground-truth segmentation, and outperforms\nexisting methods by 2-12% across three clinical registration scenarios\n(cardiac, abdominal, and lung images) even with automatic segmentation. Our\nSegReg demonstrates a near-linear dependence of registration accuracy on\nsegmentation quality, transforming the registration challenge into a\nsegmentation problem. The source code will be released upon manuscript\nacceptance.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15784v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15784v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.347,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15785",
      "title": "CBPNet: A Continual Backpropagation Prompt Network for Alleviating\n  Plasticity Loss on Edge Devices",
      "authors": [
        "Runjie Shao",
        "Boyu Diao",
        "Zijia An",
        "Ruiqi Liu",
        "Yongjun Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "To meet the demands of applications like robotics and autonomous driving that\nrequire real-time responses to dynamic environments, efficient continual\nlearning methods suitable for edge devices have attracted increasing attention.\nIn this transition, using frozen pretrained models with prompts has become a\nmainstream strategy to combat catastrophic forgetting. However, this approach\nintroduces a new critical bottleneck: plasticity loss, where the model's\nability to learn new knowledge diminishes due to the frozen backbone and the\nlimited capacity of prompt parameters. We argue that the reduction in\nplasticity stems from a lack of update vitality in underutilized parameters\nduring the training process. To this end, we propose the Continual\nBackpropagation Prompt Network (CBPNet), an effective and parameter efficient\nframework designed to restore the model's learning vitality. We innovatively\nintegrate an Efficient CBP Block that counteracts plasticity decay by\nadaptively reinitializing these underutilized parameters. Experimental results\non edge devices demonstrate CBPNet's effectiveness across multiple benchmarks.\nOn Split CIFAR-100, it improves average accuracy by over 1% against a strong\nbaseline, and on the more challenging Split ImageNet-R, it achieves a state of\nthe art accuracy of 69.41%. This is accomplished by training additional\nparameters that constitute less than 0.2% of the backbone's size, validating\nour approach.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15785v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15785v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.412,
      "datasets_score": 0.271,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the development of CBPNet, a framework for continual learning that addresses plasticity loss in prompt-based methods on edge devices. It focuses on techniques like parameter reinitialization to improve learning efficiency on resource-constrained devices, without any discussion of distributed training, parallel computing, multi-node setups, or partitioning data/computation across processors. Therefore, it does not relate to the topic of distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15786",
      "title": "Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage\n  Approach via Semantic Clustering and Multi-Agent Collaboration",
      "authors": [
        "Nan Li",
        "Bo Kang",
        "Tijl De Bie"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Creating robust occupation taxonomies, vital for applications ranging from\njob recommendation to labor market intelligence, is challenging. Manual\ncuration is slow, while existing automated methods are either not adaptive to\ndynamic regional markets (top-down) or struggle to build coherent hierarchies\nfrom noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent\ntaxonomy Builder), a framework that fully automates the creation of\nhigh-quality, data-driven taxonomies from raw job postings. CLIMB uses global\nsemantic clustering to distill core occupations, then employs a\nreflection-based multi-agent system to iteratively build a coherent hierarchy.\nOn three diverse, real-world datasets, we show that CLIMB produces taxonomies\nthat are more coherent and scalable than existing methods and successfully\ncapture unique regional characteristics. We release our code and datasets at\nhttps://anonymous.4open.science/r/CLIMB.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15786v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15786v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.323,
      "datasets_score": 0.413,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper utilizes three real-world datasets of job postings to demonstrate and evaluate the CLIMB framework, and it releases these datasets publicly to promote reproducibility. This aligns with aspects of dataset introduction and sharing for AI applications. However, the primary focus is on developing a taxonomy-building methodology using these datasets, rather than on creating, analyzing, benchmarking, or evaluating datasets as the core contribution.",
      "llm_score_status": "completed",
      "summary": "This paper introduces CLIMB, a novel framework for automating the creation of data-driven occupation taxonomies from raw job postings, addressing the limitations of existing top-down and bottom-up methods by using semantic clustering to distill core occupations from the entire dataset and a reflection-based multi-agent system to iteratively build a coherent hierarchy. The methodology ensures global consistency and adaptability to regional markets, and evaluations on three real-world datasets demonstrate that CLIMB produces more coherent, scalable taxonomies that effectively capture unique local characteristics, with the authors releasing code and datasets for reproducibility.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative multi-stage framework, CLIMB, that combines global semantic clustering with a reflection-based multi-agent system to automate bottom-up taxonomy generation, significantly advancing the state-of-the-art in handling dynamic and noisy data for occupation hierarchies.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI and information retrieval due to its practical applications in labor market analysis and job recommendation, though its influence may remain confined to specific domains rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and innovative contribution to automated taxonomy generation, making it essential for researchers in AI and information retrieval to understand its methods and implications, though it may not be critical for those outside these areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5473c9ba39febe4eef0028e07ff78b06cc3854b4",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 36,
      "average_h_index": 16.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Nan Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2215172053"
        },
        {
          "name": "Bo Kang",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/144325478"
        },
        {
          "name": "T. D. Bie",
          "h_index": 36,
          "profile_url": "https://www.semanticscholar.org/author/51204489"
        }
      ]
    },
    {
      "id": "2509.15788",
      "title": "FoBa: A Foreground-Background co-Guided Method and New Benchmark for\n  Remote Sensing Semantic Change Detection",
      "authors": [
        "Haotian Zhang",
        "Han Guo",
        "Keyan Chen",
        "Hao Chen",
        "Zhengxia Zou",
        "Zhenwei Shi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite the remarkable progress achieved in remote sensing semantic change\ndetection (SCD), two major challenges remain. At the data level, existing SCD\ndatasets suffer from limited change categories, insufficient change types, and\na lack of fine-grained class definitions, making them inadequate to fully\nsupport practical applications. At the methodological level, most current\napproaches underutilize change information, typically treating it as a\npost-processing step to enhance spatial consistency, which constrains further\nimprovements in model performance. To address these issues, we construct a new\nbenchmark for remote sensing SCD, LevirSCD. Focused on the Beijing area, the\ndataset covers 16 change categories and 210 specific change types, with more\nfine-grained class definitions (e.g., roads are divided into unpaved and paved\nroads). Furthermore, we propose a foreground-background co-guided SCD (FoBa)\nmethod, which leverages foregrounds that focus on regions of interest and\nbackgrounds enriched with contextual information to guide the model\ncollaboratively, thereby alleviating semantic ambiguity while enhancing its\nability to detect subtle changes. Considering the requirements of bi-temporal\ninteraction and spatial consistency in SCD, we introduce a Gated Interaction\nFusion (GIF) module along with a simple consistency loss to further enhance the\nmodel's detection performance. Extensive experiments on three datasets (SECOND,\nJL1, and the proposed LevirSCD) demonstrate that FoBa achieves competitive\nresults compared to current SOTA methods, with improvements of 1.48%, 3.61%,\nand 2.81% in the SeK metric, respectively. Our code and dataset are available\nat https://github.com/zmoka-zht/FoBa.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15788v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15788v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.285,
      "distributed_training_score": 0.319,
      "datasets_score": 0.429,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the creation and introduction of a new dataset, LevirSCD, specifically for remote sensing semantic change detection. It details the dataset's construction, such as its 16 change categories, 210 specific change types, and fine-grained annotations, and compares it to existing datasets in a table. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications, as it provides a new benchmark with detailed analysis.",
      "llm_score_status": "completed",
      "summary": "The paper addresses limitations in remote sensing semantic change detection (SCD) by introducing a new benchmark dataset, LevirSCD, which features 16 change categories, 210 specific change types, and fine-grained annotations based on Beijing area images, and by proposing the FoBa method that uses foreground-background co-guidance to enhance detection accuracy, incorporating a Gated Interaction Fusion (GIF) module and a consistency loss to improve bi-temporal feature interaction and spatial consistency. Through extensive experiments on LevirSCD and existing datasets like SECOND and JL1, the FoBa method demonstrates superior performance, achieving improvements of up to 3.61% in the SeK metric compared to state-of-the-art approaches, thereby advancing the field by providing a more comprehensive dataset and a novel guidance mechanism for better handling semantic ambiguity and subtle changes.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new benchmark dataset with significantly more change categories and fine-grained annotations, and a novel FoBa method that innovatively combines foreground and background guidance with a GIF module, representing a substantial advancement in SCD techniques. This addresses key gaps in existing datasets and methods, making it a significant contribution to the state-of-the-art in remote sensing.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the specific subfield of remote sensing SCD due to the new dataset and improved method, which could enhance applications in urban monitoring and disaster assessment. However, its influence may be limited to researchers focused on computer vision in remote sensing rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a strong, valuable contribution with a new dataset and method that advances SCD, making it essential for researchers in remote sensing and computer vision to be aware of for potential applications and further developments. While not groundbreaking for all audiences, it offers high-quality insights that could inform ongoing work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/78f5e987d66901807b108f5788b71583e7df0430",
      "total_authors": 6,
      "authors_found": 5,
      "highest_h_index": 23,
      "average_h_index": 10.8,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Haotian Zhang",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2276656129"
        },
        {
          "name": "Han Guo",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2307434318"
        },
        {
          "name": "Keyan Chen",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/152567984"
        },
        {
          "name": "Hao Chen",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2268166007"
        },
        {
          "name": "Zhengxia Zou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Z. Shi",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/113515560"
        }
      ]
    },
    {
      "id": "2509.15791",
      "title": "Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization",
      "authors": [
        "Tan Pan",
        "Kaiyu Guo",
        "Dongli Xu",
        "Zhaorui Tan",
        "Chen Jiang",
        "Deshu Chen",
        "Xin Guo",
        "Brian C. Lovell",
        "Limei Han",
        "Yuan Cheng",
        "Mahsa Baktashmotlagh"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The generalization ability of deep learning has been extensively studied in\nsupervised settings, yet it remains less explored in unsupervised scenarios.\nRecently, the Unsupervised Domain Generalization (UDG) task has been proposed\nto enhance the generalization of models trained with prevalent unsupervised\nlearning techniques, such as Self-Supervised Learning (SSL). UDG confronts the\nchallenge of distinguishing semantics from variations without category labels.\nAlthough some recent methods have employed domain labels to tackle this issue,\nsuch domain labels are often unavailable in real-world contexts. In this paper,\nwe address these limitations by formalizing UDG as the task of learning a\nMinimal Sufficient Semantic Representation: a representation that (i) preserves\nall semantic information shared across augmented views (sufficiency), and (ii)\nmaximally removes information irrelevant to semantics (minimality). We\ntheoretically ground these objectives from the perspective of information\ntheory, demonstrating that optimizing representations to achieve sufficiency\nand minimality directly reduces out-of-distribution risk. Practically, we\nimplement this optimization through Minimal-Sufficient UDG (MS-UDG), a\nlearnable model by integrating (a) an InfoNCE-based objective to achieve\nsufficiency; (b) two complementary components to promote minimality: a novel\nsemantic-variation disentanglement loss and a reconstruction-based mechanism\nfor capturing adequate variation. Empirically, MS-UDG sets a new\nstate-of-the-art on popular unsupervised domain-generalization benchmarks,\nconsistently outperforming existing SSL and UDG methods, without category or\ndomain labels during representation learning.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15791v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15791v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.349,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on Unsupervised Domain Generalization (UDG) using self-supervised learning techniques, such as contrastive learning, without any labels (category or domain). While weak supervision involves learning from noisy or programmatically generated labels, this paper operates in a fully unsupervised setting, relying on data augmentations for supervisory signals. There is a loose connection in that both approaches address learning with limited supervision, but the paper does not use or generate any form of labels, making it only tangentially related to weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15795",
      "title": "TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale\n  Remote Sensing Segmentation",
      "authors": [
        "Tianyang Wang",
        "Xi Xiao",
        "Gaofei Chen",
        "Hanzhang Chi",
        "Qi Zhang",
        "Guo Cheng",
        "Yingrui Ji"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Segment Anything Model (SAM) has demonstrated impressive zero-shot\nsegmentation capabilities across natural image domains, but it struggles to\ngeneralize to the unique challenges of remote sensing data, such as complex\nterrain, multi-scale objects, and temporal dynamics. In this paper, we\nintroduce TASAM, a terrain and temporally-aware extension of SAM designed\nspecifically for high-resolution remote sensing image segmentation. TASAM\nintegrates three lightweight yet effective modules: a terrain-aware adapter\nthat injects elevation priors, a temporal prompt generator that captures\nland-cover changes over time, and a multi-scale fusion strategy that enhances\nfine-grained object delineation. Without retraining the SAM backbone, our\napproach achieves substantial performance gains across three remote sensing\nbenchmarks-LoveDA, iSAID, and WHU-CD-outperforming both zero-shot SAM and\ntask-specific models with minimal computational overhead. Our results highlight\nthe value of domain-adaptive augmentation for foundation models and offer a\nscalable path toward more robust geospatial segmentation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15795v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15795v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.264,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.307,
      "distributed_training_score": 0.32,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15796",
      "title": "Monte Carlo Tree Diffusion with Multiple Experts for Protein Design",
      "authors": [
        "Xuefeng Liu",
        "Mingxuan Cao",
        "Songhao Jiang",
        "Xiao Luo",
        "Xiaotian Duan",
        "Mengdi Wang",
        "Tobin R. Sosnick",
        "Jinbo Xu",
        "Rick Stevens"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The goal of protein design is to generate amino acid sequences that fold into\nfunctional structures with desired properties. Prior methods combining\nautoregressive language models with Monte Carlo Tree Search (MCTS) struggle\nwith long-range dependencies and suffer from an impractically large search\nspace. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts,\nwhich integrates masked diffusion models with tree search to enable multi-token\nplanning and efficient exploration. Unlike autoregressive planners, MCTD-ME\nuses biophysical-fidelity-enhanced diffusion denoising as the rollout engine,\njointly revising multiple positions and scaling to large sequence spaces. It\nfurther leverages experts of varying capacities to enrich exploration, guided\nby a pLDDT-based masking schedule that targets low-confidence regions while\npreserving reliable residues. We propose a novel multi-expert selection rule\n(PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse\nfolding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and\nunguided baselines in both sequence recovery (AAR) and structural similarity\n(scTM), with gains increasing for longer proteins and benefiting from\nmulti-expert guidance. More generally, the framework is model-agnostic and\napplicable beyond inverse folding, including de novo protein engineering and\nmulti-objective molecular generation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15796v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15796v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.549,
      "distributed_training_score": 0.381,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Moderately Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper adapts diffusion models for iterative refinement in protein sequence design, using a tree-structured process to evaluate and improve partial sequences, which aligns with the iterative nature of diffusion-based reasoning. However, it focuses on generative planning for biological tasks rather than multi-step logical reasoning or holistic correction of a 'Chain-of-Thought' for complex logical tasks, making it related but not directly aligned.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces MCTD-ME, a novel framework that combines Monte Carlo Tree Diffusion with Multiple Experts to enhance protein design by generating amino acid sequences that fold into functional structures, addressing limitations in prior methods like autoregressive models and MCTS by enabling multi-token planning and efficient exploration through biophysical-fidelity-enhanced diffusion denoising, multiple experts for diverse search, and a pLDDT-guided masking schedule. The methodology involves a tree-structured diffusion process with a new multi-expert selection rule (PH-UCT-ME), and key findings demonstrate superior performance on benchmarks like CAMEO and PDB for sequence recovery and structural similarity, particularly for longer proteins, with the framework being model-agnostic and applicable to areas such as de novo protein engineering and multi-objective molecular generation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative integration of masked diffusion models with Monte Carlo Tree Search and multiple experts, which significantly advances protein design by addressing long-range dependencies and large search spaces in a way not previously explored. This represents a substantial leap beyond existing methods, making it a high-novelty contribution.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to broadly influence future research in protein design, drug discovery, and generative AI by providing a scalable framework for multi-objective planning, as evidenced by its empirical gains on benchmarks. Its model-agnostic nature and applications beyond inverse folding suggest it could lead to widespread citations and practical advancements in related fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution with innovative techniques and demonstrated improvements, making it essential for researchers in AI for protein design to be aware of. While highly insightful, it may not be groundbreaking enough for every AI practitioner to prioritize as a must-read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a7b5091ea71fe85bba6d4c652c00ec9df6c195b6",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 58,
      "average_h_index": 7.333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Xuefeng Liu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2254615881"
        },
        {
          "name": "Mingxuan Cao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381724158"
        },
        {
          "name": "Songhao Jiang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2305739785"
        },
        {
          "name": "Xiao Luo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382010495"
        },
        {
          "name": "Xiaotian Duan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382442634"
        },
        {
          "name": "Mengdi Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381366970"
        },
        {
          "name": "T. Sosnick",
          "h_index": 58,
          "profile_url": "https://www.semanticscholar.org/author/3132003"
        },
        {
          "name": "Jinbo Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354173662"
        },
        {
          "name": "Rick L. Stevens",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2253471806"
        }
      ]
    },
    {
      "id": "2509.15799",
      "title": "Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent\n  Control",
      "authors": [
        "Max Studt",
        "Georg Schildbach"
      ],
      "categories": [
        "eess.SY (Systems and Control)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)",
        "cs.SY (Systems and Control)",
        "math.OC (Optimization and Control)"
      ],
      "abstract": "Achieving safe and coordinated behavior in dynamic, constraint-rich\nenvironments remains a major challenge for learning-based control. Pure\nend-to-end learning often suffers from poor sample efficiency and limited\nreliability, while model-based methods depend on predefined references and\nstruggle to generalize. We propose a hierarchical framework that combines\ntactical decision-making via reinforcement learning (RL) with low-level\nexecution through Model Predictive Control (MPC). For the case of multi-agent\nsystems this means that high-level policies select abstract targets from\nstructured regions of interest (ROIs), while MPC ensures dynamically feasible\nand safe motion. Tested on a predator-prey benchmark, our approach outperforms\nend-to-end and shielding-based RL baselines in terms of reward, safety, and\nconsistency, underscoring the benefits of combining structured learning with\nmodel-based control.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15799v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15799v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.38,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a hierarchical reinforcement learning framework combined with Model Predictive Control for multi-agent systems, focusing on high-level decision-making and safe execution without any mention of human feedback. RLHF specifically involves using human-ranked data to train a reward model for fine-tuning AI systems, which is not addressed or utilized in this work.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15800",
      "title": "ChronoForge-RL: Chronological Forging through Reinforcement Learning for\n  Enhanced Video Understanding",
      "authors": [
        "Kehua Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Current state-of-the-art video understanding methods typically struggle with\ntwo critical challenges: (1) the computational infeasibility of processing\nevery frame in dense video content and (2) the difficulty in identifying\nsemantically significant frames through naive uniform sampling strategies. In\nthis paper, we propose a novel video understanding framework, called\nChronoForge-RL, which combines Temporal Apex Distillation (TAD) and\nKeyFrame-aware Group Relative Policy Optimization (KF-GRPO) to tackle these\nissues. Concretely, we introduce a differentiable keyframe selection mechanism\nthat systematically identifies semantic inflection points through a three-stage\nprocess to enhance computational efficiency while preserving temporal\ninformation. Then, two particular modules are proposed to enable effective\ntemporal reasoning: Firstly, TAD leverages variation scoring, inflection\ndetection, and prioritized distillation to select the most informative frames.\nSecondly, we introduce KF-GRPO which implements a contrastive learning paradigm\nwith a saliency-enhanced reward mechanism that explicitly incentivizes models\nto leverage both frame content and temporal relationships. Finally, our\nproposed ChronoForge-RL achieves 69.1% on VideoMME and 52.7% on LVBench\ncompared to baseline methods, clearly surpassing previous approaches while\nenabling our 7B parameter model to achieve performance comparable to 72B\nparameter alternatives.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15800v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15800v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.452,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.403,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning in KF-GRPO with rewards based on model performance comparisons (e.g., accurate responses on keyframe sequences), but it does not involve human feedback, such as training a reward model on human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for keyframe selection and temporal reasoning via TAD and KF-GRPO, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "The paper discusses video understanding techniques like keyframe selection and reinforcement learning, but it does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15802",
      "title": "DPC-QA Net: A No-Reference Dual-Stream Perceptual and Cellular Quality\n  Assessment Network for Histopathology Images",
      "authors": [
        "Qijun Yang",
        "Boyang Wang",
        "Hujun Yin"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Reliable whole slide imaging (WSI) hinges on image quality,yet staining\nartefacts, defocus, and cellular degradations are common. We present DPC-QA\nNet, a no-reference dual-stream network that couples wavelet-based global\ndifference perception with cellular quality assessment from nuclear and\nmembrane embeddings via an Aggr-RWKV module. Cross-attention fusion and\nmulti-term losses align perceptual and cellular cues. Across different\ndatasets, our model detects staining, membrane, and nuclear issues with >92%\naccuracy and aligns well with usability scores; on LIVEC and KonIQ it\noutperforms state-of-the-art NR-IQA. A downstream study further shows strong\npositive correlations between predicted quality and cell recognition accuracy\n(e.g., nuclei PQ/Dice, membrane boundary F-score), enabling practical\npre-screening of WSI regions for computational pathology.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15802v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15802v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.311,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15803",
      "title": "CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models",
      "authors": [
        "Fangjian Shen",
        "Zifeng Liang",
        "Chao Wang",
        "Wushao Wen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Text-to-image (T2I) models exhibit a significant yet under-explored \"brand\nbias\", a tendency to generate contents featuring dominant commercial brands\nfrom generic prompts, posing ethical and legal risks. We propose CIDER, a\nnovel, model-agnostic framework to mitigate bias at inference-time through\nprompt refinement to avoid costly retraining. CIDER uses a lightweight detector\nto identify branded content and a Vision-Language Model (VLM) to generate\nstylistically divergent alternatives. We introduce the Brand Neutrality Score\n(BNS) to quantify this issue and perform extensive experiments on leading T2I\nmodels. Results show CIDER significantly reduces both explicit and implicit\nbiases while maintaining image quality and aesthetic appeal. Our work offers a\npractical solution for more original and equitable content, contributing to the\ndevelopment of trustworthy generative AI.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15803v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15803v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.476,
      "distributed_training_score": 0.314,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework (CIDER) to mitigate brand bias in text-to-image diffusion models through prompt refinement and causal intervention, focusing on ethical issues in image generation. It does not involve adapting the iterative refinement process of diffusion models for multi-step logical reasoning or treating a Chain-of-Thought as a single entity for holistic correction. While diffusion models are used as the base for image generation, there is no component dedicated to solving complex logical tasks, making this topic unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15805",
      "title": "Boosting Active Learning with Knowledge Transfer",
      "authors": [
        "Tianyang Wang",
        "Xi Xiao",
        "Gaofei Chen",
        "Xiaoying Liao",
        "Guo Cheng",
        "Yingrui Ji"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Uncertainty estimation is at the core of Active Learning (AL). Most existing\nmethods resort to complex auxiliary models and advanced training fashions to\nestimate uncertainty for unlabeled data. These models need special design and\nhence are difficult to train especially for domain tasks, such as Cryo-Electron\nTomography (cryo-ET) classification in computational biology. To address this\nchallenge, we propose a novel method using knowledge transfer to boost\nuncertainty estimation in AL. Specifically, we exploit the teacher-student mode\nwhere the teacher is the task model in AL and the student is an auxiliary model\nthat learns from the teacher. We train the two models simultaneously in each AL\ncycle and adopt a certain distance between the model outputs to measure\nuncertainty for unlabeled data. The student model is task-agnostic and does not\nrely on special training fashions (e.g. adversarial), making our method\nsuitable for various tasks. More importantly, we demonstrate that data\nuncertainty is not tied to concrete value of task loss but closely related to\nthe upper-bound of task loss. We conduct extensive experiments to validate the\nproposed method on classical computer vision tasks and cryo-ET challenges. The\nresults demonstrate its efficacy and efficiency.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15805v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15805v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.458,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.34,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on Active Learning (AL) with knowledge transfer to estimate uncertainty and select samples for annotation, emphasizing efficient labeling in scenarios like cryo-ET classification. It does not involve programmatically generating labels from high-level, noisy, or imprecise sources, which is the core of weak supervision. Instead, it relies on accurate annotations for selected samples, making it distinct from weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15810",
      "title": "Instance Generation for Meta-Black-Box Optimization through Latent Space\n  Reverse Engineering",
      "authors": [
        "Chen Wang",
        "Zeyuan Ma",
        "Zhiguang Cao",
        "Yue-Jiao Gong"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NE (Neural and Evolutionary Computing)"
      ],
      "abstract": "To relieve intensive human-expertise required to design optimization\nalgorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage\ngeneralization strength of meta-learning to train neural network-based\nalgorithm design policies over a predefined training problem set, which\nautomates the adaptability of the low-level optimizers on unseen problem\ninstances. Currently, a common training problem set choice in existing MetaBBOs\nis well-known benchmark suites CoCo-BBOB. Although such choice facilitates the\nMetaBBO's development, problem instances in CoCo-BBOB are more or less limited\nin diversity, raising the risk of overfitting of MetaBBOs, which might further\nresults in poor generalization. In this paper, we propose an instance\ngeneration approach, termed as \\textbf{LSRE}, which could generate diverse\ntraining problem instances for MetaBBOs to learn more generalizable policies.\nLSRE first trains an autoencoder which maps high-dimensional problem features\ninto a 2-dimensional latent space. Uniform-grid sampling in this latent space\nleads to hidden representations of problem instances with sufficient diversity.\nBy leveraging a genetic-programming approach to search function formulas with\nminimal L2-distance to these hidden representations, LSRE reverse engineers a\ndiversified problem set, termed as \\textbf{Diverse-BBO}. We validate the\neffectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe\ntheir generalization performances on either synthetic or realistic scenarios.\nExtensive experimental results underscore the superiority of Diverse-BBO to\nexisting training set choices in MetaBBOs. Further ablation studies not only\ndemonstrate the effectiveness of design choices in LSRE, but also reveal\ninteresting insights on instance diversity and MetaBBO's generalization.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15810v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15810v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.368,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15811",
      "title": "Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning",
      "authors": [
        "Sara Rajaee",
        "Rochelle Choenni",
        "Ekaterina Shutova",
        "Christof Monz"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While the reasoning abilities of large language models (LLMs) continue to\nadvance, it remains unclear how such ability varies across languages in\nmultilingual LLMs and whether different languages produce reasoning paths that\ncomplement each other. To investigate this question, we train a reward model to\nrank generated responses for a given question across languages. Our results\nshow that our cross-lingual reward model substantially improves mathematical\nreasoning performance compared to using reward modeling within a single\nlanguage, benefiting even high-resource languages. While English often exhibits\nthe highest performance in multilingual models, we find that cross-lingual\nsampling particularly benefits English under low sampling budgets. Our findings\nreveal new opportunities to improve multilingual reasoning by leveraging the\ncomplementary strengths of diverse languages.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15811v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15811v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.526,
      "distributed_training_score": 0.366,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper involves training a reward model to rank responses for mathematical reasoning, which is a technique often associated with RLHF. However, it does not explicitly mention using human-ranked data or human feedback for training the reward model; instead, it references translated datasets and binary cross-entropy loss, indicating a potential deviation from core RLHF principles. Thus, while reward modeling is a shared element, the paper's approach is not a direct application of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on cross-lingual reward modeling for mathematical reasoning using large language models, with no mention of diffusion models, iterative refinement processes, or adapting diffusion techniques for logical tasks. There is no component involving multi-step logical reasoning via a diffusion model, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15812",
      "title": "Diversity of Structured Domains via k-Kemeny Scores",
      "authors": [
        "Piotr Faliszewski",
        "Krzysztof Sornat",
        "Stanisław Szufa",
        "Tomasz Wąs"
      ],
      "categories": [
        "cs.GT (Computer Science and Game Theory)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "In the k-Kemeny problem, we are given an ordinal election, i.e., a collection\nof votes ranking the candidates from best to worst, and we seek the smallest\nnumber of swaps of adjacent candidates that ensure that the election has at\nmost k different rankings. We study this problem for a number of structured\ndomains, including the single-peaked, single-crossing, group-separable, and\nEuclidean ones. We obtain two kinds of results: (1) We show that k-Kemeny\nremains intractable under most of these domains, even for k=2, and (2) we use\nk-Kemeny to rank these domains in terms of their diversity.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15812v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15812v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.235,
      "weak_supervision_score": 0.226,
      "diffusion_reasoning_score": 0.259,
      "distributed_training_score": 0.228,
      "datasets_score": 0.254,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15814",
      "title": "QWD-GAN: Quality-aware Wavelet-driven GAN for Unsupervised Medical\n  Microscopy Images Denoising",
      "authors": [
        "Qijun Yang",
        "Yating Huang",
        "Lintao Xiang",
        "Hujun Yin"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Image denoising plays a critical role in biomedical and microscopy imaging,\nespecially when acquiring wide-field fluorescence-stained images. This task\nfaces challenges in multiple fronts, including limitations in image acquisition\nconditions, complex noise types, algorithm adaptability, and clinical\napplication demands. Although many deep learning-based denoising techniques\nhave demonstrated promising results, further improvements are needed in\npreserving image details, enhancing algorithmic efficiency, and increasing\nclinical interpretability. We propose an unsupervised image denoising method\nbased on a Generative Adversarial Network (GAN) architecture. The approach\nintroduces a multi-scale adaptive generator based on the Wavelet Transform and\na dual-branch discriminator that integrates difference perception feature maps\nwith original features. Experimental results on multiple biomedical microscopy\nimage datasets show that the proposed model achieves state-of-the-art denoising\nperformance, particularly excelling in the preservation of high-frequency\ninformation. Furthermore, the dual-branch discriminator is seamlessly\ncompatible with various GAN frameworks. The proposed quality-aware,\nwavelet-driven GAN denoising model is termed as QWD-GAN.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15814v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15814v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.283,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.308,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15844",
      "title": "FedHK-MVFC: Federated Heat Kernel Multi-View Clustering",
      "authors": [
        "Kristina P. Sinaga"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "math.AG (Algebraic Geometry)"
      ],
      "abstract": "In the realm of distributed AI and privacy-focused medical applications, we\npropose a framework for multi-view clustering that links quantum field theory\nwith federated healthcare analytics. Our method uses heat-kernel coefficients\nfrom spectral analysis to convert Euclidean distances into geometry-aware\nsimilarity measures, capturing the structure of diverse medical data. We lay\nthis out through the Heat Kernel Distance (HKD) transformation with convergence\nguarantees. Two algorithms are developed: Heat Kernel-Enhanced Multi-View Fuzzy\nClustering (HK-MVFC) for central analysis, and Federated Heat Kernel Multi-View\nFuzzy Clustering (FedHK-MVFC) for secure, privacy-preserving learning across\nhospitals using differential privacy and secure aggregation to facilitate\nHIPAA-compliant collaboration. Tests on synthetic datasets of cardiovascular\npatients show an $8-12 \\%$ increase in clustering accuracy, $70 \\%$ reduced\ncommunication, and $98.2 \\%$ efficiency retention over centralized methods.\nValidated on 10,000 patient records across two hospitals, it proves useful for\ncollaborative phenotyping involving ECG, cardiac imaging, and behavioral data.\nOur theoretical contributions include update rules with proven convergence,\nadaptive view weighting, and privacy-preserving protocols. This presents a new\nstandard for geometry-aware federated learning in healthcare, turning advanced\nmath into workable solutions for analyzing sensitive medical data while\nensuring both rigor and clinical relevance.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15844v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15844v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.396,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15848",
      "title": "A Comparative Study of Rule-Based and Data-Driven Approaches in\n  Industrial Monitoring",
      "authors": [
        "Giovanni De Gasperis",
        "Sante Dino Facchini"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Industrial monitoring systems, especially when deployed in Industry 4.0\nenvironments, are experiencing a shift in paradigm from traditional rule-based\narchitectures to data-driven approaches leveraging machine learning and\nartificial intelligence. This study presents a comparison between these two\nmethodologies, analyzing their respective strengths, limitations, and\napplication scenarios, and proposes a basic framework to evaluate their key\nproperties. Rule-based systems offer high interpretability, deterministic\nbehavior, and ease of implementation in stable environments, making them ideal\nfor regulated industries and safety-critical applications. However, they face\nchallenges with scalability, adaptability, and performance in complex or\nevolving contexts. Conversely, data-driven systems excel in detecting hidden\nanomalies, enabling predictive maintenance and dynamic adaptation to new\nconditions. Despite their high accuracy, these models face challenges related\nto data availability, explainability, and integration complexity. The paper\nsuggests hybrid solutions as a possible promising direction, combining the\ntransparency of rule-based logic with the analytical power of machine learning.\nOur hypothesis is that the future of industrial monitoring lies in intelligent,\nsynergic systems that leverage both expert knowledge and data-driven insights.\nThis dual approach enhances resilience, operational efficiency, and trust,\npaving the way for smarter and more flexible industrial environments.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15848v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15848v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.318,
      "datasets_score": 0.397,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a comparison of rule-based and data-driven approaches in industrial monitoring, focusing on their strengths, limitations, and potential hybrids. It discusses general machine learning and AI applications but does not address weak supervision, which involves programmatically generating labels from noisy sources. There is no mention of training models with imprecise or high-level labels, making the paper unrelated to this topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15857",
      "title": "EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving\n  Brain Network",
      "authors": [
        "Rikuto Kotoge",
        "Zheng Chen",
        "Tasuku Kimura",
        "Yasuko Matsubara",
        "Takufumi Yanagisawa",
        "Haruhiko Kishima",
        "Yasushi Sakurai"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Dynamic GNNs, which integrate temporal and spatial features in\nElectroencephalography (EEG) data, have shown great potential in automating\nseizure detection. However, fully capturing the underlying dynamics necessary\nto represent brain states, such as seizure and non-seizure, remains a\nnon-trivial task and presents two fundamental challenges. First, most existing\ndynamic GNN methods are built on temporally fixed static graphs, which fail to\nreflect the evolving nature of brain connectivity during seizure progression.\nSecond, current efforts to jointly model temporal signals and graph structures\nand, more importantly, their interactions remain nascent, often resulting in\ninconsistent performance. To address these challenges, we present the first\ntheoretical analysis of these two problems, demonstrating the effectiveness and\nnecessity of explicit dynamic modeling and time-then-graph dynamic GNN method.\nBuilding on these insights, we propose EvoBrain, a novel seizure detection\nmodel that integrates a two-stream Mamba architecture with a GCN enhanced by\nLaplacian Positional Encoding, following neurological insights. Moreover,\nEvoBrain incorporates explicitly dynamic graph structures, allowing both nodes\nand edges to evolve over time. Our contributions include (a) a theoretical\nanalysis proving the expressivity advantage of explicit dynamic modeling and\ntime-then-graph over other approaches, (b) a novel and efficient model that\nsignificantly improves AUROC by 23% and F1 score by 30%, compared with the\ndynamic GNN baseline, and (c) broad evaluations of our method on the\nchallenging early seizure prediction tasks.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15857v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15857v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.281,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.322,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a dynamic graph neural network model for EEG-based seizure detection, emphasizing temporal and spatial modeling of brain networks. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks, as described in the topic definition.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15859",
      "title": "Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data",
      "authors": [
        "Nakul Sharma"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Imbalanced classification datasets pose significant challenges in machine\nlearning, often leading to biased models that perform poorly on\nunderrepresented classes. With the rise of foundation models, recent research\nhas focused on the full, partial, and parameter-efficient fine-tuning of these\nmodels to deal with long-tail classification. Despite the impressive\nperformance of these works on the benchmark datasets, they still fail to close\nthe gap with the networks trained using the balanced datasets and still require\nsubstantial computational resources, even for relatively smaller datasets.\nUnderscoring the importance of computational efficiency and simplicity, in this\nwork we propose a novel framework that leverages the rich semantic latent space\nof Vision Foundation Models to generate synthetic data and train a simple\nlinear classifier using a mixture of real and synthetic data for long-tail\nclassification. The computational efficiency gain arises from the number of\ntrainable parameters that are reduced to just the number of parameters in the\nlinear model. Our method sets a new state-of-the-art for the CIFAR-100-LT\nbenchmark and demonstrates strong performance on the Places-LT benchmark,\nhighlighting the effectiveness and adaptability of our simple and effective\napproach.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15859v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15859v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.439,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.374,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves generating synthetic features in the latent space to augment imbalanced datasets for long-tail classification, which programmatically creates training examples. However, it primarily focuses on data augmentation rather than generating labels from high-level, noisy, or imprecise sources, as defined in weak supervision. This makes it only loosely connected to the topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15868",
      "title": "LC-SLab -- An Object-based Deep Learning Framework for Large-scale Land\n  Cover Classification from Satellite Imagery and Sparse In-situ Labels",
      "authors": [
        "Johannes Leonhardt",
        "Juergen Gall",
        "Ribana Roscher"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Large-scale land cover maps generated using deep learning play a critical\nrole across a wide range of Earth science applications. Open in-situ datasets\nfrom principled land cover surveys offer a scalable alternative to manual\nannotation for training such models. However, their sparse spatial coverage\noften leads to fragmented and noisy predictions when used with existing deep\nlearning-based land cover mapping approaches. A promising direction to address\nthis issue is object-based classification, which assigns labels to semantically\ncoherent image regions rather than individual pixels, thereby imposing a\nminimum mapping unit. Despite this potential, object-based methods remain\nunderexplored in deep learning-based land cover mapping pipelines, especially\nin the context of medium-resolution imagery and sparse supervision. To address\nthis gap, we propose LC-SLab, the first deep learning framework for\nsystematically exploring object-based deep learning methods for large-scale\nland cover classification under sparse supervision. LC-SLab supports both\ninput-level aggregation via graph neural networks, and output-level aggregation\nby postprocessing results from established semantic segmentation models.\nAdditionally, we incorporate features from a large pre-trained network to\nimprove performance on small datasets. We evaluate the framework on annual\nSentinel-2 composites with sparse LUCAS labels, focusing on the tradeoff\nbetween accuracy and fragmentation, as well as sensitivity to dataset size. Our\nresults show that object-based methods can match or exceed the accuracy of\ncommon pixel-wise models while producing substantially more coherent maps.\nInput-level aggregation proves more robust on smaller datasets, whereas\noutput-level aggregation performs best with more data. Several configurations\nof LC-SLab also outperform existing land cover products, highlighting the\nframework's practical utility.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15868v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15868v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.458,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.403,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using sparse in-situ labels from surveys like LUCAS for training deep learning models, which aligns closely with weak supervision. It programmatically handles noisy, imprecise, or high-level labels rather than relying on dense, hand-annotated data, and incorporates techniques like object-based aggregation to improve results under these conditions.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on object-based deep learning frameworks for land cover classification and does not discuss distributed training, parallel computing, multi-node setups, or any strategies for partitioning data or computation across processors.",
      "datasets_justification": "The paper analyzes and evaluates existing datasets like Sentinel-2 composites and LUCAS for sparse supervision, including benchmarking performance, sensitivity to dataset size, and tradeoffs in accuracy. While it does not introduce a new dataset or focus primarily on dataset creation, it involves dataset analysis in the context of machine learning applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces LC-SLab, a deep learning framework designed for large-scale land cover classification from satellite imagery using sparse in-situ labels, addressing the issue of fragmented predictions by employing object-based methods. It incorporates input-level aggregation via graph neural networks and output-level aggregation through postprocessing, along with features from pre-trained models, and evaluates these on Sentinel-2 composites with LUCAS labels, demonstrating that object-based approaches achieve comparable or better accuracy with less fragmentation, perform robustly across dataset sizes, and outperform existing land cover products.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework, LC-SLab, that systematically explores object-based deep learning methods for land cover classification with sparse labels, significantly advancing the state-of-the-art by addressing underexplored challenges in medium-resolution imagery. This represents a truly new integration of techniques like graph neural networks and output-level aggregation in this specific context.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision for Earth sciences, as it provides practical improvements for handling sparse data in land cover mapping, potentially enhancing applications in climate science and disaster management. However, its influence may be limited to specific domains rather than broadly across all research or commercial areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to remote sensing and machine learning by offering an innovative framework that improves land cover classification accuracy and coherence, making it essential for researchers in these fields. While not groundbreaking for all audiences, its practical utility and empirical findings warrant attention from those working with satellite imagery and sparse labels.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8d37fbf50e213b4ce812e5c9f9468347e0eb95ef",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 26,
      "average_h_index": 9.666666666666666,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Johannes Leonhardt",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2156789684"
        },
        {
          "name": "Juergen Gall",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381259544"
        },
        {
          "name": "R. Roscher",
          "h_index": 26,
          "profile_url": "https://www.semanticscholar.org/author/46525320"
        }
      ]
    },
    {
      "id": "2509.15871",
      "title": "Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval",
      "authors": [
        "Liwei Liao",
        "Xufeng Li",
        "Xiaoyun Zheng",
        "Boning Liu",
        "Feng Gao",
        "Ronggang Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text\nprompts, which is essential for applications such as robotics. However,\nexisting 3DVG methods encounter two main challenges: first, they struggle to\nhandle the implicit representation of spatial textures in 3D Gaussian Splatting\n(3DGS), making per-scene training indispensable; second, they typically require\nlarges amounts of labeled data for effective training. To this end, we propose\n\\underline{G}rounding via \\underline{V}iew \\underline{R}etrieval (GVR), a novel\nzero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D\nretrieval task that leverages object-level view retrieval to collect grounding\nclues from multiple views, which not only avoids the costly process of 3D\nannotation, but also eliminates the need for per-scene training. Extensive\nexperiments demonstrate that our method achieves state-of-the-art visual\ngrounding performance while avoiding per-scene training, providing a solid\nfoundation for zero-shot 3DVG research. Video demos can be found in\nhttps://github.com/leviome/GVR_demos.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15871v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15871v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.314,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15872",
      "title": "DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism\n  Prediction",
      "authors": [
        "Manajit Das",
        "Ajnabiul Hoque",
        "Mayank Baranwal",
        "Raghavan B. Sunoj"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Prediction of complete step-by-step chemical reaction mechanisms (CRMs)\nremains a major challenge. Whereas the traditional approaches in CRM tasks rely\non expert-driven experiments or costly quantum chemical computations,\ncontemporary deep learning (DL) alternatives ignore key intermediates and\nmechanistic steps and often suffer from hallucinations. We present DeepMech, an\ninterpretable graph-based DL framework employing atom- and bond-level\nattention, guided by generalized templates of mechanistic operations (TMOps),\nto generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K\natom-mapped and mass-balanced elementary steps), DeepMech achieves\n98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in\ncomplete CRM tasks, besides maintaining high fidelity even in\nout-of-distribution scenarios as well as in predicting side and/or byproducts.\nExtension to multistep CRMs relevant to prebiotic chemistry, demonstrates the\nability of DeepMech in effectively reconstructing pathways from simple\nprimordial substrates to complex biomolecules such as serine and aldopentose.\nAttention analysis identifies reactive atoms/bonds in line with chemical\nintuition, rendering our model interpretable and suitable for reaction design.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15872v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15872v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.299,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces DeepMech, a graph-based deep learning framework for predicting chemical reaction mechanisms using attention mechanisms and templates, focused on accuracy in reaction steps. It does not involve diffusion models, iterative refinement processes for logical reasoning, or treating a Chain-of-Thought as a single entity for holistic correction. Thus, it lacks any components related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15874",
      "title": "ENSAM: an efficient foundation model for interactive segmentation of 3D\n  medical images",
      "authors": [
        "Elias Stenhede",
        "Agnar Martin Bjørnstad",
        "Arian Ranjbar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present ENSAM (Equivariant, Normalized, Segment Anything Model), a\nlightweight and promptable model for universal 3D medical image segmentation.\nENSAM combines a SegResNet-based encoder with a prompt encoder and mask decoder\nin a U-Net-style architecture, using latent cross-attention, relative\npositional encoding, normalized attention, and the Muon optimizer for training.\nENSAM is designed to achieve good performance under limited data and\ncomputational budgets, and is trained from scratch on under 5,000 volumes from\nmultiple modalities (CT, MRI, PET, ultrasound, microscopy) on a single 32 GB\nGPU in 6 hours. As part of the CVPR 2025 Foundation Models for Interactive 3D\nBiomedical Image Segmentation Challenge, ENSAM was evaluated on hidden test set\nwith multimodal 3D medical images, obtaining a DSC AUC of 2.404, NSD AUC of\n2.266, final DSC of 0.627, and final NSD of 0.597, outperforming two previously\npublished baseline models (VISTA3D, SAM-Med3D) and matching the third (SegVol),\nsurpassing its performance in final DSC but trailing behind in the other three\nmetrics. In the coreset track of the challenge, ENSAM ranks 5th of 10 overall\nand best among the approaches not utilizing pretrained weights. Ablation\nstudies confirm that our use of relative positional encodings and the Muon\noptimizer each substantially speed up convergence and improve segmentation\nquality.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15874v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15874v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.323,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15882",
      "title": "Self-Supervised Cross-Modal Learning for Image-to-Point Cloud\n  Registration",
      "authors": [
        "Xingmei Wang",
        "Xiaoyu Hu",
        "Chengkai Huang",
        "Ziyan Zeng",
        "Guohao Nie",
        "Quan Z. Sheng",
        "Lina Yao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Bridging 2D and 3D sensor modalities is critical for robust perception in\nautonomous systems. However, image-to-point cloud (I2P) registration remains\nchallenging due to the semantic-geometric gap between texture-rich but\ndepth-ambiguous images and sparse yet metrically precise point clouds, as well\nas the tendency of existing methods to converge to local optima. To overcome\nthese limitations, we introduce CrossI2P, a self-supervised framework that\nunifies cross-modal learning and two-stage registration in a single end-to-end\npipeline. First, we learn a geometric-semantic fused embedding space via\ndual-path contrastive learning, enabling annotation-free, bidirectional\nalignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine\nregistration paradigm: a global stage establishes superpoint-superpixel\ncorrespondences through joint intra-modal context and cross-modal interaction\nmodeling, followed by a geometry-constrained point-level refinement for precise\nregistration. Third, we employ a dynamic training mechanism with gradient\nnormalization to balance losses for feature alignment, correspondence\nrefinement, and pose estimation. Extensive experiments demonstrate that\nCrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry\nbenchmark and by 37.9% on nuScenes, significantly improving both accuracy and\nrobustness.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15882v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15882v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.346,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15883",
      "title": "RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented\n  Image Captioning",
      "authors": [
        "Xiaosheng Long",
        "Hanyu Wang",
        "Zhentao Song",
        "Kun Luo",
        "Hongde Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent retrieval-augmented image captioning methods incorporate external\nknowledge to compensate for the limitations in comprehending complex scenes.\nHowever, current approaches face challenges in relation modeling: (1) the\nrepresentation of semantic prompts is too coarse-grained to capture\nfine-grained relationships; (2) these methods lack explicit modeling of image\nobjects and their semantic relationships. To address these limitations, we\npropose RACap, a relation-aware retrieval-augmented model for image captioning,\nwhich not only mines structured relation semantics from retrieval captions, but\nalso identifies heterogeneous objects from the image. RACap effectively\nretrieves structured relation features that contain heterogeneous visual\ninformation to enhance the semantic consistency and relational expressiveness.\nExperimental results show that RACap, with only 10.8M trainable parameters,\nachieves superior performance compared to previous lightweight captioning\nmodels.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15883v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15883v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.3,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on retrieval-augmented image captioning with relation-aware prompting, emphasizing object relationships and external knowledge integration. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15886",
      "title": "RangeSAM: Leveraging Visual Foundation Models for Range-View repesented\n  LiDAR segmentation",
      "authors": [
        "Paul Julius Kühn",
        "Duc Anh Nguyen",
        "Arjan Kuijper",
        "Holger Graf",
        "Dieter Fellner",
        "Saptarshi Neil Sinha"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Point cloud segmentation is central to autonomous driving and 3D scene\nunderstanding. While voxel- and point-based methods dominate recent research\ndue to their compatibility with deep architectures and ability to capture\nfine-grained geometry, they often incur high computational cost, irregular\nmemory access, and limited real-time efficiency. In contrast, range-view\nmethods, though relatively underexplored - can leverage mature 2D semantic\nsegmentation techniques for fast and accurate predictions. Motivated by the\nrapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot\nrecognition, and multimodal tasks, we investigate whether SAM2, the current\nstate-of-the-art VFM for segmentation tasks, can serve as a strong backbone for\nLiDAR point cloud segmentation in the range view. We present , to our\nknowledge, the first range-view framework that adapts SAM2 to 3D segmentation,\ncoupling efficient 2D feature extraction with standard\nprojection/back-projection to operate on point clouds. To optimize SAM2 for\nrange-view representations, we implement several architectural modifications to\nthe encoder: (1) a novel module that emphasizes horizontal spatial dependencies\ninherent in LiDAR range images, (2) a customized configuration of tailored to\nthe geometric properties of spherical projections, and (3) an adapted mechanism\nin the encoder backbone specifically designed to capture the unique spatial\npatterns and discontinuities present in range-view pseudo-images. Our approach\nachieves competitive performance on SemanticKITTI while benefiting from the\nspeed, scalability, and deployment simplicity of 2D-centric pipelines. This\nwork highlights the viability of VFMs as general-purpose backbones for 3D\nperception and opens a path toward unified, foundation-model-driven LiDAR\nsegmentation. Results lets us conclude that range-view segmentation methods\nusing VFMs leads to promising results.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15886v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15886v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.231,
      "weak_supervision_score": 0.242,
      "diffusion_reasoning_score": 0.227,
      "distributed_training_score": 0.217,
      "datasets_score": 0.249,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15888",
      "title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation",
      "authors": [
        "Senkang Hu",
        "Xudong Han",
        "Jinqi Jiang",
        "Yihang Tao",
        "Zihan Fang",
        "Yong Dai",
        "Sam Tak Wu Kwong",
        "Yuguang Fang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Adapting billion-parameter language models to a downstream task is still\ncostly, even with parameter-efficient fine-tuning (PEFT). We re-cast task\nadaptation as output-distribution alignment: the objective is to steer the\noutput distribution toward the task distribution directly during decoding\nrather than indirectly through weight updates. Building on this view, we\nintroduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and\ntheoretically grounded method. We start with a short warm-start fine-tune and\nextract a task-aware steering vector from the Kullback-Leibler (KL) divergence\ngradient between the output distribution of the warm-started and pre-trained\nmodels. This steering vector is then used to guide the decoding process to\nsteer the model's output distribution towards the task distribution. We\ntheoretically prove that SVD is first-order equivalent to the gradient step of\nfull fine-tuning and derive a globally optimal solution for the strength of the\nsteering vector. Across three tasks and nine benchmarks, SVD paired with four\nstandard PEFT methods improves multiple-choice accuracy by up to 5 points and\nopen-ended truthfulness by 2 points, with similar gains (1-2 points) on\ncommonsense datasets without adding trainable parameters beyond the PEFT\nadapter. SVD thus offers a lightweight, theoretically grounded path to stronger\ntask adaptation for large language models.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15888v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15888v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.446,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.45,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on steering vector decoding for task adaptation in LLMs, using KL divergence and fine-tuning, without any mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses output distribution alignment and decoding adjustments for LLMs, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "The paper addresses parameter-efficient fine-tuning and decoding efficiency for LLMs, but it does not cover distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15891",
      "title": "Global Regulation and Excitation via Attention Tuning for Stereo\n  Matching",
      "authors": [
        "Jiahao Li",
        "Xinhong Chen",
        "Zhengmin Jiang",
        "Qian Zhou",
        "Yung-Hui Li",
        "Jianping Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Stereo matching achieves significant progress with iterative algorithms like\nRAFT-Stereo and IGEV-Stereo. However, these methods struggle in ill-posed\nregions with occlusions, textureless, or repetitive patterns, due to a lack of\nglobal context and geometric information for effective iterative refinement. To\nenable the existing iterative approaches to incorporate global context, we\npropose the Global Regulation and Excitation via Attention Tuning (GREAT)\nframework which encompasses three attention modules. Specifically, Spatial\nAttention (SA) captures the global context within the spatial dimension,\nMatching Attention (MA) extracts global context along epipolar lines, and\nVolume Attention (VA) works in conjunction with SA and MA to construct a more\nrobust cost-volume excited by global context and geometric details. To verify\nthe universality and effectiveness of this framework, we integrate it into\nseveral representative iterative stereo-matching methods and validate it\nthrough extensive experiments, collectively denoted as GREAT-Stereo. This\nframework demonstrates superior performance in challenging ill-posed regions.\nApplied to IGEV-Stereo, among all published methods, our GREAT-IGEV ranks first\non the Scene Flow test set, KITTI 2015, and ETH3D leaderboards, and achieves\nsecond on the Middlebury benchmark. Code is available at\nhttps://github.com/JarvisLee0423/GREAT-Stereo.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15891v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15891v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.345,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework using attention mechanisms to improve stereo matching in computer vision, specifically for handling ill-posed regions in disparity estimation. It involves iterative refinement in stereo algorithms but does not incorporate diffusion models, multi-step logical reasoning, or any adaptation of diffusion processes for tasks like Chain-of-Thought reasoning. Therefore, it lacks any connection to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15892",
      "title": "MoAngelo: Motion-Aware Neural Surface Reconstruction for Dynamic Scenes",
      "authors": [
        "Mohamed Ebbed",
        "Zorah Lähner"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Dynamic scene reconstruction from multi-view videos remains a fundamental\nchallenge in computer vision. While recent neural surface reconstruction\nmethods have achieved remarkable results in static 3D reconstruction, extending\nthese approaches with comparable quality for dynamic scenes introduces\nsignificant computational and representational challenges. Existing dynamic\nmethods focus on novel-view synthesis, therefore, their extracted meshes tend\nto be noisy. Even approaches aiming for geometric fidelity often result in too\nsmooth meshes due to the ill-posedness of the problem. We present a novel\nframework for highly detailed dynamic reconstruction that extends the static 3D\nreconstruction method NeuralAngelo to work in dynamic settings. To that end, we\nstart with a high-quality template scene reconstruction from the initial frame\nusing NeuralAngelo, and then jointly optimize deformation fields that track the\ntemplate and refine it based on the temporal sequence. This flexible template\nallows updating the geometry to include changes that cannot be modeled with the\ndeformation field, for instance occluded parts or the changes in the topology.\nWe show superior reconstruction accuracy in comparison to previous\nstate-of-the-art methods on the ActorsHQ dataset.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15892v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15892v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.327,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15895",
      "title": "From Data to Diagnosis: A Large, Comprehensive Bone Marrow Dataset and\n  AI Methods for Childhood Leukemia Prediction",
      "authors": [
        "Henning Höfener",
        "Farina Kock",
        "Martina Pontones",
        "Tabita Ghete",
        "David Pfrang",
        "Nicholas Dickel",
        "Meik Kunz",
        "Daniela P. Schacherer",
        "David A. Clunie",
        "Andrey Fedorov",
        "Max Westphal",
        "Markus Metzler"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Leukemia diagnosis primarily relies on manual microscopic analysis of bone\nmarrow morphology supported by additional laboratory parameters, making it\ncomplex and time consuming. While artificial intelligence (AI) solutions have\nbeen proposed, most utilize private datasets and only cover parts of the\ndiagnostic pipeline. Therefore, we present a large, high-quality, publicly\navailable leukemia bone marrow dataset spanning the entire diagnostic process,\nfrom cell detection to diagnosis. Using this dataset, we further propose\nmethods for cell detection, cell classification, and diagnosis prediction. The\ndataset comprises 246 pediatric patients with diagnostic, clinical and\nlaboratory information, over 40 000 cells with bounding box annotations and\nmore than 28 000 of these with high-quality class labels, making it the most\ncomprehensive dataset publicly available. Evaluation of the AI models yielded\nan average precision of 0.96 for the cell detection, an area under the curve of\n0.98, and an F1-score of 0.61 for the 33-class cell classification, and a mean\nF1-score of 0.90 for the diagnosis prediction using predicted cell counts.\nWhile the proposed approaches demonstrate their usefulness for AI-assisted\ndiagnostics, the dataset will foster further research and development in the\nfield, ultimately contributing to more precise diagnoses and improved patient\noutcomes.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15895v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15895v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.316,
      "distributed_training_score": 0.315,
      "datasets_score": 0.419,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a large, high-quality, publicly available bone marrow dataset for leukemia diagnosis, including details on its creation, annotations, and evaluation through AI methods. This directly aligns with research on creating, analyzing, and evaluating datasets for machine learning and AI applications, as it involves new dataset introduction, curation methodologies, and benchmark evaluations.",
      "llm_score_status": "completed",
      "summary": "This paper presents a large, publicly available dataset of bone marrow samples from 246 pediatric leukemia patients, including over 40,000 annotated cells with diagnostic, clinical, and laboratory information, aimed at advancing AI-assisted leukemia diagnosis. The authors develop and evaluate AI methods for cell detection, 33-class cell classification, and diagnosis prediction, achieving high performance with an average precision of 0.96 for detection, an AUC of 0.98 for classification, and an F1-score of 0.90 for diagnosis using predicted cell counts, thereby fostering further research in the field.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper offers a notable improvement by providing the most comprehensive public dataset for leukemia diagnosis and applying existing AI techniques in a more integrated way, but it does not introduce a entirely new problem or technique. This represents a clever combination of known methods with a new resource to enhance accessibility and research in medical AI.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications in AI for medical diagnostics by providing a high-quality public dataset that can accelerate developments in leukemia prediction and improve patient outcomes. Its availability could lead to broader adoption and citations across healthcare AI subfields, driving innovations in precision medicine.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper is a high-quality contribution with a valuable public dataset and effective AI methods that advance leukemia diagnostics, making it essential for researchers in medical AI to be aware of for potential applications. While impactful, it is not groundbreaking enough to be classified as must-read for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/271c4979b78ab191499922ffb01110017753c360",
      "total_authors": 32,
      "authors_found": 32,
      "highest_h_index": 36,
      "average_h_index": 4.6875,
      "notable_authors_count": 8,
      "author_h_indexes": [
        {
          "name": "Henning Hofener",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381263643"
        },
        {
          "name": "Farina Kock",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2294599607"
        },
        {
          "name": "Martina Pontones",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2294598088"
        },
        {
          "name": "Tabita Ghete",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/1695764720"
        },
        {
          "name": "David Pfrang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2333544467"
        },
        {
          "name": "Nicholas Dickel",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2134578369"
        },
        {
          "name": "M. Kunz",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381261738"
        },
        {
          "name": "D. Schacherer",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2105731835"
        },
        {
          "name": "D. Clunie",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/47750250"
        },
        {
          "name": "Andrey Fedorov",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381263676"
        },
        {
          "name": "M. Westphal",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2294599076"
        },
        {
          "name": "Markus Metzler Fraunhofer Institute for Digital Medicine Mevis",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381263876"
        },
        {
          "name": "Bremen",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/52633099"
        },
        {
          "name": "Germany",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2264654886"
        },
        {
          "name": "Department of Pediatrics",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2125176703"
        },
        {
          "name": "Adolescent Medicine",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381263438"
        },
        {
          "name": "University Hospital Erlangen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381262993"
        },
        {
          "name": "Erlangen",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/49364721"
        },
        {
          "name": "Bavarian Cancer Research Center",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381262646"
        },
        {
          "name": "Medical Informatics",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2171106488"
        },
        {
          "name": "Friedrich-Alexander-Universitat Erlangen-Nurnberg",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/1388360566"
        },
        {
          "name": "PixelMed Publishing Llc",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381262710"
        },
        {
          "name": "Bangor",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381263759"
        },
        {
          "name": "Pa",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2262828496"
        },
        {
          "name": "Usa",
          "h_index": 36,
          "profile_url": "https://www.semanticscholar.org/author/2102686766"
        },
        {
          "name": "D. Radiology",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/52226690"
        },
        {
          "name": "Brigham",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/49276653"
        },
        {
          "name": "Women's Hospital",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/1468811959"
        },
        {
          "name": "H. School",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/52001909"
        },
        {
          "name": "Boston",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2247424853"
        },
        {
          "name": "Ma.",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2057032524"
        },
        {
          "name": "Comprehensive Cancer Center Erlangen-EMN",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381262989"
        }
      ]
    },
    {
      "id": "2509.15901",
      "title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and\n  Personalization via Questions",
      "authors": [
        "Frederic Kirstein",
        "Sonu Kumar",
        "Terry Ruas",
        "Bela Gipp"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Meeting summarization with large language models (LLMs) remains error-prone,\noften producing outputs with hallucinations, omissions, and irrelevancies. We\npresent FRAME, a modular pipeline that reframes summarization as a semantic\nenrichment task. FRAME extracts and scores salient facts, organizes them\nthematically, and uses these to enrich an outline into an abstractive summary.\nTo personalize summaries, we introduce SCOPE, a reason-out-loud protocol that\nhas the model build a reasoning trace by answering nine questions before\ncontent selection. For evaluation, we propose P-MESA, a multi-dimensional,\nreference-free evaluation framework to assess if a summary fits a target\nreader. P-MESA reliably identifies error instances, achieving >= 89% balanced\naccuracy against human annotations and strongly aligns with human severity\nratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and\nomission by 2 out of 5 points (measured with MESA), while SCOPE improves\nknowledge fit and goal alignment over prompt-only baselines. Our findings\nadvocate for rethinking summarization to improve control, faithfulness, and\npersonalization.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15901v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15901v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.285,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15905",
      "title": "Deep Feedback Models",
      "authors": [
        "David Calhas",
        "Arlindo L. Oliveira"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep Feedback Models (DFMs) are a new class of stateful neural networks that\ncombine bottom up input with high level representations over time. This\nfeedback mechanism introduces dynamics into otherwise static architectures,\nenabling DFMs to iteratively refine their internal state and mimic aspects of\nbiological decision making. We model this process as a differential equation\nsolved through a recurrent neural network, stabilized via exponential decay to\nensure convergence. To evaluate their effectiveness, we measure DFMs under two\nkey conditions: robustness to noise and generalization with limited data. In\nboth object recognition and segmentation tasks, DFMs consistently outperform\ntheir feedforward counterparts, particularly in low data or high noise regimes.\nIn addition, DFMs translate to medical imaging settings, while being robust\nagainst various types of noise corruption. These findings highlight the\nimportance of feedback in achieving stable, robust, and generalizable learning.\nCode is available at https://github.com/DCalhas/deep_feedback_models.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15905v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15905v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.464,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.489,
      "distributed_training_score": 0.366,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces Deep Feedback Models, which focus on internal feedback mechanisms in neural networks for tasks like object recognition and segmentation, emphasizing robustness and generalization. There is no mention of human feedback, reward models trained on human-ranked data, or reinforcement learning techniques for aligning models with human preferences. Thus, it does not relate to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes iterative refinement in neural networks via feedback loops and differential equations, applied to vision tasks for robustness. However, it does not adapt diffusion models for multi-step logical reasoning or treat a Chain-of-Thought as a holistic entity. There is no clear component involving diffusion-based processes for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15908",
      "title": "Interpretable Nanoporous Materials Design with Symmetry-Aware Networks",
      "authors": [
        "Zhenhao Zhou",
        "Salman Bin Kashif",
        "Jin-Hu Dou",
        "Chris Wolverton",
        "Kaihang Shi",
        "Tao Deng",
        "Zhenpeng Yao"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Nanoporous materials hold promise for diverse sustainable applications, yet\ntheir vast chemical space poses challenges for efficient design. Machine\nlearning offers a compelling pathway to accelerate the exploration, but\nexisting models lack either interpretability or fidelity for elucidating the\ncorrelation between crystal geometry and property. Here, we report a\nthree-dimensional periodic space sampling method that decomposes large\nnanoporous structures into local geometrical sites for combined property\nprediction and site-wise contribution quantification. Trained with a\nconstructed database and retrieved datasets, our model achieves\nstate-of-the-art accuracy and data efficiency for property prediction on gas\nstorage, separation, and electrical conduction. Meanwhile, this approach\nenables the interpretation of the prediction and allows for accurate\nidentification of significant local sites for targeted properties. Through\nidentifying transferable high-performance sites across diverse nanoporous\nframeworks, our model paves the way for interpretable, symmetry-aware\nnanoporous materials design, which is extensible to other materials, like\nmolecular crystals and beyond.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15908v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15908v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.337,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15915",
      "title": "Foundation Models as World Models: A Foundational Study in Text-Based\n  GridWorlds",
      "authors": [
        "Remo Sasso",
        "Michelangelo Conserva",
        "Dominik Jeurissen",
        "Paulo Rauber"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While reinforcement learning from scratch has shown impressive results in\nsolving sequential decision-making tasks with efficient simulators, real-world\napplications with expensive interactions require more sample-efficient agents.\nFoundation models (FMs) are natural candidates to improve sample efficiency as\nthey possess broad knowledge and reasoning capabilities, but it is yet unclear\nhow to effectively integrate them into the reinforcement learning framework. In\nthis paper, we anticipate and, most importantly, evaluate two promising\nstrategies. First, we consider the use of foundation world models (FWMs) that\nexploit the prior knowledge of FMs to enable training and evaluating agents\nwith simulated interactions. Second, we consider the use of foundation agents\n(FAs) that exploit the reasoning capabilities of FMs for decision-making. We\nevaluate both approaches empirically in a family of grid-world environments\nthat are suitable for the current generation of large language models (LLMs).\nOur results suggest that improvements in LLMs already translate into better\nFWMs and FAs; that FAs based on current LLMs can already provide excellent\npolicies for sufficiently simple environments; and that the coupling of FWMs\nand reinforcement learning agents is highly promising for more complex settings\nwith partial observability and stochastic elements.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15915v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15915v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.459,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.337,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using foundation models for world simulation and decision-making in RL, without any mention of human feedback, reward models trained on human-ranked data, or fine-tuning based on human preferences. It relies solely on pre-trained models and simulated interactions.",
      "weak_supervision_justification": "The paper involves using foundation models to programmatically generate simulated interaction data for RL training, which shares some similarities with weak supervision's use of noisy or high-level sources for data generation. However, it is not primarily about training models via weak labels for supervised tasks, making it only loosely connected.",
      "diffusion_reasoning_justification": "The paper does not discuss diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. It evaluates foundation models for zero-shot simulation and decision-making in RL, without any components related to multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15924",
      "title": "Sparse Multiview Open-Vocabulary 3D Detection",
      "authors": [
        "Olivier Moliner",
        "Viktor Larsson",
        "Kalle Åström"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The ability to interpret and comprehend a 3D scene is essential for many\nvision and robotics systems. In numerous applications, this involves 3D object\ndetection, i.e.~identifying the location and dimensions of objects belonging to\na specific category, typically represented as bounding boxes. This has\ntraditionally been solved by training to detect a fixed set of categories,\nwhich limits its use. In this work, we investigate open-vocabulary 3D object\ndetection in the challenging yet practical sparse-view setting, where only a\nlimited number of posed RGB images are available as input. Our approach is\ntraining-free, relying on pre-trained, off-the-shelf 2D foundation models\ninstead of employing computationally expensive 3D feature fusion or requiring\n3D-specific learning. By lifting 2D detections and directly optimizing 3D\nproposals for featuremetric consistency across views, we fully leverage the\nextensive training data available in 2D compared to 3D. Through standard\nbenchmarks, we demonstrate that this simple pipeline establishes a powerful\nbaseline, performing competitively with state-of-the-art techniques in densely\nsampled scenarios while significantly outperforming them in the sparse-view\nsetting.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15924v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15924v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.354,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15927",
      "title": "Enhancing Generative Auto-bidding with Offline Reward Evaluation and\n  Policy Search",
      "authors": [
        "Zhiyu Mou",
        "Yiqin Lv",
        "Miao Xu",
        "Qi Wang",
        "Yixiu Mao",
        "Qichen Ye",
        "Chao Li",
        "Rongquan Bai",
        "Chuan Yu",
        "Jian Xu",
        "Bo Zheng"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Auto-bidding serves as a critical tool for advertisers to improve their\nadvertising performance. Recent progress has demonstrated that AI-Generated\nBidding (AIGB), which learns a conditional generative planner from offline\ndata, achieves superior performance compared to typical offline reinforcement\nlearning (RL)-based auto-bidding methods. However, existing AIGB methods still\nface a performance bottleneck due to their inherent inability to explore beyond\nthe static offline dataset. To address this, we propose {AIGB-Pearl}\n(\\emph{{P}lanning with {E}valu{A}tor via RL}), a novel method that integrates\ngenerative planning and policy optimization. The core of AIGB-Pearl lies in\nconstructing a trajectory evaluator for scoring generation quality and\ndesigning a provably sound KL-Lipschitz-constrained score maximization scheme\nto ensure safe and efficient exploration beyond the offline dataset. A\npractical algorithm incorporating the synchronous coupling technique is further\ndevised to ensure the model regularity required by the proposed scheme.\nExtensive experiments on both simulated and real-world advertising systems\ndemonstrate the state-of-the-art performance of our approach.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15927v3",
      "pdf_url": "http://arxiv.org/pdf/2509.15927v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.364,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on offline reinforcement learning and generative models for auto-bidding using static offline datasets, without any involvement of human feedback, human-ranked data, or a reward model trained on human preferences. It relies solely on automated advertising data for evaluation and optimization.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper references generative models, including potential diffusion-based approaches (e.g., via citations like zhu2023diffusion), for trajectory generation in auto-bidding, but it does not involve adapting diffusion for multi-step logical reasoning or treating a Chain-of-Thought as a holistic entity for complex tasks. It primarily addresses decision-making in advertising, not explicit reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15932",
      "title": "The Alignment Bottleneck",
      "authors": [
        "Wenjun Cao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.IT (Information Theory)",
        "math.IT (Information Theory)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Large language models improve with scale, yet feedback-based alignment still\nexhibits systematic deviations from intended behavior. Motivated by bounded\nrationality in economics and cognitive science, we view judgment as\nresource-limited and feedback as a constrained channel. On this basis, we model\nthe loop as a two-stage cascade $U \\to H \\to Y$ given $S$, with cognitive\ncapacity $C_{\\text{cog}|S}$ and average total capacity\n$\\bar{C}_{\\text{tot}|S}$. Our main result is a capacity-coupled Alignment\nPerformance Interval. It pairs a data size-independent Fano lower bound proved\non a separable codebook mixture with a PAC-Bayes upper bound whose KL term is\ncontrolled by the same channel via $m \\, \\bar{C}_{\\text{tot}|S}$. The PAC-Bayes\nbound becomes an upper bound on the same true risk when the canonical\nobservable loss is used and the dataset is drawn from the same mixture. Under\nthese matched conditions, both limits are governed by a single capacity.\nConsequences include that, with value complexity and capacity fixed, adding\nlabels alone cannot cross the bound; attaining lower risk on more complex\ntargets requires capacity that grows with $\\log M$; and once useful signal\nsaturates capacity, further optimization tends to fit channel regularities,\nconsistent with reports of sycophancy and reward hacking. The analysis views\nalignment as interface engineering: measure and allocate limited capacity,\nmanage task complexity, and decide where information is spent.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15932v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15932v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.509,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.38,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper directly addresses feedback-based alignment in large language models, including issues like sycophancy and reward hacking, which are core challenges in RLHF. It references RLHF-related works (e.g., ouyang_training_2022) and analyzes the human-AI feedback loop, providing theoretical bounds that explain limitations in aligning models with human preferences through reinforcement learning.",
      "weak_supervision_justification": "The paper focuses on the constraints of human feedback in alignment processes, such as cognitive capacity limits, but does not involve programmatically generating labels from noisy sources, which is the essence of weak supervision. There is no discussion of heuristic-based or distant supervision methods.",
      "diffusion_reasoning_justification": "The paper does not mention diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes. Its focus is on information-theoretic bounds in human-AI feedback loops, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper examines the limitations in aligning large language models with human values by modeling the feedback loop as a resource-constrained channel, drawing from bounded rationality in economics and cognitive science. Using information-theoretic approaches, including Fano's inequality and PAC-Bayes bounds, the authors derive a capacity-coupled Alignment Performance Interval that highlights how cognitive and articulation capacities impose fundamental limits on alignment, implying that increasing data alone cannot overcome these bottlenecks and explaining phenomena like sycophancy and reward hacking in AI models.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework that integrates bounded rationality with information theory to model AI alignment constraints, deriving a new capacity-coupled bound that advances understanding of systematic deviations in feedback-based alignment.",
      "impact_score": "High",
      "impact_justification": "The work provides a theoretical basis for addressing AI alignment challenges, potentially influencing research in machine learning, AI safety, and information theory by guiding the design of more robust feedback mechanisms.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers significant insights into the structural limits of AI alignment, making it valuable for researchers in AI ethics and machine learning to understand and build upon these findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6aebe394b781205ec9accd634e9395b7c21c1310",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Wenjun Cao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381336851"
        }
      ]
    },
    {
      "id": "2509.15935",
      "title": "PAN: Pillars-Attention-Based Network for 3D Object Detection",
      "authors": [
        "Ruan Bispo",
        "Dane Mitrev",
        "Letizia Mariotti",
        "Clément Botty",
        "Denver Humphrey",
        "Anthony Scanlan",
        "Ciarán Eising"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar\nfusion for the 3D object detection task in real-time under adverse weather and\nlighting conditions. However, currently, in the literature, it is possible to\nfind few works focusing on this modality and, most importantly, developing new\narchitectures to explore the advantages of the radar point cloud, such as\naccurate distance estimation and speed information. Therefore, this work\npresents a novel and efficient 3D object detection algorithm using cameras and\nradars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of\nradar before fusing the features into a detection head. A new backbone is\nintroduced, which maps the radar pillar features into an embedded dimension. A\nself-attention mechanism allows the backbone to model the dependencies between\nthe radar points. We are using a simplified convolutional layer to replace the\nFPN-based convolutional layers used in the PointPillars-based architectures\nwith the main goal of reducing inference time. Our results show that with this\nmodification, our approach achieves the new state-of-the-art in the 3D object\ndetection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,\nwhile also setting a new benchmark for inference time on the nuScenes dataset\nfor the same category.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15935v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15935v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.346,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15937",
      "title": "A Vision-Language-Action-Critic Model for Robotic Real-World\n  Reinforcement Learning",
      "authors": [
        "Shaopeng Zhai",
        "Qi Zhang",
        "Tianyi Zhang",
        "Fuxian Huang",
        "Haoran Zhang",
        "Ming Zhou",
        "Shengzhe Zhang",
        "Litao Liu",
        "Sixu Lin",
        "Jiangmiao Pang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Robotic real-world reinforcement learning (RL) with vision-language-action\n(VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient\nexploration. We introduce VLAC, a general process reward model built upon\nInternVL and trained on large scale heterogeneous datasets. Given pairwise\nobservations and a language goal, it outputs dense progress delta and done\nsignal, eliminating task-specific reward engineering, and supports one-shot\nin-context transfer to unseen tasks and environments. VLAC is trained on\nvision-language datasets to strengthen perception, dialogic and reasoning\ncapabilities, together with robot and human trajectories data that ground\naction generation and progress estimation, and additionally strengthened to\nreject irrelevant prompts as well as detect regression or stagnation by\nconstructing large numbers of negative and semantically mismatched samples.\nWith prompt control, a single VLAC model alternately generating reward and\naction tokens, unifying critic and policy. Deployed inside an asynchronous\nreal-world RL loop, we layer a graded human-in-the-loop protocol (offline\ndemonstration replay, return and explore, human guided explore) that\naccelerates exploration and stabilizes early learning. Across four distinct\nreal-world manipulation tasks, VLAC lifts success rates from about 30\\% to\nabout 90\\% within 200 real-world interaction episodes; incorporating\nhuman-in-the-loop interventions yields a further 50% improvement in sample\nefficiency and achieves up to 100% final success.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15937v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15937v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.525,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.368,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper incorporates human-in-the-loop interventions, such as offline demonstrations and guided exploration, to improve RL efficiency and success rates. However, it does not involve training a separate reward model on human-ranked data for fine-tuning, which is the core of RLHF. Instead, human feedback is used reactively in the RL loop rather than as the primary mechanism for model alignment, making it only loosely related.",
      "weak_supervision_justification": "The paper describes training the VLAC model on large-scale heterogeneous datasets, including programmatically generated labels from temporal ordering in manipulation data and constructed negative samples, which aligns with weak supervision by using noisy or imprecise sources rather than perfect hand-labeled data. This approach enhances the model's capabilities without relying on exhaustive manual annotation, though it's not the central focus of the paper.",
      "diffusion_reasoning_justification": "The paper focuses on a Vision-Language-Action-Critic model for RL, built on InternVL, and involves autoregressive generation of rewards and actions, but it does not mention or utilize diffusion models for multi-step logical reasoning or iterative refinement of a chain-of-thought. There is no evidence of diffusion-based components in the methodology.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces VLAC, a Vision-Language-Action-Critic model built on InternVL, aimed at enhancing robotic real-world reinforcement learning by providing dense progress rewards and unifying actor and critic functions to address sparse rewards and inefficient exploration. Trained on large-scale heterogeneous datasets including vision-language data and robot trajectories, VLAC outputs progress deltas and action tokens, supports one-shot transfer, and integrates into an asynchronous RL loop with human-in-the-loop protocols; experiments on four real-world manipulation tasks demonstrate success rates improving from about 30% to 90% within 200 episodes, and up to 100% with human intervention.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing vision-language models like InternVL with a unified actor-critic architecture for dense rewards in robotic RL, offering a notable improvement in generalization and efficiency but not introducing an entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in robotic reinforcement learning subfields due to its practical improvements in reward design and real-world applicability, though its influence may remain confined to specific AI and robotics applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution with empirical results that advance robotic learning efficiency, making it important for researchers in AI and robotics to be aware of, though not essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2f09aebc780958b66c203cfb36ce735d62441b15",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 1,
      "average_h_index": 0.5555555555555556,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Shaopeng Zhai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2277244188"
        },
        {
          "name": "Qi Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2277244773"
        },
        {
          "name": "Tianyi Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2277573706"
        },
        {
          "name": "Fuxian Huang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2277258301"
        },
        {
          "name": "Haoran Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2287889523"
        },
        {
          "name": "Ming Zhou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2277255677"
        },
        {
          "name": "Shengzhe Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Litao Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381311791"
        },
        {
          "name": "Sixu Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381316259"
        },
        {
          "name": "Jiangmiao Pang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377561990"
        }
      ]
    },
    {
      "id": "2509.15942",
      "title": "ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow\n  Matching",
      "authors": [
        "Graham Clyne",
        "Guillaume Couairon",
        "Guillaume Gastineau",
        "Claire Monteleoni",
        "Anastase Charantonis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Climate projections have uncertainties related to components of the climate\nsystem and their interactions. A typical approach to quantifying these\nuncertainties is to use climate models to create ensembles of repeated\nsimulations under different initial conditions. Due to the complexity of these\nsimulations, generating such ensembles of projections is computationally\nexpensive. In this work, we present ArchesClimate, a deep learning-based\nclimate model emulator that aims to reduce this cost. ArchesClimate is trained\non decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution\nof approximately 2.5x1.25 degrees. We train a flow matching model following\nArchesWeatherGen, which we adapt to predict near-term climate. Once trained,\nthe model generates states at a one-month lead time and can be used to\nauto-regressively emulate climate model simulations of any length. We show that\nfor up to 10 years, these generations are stable and physically consistent. We\nalso show that for several important climate variables, ArchesClimate generates\nsimulations that are interchangeable with the IPSL model. This work suggests\nthat climate model emulators could significantly reduce the cost of climate\nmodel simulations.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15942v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15942v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.374,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15947",
      "title": "The Missing Piece: A Case for Pre-Training in 3D Medical Object\n  Detection",
      "authors": [
        "Katharina Eckstein",
        "Constantin Ulrich",
        "Michael Baumgartner",
        "Jessica Kächele",
        "Dimitrios Bounias",
        "Tassilo Wald",
        "Ralf Floca",
        "Klaus H. Maier-Hein"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large-scale pre-training holds the promise to advance 3D medical object\ndetection, a crucial component of accurate computer-aided diagnosis. Yet, it\nremains underexplored compared to segmentation, where pre-training has already\ndemonstrated significant benefits. Existing pre-training approaches for 3D\nobject detection rely on 2D medical data or natural image pre-training, failing\nto fully leverage 3D volumetric information. In this work, we present the first\nsystematic study of how existing pre-training methods can be integrated into\nstate-of-the-art detection architectures, covering both CNNs and Transformers.\nOur results show that pre-training consistently improves detection performance\nacross various tasks and datasets. Notably, reconstruction-based\nself-supervised pre-training outperforms supervised pre-training, while\ncontrastive pre-training provides no clear benefit for 3D medical object\ndetection. Our code is publicly available at:\nhttps://github.com/MIC-DKFZ/nnDetection-finetuning.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15947v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15947v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.385,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15952",
      "title": "Compose Yourself: Average-Velocity Flow Matching for One-Step Speech\n  Enhancement",
      "authors": [
        "Gang Yang",
        "Yue Lei",
        "Wenxin Tai",
        "Jin Wu",
        "Jia Chen",
        "Ting Zhong",
        "Fan Zhou"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Diffusion and flow matching (FM) models have achieved remarkable progress in\nspeech enhancement (SE), yet their dependence on multi-step generation is\ncomputationally expensive and vulnerable to discretization errors. Recent\nadvances in one-step generative modeling, particularly MeanFlow, provide a\npromising alternative by reformulating dynamics through average velocity\nfields. In this work, we present COSE, a one-step FM framework tailored for SE.\nTo address the high training overhead of Jacobian-vector product (JVP)\ncomputations in MeanFlow, we introduce a velocity composition identity to\ncompute average velocity efficiently, eliminating expensive computation while\npreserving theoretical consistency and achieving competitive enhancement\nquality. Extensive experiments on standard benchmarks show that COSE delivers\nup to 5x faster sampling and reduces training cost by 40%, all without\ncompromising speech quality. Code is available at\nhttps://github.com/ICDM-UESTC/COSE.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15952v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15952v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.298,
      "datasets_score": 0.262,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is developing an efficient one-step flow matching framework for speech enhancement, focusing on improving generative models for audio signal processing. It mentions diffusion models briefly in the context of speech tasks but does not adapt iterative refinement processes for complex logical reasoning, Chain-of-Thought, or any reasoning-related applications. Thus, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15957",
      "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by\n  Large Language Models via Model Context Protocol",
      "authors": [
        "Kanato Masayoshi",
        "Masahiro Hashimoto",
        "Ryoichi Yokoyama",
        "Naoki Toda",
        "Yoshifumi Uwamino",
        "Shogo Fukuda",
        "Ho Namkoong",
        "Masahiro Jinzaki"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.HC (Human-Computer Interaction)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15957v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15957v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.339,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the evaluation of LLMs for clinical information retrieval from EHR systems using the Model Context Protocol (MCP), focusing on tool integration and accuracy in real-world settings. It does not involve reinforcement learning from human feedback (RLHF), as there is no mention of training a reward model, fine-tuning with human-ranked data, or using reinforcement learning techniques to align the model with human preferences. While human-generated gold standards are used for evaluation, this is for assessment purposes only and does not constitute RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15959",
      "title": "Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive\n  Interfaces and Trustworthy Human-AI Collaboration",
      "authors": [
        "Zhuoyue Zhang",
        "Haitong Xu"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Autonomous navigation in maritime domains is accelerating alongside advances\nin artificial intelligence, sensing, and connectivity. Opaque decision-making\nand poorly calibrated human-automation interaction remain key barriers to safe\nadoption. This article synthesizes 100 studies on automation transparency for\nMaritime Autonomous Surface Ships (MASS) spanning situation awareness (SA),\nhuman factors, interface design, and regulation. We (i) map the\nGuidance-Navigation-Control stack to shore-based operational modes -- remote\nsupervision (RSM) and remote control (RCM) -- and identify where human unsafe\ncontrol actions (Human-UCAs) concentrate in handover and emergency loops; (ii)\nsummarize evidence that transparency features (decision rationales,\nalternatives, confidence/uncertainty, and rule-compliance indicators) improve\nunderstanding and support trust calibration, though reliability and\npredictability often dominate trust; (iii) distill design strategies for\ntransparency at three layers: sensor/SA acquisition and fusion, HMI/eHMI\npresentation (textual/graphical overlays, color coding, conversational and\nimmersive UIs), and engineer-facing processes (resilient interaction design,\nvalidation, and standardization). We integrate methods for Human-UCA\nidentification (STPA-Cog + IDAC), quantitative trust/SA assessment, and\noperator workload monitoring, and outline regulatory and rule-based\nimplications including COLREGs formalization and route exchange. We conclude\nwith an adaptive transparency framework that couples operator state estimation\nwith explainable decision support to reduce cognitive overload and improve\ntakeover timeliness. The review highlights actionable figure-of-merit displays\n(e.g., CPA/TCPA risk bars, robustness heatmaps), transparent model outputs\n(rule traceability, confidence), and training pipelines (HIL/MIL, simulation)\nas near-term levers for safer MASS operations.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15959v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15959v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.311,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a synthesis of studies on explainable AI for Maritime Autonomous Surface Ships, focusing on transparency, human factors, interface design, and trust in human-AI collaboration. It discusses operational aspects like situation awareness, handover processes, and adaptive interfaces, but does not involve reinforcement learning, human feedback for training AI models, or aligning models with human preferences via a reward model. As such, it lacks any connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15962",
      "title": "Structured Information for Improving Spatial Relationships in\n  Text-to-Image Generation",
      "authors": [
        "Sander Schildermans",
        "Chang Tian",
        "Ying Jiao",
        "Marie-Francine Moens"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing\nspatial relationships described in natural language prompts remains a major\nchallenge. Prior efforts have addressed this issue through prompt optimization,\nspatially grounded generation, and semantic refinement. This work introduces a\nlightweight approach that augments prompts with tuple-based structured\ninformation, using a fine-tuned language model for automatic conversion and\nseamless integration into T2I pipelines. Experimental results demonstrate\nsubstantial improvements in spatial accuracy, without compromising overall\nimage quality as measured by Inception Score. Furthermore, the automatically\ngenerated tuples exhibit quality comparable to human-crafted tuples. This\nstructured information provides a practical and portable solution to enhance\nspatial relationships in T2I generation, addressing a key limitation of current\nlarge-scale generative systems.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15962v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15962v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.497,
      "distributed_training_score": 0.316,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is improving spatial relationships in text-to-image generation by augmenting prompts with structured information and using a fine-tuned language model. It employs diffusion-based models like Stable Diffusion for image synthesis but does not adapt the iterative refinement process of diffusion for multi-step logical reasoning or chain-of-thought tasks. There is no component involving holistic correction of reasoning paths, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15964",
      "title": "MoE-CE: Enhancing Generalization for Deep Learning based Channel\n  Estimation via a Mixture-of-Experts Framework",
      "authors": [
        "Tianyu Li",
        "Yan Xin",
        "Jianzhong",
        "Zhang"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reliable channel estimation (CE) is fundamental for robust communication in\ndynamic wireless environments, where models must generalize across varying\nconditions such as signal-to-noise ratios (SNRs), the number of resource blocks\n(RBs), and channel profiles. Traditional deep learning (DL)-based methods\nstruggle to generalize effectively across such diverse settings, particularly\nunder multitask and zero-shot scenarios. In this work, we propose MoE-CE, a\nflexible mixture-of-experts (MoE) framework designed to enhance the\ngeneralization capability of DL-based CE methods. MoE-CE provides an\nappropriate inductive bias by leveraging multiple expert subnetworks, each\nspecialized in distinct channel characteristics, and a learned router that\ndynamically selects the most relevant experts per input. This architecture\nenhances model capacity and adaptability without a proportional rise in\ncomputational cost while being agnostic to the choice of the backbone model and\nthe learning algorithm. Through extensive experiments on synthetic datasets\ngenerated under diverse SNRs, RB numbers, and channel profiles, including\nmultitask and zero-shot evaluations, we demonstrate that MoE-CE consistently\noutperforms conventional DL approaches, achieving significant performance gains\nwhile maintaining efficiency.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15964v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15964v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.409,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on introducing a Mixture-of-Experts (MoE) framework for improving generalization in deep learning-based channel estimation, emphasizing model architecture, expert specialization, and dynamic routing for wireless communication tasks. It does not discuss distributed training techniques, such as partitioning data or computation across multiple nodes, parallel computing for acceleration, or multi-node machine learning systems. While MoE may involve internal parallelism for experts, this is not equivalent to distributed training strategies, making the paper's contributions unrelated to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15965",
      "title": "RLinf: Flexible and Efficient Large-scale Reinforcement Learning via\n  Macro-to-Micro Flow Transformation",
      "authors": [
        "Chao Yu",
        "Yuanqing Wang",
        "Zhen Guo",
        "Hao Lin",
        "Si Xu",
        "Hongzhi Zang",
        "Quanlu Zhang",
        "Yongji Wu",
        "Chunyang Zhu",
        "Junhao Hu",
        "Zixiao Huang",
        "Mingjie Wei",
        "Yuqing Xie",
        "Ke Yang",
        "Bo Dai",
        "Zhexuan Xu",
        "Xiangyuan Wang",
        "Xu Fu",
        "Zhihao Liu",
        "Kang Chen",
        "Weilin Liu",
        "Gang Liu",
        "Boxun Li",
        "Jianlei Yang",
        "Zhi Yang",
        "Guohao Dai",
        "Yu Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Reinforcement learning (RL) has demonstrated immense potential in advancing\nartificial general intelligence, agentic intelligence, and embodied\nintelligence. However, the inherent heterogeneity and dynamicity of RL\nworkflows often lead to low hardware utilization and slow training on existing\nsystems. In this paper, we present RLinf, a high-performance RL training system\nbased on our key observation that the major roadblock to efficient RL training\nlies in system flexibility. To maximize flexibility and efficiency, RLinf is\nbuilt atop a novel RL system design paradigm called macro-to-micro flow\ntransformation (M2Flow), which automatically breaks down high-level,\neasy-to-compose RL workflows at both the temporal and spatial dimensions, and\nrecomposes them into optimized execution flows. Supported by RLinf worker's\nadaptive communication capability, we devise context switching and elastic\npipelining to realize M2Flow transformation, and a profiling-guided scheduling\npolicy to generate optimal execution plans. Extensive evaluations on both\nreasoning RL and embodied RL tasks demonstrate that RLinf consistently\noutperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in\nend-to-end training throughput.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15965v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15965v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.477,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.462,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions RLHF as an example of RL applications in the introduction, such as aligning LLMs with human preferences, but its main contribution is a general RL training system focused on efficiency, not the specific mechanisms of RLHF like reward model training or human feedback integration. Thus, it is only tangentially related.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses reasoning RL tasks in evaluations but does not mention diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for logical reasoning. Its focus is on RL system efficiency, with no components related to diffusion-based methods.",
      "distributed_training_justification": "The paper's core contribution is RLinf, a system that optimizes distributed RL training through techniques like macro-to-micro flow transformation, elastic pipelining, context switching, and profiling-guided scheduling, directly addressing parallel computing, multi-node execution, and resource partitioning for efficient RL workflows.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "RLinf is a high-performance reinforcement learning (RL) training system designed to enhance efficiency in large-scale RL workflows by addressing hardware utilization and dynamicity issues through a novel paradigm called macro-to-micro flow transformation (M2Flow). This approach automatically decomposes high-level RL workflows into optimized execution plans using mechanisms like adaptive communication, context switching, elastic pipelining, and profiling-guided scheduling, resulting in 1.1x to 2.13x speedups in end-to-end training throughput compared to state-of-the-art systems across reasoning and embodied RL tasks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new paradigm, M2Flow, which significantly advances RL system design by decoupling logical programming from physical execution, addressing key inefficiencies in heterogeneous RL workflows. This represents a substantial innovation beyond incremental improvements, potentially setting a new standard for flexible RL training systems.",
      "impact_score": "High",
      "impact_justification": "The work's demonstrated speedups and open-sourced implementation could broadly influence future RL research and commercial applications by improving training efficiency for diverse scenarios like reasoning and embodied intelligence. As RL workloads grow, RLinf's flexible design is likely to be adopted and built upon in AI and distributed computing fields, amplifying its reach.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution with innovative mechanisms and empirical evidence of efficiency gains, making it essential for researchers in RL and AI systems to understand and potentially apply. While not revolutionary across all AI, its focused advancements warrant attention for those working on scalable RL training.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/44c875bd75dfeca46096b44bd9b7ef1a53bcb28c",
      "total_authors": 27,
      "authors_found": 25,
      "highest_h_index": 24,
      "average_h_index": 2.36,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Chao Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2358454055"
        },
        {
          "name": "Yuanqing Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352563942"
        },
        {
          "name": "Zhen Guo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373325170"
        },
        {
          "name": "Hao Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372664232"
        },
        {
          "name": "Si Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2303433046"
        },
        {
          "name": "Hongzhi Zang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2332090320"
        },
        {
          "name": "Quanlu Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2315889693"
        },
        {
          "name": "Yongji Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381994329"
        },
        {
          "name": "Chunyang Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374416301"
        },
        {
          "name": "Junhao Hu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374343189"
        },
        {
          "name": "Zixiao Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2278457016"
        },
        {
          "name": "Mingjie Wei",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yuqing Xie",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2276622116"
        },
        {
          "name": "Ke Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381703849"
        },
        {
          "name": "Bo Dai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381258654"
        },
        {
          "name": "Zhexuan Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2364947689"
        },
        {
          "name": "Xiang-Yun Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2144796561"
        },
        {
          "name": "Xu Fu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhihao Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381646406"
        },
        {
          "name": "Kang Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382574462"
        },
        {
          "name": "Weilin Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2346994708"
        },
        {
          "name": "Gang Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2304099389"
        },
        {
          "name": "Boxun Li",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/2789329"
        },
        {
          "name": "Jianlei Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2146237471"
        },
        {
          "name": "Zhi Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2361708226"
        },
        {
          "name": "Guohao Dai",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/144290348"
        },
        {
          "name": "Yu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2357733528"
        }
      ]
    },
    {
      "id": "2509.15966",
      "title": "A multi-temporal multi-spectral attention-augmented deep convolution\n  neural network with contrastive learning for crop yield prediction",
      "authors": [
        "Shalini Dangi",
        "Surya Karthikeya Mullapudi",
        "Chandravardhan Singh Raghaw",
        "Shahid Shafi Dar",
        "Mohammad Zia Ur Rehman",
        "Nagendra Kumar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Precise yield prediction is essential for agricultural sustainability and\nfood security. However, climate change complicates accurate yield prediction by\naffecting major factors such as weather conditions, soil fertility, and farm\nmanagement systems. Advances in technology have played an essential role in\novercoming these challenges by leveraging satellite monitoring and data\nanalysis for precise yield estimation. Current methods rely on spatio-temporal\ndata for predicting crop yield, but they often struggle with multi-spectral\ndata, which is crucial for evaluating crop health and growth patterns. To\nresolve this challenge, we propose a novel Multi-Temporal Multi-Spectral Yield\nPrediction Network, MTMS-YieldNet, that integrates spectral data with\nspatio-temporal information to effectively capture the correlations and\ndependencies between them. While existing methods that rely on pre-trained\nmodels trained on general visual data, MTMS-YieldNet utilizes contrastive\nlearning for feature discrimination during pre-training, focusing on capturing\nspatial-spectral patterns and spatio-temporal dependencies from remote sensing\ndata. Both quantitative and qualitative assessments highlight the excellence of\nthe proposed MTMS-YieldNet over seven existing state-of-the-art methods.\nMTMS-YieldNet achieves MAPE scores of 0.336 on Sentinel-1, 0.353 on Landsat-8,\nand an outstanding 0.331 on Sentinel-2, demonstrating effective yield\nprediction performance across diverse climatic and seasonal conditions. The\noutstanding performance of MTMS-YieldNet improves yield predictions and\nprovides valuable insights that can assist farmers in making better decisions,\npotentially improving crop yields.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15966v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15966v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.352,
      "datasets_score": 0.384,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15968",
      "title": "CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for\n  Long-Tail Scenarios via Collect-and-Refine",
      "authors": [
        "Shiyu Fang",
        "Yiming Cui",
        "Haoyang Liang",
        "Chen Lv",
        "Peng Hang",
        "Jian Sun"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Autonomous Driving (AD) systems have made notable progress, but their\nperformance in long-tail, safety-critical scenarios remains limited. These rare\ncases contribute a disproportionate number of accidents. Vision-Language Action\n(VLA) models have strong reasoning abilities and offer a potential solution,\nbut their effectiveness is limited by the lack of high-quality data and\ninefficient learning in such conditions. To address these challenges, we\npropose CoReVLA, a continual learning end-to-end autonomous driving framework\nthat improves the performance in long-tail scenarios through a dual-stage\nprocess of data Collection and behavior Refinement. First, the model is jointly\nfine-tuned on a mixture of open-source driving QA datasets, allowing it to\nacquire a foundational understanding of driving scenarios. Next, CoReVLA is\ndeployed within the Cave Automatic Virtual Environment (CAVE) simulation\nplatform, where driver takeover data is collected from real-time interactions.\nEach takeover indicates a long-tail scenario that CoReVLA fails to handle\nreliably. Finally, the model is refined via Direct Preference Optimization\n(DPO), allowing it to learn directly from human preferences and thereby avoid\nreward hacking caused by manually designed rewards. Extensive open-loop and\nclosed-loop experiments demonstrate that the proposed CoReVLA model can\naccurately perceive driving scenarios and make appropriate decisions. On the\nBench2Drive benchmark, CoReVLA achieves a Driving Score (DS) of 72.18 and a\nSuccess Rate (SR) of 50%, outperforming state-of-the-art methods by 7.96 DS and\n15% SR under long-tail, safety-critical scenarios. Furthermore, case studies\ndemonstrate the model's ability to continually improve its performance in\nsimilar failure-prone scenarios by leveraging past takeover experiences. All\ncodea and preprocessed datasets are available at:\nhttps://github.com/FanGShiYuu/CoReVLA",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15968v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15968v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.447,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.404,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves using Direct Preference Optimization (DPO) with human takeover data as preference feedback to refine the model, which aligns directly with RLHF by incorporating human preferences to improve decision-making, similar to training a model on human-ranked data for fine-tuning.",
      "weak_supervision_justification": "The paper uses human takeover data and existing QA datasets for training, which rely on direct human interactions and structured data, rather than programmatically generated noisy or imprecise labels characteristic of weak supervision.",
      "diffusion_reasoning_justification": "The paper does not involve any diffusion-based mechanisms, iterative refinement for logical tasks, or treating reasoning paths as entities for correction; it focuses on fine-tuning with QA data and DPO for autonomous driving.",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or partitioning data/computation across multiple nodes; it describes standard fine-tuning and data collection processes without any mention of acceleration techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces CoReVLA, a dual-stage end-to-end autonomous driving framework aimed at improving performance in long-tail, safety-critical scenarios by leveraging vision-language-action models. It employs an initial fine-tuning stage on open-source driving QA datasets, followed by data collection from human takeovers in a simulation environment and refinement via Direct Preference Optimization, resulting in superior Driving Scores and Success Rates on benchmarks like Bench2Drive compared to state-of-the-art methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining continual learning, human-in-the-loop testing, and Direct Preference Optimization to address long-tail scenarios in autonomous driving, though it builds on existing techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of end-to-end autonomous driving, particularly for enhancing safety in rare scenarios, but its influence may be limited to specific applications rather than widespread commercial adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a strong, valuable contribution with practical advancements in handling long-tail scenarios, making it essential for researchers in robotics and computer vision to be aware of its methods and results.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6c97ec2a0de711948c01d8aa1e5cad032ac86a8d",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 6,
      "average_h_index": 2.8333333333333335,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shiyu Fang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2225434584"
        },
        {
          "name": "Yiming Cui",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2321895104"
        },
        {
          "name": "Haoyang Liang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2383819608"
        },
        {
          "name": "Chen Lv",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2348260171"
        },
        {
          "name": "Peng Hang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2243199622"
        },
        {
          "name": "Jian Sun",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2275540966"
        }
      ]
    },
    {
      "id": "2509.15974",
      "title": "BEFT: Bias-Efficient Fine-Tuning of Language Models",
      "authors": [
        "Baichuan Huang",
        "Ananth Balashankar",
        "Amir Aminifar"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Fine-tuning all-bias-terms stands out among various parameter-efficient\nfine-tuning (PEFT) techniques, owing to its out-of-the-box usability and\ncompetitive performance, especially in low-data regimes. Bias-only fine-tuning\nhas the potential for unprecedented parameter efficiency. However, the link\nbetween fine-tuning different bias terms (i.e., bias terms in the query, key,\nor value projections) and downstream performance remains unclear. The existing\napproaches, e.g., based on the magnitude of bias change or empirical Fisher\ninformation, provide limited guidance for selecting the particular bias term\nfor effective fine-tuning. In this paper, we propose an approach for selecting\nthe bias term to be fine-tuned, forming the foundation of our bias-efficient\nfine-tuning (BEFT). We extensively evaluate our bias-efficient approach against\nother bias-selection approaches, across a wide range of large language models\n(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B\nparameters. Our results demonstrate the effectiveness and superiority of our\nbias-efficient approach on diverse downstream tasks, including classification,\nmultiple-choice, and generation tasks.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15974v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15974v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.457,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.416,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on bias-efficient fine-tuning of language models, specifically selecting bias terms for parameter-efficient fine-tuning (PEFT). It does not involve human feedback, reward models, or reinforcement learning techniques for aligning models with preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses fine-tuning methods for language models using bias terms, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like chain-of-thought correction.",
      "distributed_training_justification": "The paper discusses parameter efficiency in fine-tuning language models by selecting specific bias terms, but it does not cover distributed training techniques, parallel computing across nodes, or strategies for partitioning data/computation in multi-node environments.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15980",
      "title": "Shedding Light on Depth: Explainability Assessment in Monocular Depth\n  Estimation",
      "authors": [
        "Lorenzo Cirillo",
        "Claudio Schiavella",
        "Lorenzo Papa",
        "Paolo Russo",
        "Irene Amerini"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Explainable artificial intelligence is increasingly employed to understand\nthe decision-making process of deep learning models and create trustworthiness\nin their adoption. However, the explainability of Monocular Depth Estimation\n(MDE) remains largely unexplored despite its wide deployment in real-world\napplications. In this work, we study how to analyze MDE networks to map the\ninput image to the predicted depth map. More in detail, we investigate\nwell-established feature attribution methods, Saliency Maps, Integrated\nGradients, and Attention Rollout on different computationally complex models\nfor MDE: METER, a lightweight network, and PixelFormer, a deep network. We\nassess the quality of the generated visual explanations by selectively\nperturbing the most relevant and irrelevant pixels, as identified by the\nexplainability methods, and analyzing the impact of these perturbations on the\nmodel's output. Moreover, since existing evaluation metrics can have some\nlimitations in measuring the validity of visual explanations for MDE, we\nadditionally introduce the Attribution Fidelity. This metric evaluates the\nreliability of the feature attribution by assessing their consistency with the\npredicted depth map. Experimental results demonstrate that Saliency Maps and\nIntegrated Gradients have good performance in highlighting the most important\ninput features for MDE lightweight and deep models, respectively. Furthermore,\nwe show that Attribution Fidelity effectively identifies whether an\nexplainability method fails to produce reliable visual maps, even in scenarios\nwhere conventional metrics might suggest satisfactory results.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15980v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15980v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.336,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on explainability techniques for Monocular Depth Estimation (MDE), focusing on methods like Saliency Maps, Integrated Gradients, and Attention Rollout to analyze deep learning models for depth prediction. It introduces a new metric, Attribution Fidelity, for evaluating these explanations in MDE contexts. This work is centered on computer vision tasks and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15981",
      "title": "Uncertainty-Based Smooth Policy Regularisation for Reinforcement\n  Learning with Few Demonstrations",
      "authors": [
        "Yujie Zhu",
        "Charles A. Hepburn",
        "Matthew Thorpe",
        "Giovanni Montana"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "In reinforcement learning with sparse rewards, demonstrations can accelerate\nlearning, but determining when to imitate them remains challenging. We propose\nSmooth Policy Regularisation from Demonstrations (SPReD), a framework that\naddresses the fundamental question: when should an agent imitate a\ndemonstration versus follow its own policy? SPReD uses ensemble methods to\nexplicitly model Q-value distributions for both demonstration and policy\nactions, quantifying uncertainty for comparisons. We develop two complementary\nuncertainty-aware methods: a probabilistic approach estimating the likelihood\nof demonstration superiority, and an advantage-based approach scaling imitation\nby statistical significance. Unlike prevailing methods (e.g. Q-filter) that\nmake binary imitation decisions, SPReD applies continuous,\nuncertainty-proportional regularisation weights, reducing gradient variance\nduring training. Despite its computational simplicity, SPReD achieves\nremarkable gains in experiments across eight robotics tasks, outperforming\nexisting approaches by up to a factor of 14 in complex tasks while maintaining\nrobustness to demonstration quality and quantity. Our code is available at\nhttps://github.com/YujieZhu7/SPReD.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15981v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15981v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.441,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.358,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-collected demonstrations to accelerate reinforcement learning in sparse reward environments, without involving a separate reward model trained on human-ranked data or aligning the model with human preferences. While demonstrations might originate from humans, the method does not rely on human feedback as defined, making it unrelated to RLHF.",
      "weak_supervision_justification": "The paper employs demonstrations, which can be noisy or suboptimal, as a form of guidance in reinforcement learning, similar to weak supervision's use of imprecise labels. However, it is not primarily about programmatically generating labels for supervised learning, limiting its direct applicability to weak supervision paradigms.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces SPReD (Smooth Policy Regularisation from Demonstrations), a framework for reinforcement learning that uses ensemble methods to model Q-value distributions and quantify uncertainty, enabling agents to decide when to imitate demonstrations versus follow their own policy by applying continuous, uncertainty-proportional weights. This approach addresses limitations in existing methods like Q-filter by reducing gradient variance and improving learning efficiency in sparse-reward environments, with experiments across eight robotics tasks showing up to 14x better performance and robustness to demonstration quality and quantity.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by reformulating demonstration utilization as a distributional comparison problem with uncertainty-aware continuous weights, significantly advancing state-of-the-art methods like Q-filter that rely on binary decisions.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like reinforcement learning with demonstrations, particularly for robotics tasks, due to its empirical gains and practical improvements. However, its influence may be limited to specific applications rather than broadly across AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution with innovative methods and significant empirical results that advance RL techniques, making it essential for researchers in reinforcement learning and robotics. While not groundbreaking for all AI fields, it offers high-quality insights worth engaging with.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ba1f70ea8347a229dd083d92c442b9eb3d2f89b6",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 2,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yujie Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381311467"
        },
        {
          "name": "Charles A. Hepburn",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2191626656"
        },
        {
          "name": "Matthew Thorpe",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381257339"
        },
        {
          "name": "Giovanni Montana",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381257212"
        }
      ]
    },
    {
      "id": "2509.15984",
      "title": "CoPAD : Multi-source Trajectory Fusion and Cooperative Trajectory\n  Prediction with Anchor-oriented Decoder in V2X Scenarios",
      "authors": [
        "Kangyu Wu",
        "Jiaqi Qiao",
        "Ya Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MA (Multiagent Systems)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Recently, data-driven trajectory prediction methods have achieved remarkable\nresults, significantly advancing the development of autonomous driving.\nHowever, the instability of single-vehicle perception introduces certain\nlimitations to trajectory prediction. In this paper, a novel lightweight\nframework for cooperative trajectory prediction, CoPAD, is proposed. This\nframework incorporates a fusion module based on the Hungarian algorithm and\nKalman filtering, along with the Past Time Attention (PTA) module, mode\nattention module and anchor-oriented decoder (AoD). It effectively performs\nearly fusion on multi-source trajectory data from vehicles and road\ninfrastructure, enabling the trajectories with high completeness and accuracy.\nThe PTA module can efficiently capture potential interaction information among\nhistorical trajectories, and the mode attention module is proposed to enrich\nthe diversity of predictions. Additionally, the decoder based on sparse anchors\nis designed to generate the final complete trajectories. Extensive experiments\nshow that CoPAD achieves the state-of-the-art performance on the DAIR-V2X-Seq\ndataset, validating the effectiveness of the model in cooperative trajectory\nprediction in V2X scenarios.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15984v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15984v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.398,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for cooperative trajectory prediction in V2X scenarios, using techniques like the Hungarian algorithm, Kalman filtering, attention modules, and GNNs to fuse multi-source data and predict vehicle trajectories. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15986",
      "title": "EmoHeal: An End-to-End System for Personalized Therapeutic Music\n  Retrieval from Fine-grained Emotions",
      "authors": [
        "Xinchen Wan",
        "Jinhua Liang",
        "Huan Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.HC (Human-Computer Interaction)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Existing digital mental wellness tools often overlook the nuanced emotional\nstates underlying everyday challenges. For example, pre-sleep anxiety affects\nmore than 1.5 billion people worldwide, yet current approaches remain largely\nstatic and \"one-size-fits-all\", failing to adapt to individual needs. In this\nwork, we present EmoHeal, an end-to-end system that delivers personalized,\nthree-stage supportive narratives. EmoHeal detects 27 fine-grained emotions\nfrom user text with a fine-tuned XLM-RoBERTa model, mapping them to musical\nparameters via a knowledge graph grounded in music therapy principles (GEMS,\niso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to\nguide users from their current state toward a calmer one\n(\"match-guide-target\"). A within-subjects study (N=40) demonstrated significant\nsupportive effects, with participants reporting substantial mood improvement\n(M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05,\np<0.001). A strong correlation between perceived accuracy and therapeutic\noutcome (r=0.72, p<0.001) validates our fine-grained approach. These findings\nestablish the viability of theory-driven, emotion-aware digital wellness tools\nand provides a scalable AI blueprint for operationalizing music therapy\nprinciples.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15986v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15986v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.275,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an end-to-end system for personalized therapeutic music retrieval based on emotion detection using a fine-tuned XLM-RoBERTa model, a knowledge graph, and content retrieval. It does not involve reinforcement learning from human feedback, as there is no description of training a reward model on human-ranked data or fine-tuning via reinforcement learning to align AI with human preferences. The user study evaluates system efficacy but does not use feedback for model training.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15987",
      "title": "Towards Sharper Object Boundaries in Self-Supervised Depth Estimation",
      "authors": [
        "Aurélien Cecille",
        "Stefan Duffner",
        "Franck Davoine",
        "Rémi Agier",
        "Thibault Neveu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Accurate monocular depth estimation is crucial for 3D scene understanding,\nbut existing methods often blur depth at object boundaries, introducing\nspurious intermediate 3D points. While achieving sharp edges usually requires\nvery fine-grained supervision, our method produces crisp depth discontinuities\nusing only self-supervision. Specifically, we model per-pixel depth as a\nmixture distribution, capturing multiple plausible depths and shifting\nuncertainty from direct regression to the mixture weights. This formulation\nintegrates seamlessly into existing pipelines via variance-aware loss functions\nand uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show\nthat our method achieves up to 35% higher boundary sharpness and improves point\ncloud quality compared to state-of-the-art baselines.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15987v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15987v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.357,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves self-supervised monocular depth estimation, which aligns closely with weak supervision. It uses view synthesis as a supervisory signal to generate training labels programmatically from the data itself, rather than relying on precise, hand-labeled data. This approach fits the definition of weak supervision by employing high-level, noisy, or imprecise sources for training, as seen in the mixture distribution model and variance-aware loss functions. The paper demonstrates how this method achieves strong results without fine-grained supervision, directly exemplifying weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper tackles the problem of blurred depth boundaries in self-supervised monocular depth estimation by introducing a mixture distribution representation for per-pixel depth, which captures multiple plausible depths to achieve sharper discontinuities without requiring fine-grained supervision. The method integrates seamlessly into existing pipelines through uncertainty propagation and variance-aware loss functions, resulting in up to 35% higher boundary sharpness and improved point cloud quality, as demonstrated on datasets like KITTI and VKITTIv2.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel mixture distribution for per-pixel depth, representing a significant advancement in handling depth discontinuities in self-supervised learning, which goes beyond incremental improvements. This approach addresses a key limitation in existing methods by modeling multiple depths, thus advancing the state-of-the-art in monocular depth estimation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in computer vision subfields like autonomous driving and robotics by improving depth estimation accuracy at object boundaries, potentially leading to more citations and applications. However, its impact may be confined to specific areas of depth estimation rather than broadly across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper presents a strong, innovative contribution to self-supervised depth estimation that addresses a persistent challenge, making it valuable for researchers in computer vision and related fields. While not groundbreaking across all areas, its practical improvements warrant attention from those working on 3D scene understanding.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/91ea66825056382cf2688c730656047871a2ee74",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 2,
      "average_h_index": 1.2,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Aur'elien Cecille",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2322441312"
        },
        {
          "name": "Stefan Duffner",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2322446502"
        },
        {
          "name": "Franck Davoine",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2322441310"
        },
        {
          "name": "R'emi Agier",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2322441103"
        },
        {
          "name": "Thibault Neveu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2322444692"
        }
      ]
    },
    {
      "id": "2509.15990",
      "title": "DAFTED: Decoupled Asymmetric Fusion of Tabular and Echocardiographic\n  Data for Cardiac Hypertension Diagnosis",
      "authors": [
        "Jérémie Stym-Popper",
        "Nathan Painchaud",
        "Clément Rambour",
        "Pierre-Yves Courand",
        "Nicolas Thome",
        "Olivier Bernard"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal data fusion is a key approach for enhancing diagnosis in medical\napplications. We propose an asymmetric fusion strategy starting from a primary\nmodality and integrating secondary modalities by disentangling shared and\nmodality-specific information. Validated on a dataset of 239 patients with\nechocardiographic time series and tabular records, our model outperforms\nexisting methods, achieving an AUC over 90%. This improvement marks a crucial\nbenchmark for clinical use.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.15990v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15990v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.364,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an asymmetric fusion method for tabular and echocardiographic data in medical diagnosis, using techniques like cross-attention and disentanglement. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16010",
      "title": "Fed-PISA: Federated Voice Cloning via Personalized Identity-Style\n  Adaptation",
      "authors": [
        "Qi Wang",
        "Shituo Ma",
        "Guoxin Yu",
        "Hanyang Peng",
        "Yue Yu"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Voice cloning for Text-to-Speech (TTS) aims to generate expressive and\npersonalized speech from text using limited data from a target speaker.\nFederated Learning (FL) offers a collaborative and privacy-preserving framework\nfor this task, but existing approaches suffer from high communication costs and\ntend to suppress stylistic heterogeneity, resulting in insufficient\npersonalization. To address these issues, we propose Fed-PISA, which stands for\nFederated Personalized Identity-Style Adaptation. To minimize communication\ncosts, Fed-PISA introduces a disentangled Low-Rank Adaptation (LoRA) mechanism:\nthe speaker's timbre is retained locally through a private ID-LoRA, while only\na lightweight style-LoRA is transmitted to the server, thereby minimizing\nparameter exchange. To harness heterogeneity, our aggregation method, inspired\nby collaborative filtering, is introduced to create custom models for each\nclient by learning from stylistically similar peers. Experiments show that\nFed-PISA improves style expressivity, naturalness, and speaker similarity,\noutperforming standard federated baselines with minimal communication costs.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16010v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16010v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.421,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution, Fed-PISA, is a federated learning framework for voice cloning in TTS, which inherently involves distributed training. It partitions data and model components across clients (e.g., local training of ID-LoRA and aggregation of style-LoRA), aligning directly with distributed training concepts like multi-node machine learning and strategic partitioning of computation and parameters to enhance efficiency and collaboration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Fed-PISA, a federated learning framework for voice cloning in Text-to-Speech (TTS) systems, designed to minimize communication costs and enhance personalization by addressing the limitations of existing methods. It utilizes a disentangled Low-Rank Adaptation (LoRA) mechanism, where a private ID-LoRA retains each speaker's timbre locally and a lightweight style-LoRA is shared for server aggregation, combined with a personalized aggregation strategy inspired by collaborative filtering to learn from stylistically similar peers; experiments on public datasets show that Fed-PISA improves style expressivity, naturalness, speaker similarity, and overall efficiency compared to baselines.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like LoRA and federated learning with a novel personalized aggregation inspired by collaborative filtering, offering a notable improvement for federated TTS personalization without introducing an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in federated TTS by providing an efficient method for personalization, potentially leading to citations and adaptations within the subfield of audio and speech processing.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to federated learning in TTS with clear improvements in efficiency and personalization, making it valuable for researchers in AI and speech processing to stay informed.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/22c0f3e996e53cbf74dca37f18c5654d3b4b8abc",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 2,
      "average_h_index": 0.6,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Qi Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382004243"
        },
        {
          "name": "Shituo Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381307400"
        },
        {
          "name": "Guoxin Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2383959127"
        },
        {
          "name": "Hanyang Peng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2288239335"
        },
        {
          "name": "Yue Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336376809"
        }
      ]
    },
    {
      "id": "2509.16011",
      "title": "Towards Robust Visual Continual Learning with Multi-Prototype\n  Supervision",
      "authors": [
        "Xiwei Liu",
        "Yulong Li",
        "Yichen Li",
        "Xinlin Zhuang",
        "Haolin Yang",
        "Huifa Li",
        "Imran Razzak"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Language-guided supervision, which utilizes a frozen semantic target from a\nPretrained Language Model (PLM), has emerged as a promising paradigm for visual\nContinual Learning (CL). However, relying on a single target introduces two\ncritical limitations: 1) semantic ambiguity, where a polysemous category name\nresults in conflicting visual representations, and 2) intra-class visual\ndiversity, where a single prototype fails to capture the rich variety of visual\nappearances within a class. To this end, we propose MuproCL, a novel framework\nthat replaces the single target with multiple, context-aware prototypes.\nSpecifically, we employ a lightweight LLM agent to perform category\ndisambiguation and visual-modal expansion to generate a robust set of semantic\nprototypes. A LogSumExp aggregation mechanism allows the vision model to\nadaptively align with the most relevant prototype for a given image. Extensive\nexperiments across various CL baselines demonstrate that MuproCL consistently\nenhances performance and robustness, establishing a more effective path for\nlanguage-guided continual learning.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16011v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16011v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.453,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.377,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using a pretrained language model (PLM) and a lightweight LLM agent to programmatically generate multiple semantic prototypes for supervision in visual continual learning, which aligns with weak supervision by deriving labels from high-level sources rather than hand-labeled data. However, the focus is primarily on enhancing continual learning through these prototypes, not on the broader aspects of weak supervision like handling noise or scaling label generation, making it moderately relevant rather than highly so.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"Towards Robust Visual Continual Learning with Multi-Prototype Supervision,\" addresses limitations in language-guided supervision for visual Continual Learning (CL) by proposing MuproCL, a framework that uses a lightweight LLM agent to generate multiple context-aware prototypes for each class, handling semantic ambiguity and intra-class visual diversity. This method involves category disambiguation and visual-modal expansion, followed by LogSumExp aggregation to align image features with the most relevant prototype, and extensive experiments show that MuproCL enhances performance and robustness across various CL baselines.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing multi-prototype supervision to address semantic ambiguity and visual diversity in language-guided CL, building on existing methods like LingoCL with a clever combination of techniques. However, it does not introduce a entirely new problem or architecture, making it an incremental advancement rather than a groundbreaking one.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of visual Continual Learning due to its demonstrated enhancements in robustness and performance. While it could influence applications in areas like robotics and healthcare, its impact may be confined to specific advancements in language-guided supervision rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a strong, valuable contribution by improving language-guided CL methods, making it essential for researchers focused on computer vision and continual learning to stay informed. Although not exceptional enough to be a must-read, its practical enhancements warrant attention for those in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d0b463d573bb9506e10e688f320cb5b9804bbd8d",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 6,
      "average_h_index": 1.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Xiwei Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381312501"
        },
        {
          "name": "Yulong Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381296420"
        },
        {
          "name": "Yichen Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374500166"
        },
        {
          "name": "Xinlin Zhuang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374476176"
        },
        {
          "name": "Haolin Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2351438470"
        },
        {
          "name": "Huifa Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374918691"
        },
        {
          "name": "Imran Razzak",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2240180680"
        }
      ]
    },
    {
      "id": "2509.16017",
      "title": "DistillMatch: Leveraging Knowledge Distillation from Vision Foundation\n  Model for Multimodal Image Matching",
      "authors": [
        "Meng Yang",
        "Fan Fan",
        "Zizhuo Li",
        "Songchu Deng",
        "Yong Ma",
        "Jiayi Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal image matching seeks pixel-level correspondences between images of\ndifferent modalities, crucial for cross-modal perception, fusion and analysis.\nHowever, the significant appearance differences between modalities make this\ntask challenging. Due to the scarcity of high-quality annotated datasets,\nexisting deep learning methods that extract modality-common features for\nmatching perform poorly and lack adaptability to diverse scenarios. Vision\nFoundation Model (VFM), trained on large-scale data, yields generalizable and\nrobust feature representations adapted to data and tasks of various modalities,\nincluding multimodal matching. Thus, we propose DistillMatch, a multimodal\nimage matching method using knowledge distillation from VFM. DistillMatch\nemploys knowledge distillation to build a lightweight student model that\nextracts high-level semantic features from VFM (including DINOv2 and DINOv3) to\nassist matching across modalities. To retain modality-specific information, it\nextracts and injects modality category information into the other modality's\nfeatures, which enhances the model's understanding of cross-modal correlations.\nFurthermore, we design V2I-GAN to boost the model's generalization by\ntranslating visible to pseudo-infrared images for data augmentation.\nExperiments show that DistillMatch outperforms existing algorithms on public\ndatasets.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16017v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16017v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.362,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on multimodal image matching using knowledge distillation from vision foundation models (e.g., DINOv2 and DINOv3) and includes a GAN for data augmentation. It does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning process as described in the topic. Therefore, there is no relevance to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16019",
      "title": "SLaM-DiMM: Shared Latent Modeling for Diffusion Based Missing Modality\n  Synthesis in MRI",
      "authors": [
        "Bhavesh Sandbhor",
        "Bheeshm Sharma",
        "Balamurugan Palaniappan"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Brain MRI scans are often found in four modalities, consisting of T1-weighted\nwith and without contrast enhancement (T1ce and T1w), T2-weighted imaging\n(T2w), and Flair. Leveraging complementary information from these different\nmodalities enables models to learn richer, more discriminative features for\nunderstanding brain anatomy, which could be used in downstream tasks such as\nanomaly detection. However, in clinical practice, not all MRI modalities are\nalways available due to various reasons. This makes missing modality generation\na critical challenge in medical image analysis. In this paper, we propose\nSLaM-DiMM, a novel missing modality generation framework that harnesses the\npower of diffusion models to synthesize any of the four target MRI modalities\nfrom other available modalities. Our approach not only generates high-fidelity\nimages but also ensures structural coherence across the depth of the volume\nthrough a dedicated coherence enhancement mechanism. Qualitative and\nquantitative evaluations on the BraTS-Lighthouse-2025 Challenge dataset\ndemonstrate the effectiveness of the proposed approach in synthesizing\nanatomically plausible and structurally consistent results. Code is available\nat https://github.com/BheeshmSharma/SLaM-DiMM-MICCAI-BraTS-Challenge-2025.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16019v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16019v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.531,
      "distributed_training_score": 0.305,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for synthesizing missing MRI modalities, specifically through latent diffusion for image generation and structural coherence. However, it does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks. The application is purely generative for visual data, lacking any component for holistic reasoning path correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16020",
      "title": "AI Methods for Permutation Circuit Synthesis Across Generic Topologies",
      "authors": [
        "Victor Villar",
        "Juan Cruz-Benito",
        "Ismael Faro",
        "David Kremer"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This paper investigates artificial intelligence (AI) methodologies for the\nsynthesis and transpilation of permutation circuits across generic topologies.\nOur approach uses Reinforcement Learning (RL) techniques to achieve\nnear-optimal synthesis of permutation circuits up to 25 qubits. Rather than\ndeveloping specialized models for individual topologies, we train a\nfoundational model on a generic rectangular lattice, and employ masking\nmechanisms to dynamically select subsets of topologies during the synthesis.\nThis enables the synthesis of permutation circuits on any topology that can be\nembedded within the rectangular lattice, without the need to re-train the\nmodel. In this paper we show results for 5x5 lattice and compare them to\nprevious AI topology-oriented models and classical methods, showing that they\noutperform classical heuristics, and match previous specialized AI models, and\nperforms synthesis even for topologies that were not seen during training. We\nfurther show that the model can be fine tuned to strengthen the performance for\nselected topologies of interest. This methodology allows a single trained model\nto efficiently synthesize circuits across diverse topologies, allowing its\npractical integration into transpilation workflows.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16020v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16020v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.395,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Reinforcement Learning (RL) for quantum circuit synthesis and transpilation, specifically for permutation circuits across various topologies. It employs RL techniques to optimize circuits based on computational metrics like circuit depth and fidelity, without any mention of human feedback, such as training a reward model on human-ranked data or fine-tuning based on human preferences. Since RLHF specifically requires human involvement in the reward mechanism, this paper does not align with that topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16022",
      "title": "Generalized Deep Multi-view Clustering via Causal Learning with\n  Partially Aligned Cross-view Correspondence",
      "authors": [
        "Xihong Yang",
        "Siwei Wang",
        "Jiaqi Jin",
        "Fangdi Wang",
        "Tianrui Liu",
        "Yueming Jin",
        "Xinwang Liu",
        "En Zhu",
        "Kunlun He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multi-view clustering (MVC) aims to explore the common clustering structure\nacross multiple views. Many existing MVC methods heavily rely on the assumption\nof view consistency, where alignments for corresponding samples across\ndifferent views are ordered in advance. However, real-world scenarios often\npresent a challenge as only partial data is consistently aligned across\ndifferent views, restricting the overall clustering performance. In this work,\nwe consider the model performance decreasing phenomenon caused by data order\nshift (i.e., from fully to partially aligned) as a generalized multi-view\nclustering problem. To tackle this problem, we design a causal multi-view\nclustering network, termed CauMVC. We adopt a causal modeling approach to\nunderstand multi-view clustering procedure. To be specific, we formulate the\npartially aligned data as an intervention and multi-view clustering with\npartially aligned data as an post-intervention inference. However, obtaining\ninvariant features directly can be challenging. Thus, we design a Variational\nAuto-Encoder for causal learning by incorporating an encoder from existing\ninformation to estimate the invariant features. Moreover, a decoder is designed\nto perform the post-intervention inference. Lastly, we design a contrastive\nregularizer to capture sample correlations. To the best of our knowledge, this\npaper is the first work to deal generalized multi-view clustering via causal\nlearning. Empirical experiments on both fully and partially aligned data\nillustrate the strong generalization and effectiveness of CauMVC.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16022v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16022v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.337,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16025",
      "title": "Session-Level Spoken Language Assessment with a Multimodal Foundation\n  Model via Multi-Target Learning",
      "authors": [
        "Hong-Yun Lin",
        "Jhen-Ke Lin",
        "Chung-Chun Wang",
        "Hao-Chien Lu",
        "Berlin Chen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spoken Language Assessment (SLA) estimates a learner's oral proficiency from\nspontaneous speech. The growing population of L2 English speakers has\nintensified the demand for reliable SLA, a critical component of Computer\nAssisted Language Learning (CALL). Existing efforts often rely on cascaded\npipelines, which are prone to error propagation, or end-to-end models that\noften operate on a short audio window, which might miss discourse-level\nevidence. This paper introduces a novel multimodal foundation model approach\nthat performs session-level evaluation in a single pass. Our approach couples\nmulti-target learning with a frozen, Whisper ASR model-based speech prior for\nacoustic-aware calibration, allowing for jointly learning holistic and\ntrait-level objectives of SLA without resorting to handcrafted features. By\ncoherently processing the entire response session of an L2 speaker, the model\nexcels at predicting holistic oral proficiency. Experiments conducted on the\nSpeak & Improve benchmark demonstrate that our proposed approach outperforms\nthe previous state-of-the-art cascaded system and exhibits robust cross-part\ngeneralization, producing a compact deployable grader that is tailored for CALL\napplications.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16025v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16025v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.423,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.363,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on a multimodal foundation model for spoken language assessment using multi-target learning and a Whisper ASR model, but it does not describe training with programmatically generated, noisy, or imprecise labels. Instead, it relies on benchmark data like the Speak & Improve corpus, which implies the use of human-labeled scores, without any indication of weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper introduces a session-level multimodal model for evaluating spoken language proficiency, incorporating multi-target learning and acoustic calibration, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like chain-of-thought correction. The approach is based on direct processing of audio sessions, not diffusion-based mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16028",
      "title": "Think, Verbalize, then Speak: Bridging Complex Thoughts and\n  Comprehensible Speech",
      "authors": [
        "Sang Hoon Woo",
        "Sehun Lee",
        "Kang-wook Kim",
        "Gunhee Kim"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spoken dialogue systems increasingly employ large language models (LLMs) to\nleverage their advanced reasoning capabilities. However, direct application of\nLLMs in spoken communication often yield suboptimal results due to mismatches\nbetween optimal textual and verbal delivery. While existing approaches adapt\nLLMs to produce speech-friendly outputs, their impact on reasoning performance\nremains underexplored. In this work, we propose Think-Verbalize-Speak, a\nframework that decouples reasoning from spoken delivery to preserve the full\nreasoning capacity of LLMs. Central to our method is verbalizing, an\nintermediate step that translates thoughts into natural, speech-ready text. We\nalso introduce ReVerT, a latency-efficient verbalizer based on incremental and\nasynchronous summarization. Experiments across multiple benchmarks show that\nour method enhances speech naturalness and conciseness with minimal impact on\nreasoning. The project page with the dataset and the source code is available\nat https://yhytoto12.github.io/TVS-ReVerT",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16028v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16028v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.54,
      "distributed_training_score": 0.333,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a framework for decoupling reasoning from spoken delivery in LLMs, introducing an intermediate verbalization step and a latency-efficient model for summarization. It does not involve diffusion models, iterative refinement processes, or treating chain-of-thought as a holistically corrected entity over multiple steps, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16031",
      "title": "GLip: A Global-Local Integrated Progressive Framework for Robust Visual\n  Speech Recognition",
      "authors": [
        "Tianyue Wang",
        "Shuang Yang",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual speech recognition (VSR), also known as lip reading, is the task of\nrecognizing speech from silent video. Despite significant advancements in VSR\nover recent decades, most existing methods pay limited attention to real-world\nvisual challenges such as illumination variations, occlusions, blurring, and\npose changes. To address these challenges, we propose GLip, a Global-Local\nIntegrated Progressive framework designed for robust VSR. GLip is built upon\ntwo key insights: (i) learning an initial coarse alignment between visual\nfeatures across varying conditions and corresponding speech content facilitates\nthe subsequent learning of precise visual-to-speech mappings in challenging\nenvironments; (ii) under adverse conditions, certain local regions (e.g.,\nnon-occluded areas) often exhibit more discriminative cues for lip reading than\nglobal features. To this end, GLip introduces a dual-path feature extraction\narchitecture that integrates both global and local features within a two-stage\nprogressive learning framework. In the first stage, the model learns to align\nboth global and local visual features with corresponding acoustic speech units\nusing easily accessible audio-visual data, establishing a coarse yet\nsemantically robust foundation. In the second stage, we introduce a Contextual\nEnhancement Module (CEM) to dynamically integrate local features with relevant\nglobal context across both spatial and temporal dimensions, refining the coarse\nrepresentations into precise visual-speech mappings. Our framework uniquely\nexploits discriminative local regions through a progressive learning strategy,\ndemonstrating enhanced robustness against various visual challenges and\nconsistently outperforming existing methods on the LRS2 and LRS3 benchmarks. We\nfurther validate its effectiveness on a newly introduced challenging Mandarin\ndataset.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16031v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16031v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.336,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16044",
      "title": "FMD-TransUNet: Abdominal Multi-Organ Segmentation Based on Frequency\n  Domain Multi-Axis Representation Learning and Dual Attention Mechanisms",
      "authors": [
        "Fang Lu",
        "Jingyu Xu",
        "Qinxiu Sun",
        "Qiong Lou"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate abdominal multi-organ segmentation is critical for clinical\napplications. Although numerous deep learning-based automatic segmentation\nmethods have been developed, they still struggle to segment small, irregular,\nor anatomically complex organs. Moreover, most current methods focus on\nspatial-domain analysis, often overlooking the synergistic potential of\nfrequency-domain representations. To address these limitations, we propose a\nnovel framework named FMD-TransUNet for precise abdominal multi-organ\nsegmentation. It innovatively integrates the Multi-axis External Weight Block\n(MEWB) and the improved dual attention module (DA+) into the TransUNet\nframework. The MEWB extracts multi-axis frequency-domain features to capture\nboth global anatomical structures and local boundary details, providing\ncomplementary information to spatial-domain representations. The DA+ block\nutilizes depthwise separable convolutions and incorporates spatial and channel\nattention mechanisms to enhance feature fusion, reduce redundant information,\nand narrow the semantic gap between the encoder and decoder. Experimental\nvalidation on the Synapse dataset shows that FMD-TransUNet outperforms other\nrecent state-of-the-art methods, achieving an average DSC of 81.32\\% and a HD\nof 16.35 mm across eight abdominal organs. Compared to the baseline model, the\naverage DSC increased by 3.84\\%, and the average HD decreased by 15.34 mm.\nThese results demonstrate the effectiveness of FMD-TransUNet in improving the\naccuracy of abdominal multi-organ segmentation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16044v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16044v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.273,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.359,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16050",
      "title": "Graph-based Point Cloud Surface Reconstruction using B-Splines",
      "authors": [
        "Stuti Pathak",
        "Rhys G. Evans",
        "Gunther Steenackers",
        "Rudi Penne"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Generating continuous surfaces from discrete point cloud data is a\nfundamental task in several 3D vision applications. Real-world point clouds are\ninherently noisy due to various technical and environmental factors. Existing\ndata-driven surface reconstruction algorithms rely heavily on ground truth\nnormals or compute approximate normals as an intermediate step. This dependency\nmakes them extremely unreliable for noisy point cloud datasets, even if the\navailability of ground truth training data is ensured, which is not always the\ncase. B-spline reconstruction techniques provide compact surface\nrepresentations of point clouds and are especially known for their smoothening\nproperties. However, the complexity of the surfaces approximated using\nB-splines is directly influenced by the number and location of the spline\ncontrol points. Existing spline-based modeling methods predict the locations of\na fixed number of control points for a given point cloud, which makes it very\ndifficult to match the complexity of its underlying surface. In this work, we\ndevelop a Dictionary-Guided Graph Convolutional Network-based surface\nreconstruction strategy where we simultaneously predict both the location and\nthe number of control points for noisy point cloud data to generate smooth\nsurfaces without the use of any point normals. We compare our reconstruction\nmethod with several well-known as well as recent baselines by employing\nwidely-used evaluation metrics, and demonstrate that our method outperforms all\nof them both qualitatively and quantitatively.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16050v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16050v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.268,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.295,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16053",
      "title": "Compose by Focus: Scene Graph-based Atomic Skills",
      "authors": [
        "Han Qi",
        "Changhe Chen",
        "Heng Yang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "A key requirement for generalist robots is compositional generalization - the\nability to combine atomic skills to solve complex, long-horizon tasks. While\nprior work has primarily focused on synthesizing a planner that sequences\npre-learned skills, robust execution of the individual skills themselves\nremains challenging, as visuomotor policies often fail under distribution\nshifts induced by scene composition. To address this, we introduce a scene\ngraph-based representation that focuses on task-relevant objects and relations,\nthereby mitigating sensitivity to irrelevant variation. Building on this idea,\nwe develop a scene-graph skill learning framework that integrates graph neural\nnetworks with diffusion-based imitation learning, and further combine \"focused\"\nscene-graph skills with a vision-language model (VLM) based task planner.\nExperiments in both simulation and real-world manipulation tasks demonstrate\nsubstantially higher success rates than state-of-the-art baselines,\nhighlighting improved robustness and compositional generalization in\nlong-horizon tasks.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16053v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16053v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.366,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on scene graph-based representations for robot skill learning using imitation learning, without any mention of human feedback, reward models, or reinforcement learning to align with human preferences. It relies on demonstrations for training, which is a supervised approach, not RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion-based imitation learning for generating robot actions, which involves iterative refinement, but it applies this to visuomotor policies for manipulation tasks, not to multi-step logical reasoning or Chain-of-Thought processes as defined in the topic. There is a minor connection through the use of diffusion models, but it lacks the core element of solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16054",
      "title": "Language-Instructed Reasoning for Group Activity Detection via\n  Multimodal Large Language Model",
      "authors": [
        "Jihua Peng",
        "Qianxiong Xu",
        "Yichen Liu",
        "Chenxi Liu",
        "Cheng Long",
        "Rui Zhao",
        "Ziyue Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Group activity detection (GAD) aims to simultaneously identify group members\nand categorize their collective activities within video sequences. Existing\ndeep learning-based methods develop specialized architectures (e.g.,\ntransformer networks) to model the dynamics of individual roles and semantic\ndependencies between individuals and groups. However, they rely solely on\nimplicit pattern recognition from visual features and struggle with contextual\nreasoning and explainability. In this work, we propose LIR-GAD, a novel\nframework of language-instructed reasoning for GAD via Multimodal Large\nLanguage Model (MLLM). Our approach expand the original vocabulary of MLLM by\nintroducing an activity-level <ACT> token and multiple cluster-specific <GROUP>\ntokens. We process video frames alongside two specially designed tokens and\nlanguage instructions, which are then integrated into the MLLM. The pretrained\ncommonsense knowledge embedded in the MLLM enables the <ACT> token and <GROUP>\ntokens to effectively capture the semantic information of collective activities\nand learn distinct representational features of different groups, respectively.\nAlso, we introduce a multi-label classification loss to further enhance the\n<ACT> token's ability to learn discriminative semantic representations. Then,\nwe design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates\nMLLM's hidden embeddings corresponding to the designed tokens with visual\nfeatures, significantly enhancing the performance of GAD. Both quantitative and\nqualitative experiments demonstrate the superior performance of our proposed\nmethod in GAD taks.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16054v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16054v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.469,
      "distributed_training_score": 0.361,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a framework using Multimodal Large Language Models (MLLMs) for group activity detection, focusing on language-instructed reasoning with special tokens and fusion modules. It does not involve diffusion models, iterative refinement processes, or any multi-step logical reasoning via diffusion, as required for this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16058",
      "title": "Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired\n  Approach for Attention Management in Transformers",
      "authors": [
        "Krati Saxena",
        "Federico Jurado Ruiz",
        "Guido Manzi",
        "Dianbo Liu",
        "Alex Lamb"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Attention mechanisms have become integral in AI, significantly enhancing\nmodel performance and scalability by drawing inspiration from human cognition.\nConcurrently, the Attention Schema Theory (AST) in cognitive science posits\nthat individuals manage their attention by creating a model of the attention\nitself, effectively allocating cognitive resources. Inspired by AST, we\nintroduce ASAC (Attention Schema-based Attention Control), which integrates the\nattention schema concept into artificial neural networks. Our initial\nexperiments focused on embedding the ASAC module within transformer\narchitectures. This module employs a Vector-Quantized Variational AutoEncoder\n(VQVAE) as both an attention abstractor and controller, facilitating precise\nattention management. By explicitly modeling attention allocation, our approach\naims to enhance system efficiency. We demonstrate ASAC's effectiveness in both\nthe vision and NLP domains, highlighting its ability to improve classification\naccuracy and expedite the learning process. Our experiments with vision\ntransformers across various datasets illustrate that the attention controller\nnot only boosts classification accuracy but also accelerates learning.\nFurthermore, we have demonstrated the model's robustness and generalization\ncapabilities across noisy and out-of-distribution datasets. In addition, we\nhave showcased improved performance in multi-task settings. Quick experiments\nreveal that the attention schema-based module enhances resilience to\nadversarial attacks, optimizes attention to improve learning efficiency, and\nfacilitates effective transfer learning and learning from fewer examples. These\npromising results establish a connection between cognitive science and machine\nlearning, shedding light on the efficient utilization of attention mechanisms\nin AI systems.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16058v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16058v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.343,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces ASAC, which uses VQVAE for attention control in transformers, inspired by cognitive Attention Schema Theory. It focuses on enhancing attention mechanisms for tasks like classification and learning efficiency in vision and NLP domains, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Since the paper lacks any components related to diffusion-based approaches, it does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16068",
      "title": "Communications to Circulations: 3D Wind Field Retrieval and Real-Time\n  Prediction Using 5G GNSS Signals and Deep Learning",
      "authors": [
        "Yuchen Ye",
        "Chaoxia Yuan",
        "Mingyu Li",
        "Aoqi Zhou",
        "Hong Liang",
        "Chunqing Shang",
        "Kezuan Wang",
        "Yifeng Zheng",
        "Cong Chen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate atmospheric wind field information is crucial for various\napplications, including weather forecasting, aviation safety, and disaster risk\nreduction. However, obtaining high spatiotemporal resolution wind data remains\nchallenging due to limitations in traditional in-situ observations and remote\nsensing techniques, as well as the computational expense and biases of\nnumerical weather prediction (NWP) models. This paper introduces G-WindCast, a\nnovel deep learning framework that leverages signal strength variations from 5G\nGlobal Navigation Satellite System (GNSS) signals to retrieve and forecast\nthree-dimensional (3D) atmospheric wind fields. The framework utilizes Forward\nNeural Networks (FNN) and Transformer networks to capture complex, nonlinear,\nand spatiotemporal relationships between GNSS-derived features and wind\ndynamics. Our preliminary results demonstrate promising accuracy in both wind\nretrieval and short-term wind forecasting (up to 30 minutes lead time), with\nskill scores comparable to high-resolution NWP outputs in certain scenarios.\nThe model exhibits robustness across different forecast horizons and pressure\nlevels, and its predictions for wind speed and direction show superior\nagreement with observations compared to concurrent ERA5 reanalysis data.\nFurthermore, we show that the system can maintain excellent performance for\nlocalized forecasting even with a significantly reduced number of GNSS stations\n(e.g., around 100), highlighting its cost-effectiveness and scalability. This\ninterdisciplinary approach underscores the transformative potential of\nexploiting non-traditional data sources and deep learning for advanced\nenvironmental monitoring and real-time atmospheric applications.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16068v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16068v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.358,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16078",
      "title": "MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time\n  Series Representation Learning",
      "authors": [
        "Yi Xu",
        "Yitian Zhang",
        "Yun Fu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Unsupervised multivariate time series (MTS) representation learning aims to\nextract compact and informative representations from raw sequences without\nrelying on labels, enabling efficient transfer to diverse downstream tasks. In\nthis paper, we propose Dual-Masked Autoencoder (DMAE), a novel masked\ntime-series modeling framework for unsupervised MTS representation learning.\nDMAE formulates two complementary pretext tasks: (1) reconstructing masked\nvalues based on visible attributes, and (2) estimating latent representations\nof masked features, guided by a teacher encoder. To further improve\nrepresentation quality, we introduce a feature-level alignment constraint that\nencourages the predicted latent representations to align with the teacher's\noutputs. By jointly optimizing these objectives, DMAE learns temporally\ncoherent and semantically rich representations. Comprehensive evaluations\nacross classification, regression, and forecasting tasks demonstrate that our\napproach achieves consistent and superior performance over competitive\nbaselines.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16078v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16078v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.328,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16087",
      "title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language\n  Model",
      "authors": [
        "Pengteng Li",
        "Pinhao Song",
        "Wuyang Li",
        "Weiyu Guo",
        "Huizai Yao",
        "Yijie Xu",
        "Dugang Liu",
        "Hui Xiong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce SEE&TREK, the first training-free prompting framework tailored\nto enhance the spatial understanding of Multimodal Large Language Models\n(MLLMS) under vision-only constraints. While prior efforts have incorporated\nmodalities like depth or point clouds to improve spatial reasoning, purely\nvisualspatial understanding remains underexplored. SEE&TREK addresses this gap\nby focusing on two core principles: increasing visual diversity and motion\nreconstruction. For visual diversity, we conduct Maximum Semantic Richness\nSampling, which employs an off-the-shell perception model to extract\nsemantically rich keyframes that capture scene structure. For motion\nreconstruction, we simulate visual trajectories and encode relative spatial\npositions into keyframes to preserve both spatial relations and temporal\ncoherence. Our method is training&GPU-free, requiring only a single forward\npass, and can be seamlessly integrated into existing MLLM'S. Extensive\nexperiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently\nboosts various MLLM S performance across diverse spatial reasoning tasks with\nthe most +3.5% improvement, offering a promising path toward stronger spatial\nintelligence.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16087v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16087v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.394,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces See&Trek, a framework for enhancing spatial understanding in MLLMs through visual diversity and motion reconstruction, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. As the topic specifically requires adaptation of diffusion for complex logical tasks, this paper does not align.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16091",
      "title": "Blind-Spot Guided Diffusion for Self-supervised Real-World Denoising",
      "authors": [
        "Shen Cheng",
        "Haipeng Li",
        "Haibin Huang",
        "Xiaohong Liu",
        "Shuaicheng Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this work, we present Blind-Spot Guided Diffusion, a novel self-supervised\nframework for real-world image denoising. Our approach addresses two major\nchallenges: the limitations of blind-spot networks (BSNs), which often\nsacrifice local detail and introduce pixel discontinuities due to spatial\nindependence assumptions, and the difficulty of adapting diffusion models to\nself-supervised denoising. We propose a dual-branch diffusion framework that\ncombines a BSN-based diffusion branch, generating semi-clean images, with a\nconventional diffusion branch that captures underlying noise distributions. To\nenable effective training without paired data, we use the BSN-based branch to\nguide the sampling process, capturing noise structure while preserving local\ndetails. Extensive experiments on the SIDD and DND datasets demonstrate\nstate-of-the-art performance, establishing our method as a highly effective\nself-supervised solution for real-world denoising. Code and pre-trained models\nare released at: https://github.com/Sumching/BSGD.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16091v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16091v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.447,
      "diffusion_reasoning_score": 0.512,
      "distributed_training_score": 0.343,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces a self-supervised framework for image denoising that trains models without paired clean-noisy data, relying on noisy observations and internal guidance mechanisms. This aligns directly with weak supervision, as it uses imprecise sources (noisy images) to generate training signals, fitting the definition of programmatically derived labels from high-level or noisy data.",
      "diffusion_reasoning_justification": "The paper uses diffusion models for iterative image denoising, but it does not adapt this process for complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as a single entity. The focus is solely on visual denoising, lacking any components for logical reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Blind-Spot Guided Diffusion (BSGD), a self-supervised framework for real-world image denoising that addresses the limitations of blind-spot networks by combining a BSN-based diffusion branch, which generates semi-clean images, with a conventional diffusion branch that models noise distributions. This dual-branch approach uses the BSN branch to guide sampling, preserving local details and overcoming pixel discontinuities, ultimately achieving state-of-the-art performance on datasets like SIDD and DND without requiring paired data.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by cleverly combining blind-spot networks with diffusion models in a dual-branch framework, addressing known issues in self-supervised denoising without introducing an entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of self-supervised image denoising due to its state-of-the-art results on benchmark datasets, though its influence may remain confined to computer vision applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to self-supervised denoising with innovative methodology and empirical results, making it essential for researchers in computer vision to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8e443afdbdec4033ba2fbb4285b16e4e7ced1789",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 9,
      "average_h_index": 4.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Shen Cheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2356127840"
        },
        {
          "name": "Haipeng Li",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2108481914"
        },
        {
          "name": "Haibin Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2269046040"
        },
        {
          "name": "Xiaohong Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381312714"
        },
        {
          "name": "Shuaicheng Liu",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2268797502"
        }
      ]
    },
    {
      "id": "2509.16093",
      "title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM\n  Responses",
      "authors": [
        "Fangyi Yu",
        "Nabeel Seedat",
        "Dasha Herrmannova",
        "Frank Schilder",
        "Jonathan Richard Schwarz"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Evaluating long-form answers in high-stakes domains such as law or medicine\nremains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to\ncapture semantic correctness, and current LLM-based evaluators often reduce\nnuanced aspects of answer quality into a single undifferentiated score. We\nintroduce DeCE, a decomposed LLM evaluation framework that separates precision\n(factual accuracy and relevance) and recall (coverage of required concepts),\nusing instance-specific criteria automatically extracted from gold answer\nrequirements. DeCE is model-agnostic and domain-general, requiring no\npredefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate\ndifferent LLMs on a real-world legal QA task involving multi-jurisdictional\nreasoning and citation grounding. DeCE achieves substantially stronger\ncorrelation with expert judgments ($r=0.78$), compared to traditional metrics\n($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional\nevaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist\nmodels favor recall, while specialized models favor precision. Importantly,\nonly 11.95% of LLM-generated criteria required expert revision, underscoring\nDeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation\nframework in expert domains.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16093v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16093v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.342,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces an evaluation framework for LLM responses, focusing on precision and recall metrics, but does not involve training AI models using human feedback or reinforcement learning techniques. It only references human expert judgments for validation, without any RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a decomposed evaluation framework for LLM responses, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning adaptations. It focuses solely on evaluation criteria, not reasoning mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16095",
      "title": "AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent\n  Trajectory Modeling in Sports",
      "authors": [
        "Yi Xu",
        "Yun Fu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Trajectory prediction in multi-agent sports scenarios is inherently\nchallenging due to the structural heterogeneity across agent roles (e.g.,\nplayers vs. ball) and dynamic distribution gaps across different sports\ndomains. Existing unified frameworks often fail to capture these structured\ndistributional shifts, resulting in suboptimal generalization across roles and\ndomains. We propose AdaSports-Traj, an adaptive trajectory modeling framework\nthat explicitly addresses both intra-domain and inter-domain distribution\ndiscrepancies in sports. At its core, AdaSports-Traj incorporates a Role- and\nDomain-Aware Adapter to conditionally adjust latent representations based on\nagent identity and domain context. Additionally, we introduce a Hierarchical\nContrastive Learning objective, which separately supervises role-sensitive and\ndomain-aware representations to encourage disentangled latent structures\nwithout introducing optimization conflict. Experiments on three diverse sports\ndatasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectiveness\nof our adaptive design, achieving strong performance in both unified and\ncross-domain trajectory prediction settings.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16095v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16095v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.354,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16098",
      "title": "SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and\n  Object-Level 2D Features",
      "authors": [
        "Jinyuan Qu",
        "Hongyang Li",
        "Xingyu Chen",
        "Shilong Liu",
        "Yukai Shi",
        "Tianhe Ren",
        "Ruitao Jing",
        "Lei Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we present SegDINO3D, a novel Transformer encoder-decoder\nframework for 3D instance segmentation. As 3D training data is generally not as\nsufficient as 2D training images, SegDINO3D is designed to fully leverage 2D\nrepresentation from a pre-trained 2D detection model, including both\nimage-level and object-level features, for improving 3D representation.\nSegDINO3D takes both a point cloud and its associated 2D images as input. In\nthe encoder stage, it first enriches each 3D point by retrieving 2D image\nfeatures from its corresponding image views and then leverages a 3D encoder for\n3D context fusion. In the decoder stage, it formulates 3D object queries as 3D\nanchor boxes and performs cross-attention from 3D queries to 2D object queries\nobtained from 2D images using the 2D detection model. These 2D object queries\nserve as a compact object-level representation of 2D images, effectively\navoiding the challenge of keeping thousands of image feature maps in the memory\nwhile faithfully preserving the knowledge of the pre-trained 2D model. The\nintroducing of 3D box queries also enables the model to modulate\ncross-attention using the predicted boxes for more precise querying. SegDINO3D\nachieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3D\ninstance segmentation benchmarks. Notably, on the challenging ScanNet200\ndataset, SegDINO3D significantly outperforms prior methods by +8.7 and +6.8 mAP\non the validation and hidden test sets, respectively, demonstrating its\nsuperiority.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16098v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16098v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.289,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.38,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16106",
      "title": "PRISM: Probabilistic and Robust Inverse Solver with\n  Measurement-Conditioned Diffusion Prior for Blind Inverse Problems",
      "authors": [
        "Yuanyun Hu",
        "Evan Bell",
        "Guijin Wang",
        "Yu Sun"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Diffusion models are now commonly used to solve inverse problems in\ncomputational imaging. However, most diffusion-based inverse solvers require\ncomplete knowledge of the forward operator to be used. In this work, we\nintroduce a novel probabilistic and robust inverse solver with\nmeasurement-conditioned diffusion prior (PRISM) to effectively address blind\ninverse problems. PRISM offers a technical advancement over current methods by\nincorporating a powerful measurement-conditioned diffusion model into a\ntheoretically principled posterior sampling scheme. Experiments on blind image\ndeblurring validate the effectiveness of the proposed method, demonstrating the\nsuperior performance of PRISM over state-of-the-art baselines in both image and\nblur kernel recovery.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16106v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16106v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.274,
      "datasets_score": 0.235,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on applying diffusion models to solve blind inverse problems in computational imaging, such as image deblurring, by using iterative refinement for image and kernel reconstruction. However, it does not involve adapting diffusion models for complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as an entity. The diffusion process here is used for generative tasks in image processing, not for logical reasoning, so it does not meet the topic's criteria.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16117",
      "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process",
      "authors": [
        "Kaiwen Zheng",
        "Huayu Chen",
        "Haotian Ye",
        "Haoxiang Wang",
        "Qinsheng Zhang",
        "Kai Jiang",
        "Hang Su",
        "Stefano Ermon",
        "Jun Zhu",
        "Ming-Yu Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Online reinforcement learning (RL) has been central to post-training language\nmodels, but its extension to diffusion models remains challenging due to\nintractable likelihoods. Recent works discretize the reverse sampling process\nto enable GRPO-style training, yet they inherit fundamental drawbacks,\nincluding solver restrictions, forward-reverse inconsistency, and complicated\nintegration with classifier-free guidance (CFG). We introduce Diffusion\nNegative-aware FineTuning (DiffusionNFT), a new online RL paradigm that\noptimizes diffusion models directly on the forward process via flow matching.\nDiffusionNFT contrasts positive and negative generations to define an implicit\npolicy improvement direction, naturally incorporating reinforcement signals\ninto the supervised learning objective. This formulation enables training with\narbitrary black-box solvers, eliminates the need for likelihood estimation, and\nrequires only clean images rather than sampling trajectories for policy\noptimization. DiffusionNFT is up to $25\\times$ more efficient than FlowGRPO in\nhead-to-head comparisons, while being CFG-free. For instance, DiffusionNFT\nimproves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO\nachieves 0.95 with over 5k steps and additional CFG employment. By leveraging\nmultiple reward models, DiffusionNFT significantly boosts the performance of\nSD3.5-Medium in every benchmark tested.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16117v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16117v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.455,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.607,
      "distributed_training_score": 0.398,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with reward models to fine-tune diffusion models, which shares similarities with RLHF in its optimization approach. However, it does not explicitly involve human feedback for training the reward models, relying instead on general reward signals and benchmarks, thus not fully meeting the RLHF definition.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on applying reinforcement learning to diffusion models for image generation, emphasizing forward process optimization and visual tasks, with no mention of iterative refinement for complex logical reasoning, Chain-of-Thought, or solving logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16119",
      "title": "RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D\n  Detector with 4D Automotive Radars",
      "authors": [
        "Weiyi Xiong",
        "Bing Zhu",
        "Tao Huang",
        "Zewei Zheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "4D automotive radars have gained increasing attention for autonomous driving\ndue to their low cost, robustness, and inherent velocity measurement\ncapability. However, existing 4D radar-based 3D detectors rely heavily on\npillar encoders for BEV feature extraction, where each point contributes to\nonly a single BEV grid, resulting in sparse feature maps and degraded\nrepresentation quality. In addition, they also optimize bounding box attributes\nindependently, leading to sub-optimal detection accuracy. Moreover, their\ninference speed, while sufficient for high-end GPUs, may fail to meet the\nreal-time requirement on vehicle-mounted embedded devices. To overcome these\nlimitations, an efficient and effective Gaussian-based 3D detector, namely\nRadarGaussianDet3D is introduced, leveraging Gaussian primitives and\ndistributions as intermediate representations for radar points and bounding\nboxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed\nto transform each point into a Gaussian primitive after feature aggregation and\nemploys the 3D Gaussian Splatting (3DGS) technique for BEV rasterization,\nyielding denser feature maps. PGE exhibits exceptionally low latency, owing to\nthe optimized algorithm for point feature aggregation and fast rendering of\n3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts\nbounding boxes into 3D Gaussian distributions and measures their distance to\nenable more comprehensive and consistent optimization. Extensive experiments on\nTJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves\nstate-of-the-art detection accuracy while delivering substantially faster\ninference, highlighting its potential for real-time deployment in autonomous\ndriving.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16119v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16119v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.264,
      "weak_supervision_score": 0.264,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.332,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16126",
      "title": "Network-Based Detection of Autism Spectrum Disorder Using Sustainable\n  and Non-invasive Salivary Biomarkers",
      "authors": [
        "Janayna M. Fernandes",
        "Robinson Sabino-Silva",
        "Murillo G. Carneiro"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying\nearly diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy,\nwe developed GANet, a genetic algorithm-based network optimization framework\nleveraging PageRank and Degree for importance-based feature characterization.\nGANet systematically optimizes network structure to extract meaningful patterns\nfrom high-dimensional spectral data. It achieved superior performance compared\nto linear discriminant analysis, support vector machines, and deep learning\nmodels, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74\nharmonic mean. These results demonstrate GANet's potential as a robust,\nbio-inspired, non-invasive tool for precise ASD detection and broader\nspectral-based health applications.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16126v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16126v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.298,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.32,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16127",
      "title": "BaseReward: A Strong Baseline for Multimodal Reward Model",
      "authors": [
        "Yi-Fan Zhang",
        "Haihua Yang",
        "Huanyu Zhang",
        "Yang Shi",
        "Zezhou Chen",
        "Haochen Tian",
        "Chaoyou Fu",
        "Haotian Wang",
        "Kai Wu",
        "Bo Cui",
        "Xu Wang",
        "Jianfei Pan",
        "Haotian Wang",
        "Zhang Zhang",
        "Liang Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has made\naligning them with human preferences a critical challenge. Reward Models (RMs)\nare a core technology for achieving this goal, but a systematic guide for\nbuilding state-of-the-art Multimodal Reward Models (MRMs) is currently lacking\nin both academia and industry. Through exhaustive experimental analysis, this\npaper aims to provide a clear ``recipe'' for constructing high-performance\nMRMs. We systematically investigate every crucial component in the MRM\ndevelopment pipeline, including \\textit{reward modeling paradigms} (e.g.,\nNaive-RM, Critic-based RM, and Generative RM), \\textit{reward head\narchitecture}, \\textit{training strategies}, \\textit{data curation} (covering\nover ten multimodal and text-only preference datasets), \\textit{backbone model}\nand \\textit{model scale}, and \\textit{ensemble methods}.\n  Based on these experimental insights, we introduce \\textbf{BaseReward}, a\npowerful and efficient baseline for multimodal reward modeling. BaseReward\nadopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone,\nfeaturing an optimized two-layer reward head, and is trained on a carefully\ncurated mixture of high-quality multimodal and text-only preference data. Our\nresults show that BaseReward establishes a new SOTA on major benchmarks such as\nMM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench,\noutperforming previous models. Furthermore, to validate its practical utility\nbeyond static benchmarks, we integrate BaseReward into a real-world\nreinforcement learning pipeline, successfully enhancing an MLLM's performance\nacross various perception, reasoning, and conversational tasks. This work not\nonly delivers a top-tier MRM but, more importantly, provides the community with\na clear, empirically-backed guide for developing robust reward models for the\nnext generation of MLLMs.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16127v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16127v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.538,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.384,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of a Multimodal Reward Model (BaseReward) specifically for use in RLHF to align MLLMs with human preferences. It discusses training reward models on human feedback and integrating them into RL pipelines for fine-tuning, directly matching the definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reward modeling for multimodal tasks and RLHF, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of aligning Multimodal Large Language Models (MLLMs) with human preferences by systematically analyzing key components of Multimodal Reward Models (MRMs), including reward paradigms, architecture, training strategies, data curation, backbone models, and ensemble methods, to provide a comprehensive guide for building high-performance MRMs. The authors introduce BaseReward, a robust baseline built on the Qwen2.5-VL backbone with an optimized two-layer reward head, trained on curated multimodal and text-only data, which achieves new state-of-the-art results on benchmarks like MM-RLHF-Reward Bench and demonstrates practical utility in enhancing MLLM performance through reinforcement learning.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by providing a systematic analysis and a strong baseline for MRMs, combining existing ideas in a clever way to advance the field, though it does not introduce an entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in multimodal alignment by offering a practical baseline and guide that can be built upon in the subfield of MLLMs, though its broader applicability to other areas may be limited.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a valuable contribution with empirical insights and a strong baseline for MRM development, making it important for researchers in multimodal AI to be aware of, though it is not essential for those outside the specific domain.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/06df7400cba1ef6947f72d2dae130eb74c1ac1ca",
      "total_authors": 14,
      "authors_found": 12,
      "highest_h_index": 8,
      "average_h_index": 2.1666666666666665,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Yi-Fan Zhang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2268626991"
        },
        {
          "name": "Haihua Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381342573"
        },
        {
          "name": "Huanyu Zhang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2339967968"
        },
        {
          "name": "Yang Shi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2383137537"
        },
        {
          "name": "Zezhou Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381823654"
        },
        {
          "name": "Haochen Tian",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chaoyou Fu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Haotian Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381294622"
        },
        {
          "name": "Kai Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381285085"
        },
        {
          "name": "Bo Cui",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381264775"
        },
        {
          "name": "Xu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381297824"
        },
        {
          "name": "Jianfei Pan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381315279"
        },
        {
          "name": "Zhang Zhang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2257041164"
        },
        {
          "name": "Liang Wang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2268509501"
        }
      ]
    },
    {
      "id": "2509.16131",
      "title": "Dynamic Classifier-Free Diffusion Guidance via Online Feedback",
      "authors": [
        "Pinelopi Papalampidi",
        "Olivia Wiles",
        "Ira Ktena",
        "Aleksandar Shtedritski",
        "Emanuele Bugliarello",
        "Ivana Kajic",
        "Isabela Albuquerque",
        "Aida Nematzadeh"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Classifier-free guidance (CFG) is a cornerstone of text-to-image diffusion\nmodels, yet its effectiveness is limited by the use of static guidance scales.\nThis \"one-size-fits-all\" approach fails to adapt to the diverse requirements of\ndifferent prompts; moreover, prior solutions like gradient-based correction or\nfixed heuristic schedules introduce additional complexities and fail to\ngeneralize. In this work, we challeng this static paradigm by introducing a\nframework for dynamic CFG scheduling. Our method leverages online feedback from\na suite of general-purpose and specialized small-scale latent-space\nevaluations, such as CLIP for alignment, a discriminator for fidelity and a\nhuman preference reward model, to assess generation quality at each step of the\nreverse diffusion process. Based on this feedback, we perform a greedy search\nto select the optimal CFG scale for each timestep, creating a unique guidance\nschedule tailored to every prompt and sample. We demonstrate the effectiveness\nof our approach on both small-scale models and the state-of-the-art Imagen 3,\nshowing significant improvements in text alignment, visual quality, text\nrendering and numerical reasoning. Notably, when compared against the default\nImagen 3 baseline, our method achieves up to 53.8% human preference win-rate\nfor overall preference, a figure that increases up to to 55.5% on prompts\ntargeting specific capabilities like text rendering. Our work establishes that\nthe optimal guidance schedule is inherently dynamic and prompt-dependent, and\nprovides an efficient and generalizable framework to achieve it.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16131v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16131v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.457,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.574,
      "distributed_training_score": 0.37,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses a human preference reward model, which is trained on human-ranked data, to provide feedback during the diffusion process. However, this is not full RLHF, as the method does not involve fine-tuning the main model using reinforcement learning; instead, it employs the reward model for online evaluation and greedy search to adjust CFG scales during inference.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on dynamic CFG in text-to-image diffusion models for improving image generation quality, using evaluators for aspects like alignment and fidelity. It does not adapt the diffusion process for multi-step logical reasoning or treat a Chain-of-Thought as an entity; the evaluators for text rendering and numerical reasoning are for image assessment, not reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16132",
      "title": "Recovering Parametric Scenes from Very Few Time-of-Flight Pixels",
      "authors": [
        "Carter Sifferman",
        "Yiquan Li",
        "Yiming Li",
        "Fangzhou Mu",
        "Michael Gleicher",
        "Mohit Gupta",
        "Yin Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We aim to recover the geometry of 3D parametric scenes using very few depth\nmeasurements from low-cost, commercially available time-of-flight sensors.\nThese sensors offer very low spatial resolution (i.e., a single pixel), but\nimage a wide field-of-view per pixel and capture detailed time-of-flight data\nin the form of time-resolved photon counts. This time-of-flight data encodes\nrich scene information and thus enables recovery of simple scenes from sparse\nmeasurements. We investigate the feasibility of using a distributed set of few\nmeasurements (e.g., as few as 15 pixels) to recover the geometry of simple\nparametric scenes with a strong prior, such as estimating the 6D pose of a\nknown object. To achieve this, we design a method that utilizes both\nfeed-forward prediction to infer scene parameters, and differentiable rendering\nwithin an analysis-by-synthesis framework to refine the scene parameter\nestimate. We develop hardware prototypes and demonstrate that our method\neffectively recovers object pose given an untextured 3D model in both\nsimulations and controlled real-world captures, and show promising initial\nresults for other parametric scenes. We additionally conduct experiments to\nexplore the limits and capabilities of our imaging solution.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16132v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16132v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.333,
      "datasets_score": 0.25,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16141",
      "title": "AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models",
      "authors": [
        "Vatsal Malaviya",
        "Agneet Chatterjee",
        "Maitreya Patel",
        "Yezhou Yang",
        "Chitta Baral"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-Image (T2I) models have recently achieved remarkable success in\ngenerating images from textual descriptions. However, challenges still persist\nin accurately rendering complex scenes where actions and interactions form the\nprimary semantic focus. Our key observation in this work is that T2I models\nfrequently struggle to capture nuanced and often implicit attributes inherent\nin action depiction, leading to generating images that lack key contextual\ndetails. To enable systematic evaluation, we introduce AcT2I, a benchmark\ndesigned to evaluate the performance of T2I models in generating images from\naction-centric prompts. We experimentally validate that leading T2I models do\nnot fare well on AcT2I. We further hypothesize that this shortcoming arises\nfrom the incomplete representation of the inherent attributes and contextual\ndependencies in the training corpora of existing T2I models. We build upon this\nby developing a training-free, knowledge distillation technique utilizing Large\nLanguage Models to address this limitation. Specifically, we enhance prompts by\nincorporating dense information across three dimensions, observing that\ninjecting prompts with temporal details significantly improves image generation\naccuracy, with our best model achieving an increase of 72%. Our findings\nhighlight the limitations of current T2I methods in generating images that\nrequire complex reasoning and demonstrate that integrating linguistic knowledge\nin a systematic way can notably advance the generation of nuanced and\ncontextually accurate images.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16141v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16141v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.464,
      "distributed_training_score": 0.332,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper involves Text-to-Image (T2I) models like Stable Diffusion, which are diffusion-based, and addresses limitations in handling complex actions that require reasoning. However, it focuses on enhancing prompts using Large Language Models rather than adapting the diffusion process for multi-step logical reasoning or treating a chain-of-thought as a holistic entity. Thus, while diffusion models are used, the paper does not center on modifying diffusion for reasoning tasks as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces and evaluates the AcT2I benchmark, which involves creating prompts from the Animal Kingdom dataset to assess T2I models on action-centric scenarios. This directly aligns with research on developing, benchmarking, and evaluating datasets for AI applications, including dataset curation and performance analysis on state-of-the-art models.",
      "llm_score_status": "completed",
      "summary": "This paper introduces the AcT2I benchmark to evaluate the ability of text-to-image (T2I) models in generating accurate images from action-centric prompts, highlighting that state-of-the-art models struggle with nuanced action depiction. The authors propose a training-free knowledge distillation technique using large language models to enhance prompts with spatial, temporal, and emotional details, resulting in significant improvements, such as a 72% increase in accuracy for models like Stable Diffusion.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel benchmark for evaluating action depiction in T2I models, which is a previously unaddressed problem, and proposes an innovative training-free technique using LLMs to enhance prompt effectiveness, significantly advancing the field.",
      "impact_score": "Moderate",
      "impact_justification": "The work provides a new benchmark and improvement method that could be built upon in T2I research, likely leading to citations within the subfield of computer vision and image generation, though its broader influence may be limited to specialized applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper presents a strong contribution by identifying key limitations in T2I models and offering an effective solution, making it valuable for researchers focused on AI-generated imagery to stay informed.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/53314d729f4efac6e0a25d4dc77d3fbde2d504e6",
      "total_authors": 5,
      "authors_found": 4,
      "highest_h_index": 27,
      "average_h_index": 14.25,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Vatsal Malaviya",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2294026526"
        },
        {
          "name": "Agneet Chatterjee",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/145328123"
        },
        {
          "name": "Maitreya Patel",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yezhou Yang",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/1784500"
        },
        {
          "name": "Chitta Baral",
          "h_index": 27,
          "profile_url": "https://www.semanticscholar.org/author/2064619864"
        }
      ]
    },
    {
      "id": "2509.16149",
      "title": "Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal\n  Large Language Models",
      "authors": [
        "Renjie Pi",
        "Kehao Miao",
        "Li Peihang",
        "Runtao Liu",
        "Jiahui Gao",
        "Jipeng Zhang",
        "Xiaofang Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal large language models (MLLMs) have demonstrated extraordinary\ncapabilities in conducting conversations based on image inputs. However, we\nobserve that MLLMs exhibit a pronounced form of visual sycophantic behavior.\nWhile similar behavior has also been noted in text-based large language models\n(LLMs), it becomes significantly more prominent when MLLMs process image\ninputs. We refer to this phenomenon as the \"sycophantic modality gap.\" To\nbetter understand this issue, we further analyze the factors that contribute to\nthe exacerbation of this gap. To mitigate the visual sycophantic behavior, we\nfirst experiment with naive supervised fine-tuning to help the MLLM resist\nmisleading instructions from the user. However, we find that this approach also\nmakes the MLLM overly resistant to corrective instructions (i.e., stubborn even\nif it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective\nTuning (SRT), which enables the MLLM to engage in reflective reasoning,\nallowing it to determine whether a user's instruction is misleading or\ncorrective before drawing a conclusion. After applying SRT, we observe a\nsignificant reduction in sycophantic behavior toward misleading instructions,\nwithout resulting in excessive stubbornness when receiving corrective\ninstructions.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16149v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16149v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.462,
      "weak_supervision_score": 0.428,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.346,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on supervised fine-tuning and a new method (SRT) to mitigate sycophantic behavior in MLLMs, without involving human feedback, reward models, or reinforcement learning techniques. It does not align with RLHF, which requires training via human-ranked data and reinforcement processes.",
      "weak_supervision_justification": "The paper involves creating instruction tuning data for fine-tuning, which may include programmatically generated labels, but it does not emphasize training on noisy or imprecise sources as the core method. Weak supervision is not a primary focus, making the connection indirect.",
      "diffusion_reasoning_justification": "The proposed SRT method includes a multi-step process (e.g., image textualization, reflection, and summarization) for reasoning, which somewhat resembles iterative refinement. However, it does not use diffusion models or adapt diffusion processes for logical tasks, so it is not directly related.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16163",
      "title": "Robust Vision-Language Models via Tensor Decomposition: A Defense\n  Against Adversarial Attacks",
      "authors": [
        "Het Patel",
        "Muzammil Allie",
        "Qian Zhang",
        "Jia Chen",
        "Evangelos E. Papalexakis"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Vision language models (VLMs) excel in multimodal understanding but are prone\nto adversarial attacks. Existing defenses often demand costly retraining or\nsignificant architecture changes. We introduce a lightweight defense using\ntensor decomposition suitable for any pre-trained VLM, requiring no retraining.\nBy decomposing and reconstructing vision encoder representations, it filters\nadversarial noise while preserving meaning. Experiments with CLIP on COCO and\nFlickr30K show improved robustness. On Flickr30K, it restores 12.3\\%\nperformance lost to attacks, raising Recall@1 accuracy from 7.5\\% to 19.8\\%. On\nCOCO, it recovers 8.1\\% performance, improving accuracy from 3.8\\% to 11.9\\%.\nAnalysis shows Tensor Train decomposition with low rank (8-32) and low residual\nstrength ($\\alpha=0.1-0.2$) is optimal. This method is a practical,\nplug-and-play solution with minimal overhead for existing VLMs.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16163v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16163v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.369,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16170",
      "title": "UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical\n  Self-Supervised Compensation",
      "authors": [
        "Xiaoqi Zhao",
        "Youwei Pang",
        "Chenyang Yu",
        "Lihe Zhang",
        "Huchuan Lu",
        "Shijian Lu",
        "Georges El Fakhri",
        "Xiaofeng Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multi-modal image segmentation faces real-world deployment challenges from\nincomplete/corrupted modalities degrading performance. While existing methods\naddress training-inference modality gaps via specialized per-combination\nmodels, they introduce high deployment costs by requiring exhaustive model\nsubsets and model-modality matching. In this work, we propose a unified\nmodality-relax segmentation network (UniMRSeg) through hierarchical\nself-supervised compensation (HSSC). Our approach hierarchically bridges\nrepresentation gaps between complete and incomplete modalities across input,\nfeature and output levels. % First, we adopt modality reconstruction with the\nhybrid shuffled-masking augmentation, encouraging the model to learn the\nintrinsic modality characteristics and generate meaningful representations for\nmissing modalities through cross-modal fusion. % Next, modality-invariant\ncontrastive learning implicitly compensates the feature space distance among\nincomplete-complete modality pairs. Furthermore, the proposed lightweight\nreverse attention adapter explicitly compensates for the weak perceptual\nsemantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid\nconsistency constraint to ensure stable prediction under all modality\ncombinations without large performance fluctuations. Without bells and\nwhistles, UniMRSeg significantly outperforms the state-of-the-art methods under\ndiverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D\nsemantic segmentation, RGB-D/T salient object segmentation. The code will be\nreleased at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16170v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16170v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.351,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16179",
      "title": "Fast OTSU Thresholding Using Bisection Method",
      "authors": [
        "Sai Varun Kodathala"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)",
        "math.NA (Numerical Analysis)"
      ],
      "abstract": "The Otsu thresholding algorithm represents a fundamental technique in image\nsegmentation, yet its computational efficiency is severely limited by\nexhaustive search requirements across all possible threshold values. This work\npresents an optimized implementation that leverages the bisection method to\nexploit the unimodal characteristics of the between-class variance function.\nOur approach reduces the computational complexity from O(L) to O(log L)\nevaluations while preserving segmentation accuracy. Experimental validation on\n48 standard test images demonstrates a 91.63% reduction in variance\ncomputations and 97.21% reduction in algorithmic iterations compared to\nconventional exhaustive search. The bisection method achieves exact threshold\nmatches in 66.67% of test cases, with 95.83% exhibiting deviations within 5\ngray levels. The algorithm maintains universal convergence within theoretical\nlogarithmic bounds while providing deterministic performance guarantees\nsuitable for real-time applications. This optimization addresses critical\ncomputational bottlenecks in large-scale image processing systems without\ncompromising the theoretical foundations or segmentation quality of the\noriginal Otsu method.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16179v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16179v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.2,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.232,
      "distributed_training_score": 0.256,
      "datasets_score": 0.261,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16184",
      "title": "Accelerating Atomic Fine Structure Determination with Graph\n  Reinforcement Learning",
      "authors": [
        "M. Ding",
        "V. -A. Darvariu",
        "A. N. Ryabtsev",
        "N. Hawes",
        "J. C. Pickering"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Atomic data determined by analysis of observed atomic spectra are essential\nfor plasma diagnostics. For each low-ionisation open d- and f-subshell atomic\nspecies, around $10^3$ fine structure level energies can be determined through\nyears of analysis of $10^4$ observable spectral lines. We propose the\nautomation of this task by casting the analysis procedure as a Markov decision\nprocess and solving it by graph reinforcement learning using reward functions\nlearned on historical human decisions. In our evaluations on existing spectral\nline lists and theoretical calculations for Co II and Nd II-III, hundreds of\nlevel energies were computed within hours, agreeing with published values in\n95% of cases for Co II and 54-87% for Nd II-III. As the current efficiency in\natomic fine structure determination struggles to meet growing atomic data\ndemands from astronomy and fusion science, our new artificial intelligence\napproach sets the stage for closing this gap.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16184v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16184v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.44,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.369,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves using graph reinforcement learning with reward functions trained on historical human decisions to automate atomic fine structure determination. This directly aligns with RLHF, as it trains a reward model based on human preferences (from past analyses) and uses it to guide the reinforcement learning process, thereby fine-tuning the AI agent to match human-like decision-making.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces an AI-driven method to automate the determination of atomic fine structure levels by modeling the analysis as a Markov decision process and employing graph reinforcement learning, specifically a variant of the Deep Q-network, trained on historical human decisions to maximize rewards. Their approach, evaluated on spectral line lists for Co II and Nd II-III, successfully computes hundreds of level energies in hours with high accuracy—95% agreement for Co II and 54-87% for Nd II-III—potentially accelerating atomic data generation for applications in astronomy, fusion science, and beyond by reducing the need for extensive manual labor.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel application of graph reinforcement learning to automate atomic fine structure determination, which is a truly new technique for solving a complex decision-making problem in atomic spectroscopy. This significantly advances the state-of-the-art by integrating AI methods into a field traditionally reliant on manual analysis.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and applications in atomic physics, astronomy, and fusion technology by accelerating data generation that is currently a bottleneck. Its ability to reduce analysis time from years to hours could lead to broader adoption and citations across interdisciplinary fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality and valuable contribution that advances AI applications in scientific domains, making it essential for researchers in atomic physics, machine learning, and related fields to be aware of. While not universally critical, its innovative approach and potential real-world impact justify reading for those interested in AI-driven automation.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f9c58160cec298e98ef7c4233e02c1001fc17406",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 8,
      "average_h_index": 3.6,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "M. Ding",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2282589806"
        },
        {
          "name": "Victor-Alexandru Darvariu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/41031873"
        },
        {
          "name": "A. Ryabtsev",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/50875704"
        },
        {
          "name": "N. Hawes",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381258027"
        },
        {
          "name": "J. C. Pickering",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2282649277"
        }
      ]
    },
    {
      "id": "2509.16188",
      "title": "CultureScope: A Dimensional Lens for Probing Cultural Understanding in\n  LLMs",
      "authors": [
        "Jinghao Zhang",
        "Sihang Jiang",
        "Shiwei Guo",
        "Shisong Chen",
        "Yanghua Xiao",
        "Hongwei Feng",
        "Jiaqing Liang",
        "Minggui HE",
        "Shimin Tao",
        "Hongxia Ma"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As large language models (LLMs) are increasingly deployed in diverse cultural\nenvironments, evaluating their cultural understanding capability has become\nessential for ensuring trustworthy and culturally aligned applications.\nHowever, most existing benchmarks lack comprehensiveness and are challenging to\nscale and adapt across different cultural contexts, because their frameworks\noften lack guidance from well-established cultural theories and tend to rely on\nexpert-driven manual annotations. To address these issues, we propose\nCultureScope, the most comprehensive evaluation framework to date for assessing\ncultural understanding in LLMs. Inspired by the cultural iceberg theory, we\ndesign a novel dimensional schema for cultural knowledge classification,\ncomprising 3 layers and 140 dimensions, which guides the automated construction\nof culture-specific knowledge bases and corresponding evaluation datasets for\nany given languages and cultures. Experimental results demonstrate that our\nmethod can effectively evaluate cultural understanding. They also reveal that\nexisting large language models lack comprehensive cultural competence, and\nmerely incorporating multilingual data does not necessarily enhance cultural\nunderstanding. All code and data files are available at\nhttps://github.com/HoganZinger/Culture",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16188v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16188v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.343,
      "datasets_score": 0.412,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on evaluating cultural understanding in LLMs through a new framework and datasets, with no mention of reinforcement learning, human feedback, reward models, or model fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating, analyzing, and evaluating datasets for assessing cultural understanding in LLMs, including automated construction of culture-specific knowledge bases and benchmarks, which aligns directly with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "CultureScope proposes a comprehensive framework for evaluating cultural understanding in large language models (LLMs), inspired by the cultural iceberg theory, which includes a dimensional schema with 3 layers and 140 dimensions to classify cultural knowledge and enable automated construction of culture-specific knowledge bases and datasets. The methodology involves expert-guided schema development, automated knowledge extraction for languages like Chinese and Spanish, and extensive experiments on existing LLMs, revealing that these models exhibit inconsistent cultural competence across dimensions and that incorporating multilingual data alone does not sufficiently enhance cultural understanding.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel dimensional schema and automated framework for cultural evaluation in LLMs, significantly advancing beyond existing benchmarks by incorporating well-established cultural theories and scalability.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future AI research and applications by providing a standardized, scalable method for assessing cultural alignment, which is crucial for deploying LLMs in diverse global contexts.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution to AI evaluation methods, making it valuable for researchers focused on cultural aspects of LLMs, though it may not be essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/97801a5f25055f449fecdf3b2c8f2847ae7ca85e",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 19,
      "average_h_index": 4.555555555555555,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Jinghao Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381312415"
        },
        {
          "name": "Sihang Jiang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/1999030240"
        },
        {
          "name": "Shiwei Guo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381837641"
        },
        {
          "name": "Shisong Chen",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2244180854"
        },
        {
          "name": "Yanghua Xiao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2291069226"
        },
        {
          "name": "Hongwei Feng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jiaqing Liang",
          "h_index": 19,
          "profile_url": "https://www.semanticscholar.org/author/3366523"
        },
        {
          "name": "Minggui He",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2319973253"
        },
        {
          "name": "Shimin Tao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381260162"
        },
        {
          "name": "Hongxia Ma",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2267872093"
        }
      ]
    },
    {
      "id": "2509.16195",
      "title": "FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal\n  Distillation",
      "authors": [
        "Luca Della Libera",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Neural audio codecs are a fundamental component of modern generative audio\npipelines. Although recent codecs achieve strong low-bitrate reconstruction and\nprovide powerful representations for downstream tasks, most are non-streamable,\nlimiting their use in real-time applications. We present FocalCodec-Stream, a\nhybrid codec based on focal modulation that compresses speech into a single\nbinary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our\napproach combines multi-stage causal distillation of WavLM with targeted\narchitectural improvements, including a lightweight refiner module that\nenhances quality under latency constraints. Experiments show that\nFocalCodec-Stream outperforms existing streamable codecs at comparable\nbitrates, while preserving both semantic and acoustic information. The result\nis a favorable trade-off between reconstruction quality, downstream task\nperformance, latency, and efficiency. Code and checkpoints will be released at\nhttps://github.com/lucadellalib/focalcodec.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16195v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16195v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.393,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a streaming neural audio codec for low-bitrate speech compression, using techniques like causal distillation and focal modulation. It does not involve diffusion models, iterative refinement for logical tasks, or any form of Chain-of-Thought reasoning. The core contributions are in audio processing and real-time applications, with no connection to multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16197",
      "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid\n  Vision Tokenizer",
      "authors": [
        "Yanghao Li",
        "Rui Qian",
        "Bowen Pan",
        "Haotian Zhang",
        "Haoshuo Huang",
        "Bowen Zhang",
        "Jialing Tong",
        "Haoxuan You",
        "Xianzhi Du",
        "Zhe Gan",
        "Hyunjik Kim",
        "Chao Jia",
        "Zhenbang Wang",
        "Yinfei Yang",
        "Mingfei Gao",
        "Zi-Yi Dou",
        "Wenze Hu",
        "Chang Gao",
        "Dongxu Li",
        "Philipp Dufter",
        "Zirui Wang",
        "Guoli Yin",
        "Zhengdong Zhang",
        "Chen Chen",
        "Yang Zhao",
        "Ruoming Pang",
        "Zhifeng Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Unified multimodal Large Language Models (LLMs) that can both understand and\ngenerate visual content hold immense potential. However, existing open-source\nmodels often suffer from a performance trade-off between these capabilities. We\npresent Manzano, a simple and scalable unified framework that substantially\nreduces this tension by coupling a hybrid image tokenizer with a well-curated\ntraining recipe. A single shared vision encoder feeds two lightweight adapters\nthat produce continuous embeddings for image-to-text understanding and discrete\ntokens for text-to-image generation within a common semantic space. A unified\nautoregressive LLM predicts high-level semantics in the form of text and image\ntokens, with an auxiliary diffusion decoder subsequently translating the image\ntokens into pixels. The architecture, together with a unified training recipe\nover understanding and generation data, enables scalable joint learning of both\ncapabilities. Manzano achieves state-of-the-art results among unified models,\nand is competitive with specialist models, particularly on text-rich\nevaluation. Our studies show minimal task conflicts and consistent gains from\nscaling model size, validating our design choice of a hybrid tokenizer.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16197v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16197v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.485,
      "distributed_training_score": 0.388,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MANZANO, a unified multimodal model that uses a diffusion decoder for translating image tokens into pixels during image generation. However, it does not adapt the diffusion process for multi-step logical reasoning, such as treating a Chain-of-Thought as a single entity for iterative correction. The diffusion component is solely for visual generation, not for solving complex logical tasks, so it does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16198",
      "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase\n  Generation",
      "authors": [
        "Jane Luo",
        "Xin Zhang",
        "Steven Liu",
        "Jie Wu",
        "Yiming Huang",
        "Yangyu Huang",
        "Chengyu Yin",
        "Ying Xin",
        "Jianfeng Liu",
        "Yuefeng Zhan",
        "Hao Sun",
        "Qi Chen",
        "Scarlett Li",
        "Mao Yang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Large language models excel at generating individual functions or single\nfiles of code, yet generating complete repositories from scratch remains a\nfundamental challenge. This capability is key to building coherent software\nsystems from high-level specifications and realizing the full potential of\nautomated code generation. The process requires planning at two levels:\ndeciding what features and modules to build (proposal stage) and defining their\nimplementation details (implementation stage). Current approaches rely on\nnatural language planning, which often produces unclear specifications,\nmisaligned components, and brittle designs due to its inherent ambiguity and\nlack of structure. To address these limitations, we introduce the Repository\nPlanning Graph (RPG), a structured representation that encodes capabilities,\nfile structures, data flows, and functions in a unified graph. By replacing\nfree-form natural language with an explicit blueprint, RPG enables consistent\nlong-horizon planning for repository generation. Building on RPG, we develop\nZeroRepo, a graph-driven framework that operates in three stages:\nproposal-level planning, implementation-level construction, and graph-guided\ncode generation with test validation. To evaluate, we construct RepoCraft, a\nbenchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo\nproduces nearly 36K Code Lines and 445K Code Tokens, on average 3.9$\\times$\nlarger than the strongest baseline (Claude Code), and 68$\\times$ larger than\nother baselines. It achieves 81.5% coverage and 69.7% test accuracy, improving\nover Claude Code by 27.3 and 35.8 points. Further analysis shows that RPG\nmodels complex dependencies, enables more sophisticated planning through\nnear-linear scaling, and improves agent understanding of repositories, thus\naccelerating localization.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16198v3",
      "pdf_url": "http://arxiv.org/pdf/2509.16198v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.4,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a graph-based framework for code generation and planning, focusing on structured representations like the Repository Planning Graph (RPG) for repository creation. It mentions iterative processes in code generation, such as proposal and implementation stages, but does not involve diffusion models, iterative refinement of a Chain-of-Thought as a single entity, or any adaptation of diffusion for logical reasoning tasks.",
      "distributed_training_justification": "The paper addresses code generation for software repositories using a graph-driven approach, with no discussion of distributed training, parallel computing, multi-node machine learning, or strategies for partitioning data/computation to accelerate model training. It focuses solely on automated codebase planning and generation, not on training algorithms or systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16288",
      "title": "Identifying Critical Pathways in Coronary Heart Disease via Fuzzy\n  Subgraph Connectivity",
      "authors": [
        "Shanookha Ali",
        "Nitha Niralda P C"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Coronary heart disease (CHD) arises from complex interactions among\nuncontrollable factors, controllable lifestyle factors, and clinical\nindicators, where relationships are often uncertain. Fuzzy subgraph\nconnectivity (FSC) provides a systematic tool to capture such imprecision by\nquantifying the strength of association between vertices and subgraphs in fuzzy\ngraphs. In this work, a fuzzy CHD graph is constructed with vertices for\nuncontrollable, controllable, and indicator components, and edges weighted by\nfuzzy memberships. Using FSC, we evaluate connectivity to identify strongest\ndiagnostic routes, dominant risk factors, and critical bridges. Results show\nthat FSC highlights influential pathways, bounds connectivity between weakest\nand strongest correlations, and reveals critical edges whose removal reduces\npredictive strength. Thus, FSC offers an interpretable and robust framework for\nmodeling uncertainty in CHD risk prediction and supporting clinical\ndecision-making.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16288v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16288v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.278,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.255,
      "datasets_score": 0.258,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16293",
      "title": "Robust LLM Training Infrastructure at ByteDance",
      "authors": [
        "Borui Wan",
        "Gaohong Liu",
        "Zuquan Song",
        "Jun Wang",
        "Yun Zhang",
        "Guangming Sheng",
        "Shuguang Wang",
        "Houmin Wei",
        "Chenyuan Wang",
        "Weiqiang Lou",
        "Xi Yang",
        "Mofan Zhang",
        "Kaihua Jiang",
        "Cheng Ren",
        "Xiaoyun Zhi",
        "Menghan Yu",
        "Zhe Nan",
        "Zhuolin Zheng",
        "Baoquan Zhong",
        "Qinlong Wang",
        "Huan Yu",
        "Jinxin Chi",
        "Wang Zhang",
        "Yuhan Li",
        "Zixian Du",
        "Sida Zhao",
        "Yongqiang Zhang",
        "Jingzhe Tang",
        "Zherui Liu",
        "Chuan Wu",
        "Yanghua Peng",
        "Haibin Lin",
        "Wencong Xiao",
        "Xin Liu",
        "Liang Xiang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform with over 200,000 GPUs and achieves 97% ETTR for a three-month\ntraining job on 9,600 GPUs.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16293v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16293v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.555,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a robust infrastructure system, ByteRobust, designed for large-scale distributed training of LLMs on tens of thousands of GPUs. It focuses on fault tolerance, failure detection, and recovery mechanisms in parallel computing environments, directly aligning with distributed training concepts such as partitioning computation across nodes, handling failures in multi-node setups, and optimizing for efficient parallel training. This makes it highly relevant to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces ByteRobust, a GPU infrastructure management system developed at ByteDance to enhance the robustness and stability of large-scale large language model (LLM) training by addressing failures such as CUDA errors, NaN values, and job hangs. It employs a methodology focused on rapid fault detection and isolation, hierarchical diagnostics, data-driven clustering, warm standbys, hot updates, and efficient checkpointing to minimize training interruptions and achieve high effective training time ratios (ETTR), with key findings demonstrating its deployment on over 200,000 GPUs, identification of thousands of failures, and up to 97% ETTR for a three-month job on 9,600 GPUs.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing fault tolerance techniques tailored specifically for LLM training challenges, such as implicit failures and evolving code, rather than introducing a entirely new problem or architecture. This approach offers a notable improvement in handling large-scale distributed training but remains an incremental refinement of established methods.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research and commercial applications in distributed AI training by providing practical solutions for maintaining stability at massive scales, which could be adopted by other organizations to improve efficiency in LLM development. Its real-world deployment and demonstrated results suggest it could lead to broader advancements in GPU infrastructure management.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, practical contribution to AI infrastructure for large-scale training, offering valuable techniques and insights that are particularly relevant for researchers and engineers in distributed systems. While essential for those in the field, it may not be universally critical for all readers.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/17f0ffa6295112997e758c686db6c7d4fa8071e6",
      "total_authors": 35,
      "authors_found": 33,
      "highest_h_index": 16,
      "average_h_index": 2.0606060606060606,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Borui Wan",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2158185137"
        },
        {
          "name": "Gaohong Liu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2329343961"
        },
        {
          "name": "Zuquan Song",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2306205535"
        },
        {
          "name": "Jun Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381317494"
        },
        {
          "name": "Yun Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2384059444"
        },
        {
          "name": "Guangming Sheng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2350756675"
        },
        {
          "name": "Shuguang Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2329109582"
        },
        {
          "name": "Houmin Wei",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2355119999"
        },
        {
          "name": "Chenyuan Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2356631127"
        },
        {
          "name": "Weiqiang Lou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356588152"
        },
        {
          "name": "Xi Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356798692"
        },
        {
          "name": "Mofan Zhang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2313688890"
        },
        {
          "name": "Kaihua Jiang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356398307"
        },
        {
          "name": "Cheng Ren",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381945207"
        },
        {
          "name": "Xiaoyun Zhi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356588132"
        },
        {
          "name": "Menghan Yu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2325463897"
        },
        {
          "name": "Zhe Nan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364802708"
        },
        {
          "name": "Zhuolin Zheng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2357543033"
        },
        {
          "name": "Baoquan Zhong",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2355650284"
        },
        {
          "name": "Qinlong Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356632404"
        },
        {
          "name": "Huan Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2358185473"
        },
        {
          "name": "Jinxin Chi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2356547901"
        },
        {
          "name": "Wang Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362245805"
        },
        {
          "name": "Yuhan Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2321038944"
        },
        {
          "name": "Zixian Du",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Sida Zhao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2360232000"
        },
        {
          "name": "Yongqiang Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2384055588"
        },
        {
          "name": "Jingzhe Tang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381944187"
        },
        {
          "name": "Zherui Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2287258718"
        },
        {
          "name": "Chuan Wu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2285416570"
        },
        {
          "name": "Yanghua Peng",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/9561490"
        },
        {
          "name": "Haibin Lin",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2267006156"
        },
        {
          "name": "Wencong Xiao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xin Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379976078"
        },
        {
          "name": "Liang Xiang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379610539"
        }
      ]
    },
    {
      "id": "2509.16295",
      "title": "Patterns in the Transition From Founder-Leadership to Community\n  Governance of Open Source",
      "authors": [
        "Mobina Noori",
        "Mahasweta Chakraborti",
        "Amy X Zhang",
        "Seth Frey"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Open digital public infrastructure needs community management to ensure\naccountability, sustainability, and robustness. Yet open-source projects often\nrely on centralized decision-making, and the determinants of successful\ncommunity management remain unclear. We analyze 637 GitHub repositories to\ntrace transitions from founder-led to shared governance. Specifically, we\ndocument trajectories to community governance by extracting institutional\nroles, actions, and deontic cues from version-controlled project constitutions\n(GOVERNANCE.md). With a semantic parsing pipeline, we cluster elements into\nbroader role and action types. We find roles and actions grow, and regulation\nbecomes more balanced, reflecting increases in governance scope and\ndifferentiation over time. Rather than shifting tone, communities grow by\nlayering and refining responsibilities. As transitions to community management\nmature, projects increasingly regulate ecosystem-level relationships and add\ndefinition to project oversight roles. Overall, this work offers a scalable\npipeline for tracking the growth and development of community governance\nregimes from open-source software's familiar default of founder-ownership.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16295v3",
      "pdf_url": "http://arxiv.org/pdf/2509.16295v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.283,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.293,
      "distributed_training_score": 0.27,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16297",
      "title": "How Large Language Models are Designed to Hallucinate",
      "authors": [
        "Richard Ackermann",
        "Simeon Emanuilov"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large language models (LLMs) achieve remarkable fluency across linguistic and\nreasoning tasks but remain systematically prone to hallucination. Prevailing\naccounts attribute hallucinations to data gaps, limited context, or\noptimization errors. We argue instead that hallucination is a structural\noutcome of the transformer architecture. As coherence engines, transformers are\ncompelled to produce fluent continuations, with self-attention simulating the\nrelational structure of meaning but lacking the existential grounding of\ntemporality, mood, and care that stabilizes human understanding. On this basis,\nwe distinguish ontological hallucination, arising when continuations require\ndisclosure of beings in world, and residual reasoning hallucination, where\nmodels mimic inference by recycling traces of human reasoning in text. We\nillustrate these patterns through case studies aligned with Heideggerian\ncategories and an experiment across twelve LLMs showing how simulated\n\"self-preservation\" emerges under extended prompts. Our contribution is\nthreefold: (1) a comparative account showing why existing explanations are\ninsufficient; (2) a predictive taxonomy of hallucination linked to existential\nstructures with proposed benchmarks; and (3) design directions toward\n\"truth-constrained\" architectures capable of withholding or deferring when\ndisclosure is absent. We conclude that hallucination is not an incidental\ndefect but a defining limit of transformer-based models, an outcome scaffolding\ncan mask but never resolve.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16297v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16297v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.502,
      "distributed_training_score": 0.349,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on the inherent design flaws of transformer architectures in LLMs that lead to hallucination, including theoretical analysis and experiments, but it does not mention or involve reinforcement learning, human feedback, reward models, or any alignment techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses hallucination in transformer-based LLMs and proposes a taxonomy and benchmarks, but it does not reference diffusion models, iterative refinement processes, or multi-step logical reasoning as described. It centers on transformer mechanics and existential structures instead.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16298",
      "title": "A global view of diverse construction methods of fuzzy implication\n  functions rooted on F-chains",
      "authors": [
        "Raquel Fernandez-Peralta",
        "Juan Vicente Riera"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Fuzzy implication functions are one of the most important operators used in\nthe fuzzy logic framework. While their flexible definition allows for diverse\nfamilies with distinct properties, this variety needs a deeper theoretical\nunderstanding of their structural relationships. In this work, we focus on the\nstudy of construction methods, which employ different techniques to generate\nnew fuzzy implication functions from existing ones. Particularly, we generalize\nthe $F$-chain-based construction, recently introduced by Mesiar et al. to\nextend a method for constructing aggregation functions to the context of fuzzy\nimplication functions. Our generalization employs collections of fuzzy\nimplication functions rather than single ones, and uses two different\nincreasing functions instead of a unique $F$-chain. We analyze property\npreservation under this construction and establish sufficient conditions.\nFurthermore, we demonstrate that our generalized $F$-chain-based construction\nis a unifying framework for several existing methods. In particular, we show\nthat various construction techniques, such as contraposition, aggregation, and\ngeneralized vertical/horizontal threshold methods, can be reformulated within\nour approach. This reveals structural similarities between seemingly distinct\nconstruction strategies and provides a cohesive perspective on fuzzy\nimplication construction methods.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16298v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16298v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.257,
      "weak_supervision_score": 0.251,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.209,
      "datasets_score": 0.226,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16299",
      "title": "On the Non-Uniqueness of Representation of $(U,N)$-Implications",
      "authors": [
        "Raquel Fernandez-Peralta",
        "Andrea Mesiarová-Zemánková"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Fuzzy implication functions constitute fundamental operators in fuzzy logic\nsystems, extending classical conditionals to manage uncertainty in logical\ninference. Among the extensive families of these operators, generalizations of\nthe classical material implication have received considerable theoretical\nattention, particularly $(S,N)$-implications constructed from t-conorms and\nfuzzy negations, and their further generalizations to $(U,N)$-implications\nusing disjunctive uninorms. Prior work has established characterization\ntheorems for these families under the assumption that the fuzzy negation $N$ is\ncontinuous, ensuring uniqueness of representation. In this paper, we disprove\nthis last fact for $(U,N)$-implications and we show that they do not\nnecessarily possess a unique representation, even if the fuzzy negation is\ncontinuous. Further, we provide a comprehensive study of uniqueness conditions\nfor both uninorms with continuous and non-continuous underlying functions. Our\nresults offer important theoretical insights into the structural properties of\nthese operators.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16299v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16299v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.236,
      "weak_supervision_score": 0.244,
      "diffusion_reasoning_score": 0.316,
      "distributed_training_score": 0.18,
      "datasets_score": 0.184,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16325",
      "title": "Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap",
      "authors": [
        "Andrew Zhu",
        "Chris Callison-Burch"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Imagine AI assistants that enhance conversations without interrupting them:\nquietly providing relevant information during a medical consultation,\nseamlessly preparing materials as teachers discuss lesson plans, or\nunobtrusively scheduling meetings as colleagues debate calendars. While modern\nconversational LLM agents directly assist human users with tasks through a chat\ninterface, we study this alternative paradigm for interacting with LLM agents,\nwhich we call \"overhearing agents.\" Rather than demanding the user's attention,\noverhearing agents continuously monitor ambient activity and intervene only\nwhen they can provide contextual assistance. In this paper, we present the\nfirst analysis of overhearing LLM agents as a distinct paradigm in human-AI\ninteraction and establish a taxonomy of overhearing agent interactions and\ntasks grounded in a survey of works on prior LLM-powered agents and exploratory\nHCI studies. Based on this taxonomy, we create a list of best practices for\nresearchers and developers building overhearing agent systems. Finally, we\noutline the remaining research gaps and reveal opportunities for future\nresearch in the overhearing paradigm.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16325v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16325v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.455,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.327,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on surveying, taxonomizing, and outlining best practices for overhearing LLM agents, emphasizing human-AI interaction paradigms. It does not discuss training AI models using human feedback, reinforcement learning, or any related techniques, making it unrelated to RLHF.",
      "weak_supervision_justification": "The paper examines overhearing agents through surveys, taxonomies, and HCI studies, without addressing machine learning approaches like using noisy or programmatically generated labels for training. It is centered on interaction design, not weak supervision methods.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16326",
      "title": "HARE: an entity and relation centric evaluation framework for\n  histopathology reports",
      "authors": [
        "Yunsoo Kim",
        "Michal W. S. Ong",
        "Alex Shavick",
        "Honghan Wu",
        "Adam P. Levine"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical domain automated text generation is an active area of research and\ndevelopment; however, evaluating the clinical quality of generated reports\nremains a challenge, especially in instances where domain-specific metrics are\nlacking, e.g. histopathology. We propose HARE (Histopathology Automated Report\nEvaluation), a novel entity and relation centric framework, composed of a\nbenchmark dataset, a named entity recognition (NER) model, a relation\nextraction (RE) model, and a novel metric, which prioritizes clinically\nrelevant content by aligning critical histopathology entities and relations\nbetween reference and generated reports. To develop the HARE benchmark, we\nannotated 813 de-identified clinical diagnostic histopathology reports and 652\nhistopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific\nentities and relations. We fine-tuned GatorTronS, a domain-adapted language\nmodel to develop HARE-NER and HARE-RE which achieved the highest overall\nF1-score (0.915) among the tested models. The proposed HARE metric outperformed\ntraditional metrics including ROUGE and Meteor, as well as radiology metrics\nsuch as RadGraph-XL, with the highest correlation and the best regression to\nexpert evaluations (higher than the second best method, GREEN, a large language\nmodel based radiology report evaluator, by Pearson $r = 0.168$, Spearman $\\rho\n= 0.161$, Kendall $\\tau = 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release\nHARE, datasets, and the models at https://github.com/knowlab/HARE to foster\nadvancements in histopathology report generation, providing a robust framework\nfor improving the quality of reports.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16326v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16326v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.303,
      "datasets_score": 0.398,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16330",
      "title": "Generalizability of Large Language Model-Based Agents: A Comprehensive\n  Survey",
      "authors": [
        "Minxing Zhang",
        "Yi Yang",
        "Roy Xie",
        "Bhuwan Dhingra",
        "Shuyan Zhou",
        "Jian Pei"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Model (LLM)-based agents have emerged as a new paradigm that\nextends LLMs' capabilities beyond text generation to dynamic interaction with\nexternal environments. By integrating reasoning with perception, memory, and\ntool use, agents are increasingly deployed in diverse domains like web\nnavigation and household robotics. A critical challenge, however, lies in\nensuring agent generalizability - the ability to maintain consistent\nperformance across varied instructions, tasks, environments, and domains,\nespecially those beyond agents' fine-tuning data. Despite growing interest, the\nconcept of generalizability in LLM-based agents remains underdefined, and\nsystematic approaches to measure and improve it are lacking. In this survey, we\nprovide the first comprehensive review of generalizability in LLM-based agents.\nWe begin by emphasizing agent generalizability's importance by appealing to\nstakeholders and clarifying the boundaries of agent generalizability by\nsituating it within a hierarchical domain-task ontology. We then review\ndatasets, evaluation dimensions, and metrics, highlighting their limitations.\nNext, we categorize methods for improving generalizability into three groups:\nmethods for the backbone LLM, for agent components, and for their interactions.\nMoreover, we introduce the distinction between generalizable frameworks and\ngeneralizable agents and outline how generalizable frameworks can be translated\ninto agent-level generalizability. Finally, we identify critical challenges and\nfuture directions, including developing standardized frameworks, variance- and\ncost-based metrics, and approaches that integrate methodological innovations\nwith architecture-level designs. By synthesizing progress and highlighting\nopportunities, this survey aims to establish a foundation for principled\nresearch on building LLM-based agents that generalize reliably across diverse\napplications.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16330v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16330v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.367,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a comprehensive survey of generalizability in LLM-based agents, including definitions, challenges, evaluation methods, and strategies for improvement across various components. It does not discuss or involve Reinforcement Learning from Human Feedback (RLHF), such as training with human-ranked data or using a reward model for fine-tuning. While LLMs may be trained via RLHF in general contexts, the paper's main contribution centers on agent generalizability, not alignment techniques like RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16332",
      "title": "Psychometric Personality Shaping Modulates Capabilities and Safety in\n  Language Models",
      "authors": [
        "Stephen Fitz",
        "Peter Romero",
        "Steven Basart",
        "Sipeng Chen",
        "Jose Hernandez-Orallo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large Language Models increasingly mediate high-stakes interactions,\nintensifying research on their capabilities and safety. While recent work has\nshown that LLMs exhibit consistent and measurable synthetic personality traits,\nlittle is known about how modulating these traits affects model behavior. We\naddress this gap by investigating how psychometric personality control grounded\nin the Big Five framework influences AI behavior in the context of capability\nand safety benchmarks. Our experiments reveal striking effects: for example,\nreducing conscientiousness leads to significant drops in safety-relevant\nmetrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well\nas reduction in general capabilities as measured by MMLU. These findings\nhighlight personality shaping as a powerful and underexplored axis of model\ncontrol that interacts with both safety and general competence. We discuss the\nimplications for safety evaluation, alignment strategies, steering model\nbehavior after deployment, and risks associated with possible exploitation of\nthese findings. Our findings motivate a new line of research on\npersonality-sensitive safety evaluations and dynamic behavioral control in\nLLMs.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16332v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16332v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.5,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.332,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution focuses on modulating personality traits in language models using psychometric prompting techniques and analyzing their effects on capabilities and safety benchmarks. It does not involve human feedback, reward models, or reinforcement learning for model alignment, which are core elements of RLHF. Therefore, there is no direct or indirect connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16336",
      "title": "Neural Atlas Graphs for Dynamic Scene Decomposition and Editing",
      "authors": [
        "Jan Philipp Schneider",
        "Pratik Singh Bisht",
        "Ilya Chugunov",
        "Andreas Kolb",
        "Michael Moeller",
        "Felix Heide"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Learning editable high-resolution scene representations for dynamic scenes is\nan open problem with applications across the domains from autonomous driving to\ncreative editing - the most successful approaches today make a trade-off\nbetween editability and supporting scene complexity: neural atlases represent\ndynamic scenes as two deforming image layers, foreground and background, which\nare editable in 2D, but break down when multiple objects occlude and interact.\nIn contrast, scene graph models make use of annotated data such as masks and\nbounding boxes from autonomous-driving datasets to capture complex 3D spatial\nrelationships, but their implicit volumetric node representations are\nchallenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a\nhybrid high-resolution scene representation, where every graph node is a\nview-dependent neural atlas, facilitating both 2D appearance editing and 3D\nordering and positioning of scene elements. Fit at test-time, NAGs achieve\nstate-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR\nincrease compared to existing methods - and make environmental editing possible\nin high resolution and visual quality - creating counterfactual driving\nscenarios with new backgrounds and edited vehicle appearance. We find that the\nmethod also generalizes beyond driving scenes and compares favorably - by more\nthan 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS\nvideo dataset with a diverse set of human and animal-centric scenes.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16336v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16336v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.348,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Neural Atlas Graphs for dynamic scene decomposition and editing, focusing on computer vision techniques like neural fields and scene graphs for applications in autonomous driving and video editing. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning processes as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16339",
      "title": "Highly Imbalanced Regression with Tabular Data in SEP and Other\n  Applications",
      "authors": [
        "Josias K. Moukpe",
        "Philip K. Chan",
        "Ming Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We investigate imbalanced regression with tabular data that have an imbalance\nratio larger than 1,000 (\"highly imbalanced\"). Accurately estimating the target\nvalues of rare instances is important in applications such as forecasting the\nintensity of rare harmful Solar Energetic Particle (SEP) events. For\nregression, the MSE loss does not consider the correlation between predicted\nand actual values. Typical inverse importance functions allow only convex\nfunctions. Uniform sampling might yield mini-batches that do not have rare\ninstances. We propose CISIR that incorporates correlation, Monotonically\nDecreasing Involution (MDI) importance, and stratified sampling. Based on five\ndatasets, our experimental results indicate that CISIR can achieve lower error\nand higher correlation than some recent methods. Also, adding our correlation\ncomponent to other recent methods can improve their performance. Lastly, MDI\nimportance can outperform other importance functions. Our code can be found in\nhttps://github.com/Machine-Earning/CISIR.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16339v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16339v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.364,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16343",
      "title": "Agentic Reasoning for Robust Vision Systems via Increased Test-Time\n  Compute",
      "authors": [
        "Chung-En",
        "Yu",
        "Brian Jalaian",
        "Nathaniel D. Bastian"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Developing trustworthy intelligent vision systems for high-stakes domains,\n\\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness\nwithout costly retraining. We propose \\textbf{Visual Reasoning Agent (VRA)}, a\ntraining-free, agentic reasoning framework that wraps off-the-shelf\nvision-language models \\emph{and} pure vision systems in a\n\\emph{Think--Critique--Act} loop. While VRA incurs significant additional\ntest-time computation, it achieves up to 40\\% absolute accuracy gains on\nchallenging visual reasoning benchmarks. Future work will optimize query\nrouting and early stopping to reduce inference overhead while preserving\nreliability in vision tasks.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16343v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16343v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.361,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a training-free agentic framework for vision systems, focusing on a Think-Critique-Act loop without any involvement of human feedback, reward models, or reinforcement learning techniques. It relies on off-the-shelf models and self-correction, which does not align with RLHF's core elements of using human-ranked data to train a reward model for fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an iterative Think-Critique-Act loop for visual reasoning, but it does not adapt or use diffusion models for multi-step logical tasks or holistic correction of a Chain-of-Thought. Instead, it employs agentic AI with existing vision-language models, lacking any reference to diffusion-based processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16345",
      "title": "Estimating Clinical Lab Test Result Trajectories from PPG using\n  Physiological Foundation Model and Patient-Aware State Space Model -- a\n  UNIPHY+ Approach",
      "authors": [
        "Minxiao Wang",
        "Runze Yan",
        "Carol Li",
        "Saurabh Kataria",
        "Xiao Hu",
        "Matthew Clark",
        "Timothy Ruchti",
        "Timothy G. Buchman",
        "Sivasubramanium V Bhavani",
        "Randall J. Lee"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Clinical laboratory tests provide essential biochemical measurements for\ndiagnosis and treatment, but are limited by intermittent and invasive sampling.\nIn contrast, photoplethysmogram (PPG) is a non-invasive, continuously recorded\nsignal in intensive care units (ICUs) that reflects cardiovascular dynamics and\ncan serve as a proxy for latent physiological changes. We propose UNIPHY+Lab, a\nframework that combines a large-scale PPG foundation model for local waveform\nencoding with a patient-aware Mamba model for long-range temporal modeling. Our\narchitecture addresses three challenges: (1) capturing extended temporal trends\nin laboratory values, (2) accounting for patient-specific baseline variation\nvia FiLM-modulated initial states, and (3) performing multi-task estimation for\ninterrelated biomarkers. We evaluate our method on the two ICU datasets for\npredicting the five key laboratory tests. The results show substantial\nimprovements over the LSTM and carry-forward baselines in MAE, RMSE, and $R^2$\namong most of the estimation targets. This work demonstrates the feasibility of\ncontinuous, personalized lab value estimation from routine PPG monitoring,\noffering a pathway toward non-invasive biochemical surveillance in critical\ncare.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16345v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16345v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.341,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16346",
      "title": "From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation\n  of 3D Forest Structure from Aerial-to-Terrestrial LiDAR",
      "authors": [
        "Juan Castorena",
        "E. Louise Loudermilk",
        "Scott Pokswinski",
        "Rodman Linn"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The 3D structure of living and non-living components in ecosystems plays a\ncritical role in determining ecological processes and feedbacks from both\nnatural and human-driven disturbances. Anticipating the effects of wildfire,\ndrought, disease, or atmospheric deposition depends on accurate\ncharacterization of 3D vegetation structure, yet widespread measurement remains\nprohibitively expensive and often infeasible. We introduce ForestGen3D, a novel\ngenerative modeling framework that synthesizes high-fidelity 3D forest\nstructure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on\nconditional denoising diffusion probabilistic models (DDPMs) trained on\nco-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate\nTLS-like 3D point clouds conditioned on sparse ALS observations, effectively\nreconstructing occluded sub-canopy detail at scale. To ensure ecological\nplausibility, we introduce a geometric containment prior based on the convex\nhull of ALS observations and provide theoretical and empirical guarantees that\ngenerated structures remain spatially consistent. We evaluate ForestGen3D at\ntree, plot, and landscape scales using real-world data from mixed conifer\necosystems, and show that it produces high-fidelity reconstructions that\nclosely match TLS references in terms of geometric similarity and biophysical\nmetrics, such as tree height, DBH, crown diameter and crown volume.\nAdditionally, we demonstrate that the containment property can serve as a\npractical proxy for generation quality in settings where TLS ground truth is\nunavailable. Our results position ForestGen3D as a scalable tool for ecological\nmodeling, wildfire simulation, and structural fuel characterization in ALS-only\nenvironments.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16346v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16346v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.317,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16347",
      "title": "QUINTA: Reflexive Sensibility For Responsible AI Research and\n  Data-Driven Processes",
      "authors": [
        "Alicia E. Boyd"
      ],
      "categories": [
        "cs.SI (Social and Information Networks)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As the field of artificial intelligence (AI) and machine learning (ML)\ncontinues to prioritize fairness and the concern for historically marginalized\ncommunities, the importance of intersectionality in AI research has gained\nsignificant recognition. However, few studies provide practical guidance on how\nresearchers can effectively incorporate intersectionality into critical praxis.\nIn response, this paper presents a comprehensive framework grounded in critical\nreflexivity as intersectional praxis. Operationalizing intersectionality within\nthe AI/DS (Artificial Intelligence/Data Science) pipeline, Quantitative\nIntersectional Data (QUINTA) is introduced as a methodological paradigm that\nchallenges conventional and superficial research habits, particularly in\ndata-centric processes, to identify and mitigate negative impacts such as the\ninadvertent marginalization caused by these practices. The framework centers\nresearcher reflexivity to call attention to the AI researchers' power in\ncreating and analyzing AI/DS artifacts through data-centric approaches. To\nillustrate the effectiveness of QUINTA, we provide a reflexive AI/DS researcher\ndemonstration utilizing the \\#metoo movement as a case study. Note: This paper\nwas accepted as a poster presentation at Equity and Access in Algorithms,\nMechanisms, and Optimization (EAAMO) Conference in 2023.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16347v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16347v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.357,
      "datasets_score": 0.445,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper introduces a framework for reflexive sensibility in AI research, focusing on intersectionality and data processes, but does not involve training AI models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper addresses data-centric processes in AI/DS pipelines, including analyzing and mitigating biases in data handling for fairness, as part of the QUINTA framework, though it does not primarily focus on creating, benchmarking, or evaluating datasets.",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"QUINTA: Reflexive Sensibility For Responsible AI Research and Data-Driven Processes,\" introduces a framework called QUINTA to operationalize intersectionality in AI and data science research, emphasizing critical reflexivity to address and mitigate negative impacts on marginalized communities. By challenging conventional data-centric practices, the methodology centers on researcher reflexivity and demonstrates its application through a case study on the #metoo movement, aiming to provide practical guidance for incorporating intersectionality into the AI/DS pipeline and highlighting the researcher's role in ethical AI development.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents QUINTA as a clever combination of existing ideas on intersectionality and reflexivity to offer practical guidance in AI research, representing a notable improvement over superficial approaches rather than a completely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "This work is likely to influence future research in AI ethics and fairness by providing a framework for responsible practices, potentially leading to citations and applications within subfields like social and information networks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "As a significant contribution to responsible AI and intersectionality, the paper offers valuable methodological insights that AI researchers focused on ethics should be aware of, though it may not be essential for those outside this niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ef0b5560fe681578e0df3fe833a0146f2260fb8c",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Alicia E. Boyd",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381362641"
        }
      ]
    },
    {
      "id": "2509.16348",
      "title": "A Unified AI Approach for Continuous Monitoring of Human Health and\n  Diseases from Intensive Care Unit to Home with Physiological Foundation\n  Models (UNIPHY+)",
      "authors": [
        "Minxiao Wang",
        "Saurabh Kataria",
        "Juntong Ni",
        "Timothy G. Buchman",
        "Jocelyn Grunwell",
        "Mark Mai",
        "Wei Jin",
        "Matthew Clark",
        "Stephanie Brown",
        "Michael Fundora",
        "Puneet Sharma",
        "Tony Pan",
        "Sam Khan",
        "Timothy Ruchti",
        "Naveen Muthu",
        "Kevin Maher",
        "Sivasubramanium V Bhavani",
        "Xiao Hu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present UNIPHY+, a unified physiological foundation model (physioFM)\nframework designed to enable continuous human health and diseases monitoring\nacross care settings using ubiquitously obtainable physiological data. We\npropose novel strategies for incorporating contextual information during\npretraining, fine-tuning, and lightweight model personalization via multi-modal\nlearning, feature fusion-tuning, and knowledge distillation. We advocate\ntesting UNIPHY+ with a broad set of use cases from intensive care to ambulatory\nmonitoring in order to demonstrate that UNIPHY+ can empower generalizable,\nscalable, and personalized physiological AI to support both clinical\ndecision-making and long-term health monitoring.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16348v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16348v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.443,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.395,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing a unified physiological foundation model (UNIPHY+) for health monitoring, emphasizing pretraining, fine-tuning, and personalization techniques like multi-modal learning and knowledge distillation. It does not involve reinforcement learning, human feedback, or training a reward model based on human-ranked data, which are core to RLHF. Therefore, there is no connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16352",
      "title": "Secure Confidential Business Information When Sharing Machine Learning\n  Models",
      "authors": [
        "Yunfan Yang",
        "Jiarong Xu",
        "Hongzhe Zhang",
        "Xiao Fang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Model-sharing offers significant business value by enabling firms with\nwell-established Machine Learning (ML) models to monetize and share their\nmodels with others who lack the resources to develop ML models from scratch.\nHowever, concerns over data confidentiality remain a significant barrier to\nmodel-sharing adoption, as Confidential Property Inference (CPI) attacks can\nexploit shared ML models to uncover confidential properties of the model\nprovider's private model training data. Existing defenses often assume that CPI\nattacks are non-adaptive to the specific ML model they are targeting. This\nassumption overlooks a key characteristic of real-world adversaries: their\nresponsiveness, i.e., adversaries' ability to dynamically adjust their attack\nmodels based on the information of the target and its defenses. To overcome\nthis limitation, we propose a novel defense method that explicitly accounts for\nthe responsive nature of real-world adversaries via two methodological\ninnovations: a novel Responsive CPI attack and an attack-defense arms race\nframework. The former emulates the responsive behaviors of adversaries in the\nreal world, and the latter iteratively enhances both the target and attack\nmodels, ultimately producing a secure ML model that is robust against\nresponsive CPI attacks. Furthermore, we propose and integrate a novel\napproximate strategy into our defense, which addresses a critical computational\nbottleneck of defense methods and improves defense efficiency. Through\nextensive empirical evaluations across various realistic model-sharing\nscenarios, we demonstrate that our method outperforms existing defenses by more\neffectively defending against CPI attacks, preserving ML model utility, and\nreducing computational overhead.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16352v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16352v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.394,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16363",
      "title": "Introducing Resizable Region Packing Problem in Image Generation, with a\n  Heuristic Solution",
      "authors": [
        "Hrishikesh Sharma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The problem of image data generation in computer vision has traditionally\nbeen a harder problem to solve, than discriminative problems. Such data\ngeneration entails placing relevant objects of appropriate sizes each, at\nmeaningful location in a scene canvas. There have been two classes of popular\napproaches to such generation: graphics based, and generative models-based.\nOptimization problems are known to lurk in the background for both these\nclasses of approaches. In this paper, we introduce a novel, practically useful\nmanifestation of the classical Bin Packing problem in the context of generation\nof synthetic image data. We conjecture that the newly introduced problem,\nResizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide\ndetailed arguments about our conjecture. As a first solution, we present a\nnovel heuristic algorithm that is generic enough and therefore scales and packs\narbitrary number of arbitrary-shaped regions at arbitrary locations, into an\nimage canvas. The algorithm follows greedy approach to iteratively pack region\npairs in a careful way, while obeying the optimization constraints. The\nalgorithm is validated by an implementation that was used to generate a\nlarge-scale synthetic anomaly detection dataset, with highly varying degree of\nbin packing parameters per image sample i.e. RARP instance. Visual inspection\nof such data and checking of the correctness of each solution proves the\neffectiveness of our algorithm. With generative modeling being on rise in deep\nlearning, and synthetic data generation poised to become mainstream, we expect\nthat the newly introduced problem will be valued in the imaging scientific\ncommunity.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16363v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16363v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.323,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16369",
      "title": "Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach\n  to Knowledge Retrieval and Hallucination Reduction",
      "authors": [
        "Akshay Govind Srinivasan",
        "Ryan Jacob George",
        "Jayden Koshy Joe",
        "Hrushikesh Kant",
        "Harshith M R",
        "Sachin Sundar",
        "Sudharshan Suresh",
        "Rahul Vimalkanth",
        "Vijayavallabh"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)"
      ],
      "abstract": "Accurate and reliable knowledge retrieval is vital for financial\nquestion-answering, where continually updated data sources and complex,\nhigh-stakes contexts demand precision. Traditional retrieval systems rely on a\nsingle database and retriever, but financial applications require more\nsophisticated approaches to handle intricate regulatory filings, market\nanalyses, and extensive multi-year reports. We introduce a framework for\nfinancial Retrieval Augmented Generation (RAG) that leverages agentic AI and\nthe Multi-HyDE system, an approach that generates multiple, nonequivalent\nqueries to boost the effectiveness and coverage of retrieval from large,\nstructured financial corpora. Our pipeline is optimized for token efficiency\nand multi-step financial reasoning, and we demonstrate that their combination\nimproves accuracy by 11.2% and reduces hallucinations by 15%. Our method is\nevaluated on standard financial QA benchmarks, showing that integrating\ndomain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets,\nincluding keyword and table-based retrieval, significantly enhances both the\naccuracy and reliability of answers. This research not only delivers a modular,\nadaptable retrieval framework for finance but also highlights the importance of\nstructured agent workflows and multi-perspective retrieval for trustworthy\ndeployment of AI in high-stakes financial applications.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16369v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16369v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.464,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.469,
      "distributed_training_score": 0.351,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on enhancing Retrieval Augmented Generation (RAG) for financial applications using agentic AI and Multi-HyDE, emphasizing retrieval accuracy and hallucination reduction. It does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses multi-step reasoning in agentic AI systems for financial QA, but it does not adapt diffusion processes for iterative refinement or treat reasoning paths holistically as described in diffusion-based models. There is no mention of diffusion models or related mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16372",
      "title": "Evaluation of Causal Reasoning for Large Language Models in\n  Contextualized Clinical Scenarios of Laboratory Test Interpretation",
      "authors": [
        "Balu Bhasuran",
        "Mattia Prosperi",
        "Karim Hanna",
        "John Petrilli",
        "Caretia JeLayne Washington",
        "Zhe He"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study evaluates causal reasoning in large language models (LLMs) using\n99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of\nCausation: association, intervention, and counterfactual reasoning. We examined\ncommon laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and\npaired them with relevant causal factors including age, gender, obesity, and\nsmoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with\nresponses evaluated by four medically trained human experts. GPT-o1\ndemonstrated stronger discriminative performance (AUROC overall = 0.80 +/-\n0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores\nacross association (0.75 vs 0.72), intervention (0.84 vs 0.70), and\ncounterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and\nspecificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings\nshowing similar trends. Both models performed best on intervention questions\nand worst on counterfactuals, particularly in altered outcome scenarios. These\nfindings suggest GPT-o1 provides more consistent causal reasoning, but\nrefinement is required before adoption in high-stakes clinical applications.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16372v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16372v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.285,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating causal reasoning in LLMs using clinical scenarios, specifically testing models like GPT-o1 and Llama-3.2 on association, intervention, and counterfactual tasks. It does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16382",
      "title": "Accurate Thyroid Cancer Classification using a Novel Binary Pattern\n  Driven Local Discrete Cosine Transform Descriptor",
      "authors": [
        "Saurabh Saini",
        "Kapil Ahuja",
        "Marc C. Steinbach",
        "Thomas Wick"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "In this study, we develop a new CAD system for accurate thyroid cancer\nclassification with emphasis on feature extraction. Prior studies have shown\nthat thyroid texture is important for segregating the thyroid ultrasound images\ninto different classes. Based upon our experience with breast cancer\nclassification, we first conjuncture that the Discrete Cosine Transform (DCT)\nis the best descriptor for capturing textural features. Thyroid ultrasound\nimages are particularly challenging as the gland is surrounded by multiple\ncomplex anatomical structures leading to variations in tissue density. Hence,\nwe second conjuncture the importance of localization and propose that the Local\nDCT (LDCT) descriptor captures the textural features best in this context.\nAnother disadvantage of complex anatomy around the thyroid gland is scattering\nof ultrasound waves resulting in noisy and unclear textures. Hence, we third\nconjuncture that one image descriptor is not enough to fully capture the\ntextural features and propose the integration of another popular texture\ncapturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is\nknown to be noise resilient as well. We term our novel descriptor as Binary\nPattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification\nis carried out using a non-linear SVM. The proposed CAD system is evaluated on\nthe only two publicly available thyroid cancer datasets, namely TDID and AUITD.\nThe evaluation is conducted in two stages. In Stage I, thyroid nodules are\ncategorized as benign or malignant. In Stage II, the malignant cases are\nfurther sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I\nclassification, our proposed model demonstrates exceptional performance of\nnearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed\nmodel again attains excellent classification of close to 100% on TDID and 99%\non AUITD.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16382v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16382v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.217,
      "weak_supervision_score": 0.268,
      "diffusion_reasoning_score": 0.28,
      "distributed_training_score": 0.268,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16391",
      "title": "CoUn: Empowering Machine Unlearning via Contrastive Learning",
      "authors": [
        "Yasser H. Khalil",
        "Mehdi Setayesh",
        "Hongliang Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Machine unlearning (MU) aims to remove the influence of specific \"forget\"\ndata from a trained model while preserving its knowledge of the remaining\n\"retain\" data. Existing MU methods based on label manipulation or model weight\nperturbations often achieve limited unlearning effectiveness. To address this,\nwe introduce CoUn, a novel MU framework inspired by the observation that a\nmodel retrained from scratch using only retain data classifies forget data\nbased on their semantic similarity to the retain data. CoUn emulates this\nbehavior by adjusting learned data representations through contrastive learning\n(CL) and supervised learning, applied exclusively to retain data. Specifically,\nCoUn (1) leverages semantic similarity between data samples to indirectly\nadjust forget representations using CL, and (2) maintains retain\nrepresentations within their respective clusters through supervised learning.\nExtensive experiments across various datasets and model architectures show that\nCoUn consistently outperforms state-of-the-art MU baselines in unlearning\neffectiveness. Additionally, integrating our CL module into existing baselines\nempowers their unlearning effectiveness.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16391v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16391v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.444,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.373,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on machine unlearning using contrastive learning and supervised learning to adjust data representations, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper does not involve training models with programmatically generated, noisy, or imprecise labels; instead, it uses standard retain data for supervised and contrastive learning in the machine unlearning process.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16394",
      "title": "Evaluating Behavioral Alignment in Conflict Dialogue: A\n  Multi-Dimensional Comparison of LLM Agents and Humans",
      "authors": [
        "Deuksin Kwon",
        "Kaleen Shrestha",
        "Bin Han",
        "Elena Hayoung Lee",
        "Gale Lucas"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in socially complex,\ninteraction-driven tasks, yet their ability to mirror human behavior in\nemotionally and strategically complex contexts remains underexplored. This\nstudy assesses the behavioral alignment of personality-prompted LLMs in\nadversarial dispute resolution by simulating multi-turn conflict dialogues that\nincorporate negotiation. Each LLM is guided by a matched Five-Factor\npersonality profile to control for individual variation and enhance realism. We\nevaluate alignment across three dimensions: linguistic style, emotional\nexpression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the\nclosest alignment with humans in linguistic style and emotional dynamics, while\nClaude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial\nalignment gaps persist. Our findings establish a benchmark for alignment\nbetween LLMs and humans in socially complex interactions, underscoring both the\npromise and the limitations of personality conditioning in dialogue modeling.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16394v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16394v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.485,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.318,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating the behavioral alignment of existing LLMs in conflict dialogues using personality prompts and benchmarks, without any discussion of training models via reinforcement learning from human feedback. It does not involve creating a reward model, fine-tuning with human-ranked data, or any RLHF processes, making it unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16397",
      "title": "GRID: Graph-based Reasoning for Intervention and Discovery in Built\n  Environments",
      "authors": [
        "Taqiya Ehsan",
        "Shuren Xia",
        "Jorge Ortiz"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Manual HVAC fault diagnosis in commercial buildings takes 8-12 hours per\nincident and achieves only 60 percent diagnostic accuracy, reflecting analytics\nthat stop at correlation instead of causation. To close this gap, we present\nGRID (Graph-based Reasoning for Intervention and Discovery), a three-stage\ncausal discovery pipeline that combines constraint-based search, neural\nstructural equation modeling, and language model priors to recover directed\nacyclic graphs from building sensor data. Across six benchmarks: synthetic\nrooms, EnergyPlus simulation, the ASHRAE Great Energy Predictor III dataset,\nand a live office testbed, GRID achieves F1 scores ranging from 0.65 to 1.00,\nwith exact recovery (F1 = 1.00) in three controlled environments (Base, Hidden,\nPhysical) and strong performance on real-world data (F1 = 0.89 on ASHRAE, 0.86\nin noisy conditions). The method outperforms ten baseline approaches across all\nevaluation scenarios. Intervention scheduling achieves low operational impact\nin most scenarios (cost <= 0.026) while reducing risk metrics compared to\nbaseline approaches. The framework integrates constraint-based methods, neural\narchitectures, and domain-specific language model prompts to address the\nobservational-causal gap in building analytics.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16397v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16397v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.364,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents GRID, a framework for causal discovery in building environments using a three-stage pipeline involving constraint-based search, neural structural equation modeling, and language model priors. While it includes an iterative process for refining causal graphs, it does not adapt or utilize diffusion models for multi-step logical reasoning, such as treating a Chain-of-Thought as a holistically corrected entity through iterative denoising. Thus, it lacks any clear component of diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16399",
      "title": "VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided\n  Reward Shaping",
      "authors": [
        "Guojun Xiong",
        "Milind Tambe"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In social impact optimization, AI decision systems often rely on solvers that\noptimize well-calibrated mathematical objectives. However, these solvers cannot\ndirectly accommodate evolving human preferences, typically expressed in natural\nlanguage rather than formal constraints. Recent approaches address this by\nusing large language models (LLMs) to generate new reward functions from\npreference descriptions. While flexible, they risk sacrificing the system's\ncore utility guarantees. In this paper, we propose \\texttt{VORTEX}, a\nlanguage-guided reward shaping framework that preserves established\noptimization goals while adaptively incorporating human feedback. By\nformalizing the problem as multi-objective optimization, we use LLMs to\niteratively generate shaping rewards based on verbal reinforcement and\ntext-gradient prompt updates. This allows stakeholders to steer decision\nbehavior via natural language without modifying solvers or specifying trade-off\nweights. We provide theoretical guarantees that \\texttt{VORTEX} converges to\nPareto-optimal trade-offs between utility and preference satisfaction.\nEmpirical results in real-world allocation tasks demonstrate that\n\\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage\ngoals while maintaining high task performance. This work introduces a practical\nand theoretically grounded paradigm for human-AI collaborative optimization\nguided by natural language.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16399v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16399v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.605,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.38,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper involves using human feedback, such as verbal reinforcement, to iteratively refine reward functions via LLMs in a multi-objective optimization setup. This aligns with RLHF's core idea of incorporating human preferences to guide AI behavior, but it focuses on reward shaping for existing solvers rather than training a reward model and fine-tuning a main model via reinforcement learning. Thus, it shares conceptual similarities without fully matching the standard RLHF pipeline.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an iterative framework for refining rewards using LLMs and human feedback, but it does not involve diffusion models, multi-step logical reasoning through a Chain-of-Thought process, or any adaptation of diffusion techniques for reasoning tasks. The iteration is for reward shaping in optimization, not for holistic correction of reasoning paths as in diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces VORTEX, a framework designed to integrate human preferences expressed in natural language into AI optimization systems without compromising core task utility. It achieves this by formulating the problem as multi-objective optimization, using large language models (LLMs) to iteratively generate and refine shaping rewards based on verbal feedback, and provides theoretical guarantees of convergence to Pareto-optimal trade-offs. Empirical results in real-world resource allocation tasks show that VORTEX outperforms baselines by effectively balancing preference satisfaction and high task performance.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing ideas, such as LLMs for reward generation and multi-objective optimization, to address the challenge of incorporating natural language preferences into established AI systems in a new way. While it advances the field, it builds on prior work in human-AI alignment rather than introducing a entirely novel problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI ethics and decision-making, particularly for applications in social impact areas such as public health and conservation. However, its influence may be limited to specific domains rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a strong, valuable contribution with theoretical and empirical insights into human-AI collaboration, making it important for researchers in AI optimization and related fields. While not essential for all, it provides practical advancements that warrant attention from those interested in aligning AI with human preferences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/51d8a202334ca4067d637587f97ac56f71ccc98a",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 7,
      "average_h_index": 4.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Guojun Xiong",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2325728161"
        },
        {
          "name": "Milind Tambe",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2288282318"
        }
      ]
    },
    {
      "id": "2509.16413",
      "title": "Pico: A Modular Framework for Hypothesis-Driven Small Language Model\n  Research",
      "authors": [
        "Richard Diehl Martinez",
        "David Demitri Africa",
        "Yuval Weiss",
        "Suchir Salhan",
        "Ryan Daniels",
        "Paula Buttery"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Building language models (LMs), especially small and medium ones, remains\nmore art than science. While large LMs often improve by sheer scale, it is\nstill unclear why many design choices work. For small LMs, this uncertainty is\nmore limiting: tight parameter budgets make each decision critical, yet\nresearchers still lack systematic, scientific ways to test and refine new\nideas.\n  We introduce Pico, a lightweight, modular framework that enables systematic,\nhypothesis-driven research for small and medium-scale language model\ndevelopment. Pico consists of two libraries that together provide a practical\nsandbox where researchers can make targeted changes to a model's architecture\nor training procedures and directly observe their effects on the model's\nbehavior. To support reproducible experimentation, we also release a suite of\nbaseline models, pico-decoder, trained under standardized conditions and\nopen-sourced for the community. Case studies highlight how Pico can support\niterative small LM design and analysis.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16413v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16413v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.424,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Pico, a framework for systematic training and analysis of small language models, focusing on modular architecture and hypothesis-driven experimentation. It does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning or Chain-of-Thought correction. Therefore, it lacks any connection to this topic.",
      "distributed_training_justification": "The paper describes Pico as a lightweight framework for small-scale language model research, emphasizing academic-GPU scale and low-budget training, but it does not discuss distributed training algorithms, parallel computing strategies, multi-node setups, or partitioning of data/computation across processors. Its focus is on single-model experimentation rather than distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16415",
      "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes",
      "authors": [
        "Zhengri Wu",
        "Yiran Wang",
        "Yu Wen",
        "Zeyu Zhang",
        "Biao Wu",
        "Hao Tang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Underwater stereo depth estimation provides accurate 3D geometry for robotics\ntasks such as navigation, inspection, and mapping, offering metric depth from\nlow-cost passive cameras while avoiding the scale ambiguity of monocular\nmethods. However, existing approaches face two critical challenges: (i)\nparameter-efficiently adapting large vision foundation encoders to the\nunderwater domain without extensive labeled data, and (ii) tightly fusing\nglobally coherent but scale-ambiguous monocular priors with locally metric yet\nphotometrically fragile stereo correspondences. To address these challenges, we\npropose StereoAdapter, a parameter-efficient self-supervised framework that\nintegrates a LoRA-adapted monocular foundation encoder with a recurrent stereo\nrefinement module. We further introduce dynamic LoRA adaptation for efficient\nrank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to\nenhance robustness under diverse underwater conditions. Comprehensive\nevaluations on both simulated and real-world benchmarks show improvements of\n6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,\nwhile real-world deployment with the BlueROV2 robot further demonstrates the\nconsistent robustness of our approach. Code:\nhttps://github.com/AIGeeksGroup/StereoAdapter. Website:\nhttps://aigeeksgroup.github.io/StereoAdapter.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16415v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16415v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.331,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16418",
      "title": "LenslessMic: Audio Encryption and Authentication via Lensless\n  Computational Imaging",
      "authors": [
        "Petr Grinberg",
        "Eric Bezzam",
        "Paolo Prandoni",
        "Martin Vetterli"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "With society's increasing reliance on digital data sharing, the protection of\nsensitive information has become critical. Encryption serves as one of the\nprivacy-preserving methods; however, its realization in the audio domain\npredominantly relies on signal processing or software methods embedded into\nhardware. In this paper, we introduce LenslessMic, a hybrid optical\nhardware-based encryption method that utilizes a lensless camera as a physical\nlayer of security applicable to multiple types of audio. We show that\nLenslessMic enables (1) robust authentication of audio recordings and (2)\nencryption strength that can rival the search space of 256-bit digital\nstandards, while maintaining high-quality signals and minimal loss of content\ninformation. The approach is validated with a low-cost Raspberry Pi prototype\nand is open-sourced together with datasets to facilitate research in the area.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16418v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16418v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.307,
      "distributed_training_score": 0.244,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16421",
      "title": "AHA -- Predicting What Matters Next: Online Highlight Detection Without\n  Looking Ahead",
      "authors": [
        "Aiden Chang",
        "Celso De Melo",
        "Stephanie M. Lukin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Real-time understanding of continuous video streams is essential for\nintelligent agents operating in high-stakes environments, including autonomous\nvehicles, surveillance drones, and disaster response robots. Yet, most existing\nvideo understanding and highlight detection methods assume access to the entire\nvideo during inference, making them unsuitable for online or streaming\nscenarios. In particular, current models optimize for offline summarization,\nfailing to support step-by-step reasoning needed for real-time decision-making.\nWe introduce Aha, an autoregressive highlight detection framework that predicts\nthe relevance of each video frame against a task described in natural language.\nWithout accessing future video frames, Aha utilizes a multimodal\nvision-language model and lightweight, decoupled heads trained on a large,\ncurated dataset of human-centric video labels. To enable scalability, we\nintroduce the Dynamic SinkCache mechanism that achieves constant memory usage\nacross infinite-length streams without degrading performance on standard\nbenchmarks. This encourages the hidden representation to capture high-level\ntask objectives, enabling effective frame-level rankings for informativeness,\nrelevance, and uncertainty with respect to the natural language task. Aha\nachieves state-of-the-art (SOTA) performance on highlight detection benchmarks,\nsurpassing even prior offline, full-context approaches and video-language\nmodels by +5.9% on TVSum and +8.3% on Mr. Hisum in mAP (mean Average\nPrecision). We explore Aha's potential for real-world robotics applications\ngiven a task-oriented natural language input and a continuous, robot-centric\nvideo. Both experiments demonstrate Aha's potential effectiveness as a\nreal-time reasoning module for downstream planning and long-horizon\nunderstanding.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16421v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16421v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.48,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.368,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a framework for online highlight detection using a dataset with human-centric labels, but it does not involve training a reward model or using reinforcement learning to fine-tune the model based on human preferences. Instead, it relies on supervised training with curated data, which does not align with RLHF's core elements.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's Aha framework uses an autoregressive approach for real-time video analysis, but it does not incorporate diffusion models or iterative refinement processes for multi-step logical reasoning. There is no mention of treating a chain-of-thought as an entity for holistic correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16423",
      "title": "3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction",
      "authors": [
        "Maria Taktasheva",
        "Lily Goli",
        "Alessandro Fiorini",
        "Zhen Li",
        "Daniel Rebain",
        "Andrea Tagliasacchi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in radiance fields and novel view synthesis enable creation\nof realistic digital twins from photographs. However, current methods struggle\nwith flat, texture-less surfaces, creating uneven and semi-transparent\nreconstructions, due to an ill-conditioned photometric reconstruction\nobjective. Surface reconstruction methods solve this issue but sacrifice visual\nquality. We propose a novel hybrid 2D/3D representation that jointly optimizes\nconstrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D)\nGaussians for the rest of the scene. Our end-to-end approach dynamically\ndetects and refines planar regions, improving both visual fidelity and\ngeometric accuracy. It achieves state-of-the-art depth estimation on ScanNet++\nand ScanNetv2, and excels at mesh extraction without overfitting to a specific\ncamera model, showing its effectiveness in producing high-quality\nreconstruction of indoor scenes.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16423v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16423v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.289,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.316,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16429",
      "title": "TractoTransformer: Diffusion MRI Streamline Tractography using CNN and\n  Transformer Networks",
      "authors": [
        "Itzik Waizman",
        "Yakov Gusakov",
        "Itay Benou",
        "Tammy Riklin Raviv"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "White matter tractography is an advanced neuroimaging technique that\nreconstructs the 3D white matter pathways of the brain from diffusion MRI data.\nIt can be framed as a pathfinding problem aiming to infer neural fiber\ntrajectories from noisy and ambiguous measurements, facing challenges such as\ncrossing, merging, and fanning white-matter configurations. In this paper, we\npropose a novel tractography method that leverages Transformers to model the\nsequential nature of white matter streamlines, enabling the prediction of fiber\ndirections by integrating both the trajectory context and current diffusion MRI\nmeasurements. To incorporate spatial information, we utilize CNNs that extract\nmicrostructural features from local neighborhoods around each voxel. By\ncombining these complementary sources of information, our approach improves the\nprecision and completeness of neural pathway mapping compared to traditional\ntractography models. We evaluate our method with the Tractometer toolkit,\nachieving competitive performance against state-of-the-art approaches, and\npresent qualitative results on the TractoInferno dataset, demonstrating strong\ngeneralization to real-world data.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16429v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16429v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.521,
      "distributed_training_score": 0.378,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on tractography using diffusion MRI for reconstructing brain white matter pathways, employing CNNs and Transformers for spatial and sequential modeling. It does not involve iterative refinement processes for multi-step logical reasoning, Chain-of-Thought, or diffusion models adapted for complex logical tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16431",
      "title": "Proactive Statistical Process Control Using AI: A Time Series\n  Forecasting Approach for Semiconductor Manufacturing",
      "authors": [
        "Mohammad Iqbal Rasul Seeam",
        "Victor S. Sheng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In the manufacturing industry, it is very important to keep machines and\nprocesses running smoothly and without unexpected problems. One of the most\ncommon tools used to check if everything is working properly is called\nStatistical Process Control (SPC). Traditional SPC methods work by checking\nwhether recent measurements are within acceptable limits. However, they only\nreact after a problem has already occurred. This can lead to wasted materials,\nmachine downtime, and increased costs. In this paper, we present a smarter way\nto use SPC. Instead of just reacting to issues after they happen, our system\ncan predict future problems before they occur. We use a machine learning tool\ncalled Facebook Prophet, which is designed to work with time-series data (data\nthat changes over time). Prophet looks at past data and forecasts what the next\nvalue will be. Then, we use SPC rules to decide if the predicted value is in a\nSafe zone (no problem), a Warning zone (needs attention), or a Critical zone\n(may require shutting down the process). We applied this system to real data\nfrom a semiconductor manufacturing company. One of the challenges with this\ndata is that the measurements are not taken at regular time intervals. This\nmakes it harder to predict future values accurately. Despite this, our model\nwas able to make strong predictions and correctly classify the risk level of\nfuture measurements. The main benefit of our system is that it gives engineers\nand technicians a chance to act early - before something goes wrong. This helps\nreduce unexpected failures and improves the overall stability and reliability\nof the production process. By combining machine learning with traditional SPC,\nwe make quality control more proactive, accurate, and useful for modern\nindustry.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16431v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16431v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.294,
      "distributed_training_score": 0.311,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16436",
      "title": "Improved mmFormer for Liver Fibrosis Staging via Missing-Modality\n  Compensation",
      "authors": [
        "Zhejia Zhang",
        "Junjie Wang",
        "Le Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In real-world clinical settings, magnetic resonance imaging (MRI) frequently\nsuffers from missing modalities due to equipment variability or patient\ncooperation issues, which can significantly affect model performance. To\naddress this issue, we propose a multimodal MRI classification model based on\nthe mmFormer architecture with an adaptive module for handling arbitrary\ncombinations of missing modalities. Specifically, this model retains the hybrid\nmodality-specific encoders and the modality-correlated encoder from mmFormer to\nextract consistent lesion features across available modalities. In addition, we\nintegrate a missing-modality compensation module which leverages zero-padding,\nmodality availability masks, and a Delta Function with learnable statistical\nparameters to dynamically synthesize proxy features for recovering missing\ninformation. To further improve prediction performance, we adopt a\ncross-validation ensemble strategy by training multiple models on different\nfolds and applying soft voting during inference. This method is evaluated on\nthe test set of Comprehensive Analysis & Computing of REal-world medical images\n(CARE) 2025 challenge, targeting the Liver Fibrosis Staging (LiFS) task based\non non-contrast dynamic MRI scans including T1-weighted imaging (T1WI),\nT2-weighted imaging (T2WI), and diffusion-weighted imaging (DWI). For Cirrhosis\nDetection and Substantial Fibrosis Detection on in-distribution vendors, our\nmodel obtains accuracies of 66.67%, and 74.17%, and corresponding area under\nthe curve (AUC) scores of 71.73% and 68.48%, respectively.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16436v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16436v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.28,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.297,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16437",
      "title": "SENSE-7: Taxonomy and Dataset for Measuring User Perceptions of Empathy\n  in Sustained Human-AI Conversations",
      "authors": [
        "Jina Suh",
        "Lindy Le",
        "Erfan Shayegani",
        "Gonzalo Ramos",
        "Judith Amores",
        "Desmond C. Ong",
        "Mary Czerwinski",
        "Javier Hernandez"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Empathy is increasingly recognized as a key factor in human-AI communication,\nyet conventional approaches to \"digital empathy\" often focus on simulating\ninternal, human-like emotional states while overlooking the inherently\nsubjective, contextual, and relational facets of empathy as perceived by users.\nIn this work, we propose a human-centered taxonomy that emphasizes observable\nempathic behaviors and introduce a new dataset, Sense-7, of real-world\nconversations between information workers and Large Language Models (LLMs),\nwhich includes per-turn empathy annotations directly from the users, along with\nuser characteristics, and contextual details, offering a more user-grounded\nrepresentation of empathy. Analysis of 695 conversations from 109 participants\nreveals that empathy judgments are highly individualized, context-sensitive,\nand vulnerable to disruption when conversational continuity fails or user\nexpectations go unmet. To promote further research, we provide a subset of 672\nanonymized conversation and provide exploratory classification analysis,\nshowing that an LLM-based classifier can recognize 5 levels of empathy with an\nencouraging average Spearman $\\rho$=0.369 and Accuracy=0.487 over this set.\nOverall, our findings underscore the need for AI designs that dynamically\ntailor empathic behaviors to user contexts and goals, offering a roadmap for\nfuture research and practical development of socially attuned, human-centered\nartificial agents.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16437v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16437v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.44,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.316,
      "datasets_score": 0.483,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on introducing a taxonomy and dataset for empathy in human-AI conversations, including exploratory classification analysis, but does not involve training AI models using reinforcement learning with a reward model based on human-ranked data. There is no mention of RLHF techniques for aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contributions include creating and analyzing a new dataset (Sense-7) for empathy in human-AI interactions, detailing its curation from real-world conversations, providing annotations, and performing evaluations, which directly aligns with research on dataset introduction, analysis, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a human-centered taxonomy for empathic behaviors in AI interactions and presents the SENSE-7 dataset, consisting of 695 real-world conversations between information workers and Large Language Models (LLMs), annotated for empathy by users to capture subjective perceptions. Through analysis of the data from 109 participants, it reveals that empathy judgments are highly individualized, context-sensitive, and easily disrupted, while an exploratory LLM-based classification achieves moderate performance in detecting empathy levels, advocating for AI designs that dynamically adapt to user needs.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new taxonomy and dataset focused on user-perceived empathy in human-AI conversations, significantly advancing the field by emphasizing observable behaviors over internal AI states. This represents a novel shift in perspective that addresses gaps in existing research on digital empathy.",
      "impact_score": "High",
      "impact_justification": "The work provides a new dataset and taxonomy that could influence AI development in human-computer interaction by enabling more user-adaptive empathic systems, potentially leading to broader applications in commercial AI and future research. Its provision of tools for empathy evaluation makes it likely to be widely cited and built upon in HCI and AI subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers high-quality contributions with its innovative taxonomy and dataset, offering practical value for researchers in human-AI interaction and empathy in AI. While essential for those in HCI and AI, it may not be critical for all audiences, making it a strong but not indispensable read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bfdb1422143067e91a8e8bb8c5e7214e52f37209",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 19,
      "average_h_index": 6.125,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Jina Suh",
          "h_index": 19,
          "profile_url": "https://www.semanticscholar.org/author/38972741"
        },
        {
          "name": "Lindy Le",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2273645224"
        },
        {
          "name": "Erfan Shayegani",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2225246116"
        },
        {
          "name": "Gonzalo Ramos",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2279021713"
        },
        {
          "name": "Judith Amores",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2260335753"
        },
        {
          "name": "Desmond C. Ong",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/3078792"
        },
        {
          "name": "Mary Czerwinski",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2299852863"
        },
        {
          "name": "Javier Hernandez",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2357783147"
        }
      ]
    },
    {
      "id": "2509.16438",
      "title": "AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval\n  Benchmarks",
      "authors": [
        "Mohamed Eltahir",
        "Osamah Sarraj",
        "Abdulrahman Alfrihidi",
        "Taha Alshatiri",
        "Mohammed Khurd",
        "Mohammed Bremoo",
        "Tanveer Hussain"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Video-to-text and text-to-video retrieval are dominated by English benchmarks\n(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet\nArabic remains underserved, lacking localized evaluation metrics. We introduce\na three-stage framework, AutoArabic, utilizing state-of-the-art large language\nmodels (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,\nreducing the manual revision required by nearly fourfold. The framework\nincorporates an error detection module that automatically flags potential\ntranslation errors with 97% accuracy. Applying the framework to DiDeMo, a video\nretrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent\nArabic descriptions. An analysis of the translation errors is provided and\norganized into an insightful taxonomy to guide future Arabic localization\nefforts. We train a CLIP-style baseline with identical hyperparameters on the\nArabic and English variants of the benchmark, finding a moderate performance\ngap (about 3 percentage points at Recall@1), indicating that Arabic\nlocalization preserves benchmark difficulty. We evaluate three post-editing\nbudgets (zero/ flagged-only/ full) and find that performance improves\nmonotonically with more post-editing, while the raw LLM output (zero-budget)\nremains usable. To ensure reproducibility to other languages, we made the code\navailable at https://github.com/Tahaalshatiri/AutoArabic.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16438v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16438v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.341,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16443",
      "title": "LightCode: Compiling LLM Inference for Photonic-Electronic Systems",
      "authors": [
        "Ryan Tomich",
        "Zhizhen Zhong",
        "Dirk Englund"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.PL (Programming Languages)"
      ],
      "abstract": "The growing demand for low-latency, energy-efficient inference in large\nlanguage models (LLMs) has catalyzed interest in heterogeneous architectures.\nWhile GPUs remain dominant, they are poorly suited for integration with\nemerging domain-specific accelerators like the Photonic Tensor Units (PTUs),\nwhich offer low-power, high-throughput linear computation. This motivates\nhybrid compilation strategies that combine photonic and electronic resources.\nWe present LightCode, a compiler framework and simulator for mapping LLM\ninference workloads across hybrid photonic-electronic systems. LightCode\nintroduces the Stacked Graph, an intermediate representation that encodes\nmultiple hardware-specific realizations of each tensor operation. Hardware\nassignment is formulated as a constrained subgraph selection problem optimized\nfor latency or energy under parametric cost models. We evaluate LightCode on\nthe prefill stage of GPT-2 and Llama-7B showing that under our workload and\nhardware assumptions, (i) Photonic hardware reduced energy by up to 50% in our\nsimulated workloads at maximum sequence length; (ii) multiplexing and\nassignment strategy yielded latency improvements exceeding 10x; and (iii)\nOptimizing for latency or energy resulted in distinct hardware mappings in our\nsimulations. LightCode offers a module, foundational framework and simulator\nfor compiling LLMs to emerging photonic accelerators.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16443v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16443v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.473,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on compiling LLM inference for hybrid photonic-electronic systems, emphasizing hardware optimization for latency and energy efficiency. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "The paper discusses workload partitioning and scheduling across heterogeneous hardware (e.g., GPUs and PTUs) for LLM inference, which involves elements of parallel computing. However, it is not about distributed training, model training acceleration, or multi-node setups; it centers on inference optimization, making it only loosely related to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16444",
      "title": "Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered\n  Mental Health Chatbots",
      "authors": [
        "Chenhan Lyu",
        "Yutong Song",
        "Pengfei Zhang",
        "Amir M. Rahmani"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Mental health applications have emerged as a critical area in computational\nhealth, driven by rising global rates of mental illness, the integration of AI\nin psychological care, and the need for scalable solutions in underserved\ncommunities. These include therapy chatbots, crisis detection, and wellness\nplatforms handling sensitive data, requiring specialized AI safety beyond\ngeneral safeguards due to emotional vulnerability, risks like misdiagnosis or\nsymptom exacerbation, and precise management of vulnerable states to avoid\nsevere outcomes such as self-harm or loss of trust. Despite AI safety advances,\ngeneral safeguards inadequately address mental health-specific challenges,\nincluding crisis intervention accuracy to avert escalations, therapeutic\nguideline adherence to prevent misinformation, scale limitations in\nresource-constrained settings, and adaptation to nuanced dialogues where\ngenerics may introduce biases or miss distress signals. We introduce an\napproach to apply Constitutional AI training with domain-specific mental health\nprinciples for safe, domain-adapted CAI systems in computational mental health\napplications.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16444v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16444v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.508,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.321,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions RLHF as a related technique for AI safety and alignment in the introduction, but its main contribution focuses on Constitutional AI (CAI) training with domain-specific principles, without describing the use of human feedback to train a reward model or fine-tune via reinforcement learning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on applying Constitutional AI for mental health chatbots, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16449",
      "title": "PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal\n  Summarization",
      "authors": [
        "Tsz Fung Pang",
        "Maryam Berijanian",
        "Thomas Orth",
        "Breanna Shi",
        "Charlotte S. Alexander"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Legal documents are often long, dense, and difficult to comprehend, not only\nfor laypeople but also for legal experts. While automated document\nsummarization has great potential to improve access to legal knowledge,\nprevailing task-based evaluators overlook divergent user and stakeholder needs.\nTool development is needed to encompass the technicality of a case summary for\na litigator yet be accessible for a self-help public researching for their\nlawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation\nframework that scores summaries through the lens of six personas, including\nlegal and non-legal users. We also introduce a controlled dimension-shifted\npilot dataset of U.S. civil rights case summaries that varies along depth,\naccessibility, and procedural detail as well as Diversity-Coverage Index (DCI)\nto expose divergent optima of legal summary between persona-aware and\npersona-agnostic judges. This work enables refinement of legal AI summarization\nsystems for both expert and non-expert users, with the potential to increase\naccess to legal knowledge. The code base and data are publicly available in\nGitHub.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16449v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16449v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.292,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16452",
      "title": "KRAST: Knowledge-Augmented Robotic Action Recognition with Structured\n  Text for Vision-Language Models",
      "authors": [
        "Son Hai Nguyen",
        "Diwei Wang",
        "Jinhyeok Jang",
        "Hyewon Seo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate vision-based action recognition is crucial for developing autonomous\nrobots that can operate safely and reliably in complex, real-world\nenvironments. In this work, we advance video-based recognition of indoor daily\nactions for robotic perception by leveraging vision-language models (VLMs)\nenriched with domain-specific knowledge. We adapt a prompt-learning framework\nin which class-level textual descriptions of each action are embedded as\nlearnable prompts into a frozen pre-trained VLM backbone. Several strategies\nfor structuring and encoding these textual descriptions are designed and\nevaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our\nmethod, using only RGB video inputs at test time, achieves over 95\\% accuracy\nand outperforms state-of-the-art approaches. These results highlight the\neffectiveness of knowledge-augmented prompts in enabling robust action\nrecognition with minimal supervision.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16452v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16452v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.282,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is developing a knowledge-augmented prompt-learning framework for vision-language models to improve robotic action recognition from video data. It focuses on adapting VLMs with textual descriptions for better accuracy in domestic environments, without any involvement of human feedback, reward models, or reinforcement learning techniques. Since RLHF specifically requires training models using human-ranked data and reinforcement learning for alignment with human preferences, this paper does not address or relate to that topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16454",
      "title": "A Generative AI System for Biomedical Data Discovery with Grammar-Based\n  Visualizations",
      "authors": [
        "Devin Lange",
        "Shanghua Gao",
        "Pengwei Sui",
        "Austen Money",
        "Priya Misner",
        "Marinka Zitnik",
        "Nils Gehlenborg"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We explore the potential for combining generative AI with grammar-based\nvisualizations for biomedical data discovery. In our prototype, we use a\nmulti-agent system to generate visualization specifications and apply filters.\nThese visualizations are linked together, resulting in an interactive dashboard\nthat is progressively constructed. Our system leverages the strengths of\nnatural language while maintaining the utility of traditional user interfaces.\nFurthermore, we utilize generated interactive widgets enabling user adjustment.\nFinally, we demonstrate the potential utility of this system for biomedical\ndata discovery with a case study.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16454v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16454v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.246,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a generative AI system using multi-agent approaches for creating visualizations and interactive dashboards in biomedical data discovery. It does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are core to the topic. Therefore, there is no relevant component.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16456",
      "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning",
      "authors": [
        "Jiahao Yu",
        "Zelei Cheng",
        "Xian Wu",
        "Xinyu Xing"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) are increasingly used in various domains,\nshowing impressive potential on different tasks. Recently, reasoning LLMs have\nbeen proposed to improve the \\textit{reasoning} or \\textit{thinking}\ncapabilities of LLMs to solve complex problems. Despite the promising results\nof reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs\nstill remains a significant challenge. While existing optimization methods have\nadvanced the LLM reasoning capabilities, they often treat reasoning\ntrajectories as a whole, without considering the underlying critical steps\nwithin the trajectory. In this paper, we introduce \\textbf{G}uided\n\\textbf{P}ivotal \\textbf{O}ptimization (GPO), a novel fine-tuning strategy that\ndives into the reasoning process to enable more effective improvements. GPO\nfirst identifies the `critical step' within a reasoning trajectory - a point\nthat the model must carefully proceed to succeed at the problem. We locate the\ncritical step by estimating the advantage function. GPO then resets the policy\nto the critical step, samples the new rollout and prioritizes the learning\nprocess on those rollouts. This focus allows the model to learn more\neffectively from pivotal moments within the reasoning process to improve the\nreasoning performance. We demonstrate that GPO is a general strategy that can\nbe integrated with various optimization methods to improve reasoning\nperformance. Besides theoretical analysis, our experiments across challenging\nreasoning benchmarks show that GPO can consistently and significantly enhance\nthe performance of existing optimization methods, showcasing its effectiveness\nand generalizability in improving LLM reasoning by concentrating on pivotal\nmoments within the generation process.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16456v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16456v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.474,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.561,
      "distributed_training_score": 0.402,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on GPO, a fine-tuning strategy for LLMs that uses concepts like advantage functions and integrates with methods such as PPO or DPO, but it does not involve training a reward model on human-ranked data or incorporate human feedback. Instead, it emphasizes algorithmic optimization of reasoning trajectories, which does not align with the core definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces GPO for improving LLM reasoning by identifying and optimizing critical steps in trajectories, but it does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for logical tasks. The approach is based on reinforcement learning concepts, not diffusion-based mechanisms.",
      "distributed_training_justification": "The paper discusses a fine-tuning strategy for LLMs to enhance reasoning capabilities, including integration with optimization methods, but it does not address distributed training, parallel computing, multi-node systems, or any partitioning of data/computation across processors. The focus is solely on algorithmic improvements, not training infrastructure.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16457",
      "title": "Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd\n  Simulations",
      "authors": [
        "Yunzhe Wang",
        "Gale M. Lucas",
        "Burcin Becerik-Gerber",
        "Volkan Ustun"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Language-driven generative agents have enabled large-scale social simulations\nwith transformative uses, from interpersonal training to aiding global\npolicy-making. However, recent studies indicate that generative agent behaviors\noften deviate from expert expectations and real-world data--a phenomenon we\nterm the Behavior-Realism Gap. To address this, we introduce a theoretical\nframework called Persona-Environment Behavioral Alignment (PEBA), formulated as\na distribution matching problem grounded in Lewin's behavior equation stating\nthat behavior is a function of the person and their environment. Leveraging\nPEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that\niteratively refines agent personas, implicitly aligning their collective\nbehaviors with realistic expert benchmarks within a specified environmental\ncontext. We validate PEvo in an active shooter incident simulation we\ndeveloped, achieving an 84% average reduction in distributional divergence\ncompared to no steering and a 34% improvement over explicit instruction\nbaselines. Results also show PEvo-refined personas generalize to novel, related\nsimulation scenarios. Our method greatly enhances behavioral realism and\nreliability in high-stakes social simulations. More broadly, the PEBA-PEvo\nframework provides a principled approach to developing trustworthy LLM-driven\nsocial simulations.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16457v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16457v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.476,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.366,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an LLM-based optimization algorithm (PersonaEvolve) to refine agent personas for behavioral alignment in simulations, but it does not involve training a reward model with human-ranked data or using reinforcement learning to fine-tune models based on human feedback. Instead, it relies on expert benchmarks for divergence measurement without explicit RLHF components.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's PersonaEvolve algorithm involves iterative refinement of agent personas, but it does not adapt diffusion models for multi-step logical reasoning or treat a Chain-of-Thought as a holistically corrected entity; it is an LLM-based optimization for behavioral alignment, lacking any diffusion-specific components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16463",
      "title": "Entropic Causal Inference: Graph Identifiability",
      "authors": [
        "Spencer Compton",
        "Kristjan Greenewald",
        "Dmitriy Katz",
        "Murat Kocaoglu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Entropic causal inference is a recent framework for learning the causal graph\nbetween two variables from observational data by finding the\ninformation-theoretically simplest structural explanation of the data, i.e.,\nthe model with smallest entropy. In our work, we first extend the causal graph\nidentifiability result in the two-variable setting under relaxed assumptions.\nWe then show the first identifiability result using the entropic approach for\nlearning causal graphs with more than two nodes. Our approach utilizes the\nproperty that ancestrality between a source node and its descendants can be\ndetermined using the bivariate entropic tests. We provide a sound sequential\npeeling algorithm for general graphs that relies on this property. We also\npropose a heuristic algorithm for small graphs that shows strong empirical\nperformance. We rigorously evaluate the performance of our algorithms on\nsynthetic data generated from a variety of models, observing improvement over\nprior work. Finally we test our algorithms on real-world datasets.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16463v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16463v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.266,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16471",
      "title": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes",
      "authors": [
        "Sima Zeinali Danalou",
        "Dian Yu",
        "Niher R. Sarker",
        "Hooman Chamani",
        "Jane Y. Howe",
        "Patrick C. Lee",
        "Jay R. Werber"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16471v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16471v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.221,
      "weak_supervision_score": 0.265,
      "diffusion_reasoning_score": 0.269,
      "distributed_training_score": 0.251,
      "datasets_score": 0.258,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16472",
      "title": "Explainable Gait Abnormality Detection Using Dual-Dataset CNN-LSTM\n  Models",
      "authors": [
        "Parth Agarwal",
        "Sangaa Chatterjee",
        "Md Faisal Kabir",
        "Suman Saha"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Gait is a key indicator in diagnosing movement disorders, but most models\nlack interpretability and rely on single datasets. We propose a dual-branch\nCNN-LSTM framework a 1D branch on joint-based features from GAVD and a 3D\nbranch on silhouettes from OU-MVLP. Interpretability is provided by SHAP\n(temporal attributions) and Grad-CAM (spatial localization).On held-out sets,\nthe system achieves 98.6% accuracy with strong recall and F1. This approach\nadvances explainable gait analysis across both clinical and biometric domains.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16472v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16472v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.325,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16473",
      "title": "The Iconicity of the Generated Image",
      "authors": [
        "Nanne van Noord",
        "Noa Garcia"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "How humans interpret and produce images is influenced by the images we have\nbeen exposed to. Similarly, visual generative AI models are exposed to many\ntraining images and learn to generate new images based on this. Given the\nimportance of iconic images in human visual communication, as they are widely\nseen, reproduced, and used as inspiration, we may expect that they may\nsimilarly have a proportionally large influence within the generative AI\nprocess. In this work we explore this question through a three-part analysis,\ninvolving data attribution, semantic similarity analysis, and a user-study. Our\nfindings indicate that iconic images do not have an obvious influence on the\ngenerative process, and that for many icons it is challenging to reproduce an\nimage which resembles it closely. This highlights an important difference in\nhow humans and visual generative AI models draw on and learn from prior visual\ncommunication.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.16473v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16473v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.3,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines the influence of iconic images on visual generative AI models, such as Stable Diffusion, focusing on image generation, data attribution, semantic analysis, and user studies. While Stable Diffusion is a diffusion-based model, the paper does not adapt or use the iterative refinement process of diffusion for solving complex logical tasks, chain-of-thought reasoning, or multi-step logical processes. Thus, it lacks any component related to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18189",
      "title": "Qianfan-VL: Domain-Enhanced Universal Vision-Language Models",
      "authors": [
        "Daxiang Dong",
        "Mingming Zheng",
        "Dong Xu",
        "Bairong Zhuang",
        "Wenyu Zhang",
        "Chunhua Luo",
        "Haoran Wang",
        "Zijian Zhao",
        "Jie Li",
        "Yuxuan Li",
        "Hanjun Zhong",
        "Mengyue Liu",
        "Jieting Chen",
        "Shupeng Li",
        "Lun Tian",
        "Yaping Feng",
        "Xin Li",
        "Donggang Jiang",
        "Yong Chen",
        "Yehua Xu",
        "Duohao Qin",
        "Chen Feng",
        "Dan Wang",
        "Henghua Zhang",
        "Jingjing Ha",
        "Jinhui He",
        "Yanfeng Zhai",
        "Chengxin Zheng",
        "Jiayi Mao",
        "Jiacheng Chen",
        "Ruchang Yao",
        "Ziye Yuan",
        "Jianmin Wu",
        "Guangjun Xie",
        "Dou Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present Qianfan-VL, a series of multimodal large language models ranging\nfrom 3B to 70B parameters, achieving state-of-the-art performance through\ninnovative domain enhancement techniques. Our approach employs multi-stage\nprogressive training and high-precision data synthesis pipelines, which prove\nto be critical technologies for enhancing domain-specific capabilities while\nmaintaining strong general performance. Qianfan-VL achieves comparable results\nto leading open-source models on general benchmarks, with state-of-the-art\nperformance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and\nMMStar. The domain enhancement strategy delivers significant advantages in OCR\nand document understanding, validated on both public benchmarks (OCRBench 873,\nDocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B\nvariants incorporate long chain-of-thought capabilities, demonstrating superior\nperformance on mathematical reasoning (MathVista 78.6%) and logical inference\ntasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating\nthe capability of large-scale AI infrastructure to train SOTA-level multimodal\nmodels with over 90% scaling efficiency on 5000 chips for a single task. This\nwork establishes an effective methodology for developing domain-enhanced\nmultimodal models suitable for diverse enterprise deployment scenarios.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.18189v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18189v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.418,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on domain-enhanced vision-language models with multi-stage training and chain-of-thought capabilities for tasks like mathematical reasoning. However, it does not mention or utilize diffusion-based methods, such as iterative refinement processes for reasoning or treating chain-of-thought as a holistically corrected entity. There is no component involving diffusion models for multi-step logical tasks.",
      "distributed_training_justification": "The paper explicitly describes training Qianfan-VL models on Baidu's Kunlun P800 chips using innovative parallel strategies, communication-computation fusion techniques, and achieving over 90% scaling efficiency on 5000+ chip clusters. This directly aligns with distributed training, parallel computing, and multi-node machine learning by partitioning computation across nodes for efficient large-scale model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "Qianfan-VL presents a series of domain-enhanced vision-language models ranging from 3B to 70B parameters, designed to excel in specific areas like OCR, document understanding, and mathematical reasoning while maintaining strong general performance through a four-stage progressive training pipeline and high-precision data synthesis. The models achieve state-of-the-art results on benchmarks such as DocVQA, MathVista, and OCRBench, demonstrate efficient training on Baidu's Kunlun P800 chips with over 90% scaling efficiency, and offer variants tailored for various deployment scenarios, establishing an effective methodology for enterprise-grade multimodal AI.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement through its multi-stage training pipeline and data synthesis techniques, which cleverly combine existing ideas to enhance domain-specific capabilities in vision-language models without introducing a entirely new problem or architecture.",
      "impact_score": "High",
      "impact_justification": "The work's innovations in domain-enhanced VLMs and efficient large-scale training could influence a wide range of future research in multimodal AI and commercial applications, particularly in enterprise scenarios like document processing and reasoning tasks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a strong, valuable contribution with practical insights into developing specialized VLMs, making it essential for researchers and practitioners in computer vision and AI to be aware of, though it may not be groundbreaking for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f6e81d1ce7f10c2218bba12f0afdc40a0ebe8a86",
      "total_authors": 35,
      "authors_found": 33,
      "highest_h_index": 1,
      "average_h_index": 0.21212121212121213,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Daxiang Dong",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336096468"
        },
        {
          "name": "Min-Wen Zheng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352993667"
        },
        {
          "name": "Dong Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381790018"
        },
        {
          "name": "Bairong Zhuang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381722918"
        },
        {
          "name": "Wenyu Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381793085"
        },
        {
          "name": "Chunhua Luo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382450840"
        },
        {
          "name": "Haoran Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381786622"
        },
        {
          "name": "Zijian Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382094282"
        },
        {
          "name": "Jie Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380412145"
        },
        {
          "name": "Yuxuan Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381787767"
        },
        {
          "name": "Hanjun Zhong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382445233"
        },
        {
          "name": "Mengyue Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381781328"
        },
        {
          "name": "Jieting Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382999543"
        },
        {
          "name": "Shupeng Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381789468"
        },
        {
          "name": "Lun Tian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382434528"
        },
        {
          "name": "Yaping Feng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381782489"
        },
        {
          "name": "Xin Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382078566"
        },
        {
          "name": "Dong-Mou Jiang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2170762994"
        },
        {
          "name": "Yong Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362142806"
        },
        {
          "name": "Yehua Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381791188"
        },
        {
          "name": "Duohao Qin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381721964"
        },
        {
          "name": "Chen Feng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Dan Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2334351119"
        },
        {
          "name": "He-long Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2356696934"
        },
        {
          "name": "Jingjing Ha",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2383982648"
        },
        {
          "name": "Jinhui He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381938324"
        },
        {
          "name": "Yan Zhai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370738014"
        },
        {
          "name": "Chengxin Zheng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "J. Mao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2051236927"
        },
        {
          "name": "Jiacheng Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2238399159"
        },
        {
          "name": "Ruchang Yao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381722049"
        },
        {
          "name": "Ziye Yuan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2384335172"
        },
        {
          "name": "Jianmin Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381785286"
        },
        {
          "name": "Guangjun Xie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2288488120"
        },
        {
          "name": "Dou Shen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336739398"
        }
      ]
    },
    {
      "id": "2509.18190",
      "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze\n  Generation for Real-World Dehazing",
      "authors": [
        "Junseong Shin",
        "Seungwoo Chung",
        "Yunjeong Yang",
        "Tae Hyun Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Dehazing involves removing haze or fog from images to restore clarity and\nimprove visibility by estimating atmospheric scattering effects. While deep\nlearning methods show promise, the lack of paired real-world training data and\nthe resulting domain gap hinder generalization to real-world scenarios. In this\ncontext, physics-grounded learning becomes crucial; however, traditional\nmethods based on the Atmospheric Scattering Model (ASM) often fall short in\nhandling real-world complexities and diverse haze patterns. To solve this\nproblem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM\nas an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),\nHazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,\nenhancing real-world dehazing performance with only a single inference step.\nAdditionally, we introduce a non-homogeneous haze generation method using\nMarkov Chain Brownian Motion (MCBM) to address the scarcity of paired\nreal-world data. By simulating realistic haze patterns through MCBM, we enhance\nthe adaptability of HazeFlow to diverse real-world scenarios. Through extensive\nexperiments, we demonstrate that HazeFlow achieves state-of-the-art performance\nacross various real-world dehazing benchmark datasets.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.18190v2",
      "pdf_url": "http://arxiv.org/pdf/2509.18190v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.319,
      "datasets_score": 0.251,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces HazeFlow, an ODE-based framework for image dehazing inspired by Rectified Flow, which is related to diffusion models. However, it applies this to visual image processing tasks, such as mapping hazy images to clean ones, and does not involve adapting diffusion for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18193",
      "title": "TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed\n  Detection",
      "authors": [
        "Omar H. Khater",
        "Abdul Jabbar Siddiqui",
        "Aiman El-Maleh",
        "M. Shamim Hossain"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deploying deep learning models in agriculture is difficult because edge\ndevices have limited resources, but this work presents a compressed version of\nEcoWeedNet using structured channel pruning, quantization-aware training (QAT),\nand acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the\nchallenges of pruning complex architectures with residual shortcuts, attention\nmechanisms, concatenations, and CSP blocks, the model size was reduced by up to\n68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at\nFP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the\npruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n\n(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%\nmAP50, proving it to be both efficient and effective for precision agriculture.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.18193v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18193v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.262,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.271,
      "distributed_training_score": 0.406,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on model compression techniques such as channel pruning, quantization-aware training, and inference acceleration using NVIDIA TensorRT for edge devices like the Jetson Orin Nano. It does not discuss distributed training, parallel computing across multiple nodes, or strategies for partitioning data or computation to accelerate model training. The main contributions are centered on optimizing inference efficiency for real-time weed detection, with no mention of multi-node or distributed learning approaches.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18196",
      "title": "MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal\n  Vocalization Recognition in Speech",
      "authors": [
        "Jialong Mai",
        "Jinxin Ji",
        "Xiaofen Xing",
        "Chen Yang",
        "Weidong Chen",
        "Jingyuan Xing",
        "Xiangmin Xu"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing\nlexical content, but largely fail to recognize nonverbal vocalizations (NVs)\nembedded in speech, such as sighs, laughs, and coughs. This capability is\nimportant for a comprehensive understanding of human communication, as NVs\nconvey crucial emotional and intentional cues. Progress in NV-aware ASR has\nbeen hindered by the lack of high-quality, well-annotated datasets. To address\nthis gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech\ndataset. Unlike most existing corpora that rely on model-based detection,\nMNV-17's performative nature ensures high-fidelity, clearly articulated NV\ninstances. To the best of our knowledge, MNV-17 provides the most extensive set\nof nonverbal vocalization categories, comprising 17 distinct and well-balanced\nclasses of common NVs. We benchmarked MNV-17 on four mainstream ASR\narchitectures, evaluating their joint performance on semantic transcription and\nNV classification. The dataset and the pretrained model checkpoints will be\nmade publicly available to facilitate future research in expressive ASR.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.18196v2",
      "pdf_url": "http://arxiv.org/pdf/2509.18196v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.294,
      "distributed_training_score": 0.314,
      "datasets_score": 0.424,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of MNV-17, a new dataset for nonverbal vocalization recognition in Mandarin speech, which directly aligns with research on creating and introducing datasets for AI applications. It details dataset curation methodologies, such as using scripted performative recordings to ensure high-quality annotations and balanced classes across 17 categories. The paper also includes benchmarking and evaluation of the dataset on mainstream ASR architectures, assessing its utility for joint semantic transcription and NV classification tasks. This comprehensive focus on dataset creation, curation, benchmarking, and analysis makes it highly relevant to the topic.",
      "llm_score_status": "completed",
      "summary": "The paper introduces MNV-17, a 7.55-hour performative Mandarin speech dataset designed to address the lack of high-quality resources for recognizing nonverbal vocalizations (NVs) in Automatic Speech Recognition (ASR) systems. It features 17 well-balanced NV categories, recorded through scripted performances by native speakers to ensure clear and reliable annotations, and benchmarks these on mainstream ASR architectures to demonstrate effective integration of NV detection without compromising lexical transcription accuracy.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a comprehensive, well-annotated dataset for NV recognition in Mandarin ASR, combining existing dataset creation techniques with a focus on balanced categories to address a known gap. While NV recognition is not a new problem, MNV-17's extensive category coverage and performative approach offer a clever adaptation rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of expressive ASR and audio processing, as the publicly available dataset could enable further research on NV-aware systems for Mandarin speech. However, its specific focus on Mandarin limits broader applicability to non-Mandarin contexts or general AI fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a valuable contribution by providing a high-quality dataset that advances NV recognition in ASR, making it essential for researchers in speech processing and AI. While not universally groundbreaking, its practical utility and benchmarking insights warrant attention from relevant specialists.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a6c7d5dca48b53edd5bdeb621371df776dc9b62c",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 23,
      "average_h_index": 5.285714285714286,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Jialong Mai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2360700882"
        },
        {
          "name": "Jinxin Ji",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382443719"
        },
        {
          "name": "Xiaofen Xing",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/2667341"
        },
        {
          "name": "Chen Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382105912"
        },
        {
          "name": "Weidong Chen",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2111830888"
        },
        {
          "name": "Jingyuan Xing",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2349861947"
        },
        {
          "name": "Xiangmin Xu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2253898399"
        }
      ]
    },
    {
      "id": "2509.18198",
      "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy\n  with Knowledge Distillation",
      "authors": [
        "Rui Liu",
        "Zikang Wang",
        "Peng Gao",
        "Yu Shen",
        "Pratap Tokekar",
        "Ming Lin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Autonomous systems have advanced significantly, but challenges persist in\naccident-prone environments where robust decision-making is crucial. A single\nvehicle's limited sensor range and obstructed views increase the likelihood of\naccidents. Multi-vehicle connected systems and multi-modal approaches,\nleveraging RGB images and LiDAR point clouds, have emerged as promising\nsolutions. However, existing methods often assume the availability of all data\nmodalities and connected vehicles during both training and testing, which is\nimpractical due to potential sensor failures or missing connected vehicles. To\naddress these challenges, we introduce a novel framework MMCD (Multi-Modal\nCollaborative Decision-making) for connected autonomy. Our framework fuses\nmulti-modal observations from ego and collaborative vehicles to enhance\ndecision-making under challenging conditions. To ensure robust performance when\ncertain data modalities are unavailable during testing, we propose an approach\nbased on cross-modal knowledge distillation with a teacher-student model\nstructure. The teacher model is trained with multiple data modalities, while\nthe student model is designed to operate effectively with reduced modalities.\nIn experiments on $\\textit{connected autonomous driving with ground vehicles}$\nand $\\textit{aerial-ground vehicles collaboration}$, our method improves\ndriving safety by up to ${\\it 20.7}\\%$, surpassing the best-existing baseline\nin detecting potential accidents and making safe driving decisions. More\ninformation can be found on our website https://ruiiu.github.io/mmcd.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.18198v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18198v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.41,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a multi-modal collaborative decision-making framework using knowledge distillation for autonomous systems, focusing on fusing sensor data and handling missing modalities. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic.",
      "distributed_training_justification": "The paper discusses training a teacher-student model for knowledge distillation in autonomous driving but does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors. Its focus is on decision-making and modality handling, not training acceleration techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19366",
      "title": "Unsupervised Outlier Detection in Audit Analytics: A Case Study Using\n  USA Spending Data",
      "authors": [
        "Buhe Li",
        "Berkay Kaplan",
        "Maksym Lazirko",
        "Aleksandr Kogan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study investigates the effectiveness of unsupervised outlier detection\nmethods in audit analytics, utilizing USA spending data from the U.S.\nDepartment of Health and Human Services (DHHS) as a case example. We employ and\ncompare multiple outlier detection algorithms, including Histogram-based\nOutlier Score (HBOS), Robust Principal Component Analysis (PCA), Minimum\nCovariance Determinant (MCD), and K-Nearest Neighbors (KNN) to identify\nanomalies in federal spending patterns. The research addresses the growing need\nfor efficient and accurate anomaly detection in large-scale governmental\ndatasets, where traditional auditing methods may fall short. Our methodology\ninvolves data preparation, algorithm implementation, and performance evaluation\nusing precision, recall, and F1 scores. Results indicate that a hybrid\napproach, combining multiple detection strategies, enhances the robustness and\naccuracy of outlier identification in complex financial data. This study\ncontributes to the field of audit analytics by providing insights into the\ncomparative effectiveness of various outlier detection models and demonstrating\nthe potential of unsupervised learning techniques in improving audit quality\nand efficiency. The findings have implications for auditors, policymakers, and\nresearchers seeking to leverage advanced analytics in governmental financial\noversight and risk management.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.19366v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19366v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.214,
      "distributed_training_score": 0.241,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19368",
      "title": "Pipeline Parallelism is All You Need for Optimized Early-Exit Based\n  Self-Speculative Decoding",
      "authors": [
        "Ruanjun Li",
        "Ziheng Liu",
        "Yuanming Shi",
        "Jiawei Shao",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) deliver impressive generation quality, but incur\nvery high inference cost because each output token is generated\nauto-regressively through all model layers. Early-exit based self-speculative\ndecoding (EESD) has emerged to mitigate this cost. However, in practice, many\napproaches struggle to achieve the expected acceleration in such\ndraft-then-verify paradigm even with a well-aligned early-exit head and\nselected exit position. Our analysis reveals that EESD only pays off when the\nvast majority of draft tokens are accepted by the LLM. Otherwise, the draft\ncost may overcome the acceleration gain and lead to a negative speedup. To\nmitigate this, we propose Pipeline-Parallel Self-Speculative Decoding (PPSD)\nthat fully pipelines the draft and verification work so that no effort is\nwasted on failed predictions. It has two key innovations. We configure the\nmodel layers as a pipeline in which early-exit (draft) computations and\nremaining-layer (verification) computations overlap. We interleave drafting and\nverification per token. While the LLM is verifying the current token in its\nfinal layers, the early-exit path simultaneously drafts the next token. Such a\nverify-while-draft scheme keeps all units busy and validates tokens on-the-fly\nanalogous to pipelining the speculation and verification stages. Empirical\nresults confirm that PPSD achieves state-of-the-art acceleration in\nself-speculative LLM inference. On diverse benchmarks, PPSD achieves speedup\nratios in the range of 2.01x~3.81x, which gains almost the optimal acceleration\nat the fixed acceptance rate and exit position, showcasing its advancement in\nproviding efficient self-speculation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.19368v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19368v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.467,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on optimizing inference in large language models using pipeline parallelism and early-exit based self-speculative decoding, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "The paper utilizes pipeline parallelism to overlap computations in LLM inference, which involves parallel computing techniques, but it primarily addresses inference optimization rather than distributed training, parallel computing for model training, or multi-node machine learning algorithms.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19369",
      "title": "SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use",
      "authors": [
        "Changhyun Jeon",
        "Jinhee Park",
        "Jungwoo Choi",
        "Keonwoo Kim",
        "Jisu Kim",
        "Minji Hong"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose a small-scale language model (SLM) based agent architecture,\nPlanner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G\nseparates planning, calling, and generation by role: the Planner produces an\ninitial batch plan with limited on-demand replanning; the Caller returns a\nnormalized call object after joint schema-value validation; and the Generator\nintegrates tool outputs to produce the final answer. We apply a Korean-first\nvalue policy to reduce execution failures caused by frequent Korean-to-English\ncode switching in Korean settings. Evaluation assumes Korean queries and Korean\ntool/parameter specifications; it covers single-chain, multi-chain,\nmissing-parameters, and missing-functions scenarios, and is conducted via an\nLLM-as-a-Judge protocol averaged over five runs under a unified I/O interface.\nResults show that P-C-G delivers competitive tool-use accuracy and end-to-end\nquality while reducing tokens and maintaining acceptable latency, indicating\nthat role-specialized SLMs are a cost-effective alternative for Korean tool-use\nagents.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.19369v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19369v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.366,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an SLM-based agent architecture for Korean tool use, emphasizing role separation and efficiency, but it does not involve training models with human feedback, reward models, or reinforcement learning techniques. There is no mention of aligning AI with human preferences through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a Planner-Caller-Generator architecture for multi-step reasoning and tool use, but it does not adapt diffusion models or their iterative refinement processes for logical tasks. There is no component involving holistic correction of reasoning paths via diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19370",
      "title": "Meow: End-to-End Outline Writing for Automatic Academic Survey",
      "authors": [
        "Zhaoyu Ma",
        "Yuan Shan",
        "Jiahao Zhao",
        "Nan Xu",
        "Lei Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As academic paper publication numbers grow exponentially, conducting in-depth\nsurveys with LLMs automatically has become an inevitable trend. Outline\nwriting, which aims to systematically organize related works, is critical for\nautomated survey generation. Yet existing automatic survey methods treat\noutline writing as mere workflow steps in the overall pipeline. Such\ntemplate-based workflows produce outlines that lack in-depth understanding of\nthe survey topic and fine-grained styles. To address these limitations, we\npropose Meow, the first metadata-driven outline writing framework that produces\norganized and faithful outlines efficiently. Specifically, we first formulate\noutline writing as an end-to-end task that generates hierarchical structured\noutlines from paper metadata. We then curate a high-quality dataset of surveys\nfrom arXiv, bioRxiv, and medRxiv, and establish systematic evaluation metrics\nfor outline quality assessment. Finally, we employ a two-stage training\napproach combining supervised fine-tuning and reinforcement learning. Our 8B\nreasoning model demonstrates strong performance with high structural fidelity\nand stylistic coherence.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.19370v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19370v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.289,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19371",
      "title": "How to inject knowledge efficiently? Knowledge Infusion Scaling Law for\n  Pre-training Large Language Models",
      "authors": [
        "Kangtao Lv",
        "Haibin Chen",
        "Yujin Yuan",
        "Langming Liu",
        "Shilei Liu",
        "Yongwei Wang",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have attracted significant attention due to\ntheir impressive general capabilities across diverse downstream tasks. However,\nwithout domain-specific optimization, they often underperform on specialized\nknowledge benchmarks and even produce hallucination. Recent studies show that\nstrategically infusing domain knowledge during pretraining can substantially\nimprove downstream performance. A critical challenge lies in balancing this\ninfusion trade-off: injecting too little domain-specific data yields\ninsufficient specialization, whereas excessive infusion triggers catastrophic\nforgetting of previously acquired knowledge. In this work, we focus on the\nphenomenon of memory collapse induced by over-infusion. Through systematic\nexperiments, we make two key observations, i.e. 1) Critical collapse point:\neach model exhibits a threshold beyond which its knowledge retention\ncapabilities sharply degrade. 2) Scale correlation: these collapse points scale\nconsistently with the model's size. Building on these insights, we propose a\nknowledge infusion scaling law that predicts the optimal amount of domain\nknowledge to inject into large LLMs by analyzing their smaller counterparts.\nExtensive experiments across different model sizes and pertaining token budgets\nvalidate both the effectiveness and generalizability of our scaling law.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.19371v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19371v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.412,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on knowledge infusion during pre-training of LLMs to optimize memorization and generalization, without any mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper involves programmatically generating a corpus from Wikidata for knowledge infusion, which somewhat resembles using high-level sources for training data, but it does not center on training models with noisy or imprecise labels as in weak supervision; instead, it emphasizes pre-training dynamics.",
      "diffusion_reasoning_justification": "The paper addresses knowledge infusion and scaling laws for LLMs during pre-training, with no discussion of diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "The paper mentions conducting large-scale experiments across model sizes, which may imply the use of distributed training setups, but its main contribution is on knowledge infusion scaling laws, not on algorithms or systems for parallel computing or data partitioning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19372",
      "title": "Representation-based Broad Hallucination Detectors Fail to Generalize\n  Out of Distribution",
      "authors": [
        "Zuzanna Dubanowska",
        "Maciej Żelaszczyk",
        "Michał Brzozowski",
        "Paolo Mandica",
        "Michał Karpowicz"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We critically assess the efficacy of the current SOTA in hallucination\ndetection and find that its performance on the RAGTruth dataset is largely\ndriven by a spurious correlation with data. Controlling for this effect,\nstate-of-the-art performs no better than supervised linear probes, while\nrequiring extensive hyperparameter tuning across datasets. Out-of-distribution\ngeneralization is currently out of reach, with all of the analyzed methods\nperforming close to random. We propose a set of guidelines for hallucination\ndetection and its evaluation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.19372v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19372v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.355,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating and critiquing state-of-the-art methods for detecting hallucinations in large language models (LLMs), particularly using internal representations like linear probes and attention mechanisms. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning or Chain-of-Thought tasks. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19375",
      "title": "Uncertainty Quantification of Large Language Models using Approximate\n  Bayesian Computation",
      "authors": [
        "Mridul Sharma",
        "Adeetya Patel",
        "Zaneta D' Souza",
        "Samira Abbasgholizadeh Rahimi",
        "Siva Reddy",
        "Sreenath Madathil"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Despite their widespread applications, Large Language Models (LLMs) often\nstruggle to express uncertainty, posing a challenge for reliable deployment in\nhigh stakes and safety critical domains like clinical diagnostics. Existing\nstandard baseline methods such as model logits and elicited probabilities\nproduce overconfident and poorly calibrated estimates. In this work, we propose\nApproximate Bayesian Computation (ABC), a likelihood-free Bayesian inference,\nbased approach that treats LLMs as a stochastic simulator to infer posterior\ndistributions over predictive probabilities. We evaluate our ABC approach on\ntwo clinically relevant benchmarks: a synthetic oral lesion diagnosis dataset\nand the publicly available GretelAI symptom-to-diagnosis dataset. Compared to\nstandard baselines, our approach improves accuracy by up to 46.9\\%, reduces\nBrier scores by 74.4\\%, and enhances calibration as measured by Expected\nCalibration Error (ECE) and predictive entropy.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.19375v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19375v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.333,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses issues with LLMs fine-tuned using RLHF, such as poor calibration, but its main contribution is proposing Approximate Bayesian Computation (ABC) for uncertainty quantification, not developing, implementing, or evaluating RLHF methods. There is no focus on training models with human feedback or reward models.",
      "weak_supervision_justification": "The paper mentions curating a synthetic dataset for benchmarking, which could involve programmatically generated labels similar to weak supervision, but the primary contribution is the ABC method for uncertainty quantification in LLMs, not techniques for training models with noisy or imprecise labels. Weak supervision is not a core element of the work.",
      "diffusion_reasoning_justification": "The paper focuses on Approximate Bayesian Computation (ABC) for inferring uncertainty in LLMs through simulation-based methods, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.20374",
      "title": "CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in\n  Computational Fluid Dynamics",
      "authors": [
        "Nithin Somasekharan",
        "Ling Yue",
        "Yadi Cao",
        "Weichao Li",
        "Patrick Emami",
        "Pochinapeddi Sai Bhargav",
        "Anurag Acharya",
        "Xingyu Xie",
        "Shaowu Pan"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong performance across\ngeneral NLP tasks, but their utility in automating numerical experiments of\ncomplex physical system -- a critical and labor-intensive component -- remains\nunderexplored. As the major workhorse of computational science over the past\ndecades, Computational Fluid Dynamics (CFD) offers a uniquely challenging\ntestbed for evaluating the scientific capabilities of LLMs. We introduce\nCFDLLMBench, a benchmark suite comprising three complementary components --\nCFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM\nperformance across three key competencies: graduate-level CFD knowledge,\nnumerical and physical reasoning of CFD, and context-dependent implementation\nof CFD workflows. Grounded in real-world CFD practices, our benchmark combines\na detailed task taxonomy with a rigorous evaluation framework to deliver\nreproducible results and quantify LLM performance across code executability,\nsolution accuracy, and numerical convergence behavior. CFDLLMBench establishes\na solid foundation for the development and evaluation of LLM-driven automation\nof numerical experiments for complex physical systems. Code and data are\navailable at https://github.com/NREL-Theseus/cfdllmbench/.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.20374v1",
      "pdf_url": "http://arxiv.org/pdf/2509.20374v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.417,
      "datasets_score": 0.42,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on benchmarking LLMs for CFD tasks, such as knowledge assessment and code generation, without any mention of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper introduces curated datasets for evaluating LLMs in CFD, but it does not involve training models with programmatically generated labels, noisy sources, or weak supervision methods; instead, it emphasizes benchmark creation and evaluation.",
      "diffusion_reasoning_justification": "The paper evaluates LLMs on CFD tasks using standard prompting and multi-agent frameworks, but it does not incorporate diffusion models, iterative refinement for reasoning, or multi-step logical processes based on diffusion techniques.",
      "distributed_training_justification": "The paper is centered on benchmarking LLMs for CFD applications and does not discuss training methods, parallel computing, data partitioning, or accelerating model training across multiple nodes.",
      "datasets_justification": "The paper's main contribution is the introduction and evaluation of CFDLLMBench, a new benchmark suite with curated datasets (e.g., CFDQuery, CFDCodeBench, and FoamBench) for assessing LLMs in scientific domains, directly aligning with dataset creation, curation, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "CFDLLMBench is a benchmark suite introduced to evaluate Large Language Models (LLMs) in Computational Fluid Dynamics (CFD), focusing on three competencies: graduate-level CFD knowledge, numerical and physical reasoning, and context-dependent implementation of CFD workflows. The methodology includes three components—CFDQuery for multiple-choice questions, CFDCodeBench for generating simulation code, and FoamBench for configuring OpenFOAM tasks—with evaluations revealing strong LLM performance on knowledge-based tasks but poor results on reasoning and implementation, which improve with multi-agent frameworks, thus highlighting challenges and opportunities for LLM applications in scientific automation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark suite tailored for evaluating LLMs in CFD, addressing an underexplored area in AI for scientific computing and significantly advancing the state-of-the-art in assessing LLMs' capabilities for numerical simulations.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within subfields like AI and computational science, as it provides a foundational tool for developing LLM-driven automation in CFD workflows, though its influence may be limited to specialized applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by establishing a new benchmark for LLM evaluation in CFD, making it essential for researchers in AI and computational fields to be aware of for advancing scientific automation efforts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/410c3cbdae383e5fb0a4d870908de89a5cb3924f",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 2,
      "average_h_index": 0.5555555555555556,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Nithin Somasekharan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/101044471"
        },
        {
          "name": "Ling Yue",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359688261"
        },
        {
          "name": "Yadi Cao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359725151"
        },
        {
          "name": "Weichao Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382293849"
        },
        {
          "name": "Patrick Emami",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381989578"
        },
        {
          "name": "Pochinapeddi Sai Bhargav",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381989433"
        },
        {
          "name": "Anurag Acharya",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381989296"
        },
        {
          "name": "Xingyu Xie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382316426"
        },
        {
          "name": "Shaowu Pan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2330945325"
        }
      ]
    },
    {
      "id": "2509.21351",
      "title": "Random Direct Preference Optimization for Radiography Report Generation",
      "authors": [
        "Valentin Samokhin",
        "Boris Shirokikh",
        "Mikhail Goncharov",
        "Dmitriy Umerenkov",
        "Maksim Bobrin",
        "Ivan Oseledets",
        "Dmitry Dylov",
        "Mikhail Belyaev"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Radiography Report Generation (RRG) has gained significant attention in\nmedical image analysis as a promising tool for alleviating the growing workload\nof radiologists. However, despite numerous advancements, existing methods have\nyet to achieve the quality required for deployment in real-world clinical\nsettings. Meanwhile, large Visual Language Models (VLMs) have demonstrated\nremarkable progress in the general domain by adopting training strategies\noriginally designed for Large Language Models (LLMs), such as alignment\ntechniques. In this paper, we introduce a model-agnostic framework to enhance\nRRG accuracy using Direct Preference Optimization (DPO). Our approach leverages\nrandom contrastive sampling to construct training pairs, eliminating the need\nfor reward models or human preference annotations. Experiments on supplementing\nthree state-of-the-art models with our Random DPO show that our method improves\nclinical performance metrics by up to 5%, without requiring any additional\ntraining data.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.21351v1",
      "pdf_url": "http://arxiv.org/pdf/2509.21351v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.465,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.379,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Direct Preference Optimization (DPO), which is inspired by preference-based learning similar to RLHF, as it optimizes model outputs based on comparisons. However, it relies on random sampling rather than human-ranked data or a separate reward model, so it does not meet the core criteria of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing Radiography Report Generation using Random DPO, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.21352",
      "title": "Improving Autism Detection with Multimodal Behavioral Analysis",
      "authors": [
        "William Saakyan",
        "Matthias Norden",
        "Lola Eversmann",
        "Simon Kirsch",
        "Muyu Lin",
        "Simon Guendelman",
        "Isabel Dziobek",
        "Hanna Drimalla"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Due to the complex and resource-intensive nature of diagnosing Autism\nSpectrum Condition (ASC), several computer-aided diagnostic support methods\nhave been proposed to detect autism by analyzing behavioral cues in patient\nvideo data. While these models show promising results on some datasets, they\nstruggle with poor gaze feature performance and lack of real-world\ngeneralizability. To tackle these challenges, we analyze a standardized video\ndataset comprising 168 participants with ASC (46% female) and 157 non-autistic\nparticipants (46% female), making it, to our knowledge, the largest and most\nbalanced dataset available. We conduct a multimodal analysis of facial\nexpressions, voice prosody, head motion, heart rate variability (HRV), and gaze\nbehavior. To address the limitations of prior gaze models, we introduce novel\nstatistical descriptors that quantify variability in eye gaze angles, improving\ngaze-based classification accuracy from 64% to 69% and aligning computational\nfindings with clinical research on gaze aversion in ASC. Using late fusion, we\nachieve a classification accuracy of 74%, demonstrating the effectiveness of\nintegrating behavioral markers across multiple modalities. Our findings\nhighlight the potential for scalable, video-based screening tools to support\nautism assessment.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.21352v1",
      "pdf_url": "http://arxiv.org/pdf/2509.21352v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.316,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.22688",
      "title": "Robust Object Detection for Autonomous Driving via Curriculum-Guided\n  Group Relative Policy Optimization",
      "authors": [
        "Xu Jia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) excel in vision-language reasoning\nbut often struggle with structured perception tasks requiring precise\nlocalization and robustness. We propose a reinforcement learning framework that\naugments Group Relative Policy Optimization (GRPO) with curriculum-based data\nscheduling and difficulty-aware filtering. This approach stabilizes\noptimization under sparse, noisy rewards and enables progressive adaptation to\ncomplex samples. Evaluations on autonomous driving benchmarks demonstrate\nsubstantial improvements in detection accuracy and robustness. Ablation studies\nconfirm the importance of reward design, KL regularization, and curriculum\npacing for convergence stability and generalization. Our findings highlight\nreinforcement-driven optimization with structured data curricula as a scalable\npath toward robust and interpretable multimodal detection.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.22688v2",
      "pdf_url": "http://arxiv.org/pdf/2509.22688v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.446,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.409,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a reinforcement learning framework using GRPO with automated rewards like Accuracy-IoU for object detection, but it does not involve human-ranked data or a separate reward model trained on human preferences. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "The paper addresses sparse and noisy rewards, uses difficulty-aware filtering to handle annotation noise and distributional shifts, and programmatically prioritizes high-quality samples, which relates to weak supervision techniques. However, it is not the primary focus, as the core contribution is the RL framework rather than label generation from noisy sources.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes. It centers on reinforcement learning and curriculum-based optimization for object detection, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "The paper discusses optimization techniques for RL but does not mention distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors. The focus is on algorithmic improvements, not scaling hardware.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper proposes a reinforcement learning framework that enhances Group Relative Policy Optimization (GRPO) with curriculum-based data scheduling and difficulty-aware filtering to improve the robustness of Multimodal Large Language Models (MLLMs) in object detection for autonomous driving. By addressing challenges such as sparse rewards and early exposure to complex samples, the methodology enables progressive learning and achieves significant improvements in detection accuracy, including up to 9.4% gains in Intersection over Union (IoU) on benchmarks like BDD-100K and CODA.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining GRPO with curriculum learning to stabilize training for MLLMs in detection tasks, offering a clever adaptation of existing techniques rather than a completely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of multimodal perception for autonomous driving, given its demonstrated enhancements in accuracy and robustness on real-world benchmarks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with practical advancements in MLLM optimization for critical applications like autonomous driving, making it essential for researchers in computer vision to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b50c729538426b2cdbbf1e08731b640aaeb5723b",
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.22689",
      "title": "Graph-Theoretic Consistency for Robust and Topology-Aware\n  Semi-Supervised Histopathology Segmentation",
      "authors": [
        "Ha-Hieu Pham",
        "Minh Le",
        "Han Huynh",
        "Nguyen Quoc Khanh Le",
        "Huy-Hieu Pham"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Semi-supervised semantic segmentation (SSSS) is vital in computational\npathology, where dense annotations are costly and limited. Existing methods\noften rely on pixel-level consistency, which propagates noisy pseudo-labels and\nproduces fragmented or topologically invalid masks. We propose Topology Graph\nConsistency (TGC), a framework that integrates graph-theoretic constraints by\naligning Laplacian spectra, component counts, and adjacency statistics between\nprediction graphs and references. This enforces global topology and improves\nsegmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC\nachieves state-of-the-art performance under 5-10% supervision and significantly\nnarrows the gap to full supervision. Code is available at\nhttps://github.com/hieuphamha19/TGC.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.22689v1",
      "pdf_url": "http://arxiv.org/pdf/2509.22689v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.291,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on semi-supervised semantic segmentation, which uses a small amount of labeled data (5-10%) alongside unlabeled data to train models, aligning with weak supervision by relying on potentially noisy pseudo-labels derived from limited annotations. However, the main contribution emphasizes graph-theoretic constraints for improving segmentation accuracy rather than directly addressing programmatic label generation from high-level sources.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Topology Graph Consistency (TGC), a novel framework for semi-supervised semantic segmentation in histopathology, aiming to address the limitations of pixel-level consistency methods by incorporating graph-theoretic constraints to preserve global topology and reduce noisy pseudo-labels. It achieves this by aligning Laplacian spectra, component counts, and adjacency statistics between prediction graphs and references in a dual-network setup, resulting in state-of-the-art performance on GlaS and CRAG datasets with only 5-10% supervision, thereby narrowing the gap to fully supervised approaches.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by integrating graph-theoretic constraints like Laplacian spectra alignment into semi-supervised segmentation, significantly advancing the state-of-the-art in handling topological structures in histopathology.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computational pathology and medical image segmentation due to its improvements in semi-supervised methods, though its influence may be limited to specialized applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution with state-of-the-art results in a relevant field, making it essential for researchers in computer vision and pathology to be aware of its advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/99681a18adfe1a28d7459107541e9707e653097c",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 1,
      "average_h_index": 0.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ha-Hieu Pham",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355342886"
        },
        {
          "name": "M. Le",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355310231"
        },
        {
          "name": "H. Huynh",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2341133413"
        },
        {
          "name": "Nguyen Quoc Khanh Le",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353487340"
        },
        {
          "name": "Huy-Hieu Pham",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372396334"
        }
      ]
    },
    {
      "id": "2509.22690",
      "title": "A review of Recent Techniques for Person Re-Identification",
      "authors": [
        "Andrea Asperti",
        "Salvatore Fiorilla",
        "Simone Nardi",
        "Lorenzo Orsini"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Person re-identification (ReId), a crucial task in surveillance, involves\nmatching individuals across different camera views. The advent of Deep\nLearning, especially supervised techniques like Convolutional Neural Networks\nand Attention Mechanisms, has significantly enhanced person Re-ID. However, the\nsuccess of supervised approaches hinges on vast amounts of annotated data,\nposing scalability challenges in data labeling and computational costs. To\naddress these limitations, recent research has shifted towards unsupervised\nperson re-identification. Leveraging abundant unlabeled data, unsupervised\nmethods aim to overcome the need for pairwise labelled data. Although\ntraditionally trailing behind supervised approaches, unsupervised techniques\nhave shown promising developments in recent years, signalling a narrowing\nperformance gap. Motivated by this evolving landscape, our survey pursues two\nprimary objectives. First, we review and categorize significant publications in\nsupervised person re-identification, providing an in-depth overview of the\ncurrent state-of-the-art and emphasizing little room for further improvement in\nthis domain. Second, we explore the latest advancements in unsupervised person\nre-identification over the past three years, offering insights into emerging\ntrends and shedding light on the potential convergence of performance between\nsupervised and unsupervised paradigms. This dual-focus survey aims to\ncontribute to the evolving narrative of person re-identification, capturing\nboth the mature landscape of supervised techniques and the promising outcomes\nin the realm of unsupervised learning.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.22690v1",
      "pdf_url": "http://arxiv.org/pdf/2509.22690v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.331,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.22691",
      "title": "Sequential Token Merging: Revisiting Hidden States",
      "authors": [
        "Yan Wen",
        "Peng Ye",
        "Lin Zhang",
        "Baopu Li",
        "Jiakang Yuan",
        "Yaoxin Yang",
        "Tao Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision Mambas (ViMs) achieve remarkable success with sub-quadratic\ncomplexity, but their efficiency remains constrained by quadratic token scaling\nwith image resolution. While existing methods address token redundancy, they\noverlook ViMs' intrinsic Limited Directional Sequential Dependence (LDSD) - a\ncritical information flow mechanism revealed in our analysis. We further\nidentify Mamba's selective scan enables gradual information aggregation in\nhidden states. Based on these insights, we propose Sequential Token Merging\n(STM), featuring: 1) Bidirectional nearest neighbor merging to preserve\nsequential dependencies through symmetric spatial aggregation, and 2) Hidden\nstates protection to stabilize the hidden states around the class token. STM\nstrategically leverages Mamba's layer-wise loss convergence to convert temporal\nforgetfulness into stability. Experiments demonstrate STM's superiority: 1.0%\naccuracy drop for ViM-Ti at 20% token reduction, and only 1.4% degradation for\nViM-S at 40% reduction. Our method achieves state-of-the-art efficiency with\nminimal complexity, while providing new insights into state-space model\ndynamics. Codes will be released soon.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.22691v1",
      "pdf_url": "http://arxiv.org/pdf/2509.22691v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.396,
      "datasets_score": 0.263,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving efficiency in Vision Mambas (ViMs) through sequential token merging and hidden state management, which is centered on state-space models for computer vision tasks. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning or Chain-of-Thought tasks. Therefore, there is no connection to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.22692",
      "title": "Deep Learning Empowered Super-Resolution: A Comprehensive Survey and\n  Future Prospects",
      "authors": [
        "Le Zhang",
        "Ao Li",
        "Qibin Hou",
        "Ce Zhu",
        "Yonina C. Eldar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Super-resolution (SR) has garnered significant attention within the computer\nvision community, driven by advances in deep learning (DL) techniques and the\ngrowing demand for high-quality visual applications. With the expansion of this\nfield, numerous surveys have emerged. Most existing surveys focus on specific\ndomains, lacking a comprehensive overview of this field. Here, we present an\nin-depth review of diverse SR methods, encompassing single image\nsuper-resolution (SISR), video super-resolution (VSR), stereo super-resolution\n(SSR), and light field super-resolution (LFSR). We extensively cover over 150\nSISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR\nand LFSR. We analyze methodologies, datasets, evaluation protocols, empirical\nresults, and complexity. In addition, we conducted a taxonomy based on each\nbackbone structure according to the diverse purposes. We also explore valuable\nyet under-studied open issues in the field. We believe that this work will\nserve as a valuable resource and offer guidance to researchers in this domain.\nTo facilitate access to related work, we created a dedicated repository\navailable at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.22692v1",
      "pdf_url": "http://arxiv.org/pdf/2509.22692v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.37,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.25203",
      "title": "Generating High-Quality Datasets for Code Editing via Open-Source\n  Language Models",
      "authors": [
        "Zekai Zhang",
        "Mingwei Liu",
        "Zhenxi Chen",
        "Linxi Liang",
        "Yuxuan Chen",
        "Guangsheng Ou",
        "Yanlin Wang",
        "Dan Li",
        "Xin Peng",
        "Zibin Zheng"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Code editing plays a vital role in software engineering, requiring developers\nto adjust existing code according to natural language instructions while\nkeeping functionality intact and avoiding unnecessary modifications. However,\ncommit-based datasets commonly used for this task are often noisy, lack\ndiversity, and fail to reflect the style of real-world edit instructions. To\naddress this, we introduce OpenCodeEdit, an open-source pipeline that leverages\nmultiple LLMs to synthesize realistic code-edit triplets. The pipeline produces\nboth concise \"lazy\" instructions and more detailed \"descriptive\" ones, and\napplies filtering based on diffs and topics to guarantee data quality and\nvariety. Using this process, we construct OCEDataFT, a curated dataset of 20K\nsamples. Fine-tuning three advanced base models on OCEDataFT leads to\nsignificant performance boosts on the CanItEdit benchmark, with relative pass@1\nimprovements ranging from 4.50% to 20.79%. Notably, the resulting models\nachieve performance close to closed-source systems, narrowing the gap to GPT-4\nto just 3.54%, without relying on proprietary resources or manual annotation.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.25203v3",
      "pdf_url": "http://arxiv.org/pdf/2509.25203v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.433,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.359,
      "datasets_score": 0.439,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper uses LLMs to programmatically generate synthetic code-edit triplets, which aligns with weak supervision by creating labels automatically from high-level sources without manual annotation. However, it focuses more on dataset synthesis for code editing rather than deeply exploring weak supervision techniques as a primary method.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and curating a new dataset (OCEDataFT) for code editing, including methodologies for synthesis, filtering, and evaluation, which directly matches research on dataset introduction, curation, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces OpenCodeEdit, an open-source pipeline that utilizes multiple large language models (LLMs) to synthesize high-quality code editing datasets by generating realistic code-edit triplets from existing code snippets, incorporating both concise and detailed instructions, and applying filtering for quality and diversity. The methodology results in the OCEDataFT dataset with 20,000 samples, and fine-tuning advanced base models on this dataset demonstrates significant performance improvements on the CanItEdit benchmark, with relative pass@1 gains of 4.50% to 20.79% and narrowing the gap with GPT-4 to just 3.54%, highlighting the effectiveness of using open-source LLMs for code editing tasks without proprietary resources.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing an open-source pipeline for synthesizing code editing datasets, which cleverly combines existing LLMs to address gaps in diversity and quality of prior datasets, though it builds on established synthetic data techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI for software engineering, as it provides an accessible pipeline and dataset for enhancing code editing models, potentially influencing research and practical applications in code maintenance.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a strong, valuable contribution with practical tools for generating code editing datasets using open-source resources, making it essential for researchers and practitioners focused on AI in software engineering to stay informed and potentially apply these methods.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/212072d3466f1add14734cd7f9586a71c62b989b",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 9,
      "average_h_index": 2.2222222222222223,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Zekai Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2257893637"
        },
        {
          "name": "Mingwei Liu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2309320008"
        },
        {
          "name": "Zhenxi Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2383051858"
        },
        {
          "name": "Linxi Liang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yuxuan Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2328245154"
        },
        {
          "name": "Guangsheng Ou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2328091513"
        },
        {
          "name": "Yanlin Wang",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2239164852"
        },
        {
          "name": "Dan Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2383511993"
        },
        {
          "name": "Xing Peng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2334527641"
        },
        {
          "name": "Zibin Zheng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2283261402"
        }
      ]
    },
    {
      "id": "2509.25204",
      "title": "Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for\n  Controlled Text Generation",
      "authors": [
        "Jin Li",
        "Zhebo Wang",
        "Tianliang Lu",
        "Mohan Li",
        "Wenpeng Xing",
        "Meng Han"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Entropy-based inference methods have gained traction for improving the\nreliability of Large Language Models (LLMs). However, many existing approaches,\nsuch as entropy minimization techniques, suffer from high computational\noverhead and fail to leverage historical token context effectively. To address\nthese limitations, we propose Spectral Logit Sculpting (SLS), a lightweight\ninference-time optimization method that dynamically modulates token\ndistributions using spectral and entropic properties of recent logits. SLS\nmaintains a sliding buffer of top-K logits, performs on-the-fly Singular Value\nDecomposition (SVD) to identify dominant spectral directions, and adaptively\nrescales logits based on both entropy and logit gap statistics--only activating\nwhen uncertainty is high. Without updating any model parameters, SLS\neffectively sharpens the output distribution while preserving contextual\nconsistency. Experimental results on multiple public benchmarks demonstrate\nthat SLS consistently outperforms existing baseline methods, achieving superior\naccuracy in mathematical, coding, and scientific reasoning tasks.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.25204v1",
      "pdf_url": "http://arxiv.org/pdf/2509.25204v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.349,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution, Spectral Logit Sculpting (SLS), is an inference-time optimization method that adjusts token distributions using spectral and entropic properties without updating model parameters. It does not involve training a reward model with human-ranked data or using reinforcement learning based on human feedback. While the introduction references other methods comparable to reinforcement learning, SLS itself does not incorporate RLHF elements.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on SLS, which uses Singular Value Decomposition (SVD) on logits for adaptive rescaling, but it does not adapt the iterative refinement process of diffusion models for multi-step logical reasoning or treat Chain-of-Thought as a holistically corrected entity. There is no mention of diffusion-based mechanisms, making this topic unrelated to the paper's core contributions.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.25534",
      "title": "Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended\n  Reasoning",
      "authors": [
        "Zhiling Ye",
        "Yun Yue",
        "Haowen Wang",
        "Xudong Han",
        "Jiadi Jiang",
        "Cheng Wei",
        "Lei Fan",
        "Jiaxin Liang",
        "Shuowen Zhang",
        "Ji Li",
        "Chunxiao Guo",
        "Jian Wang",
        "Peng Wei",
        "Jinjie Gu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Open-ended evaluation is essential for deploying large language models in\nreal-world settings. In studying HealthBench, we observe that using the model\nitself as a grader and generating rubric-based reward signals substantially\nimproves reasoning performance. Remarkably, the trained model also becomes a\nstronger grader. Motivated by this, we introduce Self-Rewarding Rubric-Based\nReinforcement Learning for Open-Ended Reasoning, a lightweight framework that\nenables faster and more resource-efficient training while surpassing baselines.\nRemarkably, on Qwen3-32B, training with just the 4000-sample HealthBench Easy\nsubset is sufficient to obtain a model that exceeds GPT-5 on HealthBench Hard.\nIncorporating a small amount of teacher-graded data further enhances\nperformance for less capable models.",
      "published_date": "2025-09-19",
      "arxiv_url": "http://arxiv.org/abs/2509.25534v1",
      "pdf_url": "http://arxiv.org/pdf/2509.25534v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.508,
      "weak_supervision_score": 0.437,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.376,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with rewards generated by the model itself via rubrics, rather than a separate reward model trained on human-ranked data. While it incorporates a small amount of teacher-graded data (e.g., from GPT-4), this is not the primary mechanism, making it only loosely related to RLHF.",
      "weak_supervision_justification": "The paper employs the model to programmatically generate reward signals based on rubrics, which aligns with weak supervision by using noisy or self-derived labels instead of perfect human annotations. However, it is not the central focus, as the method also integrates some teacher-graded data.",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for reasoning tasks without any mention of diffusion models, iterative refinement processes, or treating chain-of-thought as a single entity for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Self-Rewarding Rubric-Based Reinforcement Learning, a framework designed to enhance open-ended reasoning in large language models (LLMs) by using the model itself as a grader to generate rubric-based reward signals, particularly for the HealthBench benchmark in healthcare. The methodology involves training models like Qwen3-32B on a small subset of data, demonstrating that this approach improves reasoning performance, enables the model to become a stronger grader, and surpasses advanced models like GPT-5 on harder tasks, with additional benefits from incorporating teacher-graded data for less capable models.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing reinforcement learning techniques with self-grading and rubric-based rewards for open-ended reasoning, offering a new way to address challenges in real-world applications like healthcare. However, it builds on established concepts such as RLVR and LLM-as-Judge rather than introducing a entirely new problem or architecture.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research and applications in LLM training by demonstrating resource-efficient methods that enhance performance on open-ended tasks, potentially extending to other domains beyond healthcare. Its ability to surpass leading models with minimal data could lead to broader adoption in scalable AI development.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to AI and machine learning by advancing efficient training techniques for LLMs in open-ended reasoning, making it essential for researchers in the field to be aware of its insights and findings. While impactful, it may not be universally critical for all readers outside of specific subfields like reinforcement learning and healthcare applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a6fbcfabbb20c8848364ec44c8a1dea18be00efe",
      "total_authors": 14,
      "authors_found": 10,
      "highest_h_index": 7,
      "average_h_index": 1.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Zhiling Ye",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yun Yue",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2269463065"
        },
        {
          "name": "Haowen Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2268638594"
        },
        {
          "name": "Xudong Han",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2351140507"
        },
        {
          "name": "Jiadi Jiang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2269743503"
        },
        {
          "name": "Cheng Wei",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Lei Fan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jiaxin Liang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2338154809"
        },
        {
          "name": "Shuowen Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2375393137"
        },
        {
          "name": "Ji Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375399756"
        },
        {
          "name": "Chunxiao Guo",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jian Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2316666144"
        },
        {
          "name": "Peng Wei",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376330307"
        },
        {
          "name": "Jinjie Gu",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2269769748"
        }
      ]
    }
  ],
  "total_papers": 232,
  "date": "2025-09-19"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
