<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 04 September 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 04 September 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 04 September 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2509.03780",
      "title": "Natural Latents: Latent Variables Stable Across Ontologies",
      "authors": [
        "John Wentworth",
        "David Lorell"
      ],
      "categories": [
        "math.PR (Probability)",
        "cs.AI (Artificial Intelligence)",
        "cs.IT (Information Theory)",
        "cs.LG (Machine Learning)",
        "math.IT (Information Theory)"
      ],
      "abstract": "Suppose two Bayesian agents each learn a generative model of the same\nenvironment. We will assume the two have converged on the predictive\ndistribution, i.e. distribution over some observables in the environment, but\nmay have different generative models containing different latent variables.\nUnder what conditions can one agent guarantee that their latents are a function\nof the other agents latents?\n  We give simple conditions under which such translation is guaranteed to be\npossible: the natural latent conditions. We also show that, absent further\nconstraints, these are the most general conditions under which translatability\nis guaranteed. Crucially for practical application, our theorems are robust to\napproximation error in the natural latent conditions.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03780v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03780v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.278,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses Bayesian agents, generative models, and conditions for translating latent variables across ontologies, focusing on stability and approximation errors. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are central to diffusion-based reasoning. Therefore, there is no overlap in concepts or methodologies.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03786",
      "title": "SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object\n  Detection",
      "authors": [
        "Xinxin Huang",
        "Han Sun",
        "Ningzhong Liu",
        "Huiyu Zhou",
        "Yinan Yao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Underwater Camouflaged Object Detection (UCOD) aims to identify objects that\nblend seamlessly into underwater environments. This task is critically\nimportant to marine ecology. However, it remains largely underexplored and\naccurate identification is severely hindered by optical distortions, water\nturbidity, and the complex traits of marine organisms. To address these\nchallenges, we introduce the UCOD task and present DeepCamo, a benchmark\ndataset designed for this domain. We also propose Semantic Localization and\nEnhancement Network (SLENet), a novel framework for UCOD. We first benchmark\nstate-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet\nis built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)\nmodule and a Localization Guidance Branch (LGB) to enhance multi-scale feature\nrepresentation while generating a location map enriched with global semantic\ninformation. This map guides the Multi-Scale Supervised Decoder (MSSD) to\nproduce more accurate predictions. Experiments on our DeepCamo dataset and\nthree benchmark COD datasets confirm SLENet's superior performance over SOTA\nmethods, and underscore its high generality for the broader COD task.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03786v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03786v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.265,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.288,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03790",
      "title": "What Fundamental Structure in Reward Functions Enables Efficient\n  Sparse-Reward Learning?",
      "authors": [
        "Ibne Farabi Shihab",
        "Sanjeda Akter",
        "Anuj Sharma"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Sparse-reward reinforcement learning (RL) remains fundamentally hard: without\nstructure, any agent needs $\\Omega(|\\mathcal{S}||\\mathcal{A}|/p)$ samples to\nrecover rewards. We introduce Policy-Aware Matrix Completion (PAMC) as a first\nconcrete step toward a structural reward learning framework. Our key idea is to\nexploit approximate low-rank + sparse structure in the reward matrix, under\npolicy-biased (MNAR) sampling. We prove recovery guarantees with\ninverse-propensity weighting, and establish a visitation-weighted\nerror-to-regret bound linking completion error to control performance.\nImportantly, when assumptions weaken, PAMC degrades gracefully: confidence\nintervals widen and the algorithm abstains, ensuring safe fallback to\nexploration. Empirically, PAMC improves sample efficiency across Atari-26 (10M\nsteps), DM Control, MetaWorld MT50, D4RL offline RL, and preference-based RL\nbenchmarks, outperforming DrQ-v2, DreamerV3, Agent57, T-REX/D-REX, and PrefPPO\nunder compute-normalized comparisons. Our results highlight PAMC as a practical\nand principled tool when structural rewards exist, and as a concrete first\ninstantiation of a broader structural reward learning perspective.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03790v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03790v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.457,
      "weak_supervision_score": 0.435,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.365,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on exploiting structure in sparse environmental rewards using Policy-Aware Matrix Completion (PAMC), without involving human feedback, ranked data, or a separate reward model trained on human preferences. While it mentions reward modeling in passing as a related approach, the core method does not rely on or incorporate human feedback, making it unrelated to RLHF.",
      "weak_supervision_justification": "The paper's approach to handling missing rewards through matrix completion and inverse-propensity weighting from partial, policy-biased observations resembles weak supervision by programmatically inferring incomplete data. However, it is not a direct application, as weak supervision typically involves label generation for supervised learning, whereas this is tailored to RL reward structures.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates the structural properties of reward functions that enable efficient learning in sparse-reward reinforcement learning, hypothesizing that low-rank and sparse structures can significantly reduce sample complexity. It introduces Policy-Aware Matrix Completion (PAMC), a method that exploits these structures under policy-biased sampling, provides theoretical guarantees for reward recovery and error-to-regret bounds, and demonstrates empirical improvements in sample efficiency across benchmarks like Atari-26, DM Control, and others, positioning PAMC as a foundational tool for structural reward learning.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework called Policy-Aware Matrix Completion that advances the state-of-the-art by providing the first systematic theoretical foundation for exploiting structural properties in reward functions for sparse-reward RL.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future RL research and applications by enabling more efficient learning in sparse-reward environments, as demonstrated by its superior performance on diverse benchmarks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, significant contribution with novel theoretical insights and empirical results that advance RL, making it essential for researchers in machine learning and AI to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c84c48399db70e99eadb5f081e91a9c4a29a2267",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 1.6666666666666667,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ibne Farabi Shihab",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/66226596"
        },
        {
          "name": "Sanjeda Akter",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359252749"
        },
        {
          "name": "Anuj Sharma",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2297251026"
        }
      ]
    },
    {
      "id": "2509.03791",
      "title": "SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation\n  Evaluation",
      "authors": [
        "Saki Imai",
        "Mert İnan",
        "Anthony Sicilia",
        "Malihe Alikhani"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Evaluating sign language generation is often done through back-translation,\nwhere generated signs are first recognized back to text and then compared to a\nreference using text-based metrics. However, this two-step evaluation pipeline\nintroduces ambiguity: it not only fails to capture the multimodal nature of\nsign language-such as facial expressions, spatial grammar, and prosody-but also\nmakes it hard to pinpoint whether evaluation errors come from sign generation\nmodel or the translation system used to assess it. In this work, we propose\nSiLVERScore, a novel semantically-aware embedding-based evaluation metric that\nassesses sign language generation in a joint embedding space. Our contributions\ninclude: (1) identifying limitations of existing metrics, (2) introducing\nSiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness\nto semantic and prosodic variations, and (4) exploring generalization\nchallenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore\nachieves near-perfect discrimination between correct and random pairs (ROC AUC\n= 0.99, overlap < 7%), substantially outperforming traditional metrics.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03791v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03791v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.288,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03793",
      "title": "SAMVAD: A Multi-Agent System for Simulating Judicial Deliberation\n  Dynamics in India",
      "authors": [
        "Prathamesh Devadiga",
        "Omkaar Jayadev Shetty",
        "Pooja Agarwal"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Understanding the complexities of judicial deliberation is crucial for\nassessing the efficacy and fairness of a justice system. However, empirical\nstudies of judicial panels are constrained by significant ethical and practical\nbarriers. This paper introduces SAMVAD, an innovative Multi-Agent System (MAS)\ndesigned to simulate the deliberation process within the framework of the\nIndian justice system.\n  Our system comprises agents representing key judicial roles: a Judge, a\nProsecution Counsel, a Defense Counsel, and multiple Adjudicators (simulating a\njudicial bench), all powered by large language models (LLMs). A primary\ncontribution of this work is the integration of Retrieval-Augmented Generation\n(RAG), grounded in a domain-specific knowledge base of landmark Indian legal\ndocuments, including the Indian Penal Code and the Constitution of India. This\nRAG functionality enables the Judge and Counsel agents to generate legally\nsound instructions and arguments, complete with source citations, thereby\nenhancing both the fidelity and transparency of the simulation.\n  The Adjudicator agents engage in iterative deliberation rounds, processing\ncase facts, legal instructions, and arguments to reach a consensus-based\nverdict. We detail the system architecture, agent communication protocols, the\nRAG pipeline, the simulation workflow, and a comprehensive evaluation plan\ndesigned to assess performance, deliberation quality, and outcome consistency.\n  This work provides a configurable and explainable MAS platform for exploring\nlegal reasoning and group decision-making dynamics in judicial simulations,\nspecifically tailored to the Indian legal context and augmented with verifiable\nlegal grounding via RAG.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03793v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03793v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.291,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a Multi-Agent System (MAS) for simulating judicial deliberation using LLMs and RAG, with iterative rounds for consensus-building. However, it does not involve diffusion models or adapt the iterative refinement process of diffusion for logical tasks. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction, making the paper unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03794",
      "title": "Fitting Image Diffusion Models on Video Datasets",
      "authors": [
        "Juhun Lee",
        "Simon S. Woo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Image diffusion models are trained on independently sampled static images.\nWhile this is the bedrock task protocol in generative modeling, capturing the\ntemporal world through the lens of static snapshots is information-deficient by\ndesign. This limitation leads to slower convergence, limited distributional\ncoverage, and reduced generalization. In this work, we propose a simple and\neffective training strategy that leverages the temporal inductive bias present\nin continuous video frames to improve diffusion training. Notably, the proposed\nmethod requires no architectural modification and can be seamlessly integrated\ninto standard diffusion training pipelines. We evaluate our method on the\nHandCo dataset, where hand-object interactions exhibit dense temporal coherence\nand subtle variations in finger articulation often result in semantically\ndistinct motions. Empirically, our method accelerates convergence by over\n2$\\text{x}$ faster and achieves lower FID on both training and validation\ndistributions. It also improves generative diversity by encouraging the model\nto capture meaningful temporal variations. We further provide an optimization\nanalysis showing that our regularization reduces the gradient variance, which\ncontributes to faster convergence.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03794v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03794v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.548,
      "distributed_training_score": 0.364,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is improving image diffusion models by incorporating temporal data from videos for better training efficiency and generative performance, without any focus on multi-step logical reasoning or adapting diffusion for complex tasks like chain-of-thought processes. It deals solely with generative modeling for visual data, lacking any components related to reasoning or iterative refinement for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03800",
      "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in\n  3D CT Disease Detection, Understanding and Reporting",
      "authors": [
        "Yuheng Li",
        "Yenho Chen",
        "Yuxiang Lai",
        "Jike Zhong",
        "Vanessa Wildman",
        "Xiaofeng Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Radiologic diagnostic errors-under-reading errors, inattentional blindness,\nand communication failures-remain prevalent in clinical practice. These issues\noften stem from missed localized abnormalities, limited global context, and\nvariability in report language. These challenges are amplified in 3D imaging,\nwhere clinicians must examine hundreds of slices per scan. Addressing them\nrequires systems with precise localized detection, global volume-level\nreasoning, and semantically consistent natural language reporting. However,\nexisting 3D vision-language models are unable to meet all three needs jointly,\nlacking local-global understanding for spatial reasoning and struggling with\nthe variability and noise of uncurated radiology reports. We present\nMedVista3D, a multi-scale semantic-enriched vision-language pretraining\nframework for 3D CT analysis. To enable joint disease detection and holistic\ninterpretation, MedVista3D performs local and global image-text alignment for\nfine-grained representation learning within full-volume context. To address\nreport variability, we apply language model rewrites and introduce a Radiology\nSemantic Matching Bank for semantics-aware alignment. MedVista3D achieves\nstate-of-the-art performance on zero-shot disease classification, report\nretrieval, and medical visual question answering, while transferring well to\norgan segmentation and prognosis prediction. Code and datasets will be\nreleased.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03800v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03800v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.364,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on MedVista3D, a vision-language model for 3D CT analysis that emphasizes multi-scale image-text alignment, semantic enrichment, and report refinement using LLMs. It does not involve diffusion models, iterative refinement for logical tasks, or treating a Chain-of-Thought as an entity for holistic correction. Thus, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03803",
      "title": "Causality-guided Prompt Learning for Vision-language Models via Visual\n  Granulation",
      "authors": [
        "Mengyu Gao",
        "Qiulei Dong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Prompt learning has recently attracted much attention for adapting\npre-trained vision-language models (e.g., CLIP) to downstream recognition\ntasks. However, most of the existing CLIP-based prompt learning methods only\nshow a limited ability for handling fine-grained datasets. To address this\nissue, we propose a causality-guided text prompt learning method via visual\ngranulation for CLIP, called CaPL, where the explored visual granulation\ntechnique could construct sets of visual granules for the text prompt to\ncapture subtle discrepancies among different fine-grained classes through\ncasual inference. The CaPL method contains the following two modules: (1) An\nattribute disentanglement module is proposed to decompose visual features into\nnon-individualized attributes (shared by some classes) and individualized\nattributes (specific to single classes) using a Brownian Bridge Diffusion\nModel; (2) A granule learning module is proposed to construct visual granules\nby integrating the aforementioned attributes for recognition under two causal\ninference strategies. Thanks to the learned visual granules, more\ndiscriminative text prompt is expected to be learned. Extensive experimental\nresults on 15 datasets demonstrate that our CaPL method significantly\noutperforms the state-of-the-art prompt learning methods, especially on\nfine-grained datasets.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03803v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03803v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.322,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a Brownian Bridge Diffusion Model (BBDM) for attribute disentanglement in visual features, which involves iterative refinement processes typical of diffusion models. However, this application is focused on vision-language tasks for fine-grained recognition, not on adapting diffusion for multi-step logical reasoning or holistic correction of a Chain-of-Thought as specified in the topic. Thus, while diffusion is present, it is not used for complex logical tasks, making the relevance tangential.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03805",
      "title": "Measuring How (Not Just Whether) VLMs Build Common Ground",
      "authors": [
        "Saki Imai",
        "Mert İnan",
        "Anthony Sicilia",
        "Malihe Alikhani"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large vision language models (VLMs) increasingly claim reasoning skills, yet\ncurrent benchmarks evaluate them in single-turn or question answering settings.\nHowever, grounding is an interactive process in which people gradually develop\nshared understanding through ongoing communication. We introduce a four-metric\nsuite (grounding efficiency, content alignment, lexical adaptation, and\nhuman-likeness) to systematically evaluate VLM performance in interactive\ngrounding contexts. We deploy the suite on 150 self-play sessions of\ninteractive referential games between three proprietary VLMs and compare them\nwith human dyads. All three models diverge from human patterns on at least\nthree metrics, while GPT4o-mini is the closest overall. We find that (i) task\nsuccess scores do not indicate successful grounding and (ii) high\nimage-utterance alignment does not necessarily predict task success. Our metric\nsuite and findings offer a framework for future research on VLM grounding.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03805v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03805v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.351,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions RLHF as part of current training pipelines for VLMs but does not focus on implementing, advancing, or evaluating RLHF. Instead, it critiques the lack of emphasis on interactive skills in such methods, making it only indirectly related to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a metric suite for evaluating VLMs in interactive grounding contexts and does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03808",
      "title": "EGTM: Event-guided Efficient Turbulence Mitigation",
      "authors": [
        "Huanan Li",
        "Rui Fan",
        "Juntao Guan",
        "Weidong Hao",
        "Lai Rui",
        "Tong Wu",
        "Yikai Wang",
        "Lin Gu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Turbulence mitigation (TM) aims to remove the stochastic distortions and\nblurs introduced by atmospheric turbulence into frame cameras. Existing\nstate-of-the-art deep-learning TM methods extract turbulence cues from multiple\ndegraded frames to find the so-called \"lucky'', not distorted patch, for \"lucky\nfusion''. However, it requires high-capacity network to learn from\ncoarse-grained turbulence dynamics between synchronous frames with limited\nframe-rate, thus fall short in computational and storage efficiency. Event\ncameras, with microsecond-level temporal resolution, have the potential to\nfundamentally address this bottleneck with efficient sparse and asynchronous\nimaging mechanism. In light of this, we (i) present the fundamental\n\\textbf{``event-lucky insight''} to reveal the correlation between turbulence\ndistortions and inverse spatiotemporal distribution of event streams. Then,\nbuild upon this insight, we (ii) propose a novel EGTM framework that extracts\npixel-level reliable turbulence-free guidance from the explicit but noisy\nturbulent events for temporal lucky fusion. Moreover, we (iii) build the first\nturbulence data acquisition system to contribute the first real-world\nevent-driven TM dataset. Extensive experimental results demonstrate that our\napproach significantly surpass the existing SOTA TM method by 710 times, 214\ntimes and 224 times in model size, inference latency and model complexity\nrespectively, while achieving the state-of-the-art in restoration quality\n(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating\nthe great efficiency merit of introducing event modality into TM task. Demo\ncode and data have been uploaded in supplementary material and will be released\nonce accepted.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03808v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03808v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.331,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03809",
      "title": "Align-then-Slide: A complete evaluation framework for Ultra-Long\n  Document-Level Machine Translation",
      "authors": [
        "Jiaxin Guo",
        "Daimeng Wei",
        "Yuanchang Luo",
        "Xiaoyu Chen",
        "Zhanglin Wu",
        "Huan Yang",
        "Hengchao Shang",
        "Zongyao Li",
        "Zhiqiang Rao",
        "Jinlong Yang",
        "Hao Yang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have ushered in a new era for document-level\nmachine translation (\\textit{doc}-mt), yet their whole-document outputs\nchallenge existing evaluation methods that assume sentence-by-sentence\nalignment. We introduce \\textit{\\textbf{Align-then-Slide}}, a complete\nevaluation framework for ultra-long doc-mt. In the Align stage, we\nautomatically infer sentence-level source-target correspondences and rebuild\nthe target to match the source sentence number, resolving omissions and\nmany-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we\ncalculate averaged metric scores under 1-, 2-, 3- and 4-chunk for\nmulti-granularity assessment. Experiments on the WMT benchmark show a Pearson\ncorrelation of 0.929 between our method with expert MQM rankings. On a newly\ncurated real-world test set, our method again aligns closely with human\njudgments. Furthermore, preference data produced by Align-then-Slide enables\neffective CPO training and its direct use as a reward model for GRPO, both\nyielding translations preferred over a vanilla SFT baseline. The results\nvalidate our framework as an accurate, robust, and actionable evaluation tool\nfor doc-mt systems.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03809v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03809v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.374,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses using preference data from the Align-then-Slide framework for CPO training and as a reward model for GRPO, which are techniques similar to RLHF for fine-tuning models based on preferences. However, it relies on an automated metric aligned with human judgments rather than direct human feedback, making it not a full RLHF implementation but related in spirit.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluation frameworks for document-level machine translation, including alignment and metric computation, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Align-then-Slide, a comprehensive evaluation framework for ultra-long document-level machine translation (doc-mt), addressing challenges like sentence omissions and mismatched mappings by first aligning source and target sentences using dynamic programming to rebuild the target, then performing multi-granularity assessments via n-chunk sliding evaluations. Key findings demonstrate high correlation (0.929 Pearson) with expert judgments on benchmarks, effective use in training models like CPO and GRPO which outperform baselines, validating the framework as an accurate and robust tool for doc-mt evaluation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new evaluation framework that addresses significant real-world challenges in doc-mt, such as sentence omissions and non-bijective mappings, advancing the state-of-the-art beyond existing methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the specific subfield of document-level machine translation evaluation and training, given its demonstrated accuracy and utility in improving models. However, its influence may be limited to AI and language processing communities rather than broader applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to machine translation evaluation, offering practical advancements that are essential for researchers in the field. While not universally groundbreaking, it provides high-quality insights that warrant attention from relevant experts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6163becee21585d8d1498ef119d1988262eb4813",
      "total_authors": 11,
      "authors_found": 11,
      "highest_h_index": 11,
      "average_h_index": 7.0,
      "notable_authors_count": 7,
      "author_h_indexes": [
        {
          "name": "Jiaxin Guo",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/1838933693"
        },
        {
          "name": "Daimeng Wei",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/8884457"
        },
        {
          "name": "Yuanchang Luo",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2203791738"
        },
        {
          "name": "Xiaoyu Chen",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2109385839"
        },
        {
          "name": "Zhanglin Wu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2109666645"
        },
        {
          "name": "Huan Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379672499"
        },
        {
          "name": "Hengchao Shang",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/1768147214"
        },
        {
          "name": "Zongyao Li",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2118208290"
        },
        {
          "name": "Zhiqiang Rao",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2057549774"
        },
        {
          "name": "Jinlong Yang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2186400195"
        },
        {
          "name": "Hao Yang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2257352292"
        }
      ]
    },
    {
      "id": "2509.03811",
      "title": "Leveraging LLM-Based Agents for Intelligent Supply Chain Planning",
      "authors": [
        "Yongzhi Qi",
        "Jiaheng Yin",
        "Jianshen Zhang",
        "Dongyang Geng",
        "Zhengyu Chen",
        "Hao Hu",
        "Wei Qi",
        "Zuo-Jun Max Shen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In supply chain management, planning is a critical concept. The movement of\nphysical products across different categories, from suppliers to warehouse\nmanagement, to sales, and logistics transporting them to customers, entails the\ninvolvement of many entities. It covers various aspects such as demand\nforecasting, inventory management, sales operations, and replenishment. How to\ncollect relevant data from an e-commerce platform's perspective, formulate\nlong-term plans, and dynamically adjust them based on environmental changes,\nwhile ensuring interpretability, efficiency, and reliability, is a practical\nand challenging problem. In recent years, the development of AI technologies,\nespecially the rapid progress of large language models, has provided new tools\nto address real-world issues. In this work, we construct a Supply Chain\nPlanning Agent (SCPA) framework that can understand domain knowledge,\ncomprehend the operator's needs, decompose tasks, leverage or create new tools,\nand return evidence-based planning reports. We deploy this framework in\nJD.com's real-world scenario, demonstrating the feasibility of LLM-agent\napplications in the supply chain. It effectively reduced labor and improved\naccuracy, stock availability, and other key metrics.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03811v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03811v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.35,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an LLM-based agent framework for supply chain planning, emphasizing iterative task decomposition and adaptive decision-making. While it involves multi-step reasoning and refinement, such as iterative plan correction, it does not reference or utilize diffusion models or the specific iterative refinement process characteristic of diffusion-based reasoning. The core contributions rely on LLMs for task orchestration, not on adapting diffusion processes for logical tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03817",
      "title": "Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with\n  Multi-agent Reinforcement Learning",
      "authors": [
        "Wei Yang",
        "Jesse Thomason"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Multi-agent systems of large language models (LLMs) show promise for complex\nreasoning, but their effectiveness is often limited by fixed collaboration\nprotocols. These frameworks typically focus on macro-level orchestration while\noverlooking agents' internal deliberative capabilities. This critical\nmeta-cognitive blindspot treats agents as passive executors unable to adapt\ntheir strategy based on internal cognitive states like uncertainty or\nconfidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where\nagents learn a decentralized policy over a set of high-level meta-cognitive\nactions: Persist, Refine, and Concede. To overcome the instability of\ntraditional policy gradients in this setting, we develop SoftRankPO, a novel\nreinforcement learning algorithm. SoftRankPO stabilizes training by shaping\nadvantages based on the rank of rewards mapped through smooth normal quantiles,\nmaking the learning process robust to reward variance. Experiments show that\nMPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across\nfive mathematical and general reasoning benchmarks compared to six\nstate-of-the-art heuristic and learning-based multi-agent reasoning algorithms.\nOur work presents a paradigm for learning adaptive, meta-cognitive policies for\nmulti-agent LLM systems, shifting the focus from designing fixed protocols to\nlearning dynamic, deliberative strategies.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03817v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03817v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.48,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.5,
      "distributed_training_score": 0.409,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on multi-agent reinforcement learning with SoftRankPO for policy optimization based on benchmark rewards, not on aligning models with human preferences using human-ranked data or a separate reward model. There is no mention of human feedback in the training process.",
      "weak_supervision_justification": "The paper does not involve training models with programmatically generated labels from noisy or imprecise sources; instead, it centers on reinforcement learning for meta-cognitive policies in multi-agent systems, without any reference to weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper introduces a framework for adaptive meta-cognitive actions in multi-agent LLMs but does not adapt diffusion models for iterative refinement in logical tasks or treat Chain-of-Thought as a holistically corrected entity; it relies on reinforcement learning instead.",
      "distributed_training_justification": "The paper involves multi-agent systems that could imply distributed computation across agents, but its main contribution is on learning meta-policies via reinforcement learning, not on techniques for accelerating training through data partitioning, parallel computing, or multi-node systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03827",
      "title": "What Would an LLM Do? Evaluating Policymaking Capabilities of Large\n  Language Models",
      "authors": [
        "Pierre Le Coz",
        "Jia An Liu",
        "Debarun Bhattacharjya",
        "Georgina Curto",
        "Serge Stinckwich"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) are increasingly being adopted in high-stakes\ndomains. Their capacity to process vast amounts of unstructured data, explore\nflexible scenarios, and handle a diversity of contextual factors can make them\nuniquely suited to provide new insights for the complexity of social\npolicymaking. This article evaluates whether LLMs' are aligned with domain\nexperts (and among themselves) to inform social policymaking on the subject of\nhomelessness alleviation - a challenge affecting over 150 million people\nworldwide. We develop a novel benchmark comprised of decision scenarios with\npolicy choices across four geographies (South Bend, USA; Barcelona, Spain;\nJohannesburg, South Africa; Macau SAR, China). The policies in scope are\ngrounded in the conceptual framework of the Capability Approach for human\ndevelopment. We also present an automated pipeline that connects the\nbenchmarked policies to an agent-based model, and we explore the social impact\nof the recommended policies through simulated social scenarios. The paper\nresults reveal promising potential to leverage LLMs for social policy making.\nIf responsible guardrails and contextual calibrations are introduced in\ncollaboration with local domain experts, LLMs can provide humans with valuable\ninsights, in the form of alternative policies at scale.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03827v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03827v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.483,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.35,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates LLMs by comparing their policy recommendations on homelessness to those of domain experts, but it does not involve training or fine-tuning LLMs using human feedback. There is no mention of creating a reward model, reinforcement learning techniques, or aligning models through iterative human-ranked data processes. The focus is on assessment and simulation, not RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses LLMs to generate and evaluate policy choices for homelessness and connects them to agent-based models for simulation, but it does not employ diffusion-based methods for iterative refinement of reasoning paths. There is no description of treating a chain-of-thought as a holistic entity for multi-step correction, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03828",
      "title": "An Agentic Model Context Protocol Framework for Medical Concept\n  Standardization",
      "authors": [
        "Jaerong Ahn",
        "Andrew Wen",
        "Nan Wang",
        "Heling Jia",
        "Zhiyi Yue",
        "Sunyang Fu",
        "Hongfang Liu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)\nprovides a standardized representation of heterogeneous health data to support\nlarge-scale, multi-institutional research. One critical step in data\nstandardization using OMOP CDM is the mapping of source medical terms to OMOP\nstandard concepts, a procedure that is resource-intensive and error-prone.\nWhile large language models (LLMs) have the potential to facilitate this\nprocess, their tendency toward hallucination makes them unsuitable for clinical\ndeployment without training and expert validation. Here, we developed a\nzero-training, hallucination-preventive mapping system based on the Model\nContext Protocol (MCP), a standardized and secure framework allowing LLMs to\ninteract with external resources and tools. The system enables explainable\nmapping and significantly improves efficiency and accuracy with minimal effort.\nIt provides real-time vocabulary lookups and structured reasoning outputs\nsuitable for immediate use in both exploratory and production environments.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03828v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03828v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.355,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a framework using LLMs and the Model Context Protocol for medical term mapping, focusing on preventing hallucinations and enabling structured reasoning. It does not mention or utilize diffusion models, iterative refinement processes for logical tasks, or treat reasoning as a holistically corrected entity over steps, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03830",
      "title": "A Multidimensional AI-powered Framework for Analyzing Tourist Perception\n  in Historic Urban Quarters: A Case Study in Shanghai",
      "authors": [
        "Kaizhen Tan",
        "Yufan Wu",
        "Yuxuan Liu",
        "Haoran Zeng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Historic urban quarters play a vital role in preserving cultural heritage\nwhile serving as vibrant spaces for tourism and everyday life. Understanding\nhow tourists perceive these environments is essential for sustainable,\nhuman-centered urban planning. This study proposes a multidimensional\nAI-powered framework for analyzing tourist perception in historic urban\nquarters using multimodal data from social media. Applied to twelve historic\nquarters in central Shanghai, the framework integrates focal point extraction,\ncolor theme analysis, and sentiment mining. Visual focus areas are identified\nfrom tourist-shared photos using a fine-tuned semantic segmentation model. To\nassess aesthetic preferences, dominant colors are extracted using a clustering\nmethod, and their spatial distribution across quarters is analyzed. Color\nthemes are further compared between social media photos and real-world street\nviews, revealing notable shifts. This divergence highlights potential gaps\nbetween visual expectations and the built environment, reflecting both\nstylistic preferences and perceptual bias. Tourist reviews are evaluated\nthrough a hybrid sentiment analysis approach combining a rule-based method and\na multi-task BERT model. Satisfaction is assessed across four dimensions:\ntourist activities, built environment, service facilities, and business\nformats. The results reveal spatial variations in aesthetic appeal and\nemotional response. Rather than focusing on a single technical innovation, this\nframework offers an integrated, data-driven approach to decoding tourist\nperception and contributes to informed decision-making in tourism, heritage\nconservation, and the design of aesthetically engaging public spaces.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03830v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03830v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.286,
      "datasets_score": 0.401,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper utilizes multimodal data from social media (e.g., photos and reviews) as input for its AI framework, which involves some level of data curation and analysis, such as extracting features from images. However, the primary focus is on developing and applying the AI-powered framework for tourist perception analysis, not on creating, benchmarking, or evaluating datasets themselves. Thus, dataset-related aspects are secondary and not the core contribution.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03832",
      "title": "Gravity Well Echo Chamber Modeling With An LLM-Based Confirmation Bias\n  Model",
      "authors": [
        "Joseph Jackson",
        "Georgiy Lapin",
        "Jeremy E. Thompson"
      ],
      "categories": [
        "cs.SI (Social and Information Networks)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Social media echo chambers play a central role in the spread of\nmisinformation, yet existing models often overlook the influence of individual\nconfirmation bias. An existing model of echo chambers is the \"gravity well\"\nmodel, which creates an analog between echo chambers and spatial gravity wells.\nWe extend this established model by introducing a dynamic confirmation bias\nvariable that adjusts the strength of pull based on a user's susceptibility to\nbelief-reinforcing content. This variable is calculated for each user through\ncomparisons between their posting history and their responses to posts of a\nwide range of viewpoints.\n  Incorporating this factor produces a confirmation-bias-integrated gravity\nwell model that more accurately identifies echo chambers and reveals\ncommunity-level markers of information health. We validated the approach on\nnineteen Reddit communities, demonstrating improved detection of echo chambers.\n  Our contribution is a framework for systematically capturing the role of\nconfirmation bias in online group dynamics, enabling more effective\nidentification of echo chambers. By flagging these high-risk environments, the\nmodel supports efforts to curb the spread of misinformation at its most common\npoints of amplification.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03832v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03832v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.273,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03834",
      "title": "From Leiden to Pleasure Island: The Constant Potts Model for Community\n  Detection as a Hedonic Game",
      "authors": [
        "Lucas Lopes Felipe",
        "Konstantin Avrachenkov",
        "Daniel Sadoc Menasche"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.GT (Computer Science and Game Theory)"
      ],
      "abstract": "Community detection is one of the fundamental problems in data science which\nconsists of partitioning nodes into disjoint communities. We present a\ngame-theoretic perspective on the Constant Potts Model (CPM) for partitioning\nnetworks into disjoint communities, emphasizing its efficiency, robustness, and\naccuracy. Efficiency: We reinterpret CPM as a potential hedonic game by\ndecomposing its global Hamiltonian into local utility functions, where the\nlocal utility gain of each agent matches the corresponding increase in global\nutility. Leveraging this equivalence, we prove that local optimization of the\nCPM objective via better-response dynamics converges in pseudo-polynomial time\nto an equilibrium partition. Robustness: We introduce and relate two stability\ncriteria: a strict criterion based on a novel notion of robustness, requiring\nnodes to simultaneously maximize neighbors and minimize non-neighbors within\ncommunities, and a relaxed utility function based on a weighted sum of these\nobjectives, controlled by a resolution parameter. Accuracy: In community\ntracking scenarios, where initial partitions are used to bootstrap the Leiden\nalgorithm with partial ground-truth information, our experiments reveal that\nrobust partitions yield higher accuracy in recovering ground-truth communities.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03834v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03834v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.267,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.319,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03842",
      "title": "INGRID: Intelligent Generative Robotic Design Using Large Language\n  Models",
      "authors": [
        "Guanglu Jia",
        "Ceng Zhang",
        "Gregory S. Chirikjian"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The integration of large language models (LLMs) into robotic systems has\naccelerated progress in embodied artificial intelligence, yet current\napproaches remain constrained by existing robotic architectures, particularly\nserial mechanisms. This hardware dependency fundamentally limits the scope of\nrobotic intelligence. Here, we present INGRID (Intelligent Generative Robotic\nDesign), a framework that enables the automated design of parallel robotic\nmechanisms through deep integration with reciprocal screw theory and kinematic\nsynthesis methods. We decompose the design challenge into four progressive\ntasks: constraint analysis, kinematic joint generation, chain construction, and\ncomplete mechanism design. INGRID demonstrates the ability to generate novel\nparallel mechanisms with both fixed and variable mobility, discovering\nkinematic configurations not previously documented in the literature. We\nvalidate our approach through three case studies demonstrating how INGRID\nassists users in designing task-specific parallel robots based on desired\nmobility requirements. By bridging the gap between mechanism theory and machine\nlearning, INGRID enables researchers without specialized robotics training to\ncreate custom parallel mechanisms, thereby decoupling advances in robotic\nintelligence from hardware constraints. This work establishes a foundation for\nmechanism intelligence, where AI systems actively design robotic hardware,\npotentially transforming the development of embodied AI systems.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03842v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03842v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.383,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using large language models for automated robotic mechanism design via screw theory, with no mention of training models using human feedback, reward models, or reinforcement learning techniques. While it references general AI applications in robotics, such as reinforcement learning for control, these are not central to the paper's contribution and do not involve RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a framework for generative robotic design using LLMs and kinematic synthesis, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistic entity. There is no component adapting diffusion techniques for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03845",
      "title": "Meta-Inverse Reinforcement Learning for Mean Field Games via\n  Probabilistic Context Variables",
      "authors": [
        "Yang Chen",
        "Xiao Lin",
        "Bo Yan",
        "Libo Zhang",
        "Jiamou Liu",
        "Neset Özkan Tan",
        "Michael Witbrock"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.GT (Computer Science and Game Theory)"
      ],
      "abstract": "Designing suitable reward functions for numerous interacting intelligent\nagents is challenging in real-world applications. Inverse reinforcement\nlearning (IRL) in mean field games (MFGs) offers a practical framework to infer\nreward functions from expert demonstrations. While promising, the assumption of\nagent homogeneity limits the capability of existing methods to handle\ndemonstrations with heterogeneous and unknown objectives, which are common in\npractice. To this end, we propose a deep latent variable MFG model and an\nassociated IRL method. Critically, our method can infer rewards from different\nyet structurally similar tasks without prior knowledge about underlying\ncontexts or modifying the MFG model itself. Our experiments, conducted on\nsimulated scenarios and a real-world spatial taxi-ride pricing problem,\ndemonstrate the superiority of our approach over state-of-the-art IRL methods\nin MFGs.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03845v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03845v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.328,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is on Meta-Inverse Reinforcement Learning for Mean Field Games, focusing on inferring reward functions from expert demonstrations in multi-agent systems. It does not involve training a reward model on human-ranked data or using human feedback to fine-tune an AI model, which are essential elements of RLHF. Instead, it deals with expert behaviors in simulated and real-world scenarios, without any direct alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03850",
      "title": "Data-Augmented Quantization-Aware Knowledge Distillation",
      "authors": [
        "Justin Kur",
        "Kaiqi Zhao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Quantization-aware training (QAT) and Knowledge Distillation (KD) are\ncombined to achieve competitive performance in creating low-bit deep learning\nmodels. Existing KD and QAT works focus on improving the accuracy of quantized\nmodels from the network output perspective by designing better KD loss\nfunctions or optimizing QAT's forward and backward propagation. However,\nlimited attention has been given to understanding the impact of input\ntransformations, such as data augmentation (DA). The relationship between\nquantization-aware KD and DA remains unexplored. In this paper, we address the\nquestion: how to select a good DA in quantization-aware KD, especially for the\nmodels with low precisions? We propose a novel metric which evaluates DAs\naccording to their capacity to maximize the Contextual Mutual Information--the\ninformation not directly related to an image's label--while also ensuring the\npredictions for each class are close to the ground truth labels on average. The\nproposed method automatically ranks and selects DAs, requiring minimal training\noverhead, and it is compatible with any KD or QAT algorithm. Extensive\nevaluations demonstrate that selecting DA strategies using our metric\nsignificantly improves state-of-the-art QAT and KD works across various model\narchitectures and datasets.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03850v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03850v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.413,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on data augmentation for quantization-aware knowledge distillation, which involves techniques to improve model accuracy through input transformations and training methods. It does not address weak supervision, as there is no discussion of programmatically generating labels from noisy or imprecise sources or relying on anything other than standard, presumably accurate labels.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is on improving quantized models via knowledge distillation and data augmentation, with evaluations on standard datasets. It does not involve distributed training, parallel computing, multi-node setups, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03852",
      "title": "MillGNN: Learning Multi-Scale Lead-Lag Dependencies for Multi-Variate\n  Time Series Forecasting",
      "authors": [
        "Binqing Wu",
        "Zongjiang Shang",
        "Jianlong Huang",
        "Ling Chen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multi-variate time series (MTS) forecasting is crucial for various\napplications. Existing methods have shown promising results owing to their\nstrong ability to capture intra- and inter-variate dependencies. However, these\nmethods often overlook lead-lag dependencies at multiple grouping scales,\nfailing to capture hierarchical lead-lag effects in complex systems. To this\nend, we propose MillGNN, a novel \\underline{g}raph \\underline{n}eural\n\\underline{n}etwork-based method that learns \\underline{m}ult\\underline{i}ple\ngrouping scale \\underline{l}ead-\\underline{l}ag dependencies for MTS\nforecasting, which can comprehensively capture lead-lag effects considering\nvariate-wise and group-wise dynamics and decays. Specifically, MillGNN\nintroduces two key innovations: (1) a scale-specific lead-lag graph learning\nmodule that integrates cross-correlation coefficients and dynamic decaying\nfeatures derived from real-time inputs and time lags to learn lead-lag\ndependencies for each scale, which can model evolving lead-lag dependencies\nwith statistical interpretability and data-driven flexibility; (2) a\nhierarchical lead-lag message passing module that passes lead-lag messages at\nmultiple grouping scales in a structured way to simultaneously propagate intra-\nand inter-scale lead-lag effects, which can capture multi-scale lead-lag\neffects with a balance of comprehensiveness and efficiency. Experimental\nresults on 11 datasets demonstrate the superiority of MillGNN for long-term and\nshort-term MTS forecasting, compared with 16 state-of-the-art methods.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03852v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03852v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.362,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03857",
      "title": "Continuous Monitoring of Large-Scale Generative AI via Deterministic\n  Knowledge Graph Structures",
      "authors": [
        "Kishor Datta Gupta",
        "Mohd Ariful Haque",
        "Hasmot Ali",
        "Marufa Kamal",
        "Syed Bahauddin Alam",
        "Mohammad Ashiqur Rahman"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative AI (GEN AI) models have revolutionized diverse application domains\nbut present substantial challenges due to reliability concerns, including\nhallucinations, semantic drift, and inherent biases. These models typically\noperate as black-boxes, complicating transparent and objective evaluation.\nCurrent evaluation methods primarily depend on subjective human assessment,\nlimiting scalability, transparency, and effectiveness. This research proposes a\nsystematic methodology using deterministic and Large Language Model\n(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN\nAI reliability. We construct two parallel KGs: (i) a deterministic KG built\nusing explicit rule-based methods, predefined ontologies, domain-specific\ndictionaries, and structured entity-relation extraction rules, and (ii) an\nLLM-generated KG dynamically derived from real-time textual data streams such\nas live news articles. Utilizing real-time news streams ensures authenticity,\nmitigates biases from repetitive training, and prevents adaptive LLMs from\nbypassing predefined benchmarks through feedback memorization. To quantify\nstructural deviations and semantic discrepancies, we employ several established\nKG metrics, including Instantiated Class Ratio (ICR), Instantiated Property\nRatio (IPR), and Class Instantiation (CI). An automated real-time monitoring\nframework continuously computes deviations between deterministic and\nLLM-generated KGs. By establishing dynamic anomaly thresholds based on\nhistorical structural metric distributions, our method proactively identifies\nand flags significant deviations, thus promptly detecting semantic anomalies or\nhallucinations. This structured, metric-driven comparison between deterministic\nand dynamically generated KGs delivers a robust and scalable evaluation\nframework.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03857v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03857v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.452,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.479,
      "distributed_training_score": 0.38,
      "datasets_score": 0.453,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on monitoring and evaluating Generative AI using Knowledge Graphs, without any mention of training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper describes building Knowledge Graphs using rule-based and LLM methods, but it does not involve training machine learning models with programmatically generated labels or noisy sources as in weak supervision.",
      "diffusion_reasoning_justification": "The paper does not reference diffusion models, iterative refinement for reasoning, or multi-step logical processes; it instead uses Knowledge Graphs for evaluation and anomaly detection.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves using real-time data streams like news articles to generate Knowledge Graphs, which relates indirectly to dataset creation and analysis, but its primary focus is on AI evaluation methodology rather than datasets themselves.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03863",
      "title": "Expedition & Expansion: Leveraging Semantic Representations for\n  Goal-Directed Exploration in Continuous Cellular Automata",
      "authors": [
        "Sina Khajehabdollahi",
        "Gautier Hamon",
        "Marko Cvjetko",
        "Pierre-Yves Oudeyer",
        "Clément Moulin-Frier",
        "Cédric Colas"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Discovering diverse visual patterns in continuous cellular automata (CA) is\nchallenging due to the vastness and redundancy of high-dimensional behavioral\nspaces. Traditional exploration methods like Novelty Search (NS) expand locally\nby mutating known novel solutions but often plateau when local novelty is\nexhausted, failing to reach distant, unexplored regions. We introduce\nExpedition and Expansion (E&E), a hybrid strategy where exploration alternates\nbetween local novelty-driven expansions and goal-directed expeditions. During\nexpeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic\ngoals--descriptions of interesting but hypothetical patterns that drive\nexploration toward uncharted regions. By operating in semantic spaces that\nalign with human perception, E&E both evaluates novelty and generates goals in\nconceptually meaningful ways, enhancing the interpretability and relevance of\ndiscovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,\nemergent behaviors, E&E consistently uncovers more diverse solutions than\nexisting exploration methods. A genealogical analysis further reveals that\nsolutions originating from expeditions disproportionately influence long-term\nexploration, unlocking new behavioral niches that serve as stepping stones for\nsubsequent search. These findings highlight E&E's capacity to break through\nlocal novelty boundaries and explore behavioral landscapes in human-aligned,\ninterpretable ways, offering a promising template for open-ended exploration in\nartificial life and beyond.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03863v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03863v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.483,
      "distributed_training_score": 0.328,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a hybrid exploration strategy for cellular automata using novelty search and Vision-Language Models, with no mention of reinforcement learning, human feedback, or training models based on human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on exploration in cellular automata with VLMs for goal generation, without any use of diffusion models, iterative refinement processes, or multi-step logical reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03871",
      "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large\n  Language Models",
      "authors": [
        "Yanbo Wang",
        "Yongcan Yu",
        "Jian Liang",
        "Ran He"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "The development of Long-CoT reasoning has advanced LLM performance across\nvarious tasks, including language understanding, complex problem solving, and\ncode generation. This paradigm enables models to generate intermediate\nreasoning steps, thereby improving both accuracy and interpretability. However,\ndespite these advancements, a comprehensive understanding of how CoT-based\nreasoning affects the trustworthiness of language models remains\nunderdeveloped. In this paper, we survey recent work on reasoning models and\nCoT techniques, focusing on five core dimensions of trustworthy reasoning:\ntruthfulness, safety, robustness, fairness, and privacy. For each aspect, we\nprovide a clear and structured overview of recent studies in chronological\norder, along with detailed analyses of their methodologies, findings, and\nlimitations. Future research directions are also appended at the end for\nreference and discussion. Overall, while reasoning techniques hold promise for\nenhancing model trustworthiness through hallucination mitigation, harmful\ncontent detection, and robustness improvement, cutting-edge reasoning models\nthemselves often suffer from comparable or even greater vulnerabilities in\nsafety, robustness, and privacy. By synthesizing these insights, we hope this\nwork serves as a valuable and timely resource for the AI safety community to\nstay informed on the latest progress in reasoning trustworthiness. A full list\nof related papers can be found at\n\\href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03871v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03871v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.536,
      "distributed_training_score": 0.345,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions innovations in reinforcement learning algorithms in the context of developing reasoning models (e.g., OpenAI's o1 series), but it does not specifically focus on RLHF as defined, which requires training with human-ranked data for alignment. The survey's main contribution is on trustworthiness in reasoning, with RLHF only briefly referenced indirectly.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's content focuses on Chain-of-Thought techniques and trustworthiness in LLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning adapted from diffusion. It does not include any components related to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03872",
      "title": "Focus Through Motion: RGB-Event Collaborative Token Sparsification for\n  Efficient Object Detection",
      "authors": [
        "Nan Yang",
        "Yang Wang",
        "Zhanwen Liu",
        "Yuchao Dai",
        "Yang Liu",
        "Xiangmo Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing RGB-Event detection methods process the low-information regions of\nboth modalities (background in images and non-event regions in event data)\nuniformly during feature extraction and fusion, resulting in high computational\ncosts and suboptimal performance. To mitigate the computational redundancy\nduring feature extraction, researchers have respectively proposed token\nsparsification methods for the image and event modalities. However, these\nmethods employ a fixed number or threshold for token selection, hindering the\nretention of informative tokens for samples with varying complexity. To achieve\na better balance between accuracy and efficiency, we propose FocusMamba, which\nperforms adaptive collaborative sparsification of multimodal features and\nefficiently integrates complementary information. Specifically, an Event-Guided\nMultimodal Sparsification (EGMS) strategy is designed to identify and\nadaptively discard low-information regions within each modality by leveraging\nscene content changes perceived by the event camera. Based on the\nsparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed\nto effectively capture and integrate complementary features from both\nmodalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate\nthat the proposed method achieves superior performance in both accuracy and\nefficiency compared to existing methods. The code will be available at\nhttps://github.com/Zizzzzzzz/FocusMamba.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03872v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03872v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.372,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03873",
      "title": "SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition",
      "authors": [
        "Jiajun Song",
        "Xiaoou Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Food recognition has gained significant attention, but the rapid emergence of\nnew dishes requires methods for recognizing unseen food categories, motivating\nZero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot\nFood Recognition (CZSFR), where cuisines and ingredients naturally align with\nattributes and objects in Compositional Zero-Shot learning (CZSL). However,\nCZSFR faces three challenges: (1) Redundant background information distracts\nmodels from learning meaningful food features, (2) Role confusion between\nstaple and side dishes leads to misclassification, and (3) Semantic bias in a\nsingle attribute can lead to confusion of understanding. Therefore, we propose\nSalientFusion, a context-aware CZSFR method with two components: SalientFormer,\nwhich removes background redundancy and uses depth features to resolve role\nconfusion; DebiasAT, which reduces the semantic bias by aligning prompts with\nvisual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we\nshow that SalientFusion achieves state-of-the-art results on these benchmarks\nand the most popular general datasets for the general CZSL. The code is\navaliable at https://github.com/Jiajun-RUC/SalientFusion.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03873v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03873v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.329,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03883",
      "title": "Human Motion Video Generation: A Survey",
      "authors": [
        "Haiwei Xue",
        "Xiangyang Luo",
        "Zhanghao Hu",
        "Xin Zhang",
        "Xunzhi Xiang",
        "Yuqin Dai",
        "Jianzhuang Liu",
        "Zhensong Zhang",
        "Minglei Li",
        "Jian Yang",
        "Fei Ma",
        "Zhiyong Wu",
        "Changpeng Yang",
        "Zonghong Dai",
        "Fei Richard Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Human motion video generation has garnered significant research interest due\nto its broad applications, enabling innovations such as photorealistic singing\nheads or dynamic avatars that seamlessly dance to music. However, existing\nsurveys in this field focus on individual methods, lacking a comprehensive\noverview of the entire generative process. This paper addresses this gap by\nproviding an in-depth survey of human motion video generation, encompassing\nover ten sub-tasks, and detailing the five key phases of the generation\nprocess: input, motion planning, motion video generation, refinement, and\noutput. Notably, this is the first survey that discusses the potential of large\nlanguage models in enhancing human motion video generation. Our survey reviews\nthe latest developments and technological trends in human motion video\ngeneration across three primary modalities: vision, text, and audio. By\ncovering over two hundred papers, we offer a thorough overview of the field and\nhighlight milestone works that have driven significant technological\nbreakthroughs. Our goal for this survey is to unveil the prospects of human\nmotion video generation and serve as a valuable resource for advancing the\ncomprehensive applications of digital humans. A complete list of the models\nexamined in this survey is available in Our Repository\nhttps://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03883v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03883v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.313,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper is a survey on human motion video generation, focusing on aspects like input processing, motion planning, and generation phases across vision, text, and audio modalities. It does not discuss or involve diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03884",
      "title": "Peptidomic-Based Prediction Model for Coronary Heart Disease Using a\n  Multilayer Perceptron Neural Network",
      "authors": [
        "Jesus Celis-Porras"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Coronary heart disease (CHD) is a leading cause of death worldwide and\ncontributes significantly to annual healthcare expenditures. To develop a\nnon-invasive diagnostic approach, we designed a model based on a multilayer\nperceptron (MLP) neural network, trained on 50 key urinary peptide biomarkers\nselected via genetic algorithms. Treatment and control groups, each comprising\n345 individuals, were balanced using the Synthetic Minority Over-sampling\nTechnique (SMOTE). The neural network was trained using a stratified validation\nstrategy. Using a network with three hidden layers of 60 neurons each and an\noutput layer of two neurons, the model achieved a precision, sensitivity, and\nspecificity of 95.67 percent, with an F1-score of 0.9565. The area under the\nROC curve (AUC) reached 0.9748 for both classes, while the Matthews correlation\ncoefficient (MCC) and Cohen's kappa coefficient were 0.9134 and 0.9131,\nrespectively, demonstrating its reliability in detecting CHD. These results\nindicate that the model provides a highly accurate and robust non-invasive\ndiagnostic tool for coronary heart disease.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03884v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03884v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.3,
      "distributed_training_score": 0.321,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03887",
      "title": "OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction",
      "authors": [
        "Bu Jin",
        "Songen Gu",
        "Xiaotao Hu",
        "Yupeng Zheng",
        "Xiaoyang Guo",
        "Qian Zhang",
        "Xiaoxiao Long",
        "Wei Yin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we propose OccTENS, a generative occupancy world model that\nenables controllable, high-fidelity long-term occupancy generation while\nmaintaining computational efficiency. Different from visual generation, the\noccupancy world model must capture the fine-grained 3D geometry and dynamic\nevolution of the 3D scenes, posing great challenges for the generative models.\nRecent approaches based on autoregression (AR) have demonstrated the potential\nto predict vehicle movement and future occupancy scenes simultaneously from\nhistorical observations, but they typically suffer from \\textbf{inefficiency},\n\\textbf{temporal degradation} in long-term generation and \\textbf{lack of\ncontrollability}. To holistically address these issues, we reformulate the\noccupancy world model as a temporal next-scale prediction (TENS) task, which\ndecomposes the temporal sequence modeling problem into the modeling of spatial\nscale-by-scale generation and temporal scene-by-scene prediction. With a\n\\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and\nspatial relationships of occupancy sequences in a flexible and scalable way. To\nenhance the pose controllability, we further propose a holistic pose\naggregation strategy, which features a unified sequence modeling for occupancy\nand ego-motion. Experiments show that OccTENS outperforms the state-of-the-art\nmethod with both higher occupancy quality and faster inference time.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03887v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03887v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.333,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces OccTENS, an autoregressive model for 3D occupancy generation using temporal next-scale prediction, focusing on efficiency and controllability in autonomous driving scenarios. It does not involve diffusion models, iterative refinement for logical tasks, or any Chain-of-Thought reasoning process, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03889",
      "title": "Reactive In-Air Clothing Manipulation with Confidence-Aware Dense\n  Correspondence and Visuotactile Affordance",
      "authors": [
        "Neha Sunil",
        "Megha Tippur",
        "Arnau Saumell",
        "Edward Adelson",
        "Alberto Rodriguez"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Manipulating clothing is challenging due to complex configurations, variable\nmaterial dynamics, and frequent self-occlusion. Prior systems often flatten\ngarments or assume visibility of key features. We present a dual-arm\nvisuotactile framework that combines confidence-aware dense visual\ncorrespondence and tactile-supervised grasp affordance to operate directly on\ncrumpled and suspended garments. The correspondence model is trained on a\ncustom, high-fidelity simulated dataset using a distributional loss that\ncaptures cloth symmetries and generates correspondence confidence estimates.\nThese estimates guide a reactive state machine that adapts folding strategies\nbased on perceptual uncertainty. In parallel, a visuotactile grasp affordance\nnetwork, self-supervised using high-resolution tactile feedback, determines\nwhich regions are physically graspable. The same tactile classifier is used\nduring execution for real-time grasp validation. By deferring action in\nlow-confidence states, the system handles highly occluded table-top and in-air\nconfigurations. We demonstrate our task-agnostic grasp selection module in\nfolding and hanging tasks. Moreover, our dense descriptors provide a reusable\nintermediate representation for other planning modalities, such as extracting\ngrasp targets from human video demonstrations, paving the way for more\ngeneralizable and scalable garment manipulation.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03889v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03889v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.331,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03890",
      "title": "FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer\n  Marketplace",
      "authors": [
        "Yineng Yan",
        "Xidong Wang",
        "Jin Seng Cheng",
        "Ran Hu",
        "Wentao Guan",
        "Nahid Farahmand",
        "Hengte Lin",
        "Yue Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The emergence of agentic AI, powered by Large Language Models (LLMs), marks a\nparadigm shift from reactive generative systems to proactive, goal-oriented\nautonomous agents capable of sophisticated planning, memory, and tool use. This\nevolution presents a novel opportunity to address long-standing challenges in\ncomplex digital environments. Core tasks on Consumer-to-Consumer (C2C)\ne-commerce platforms often require users to navigate complex Graphical User\nInterfaces (GUIs), making the experience time-consuming for both buyers and\nsellers. This paper introduces a novel approach to simplify these interactions\nthrough an LLM-powered agentic assistant. This agent functions as a new,\nconversational entry point to the marketplace, shifting the primary interaction\nmodel from a complex GUI to an intuitive AI agent. By interpreting natural\nlanguage commands, the agent automates key high-friction workflows. For\nsellers, this includes simplified updating and renewal of listings, and the\nability to send bulk messages. For buyers, the agent facilitates a more\nefficient product discovery process through conversational search. We present\nthe architecture for Facebook Marketplace Assistant (FaMA), arguing that this\nagentic, conversational paradigm provides a lightweight and more accessible\nalternative to traditional app interfaces, allowing users to manage their\nmarketplace activities with greater efficiency. Experiments show FaMA achieves\na 98% task success rate on solving complex tasks on the marketplace and enables\nup to a 2x speedup on interaction time.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03890v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03890v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.307,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03891",
      "title": "MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation",
      "authors": [
        "Gowen Loo",
        "Chang Liu",
        "Qinghong Yin",
        "Xiang Chen",
        "Jiawei Chen",
        "Jingyuan Zhang",
        "Yu Tian"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Smartphones have become indispensable in people's daily lives, permeating\nnearly every aspect of modern society. With the continuous advancement of large\nlanguage models (LLMs), numerous LLM-based mobile agents have emerged. These\nagents are capable of accurately parsing diverse user queries and automatically\nassisting users in completing complex or repetitive operations. However,\ncurrent agents 1) heavily rely on the comprehension ability of LLMs, which can\nlead to errors caused by misoperations or omitted steps during tasks, 2) lack\ninteraction with the external environment, often terminating tasks when an app\ncannot fulfill user queries, and 3) lack memory capabilities, requiring each\ninstruction to reconstruct the interface and being unable to learn from and\ncorrect previous mistakes. To alleviate the above issues, we propose MobileRAG,\na mobile agents framework enhanced by Retrieval-Augmented Generation (RAG),\nwhich includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly\nand accurately identify user queries and accomplish complex and long-sequence\nmobile tasks. Additionally, to more comprehensively assess the performance of\nMobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark\ncharacterized by numerous complex, real-world mobile tasks that require\nexternal knowledge assistance. Extensive experimental results on MobileRAG-Eval\ndemonstrate that MobileRAG can easily handle real-world mobile tasks, achieving\n10.3\\% improvement over state-of-the-art methods with fewer operational steps.\nOur code is publicly available at:\nhttps://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03891v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03891v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.333,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of MobileRAG, a framework that enhances mobile agents using Retrieval-Augmented Generation (RAG) to improve query understanding, external interaction, and memory capabilities. It focuses on RAG techniques, which involve retrieving and integrating external knowledge, rather than diffusion-based models that use iterative refinement for multi-step logical reasoning. There is no mention of diffusion models, iterative refinement processes, or adapting diffusion to handle chain-of-thought reasoning, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03893",
      "title": "Weakly-Supervised Learning of Dense Functional Correspondences",
      "authors": [
        "Stefan Stojanov",
        "Linan Zhao",
        "Yunzhi Zhang",
        "Daniel L. K. Yamins",
        "Jiajun Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Establishing dense correspondences across image pairs is essential for tasks\nsuch as shape reconstruction and robot manipulation. In the challenging setting\nof matching across different categories, the function of an object, i.e., the\neffect that an object can cause on other objects, can guide how correspondences\nshould be established. This is because object parts that enable specific\nfunctions often share similarities in shape and appearance. We derive the\ndefinition of dense functional correspondence based on this observation and\npropose a weakly-supervised learning paradigm to tackle the prediction task.\nThe main insight behind our approach is that we can leverage vision-language\nmodels to pseudo-label multi-view images to obtain functional parts. We then\nintegrate this with dense contrastive learning from pixel correspondences to\ndistill both functional and spatial knowledge into a new model that can\nestablish dense functional correspondence. Further, we curate synthetic and\nreal evaluation datasets as task benchmarks. Our results demonstrate the\nadvantages of our approach over baseline solutions consisting of off-the-shelf\nself-supervised image representations and grounded vision language models.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03893v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03893v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.457,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.343,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a weakly-supervised learning paradigm that uses vision-language models to generate pseudo-labels for multi-view images, aligning directly with weak supervision by programmatically creating noisy or imprecise labels instead of relying on manual annotation. This approach enables training for dense functional correspondences without perfect hand-labeled data.",
      "diffusion_reasoning_justification": "The paper focuses on weakly-supervised learning with vision-language models and contrastive learning for dense correspondences, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. There is no component involving Chain-of-Thought or holistic correction in a diffusion framework.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of establishing dense correspondences across image pairs of objects from different categories by focusing on functional similarity, defining dense functional correspondence as matching parts that perform similar functions despite visual differences. The authors propose a weakly-supervised learning approach that uses vision-language models to pseudo-label functional parts in multi-view images, integrates this with dense contrastive learning to combine functional and spatial knowledge, and evaluates the method on newly curated synthetic and real datasets, demonstrating superior performance over baseline methods in tasks like shape reconstruction and robot manipulation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel concept of dense functional correspondence, which combines functional similarity with spatial alignment in a new weakly-supervised learning paradigm using vision-language models and dense contrastive learning, significantly advancing the state-of-the-art in cross-category object matching.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields of computer vision, such as object correspondence for robotics and shape analysis, due to its practical applications, though its influence may be limited to specific domains rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution to computer vision by addressing a challenging problem with effective methodology, making it valuable for researchers in related areas, though it may not be essential for those outside the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2582b29c5d2af4cbeab32a0589fd516fdefbf4b5",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 10,
      "average_h_index": 2.8,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Stefan Stojanov",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2352104403"
        },
        {
          "name": "Linan Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379453798"
        },
        {
          "name": "Yunzhi Zhang",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2261420360"
        },
        {
          "name": "Daniel L. K. Yamins",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2239206602"
        },
        {
          "name": "Jiajun Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2352191188"
        }
      ]
    },
    {
      "id": "2509.03895",
      "title": "Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of\n  Vision-Language Model",
      "authors": [
        "Phuoc-Nguyen Bui",
        "Khanh-Binh Nguyen",
        "Hyunseung Choo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Contrastive vision-language models excel in zero-shot image recognition but\nface challenges in few-shot scenarios due to computationally intensive offline\nfine-tuning using prompt learning, which risks overfitting. To overcome these\nlimitations, we propose Attn-Adapter, a novel online few-shot learning\nframework that enhances CLIP's adaptability via a dual attention mechanism. Our\ndesign incorporates dataset-specific information through two components: the\nMemory Attn-Adapter, which refines category embeddings using support examples,\nand the Local-Global Attn-Adapter, which enriches image embeddings by\nintegrating local and global features. This architecture enables dynamic\nadaptation from a few labeled samples without retraining the base model.\nAttn-Adapter outperforms state-of-the-art methods in cross-category and\ncross-dataset generalization, maintaining efficient inference and scaling\nacross CLIP backbones.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03895v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03895v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.35,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03897",
      "title": "SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation",
      "authors": [
        "Xiaofu Chen",
        "Israfel Salazar",
        "Yova Kementchedjhieva"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "As interest grows in generating long, detailed image captions, standard\nevaluation metrics become increasingly unreliable. N-gram-based metrics though\nefficient, fail to capture semantic correctness. Representational Similarity\n(RS) metrics, designed to address this, initially saw limited use due to high\ncomputational costs, while today, despite advances in hardware, they remain\nunpopular due to low correlation to human judgments. Meanwhile, metrics based\non large language models (LLMs) show strong correlation with human judgments,\nbut remain too expensive for iterative use during model development.\n  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS\nmetric tailored to long image captioning. SPECS modifies CLIP with a new\nobjective that emphasizes specificity: rewarding correct details and penalizing\nincorrect ones. We show that SPECS matches the performance of open-source\nLLM-based metrics in correlation to human judgments, while being far more\nefficient. This makes it a practical alternative for iterative checkpoint\nevaluation during image captioning model development.Our code can be found at\nhttps://github.com/mbzuai-nlp/SPECS.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03897v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03897v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.304,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03898",
      "title": "Diffusion Generative Models Meet Compressed Sensing, with Applications\n  to Image Data and Financial Time Series",
      "authors": [
        "Zhengyi Guo",
        "Jiatu Li",
        "Wenpin Tang",
        "David D. Yao"
      ],
      "categories": [
        "stat.ML (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This paper develops dimension reduction techniques for accelerating diffusion\nmodel inference in the context of synthetic data generation. The idea is to\nintegrate compressed sensing into diffusion models: (i) compress the data into\na latent space, (ii) train a diffusion model in the latent space, and (iii)\napply a compressed sensing algorithm to the samples generated in the latent\nspace, facilitating the efficiency of both model training and inference. Under\nsuitable sparsity assumptions on data, the proposed algorithm is proved to\nenjoy faster convergence by combining diffusion model inference with sparse\nrecovery. As a byproduct, we obtain an optimal value for the latent space\ndimension. We also conduct numerical experiments on a range of datasets,\nincluding image data (handwritten digits, medical images, and climate data) and\nfinancial time series for stress testing.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03898v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03898v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.581,
      "distributed_training_score": 0.395,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is developing a pipeline that integrates compressed sensing with diffusion generative models to accelerate inference for synthetic data generation, focusing on image and financial time series data. It does not involve adapting the iterative refinement process of diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. Instead, it centers on generative tasks like data compression and generation, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03903",
      "title": "A Generative Foundation Model for Chest Radiography",
      "authors": [
        "Yuanfeng Ji",
        "Dan Lin",
        "Xiyue Wang",
        "Lu Zhang",
        "Wenhui Zhou",
        "Chongjian Ge",
        "Ruihang Chu",
        "Xiaoli Yang",
        "Junhan Zhao",
        "Junsong Chen",
        "Xiangde Luo",
        "Sen Yang",
        "Jin Fang",
        "Ping Luo",
        "Ruijiang Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The scarcity of well-annotated diverse medical images is a major hurdle for\ndeveloping reliable AI models in healthcare. Substantial technical advances\nhave been made in generative foundation models for natural images. Here we\ndevelop `ChexGen', a generative vision-language foundation model that\nintroduces a unified framework for text-, mask-, and bounding box-guided\nsynthesis of chest radiographs. Built upon the latent diffusion transformer\narchitecture, ChexGen was pretrained on the largest curated chest X-ray dataset\nto date, consisting of 960,000 radiograph-report pairs. ChexGen achieves\naccurate synthesis of radiographs through expert evaluations and quantitative\nmetrics. We demonstrate the utility of ChexGen for training data augmentation\nand supervised pretraining, which led to performance improvements across\ndisease classification, detection, and segmentation tasks using a small\nfraction of training data. Further, our model enables the creation of diverse\npatient cohorts that enhance model fairness by detecting and mitigating\ndemographic biases. Our study supports the transformative role of generative\nfoundation models in building more accurate, data-efficient, and equitable\nmedical AI systems.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03903v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03903v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.338,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using a latent diffusion transformer architecture for generating chest radiographs, emphasizing image synthesis, data augmentation, and medical applications. However, it does not involve adapting diffusion models for multi-step logical reasoning, such as treating a 'Chain-of-Thought' as an entity for holistic correction in complex tasks. The core contribution is generative image creation, not reasoning, so it does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03906",
      "title": "A Foundation Model for Chest X-ray Interpretation with Grounded\n  Reasoning via Online Reinforcement Learning",
      "authors": [
        "Qika Lin",
        "Yifan Zhu",
        "Bin Pu",
        "Ling Huang",
        "Haoran Luo",
        "Jingying Ma",
        "Zhen Peng",
        "Tianzhe Zhao",
        "Fangzhi Xu",
        "Jian Zhang",
        "Kai He",
        "Zhonghong Ou",
        "Swapnil Mishra",
        "Mengling Feng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Medical foundation models (FMs) have shown tremendous promise amid the rapid\nadvancements in artificial intelligence (AI) technologies. However, current\nmedical FMs typically generate answers in a black-box manner, lacking\ntransparent reasoning processes and locally grounded interpretability, which\nhinders their practical clinical deployments. To this end, we introduce\nDeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It\nleverages a sequential training pipeline: initially fine-tuned on curated CXR\ninstruction data to equip with fundamental CXR interpretation capabilities,\nthen exposed to high-quality synthetic reasoning samples to enable cold-start\nreasoning, and finally refined via online reinforcement learning to enhance\nboth grounded reasoning quality and generation performance. Thus, the model\nproduces both an answer and reasoning steps tied to the image's local regions\nfor each query. Quantitative evaluation demonstrates substantial improvements\nin report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and\nvisual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)\ntasks. To facilitate robust assessment, we propose Report Arena, a benchmarking\nframework using advanced language models to evaluate answer quality, further\nhighlighting the superiority of DeepMedix-R1. Expert review of generated\nreasoning steps reveals greater interpretability and clinical plausibility\ncompared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall\npreference). Collectively, our work advances medical FM development toward\nholistic, transparent, and clinically actionable modeling for CXR\ninterpretation.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03906v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03906v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.52,
      "distributed_training_score": 0.34,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes using online reinforcement learning (specifically GRPO) to refine the model based on a scoring mechanism for outputs and reasoning processes, which aligns somewhat with RLHF concepts. However, it does not explicitly indicate the use of human-ranked data or a separate reward model trained on human feedback; instead, the feedback appears to be derived from automated scoring during training. This makes the paper relevant but not a direct match to the strict definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a sequential training pipeline involving fine-tuning and reinforcement learning for chest X-ray interpretation, with no mention of diffusion models, iterative refinement processes for logical tasks, or treating chain-of-thought as a refinable entity. Thus, it lacks any components related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces DeepMedix-R1, a foundation model for chest X-ray interpretation that addresses the limitations of existing medical foundation models by incorporating grounded reasoning and transparent processes. The methodology involves a sequential training pipeline: initial fine-tuning on curated CXR instruction data, exposure to synthetic reasoning samples for cold-start capabilities, and refinement via online reinforcement learning to enhance reasoning quality and performance, resulting in improved report generation and visual question answering tasks as demonstrated by quantitative metrics, expert evaluations, and a new benchmarking framework called Report Arena.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel sequential training pipeline with online reinforcement learning for grounded reasoning in chest X-ray interpretation, significantly advancing the state-of-the-art in interpretable medical foundation models by addressing transparency and localization issues.",
      "impact_score": "High",
      "impact_justification": "The work's integration of grounded reasoning could broadly influence future medical AI research and clinical applications by improving model interpretability and reliability, potentially leading to wider adoption in healthcare settings.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution to interpretable medical AI with innovative methods and strong empirical results, making it valuable for researchers and practitioners in the field to understand its advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b41c22f9cc7a1c10cfecf3c911de94601a77ba9e",
      "total_authors": 14,
      "authors_found": 13,
      "highest_h_index": 10,
      "average_h_index": 3.923076923076923,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Qika Lin",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2316927371"
        },
        {
          "name": "Yifan Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355137218"
        },
        {
          "name": "Bin Pu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378953091"
        },
        {
          "name": "Ling Huang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2257135076"
        },
        {
          "name": "Haoran Luo",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2314326386"
        },
        {
          "name": "Jingying Ma",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2316526648"
        },
        {
          "name": "Zhen Peng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Tianzhe Zhao",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2153707350"
        },
        {
          "name": "Fangzhi Xu",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2143943979"
        },
        {
          "name": "Jian Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378955639"
        },
        {
          "name": "Kai He",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2316529383"
        },
        {
          "name": "Z. Ou",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2154089240"
        },
        {
          "name": "Swapnil Mishra",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2330780432"
        },
        {
          "name": "Mengling Feng",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2316641362"
        }
      ]
    },
    {
      "id": "2509.03918",
      "title": "MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question\n  Answering",
      "authors": [
        "Fengxiao Tang",
        "Yufeng Li",
        "Zongzong Wu",
        "Ming Zhao"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Complex Question Answering (QA) is a fundamental and challenging task in NLP.\nWhile large language models (LLMs) exhibit impressive performance in QA, they\nsuffer from significant performance degradation when facing complex and\nabstract QA tasks due to insufficient reasoning capabilities. Works such as\nChain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning\nabilities, but they face issues such as in-layer redundancy in tree structures\nand single paths in chain structures. Although some studies utilize\nRetrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the\nchallenge of effectively utilizing large amounts of information involving\nmultiple entities and hops remains critical. To address this, we propose the\nMatrix of Thought (MoT), a novel and efficient LLM thought structure. MoT\nexplores the problem in both horizontal and vertical dimensions through the\n\"column-cell communication\" mechanism, enabling LLMs to actively engage in\nmulti-strategy and deep-level thinking, reducing redundancy within the column\ncells and enhancing reasoning capabilities. Furthermore, we develop a\nfact-correction mechanism by constructing knowledge units from retrieved\nknowledge graph triples and raw text to enhance the initial knowledge for LLM\nreasoning and correct erroneous answers. This leads to the development of an\nefficient and accurate QA framework (MTQA). Experimental results show that our\nframework outperforms state-of-the-art methods on four widely-used datasets in\nterms of F1 and EM scores, with reasoning time only 14.4\\% of the baseline\nmethods, demonstrating both its efficiency and accuracy. The code for this\nframework is available at https://github.com/lyfiter/mtqa.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03918v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03918v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.541,
      "distributed_training_score": 0.333,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces the Matrix of Thought (MoT) for enhancing reasoning in LLMs for complex question answering, focusing on matrix-based structures, column-cell communication, and integration with RAG for fact correction. It does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03922",
      "title": "LMVC: An End-to-End Learned Multiview Video Coding Framework",
      "authors": [
        "Xihua Sheng",
        "Yingwen Zhang",
        "Long Xu",
        "Shiqi Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multiview video is a key data source for volumetric video, enabling immersive\n3D scene reconstruction but posing significant challenges in storage and\ntransmission due to its massive data volume. Recently, deep learning-based\nend-to-end video coding has achieved great success, yet most focus on\nsingle-view or stereo videos, leaving general multiview scenarios\nunderexplored. This paper proposes an end-to-end learned multiview video coding\n(LMVC) framework that ensures random access and backward compatibility while\nenhancing compression efficiency. Our key innovation lies in effectively\nleveraging independent-view motion and content information to enhance\ndependent-view compression. Specifically, to exploit the inter-view motion\ncorrelation, we propose a feature-based inter-view motion vector prediction\nmethod that conditions dependent-view motion encoding on decoded\nindependent-view motion features, along with an inter-view motion entropy model\nthat learns inter-view motion priors. To exploit the inter-view content\ncorrelation, we propose a disparity-free inter-view context prediction module\nthat predicts inter-view contexts from decoded independent-view content\nfeatures, combined with an inter-view contextual entropy model that captures\ninter-view context priors. Experimental results show that our proposed LMVC\nframework outperforms the reference software of the traditional MV-HEVC\nstandard by a large margin, establishing a strong baseline for future research\nin this field.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03922v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03922v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.357,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03934",
      "title": "SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented\n  Generation via Distribution Self-Alignment",
      "authors": [
        "Yuqing Huang",
        "Rongyang Zhang",
        "Qimeng Wang",
        "Chengqiang Lu",
        "Yan Gao",
        "Yi Wu",
        "Yao Hu",
        "Xuyang Zhi",
        "Guiquan Liu",
        "Xin Li",
        "Hao Wang",
        "Enhong Chen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have revolutionized\nnatural language processing through their remarkable capabilities in\nunderstanding and executing diverse tasks. While supervised fine-tuning,\nparticularly in Retrieval-Augmented Generation (RAG) scenarios, effectively\nenhances task-specific performance, it often leads to catastrophic forgetting,\nwhere models lose their previously acquired knowledge and general capabilities.\nExisting solutions either require access to general instruction data or face\nlimitations in preserving the model's original distribution. To overcome these\nlimitations, we propose SelfAug, a self-distribution alignment method that\naligns input sequence logits to preserve the model's semantic distribution,\nthereby mitigating catastrophic forgetting and improving downstream\nperformance. Extensive experiments demonstrate that SelfAug achieves a superior\nbalance between downstream learning and general capability retention. Our\ncomprehensive empirical analysis reveals a direct correlation between\ndistribution shifts and the severity of catastrophic forgetting in RAG\nscenarios, highlighting how the absence of RAG capabilities in general\ninstruction tuning leads to significant distribution shifts during fine-tuning.\nOur findings not only advance the understanding of catastrophic forgetting in\nRAG contexts but also provide a practical solution applicable across diverse\nfine-tuning scenarios. Our code is publicly available at\nhttps://github.com/USTC-StarTeam/SelfAug.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03934v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03934v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.426,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces SelfAug, a method for mitigating catastrophic forgetting in RAG by aligning logits, without involving human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on logit alignment for preserving model capabilities in RAG, and does not mention diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "The paper addresses fine-tuning techniques for RAG to prevent forgetting, with no discussion of distributed training, parallel computing, or multi-node machine learning strategies.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03937",
      "title": "SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by\n  Self-Play Fine-Tuning",
      "authors": [
        "Yuhao Zhang",
        "Shaoming Duan",
        "Jinhang Su",
        "Chuanyi Liu",
        "Peiyi Han"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite the significant advancements of self-play fine-tuning (SPIN), which\ncan transform a weak large language model (LLM) into a strong one through\ncompetitive interactions between models of varying capabilities, it still faces\nchallenges in the Text-to-SQL task. SPIN does not generate new information, and\nthe large number of correct SQL queries produced by the opponent model during\nself-play reduces the main model's ability to generate accurate SQL queries. To\naddress this challenge, we propose a new self-play fine-tuning method tailored\nfor the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a\nverification-based iterative fine-tuning approach, which synthesizes\nhigh-quality fine-tuning data iteratively based on the database schema and\nvalidation feedback to enhance model performance, while building a model base\nwith varying capabilities. During the self-play fine-tuning phase, we propose\nan error-driven loss method that incentivizes incorrect outputs from the\nopponent model, enabling the main model to distinguish between correct SQL and\nerroneous SQL generated by the opponent model, thereby improving its ability to\ngenerate correct SQL. Extensive experiments and in-depth analyses on six\nopen-source LLMs and five widely used benchmarks demonstrate that our approach\noutperforms existing state-of-the-art (SOTA) methods.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03937v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03937v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.371,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03938",
      "title": "TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained\n  Tubular Shapes",
      "authors": [
        "Minghui Zhang",
        "Yaoyu Liu",
        "Junyang Wu",
        "Xin You",
        "Hanxiao Zhang",
        "Junjun He",
        "Yun Gu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical tubular anatomical structures are inherently three-dimensional\nconduits with lumens, enclosing walls, and complex branching topologies.\nAccurate reconstruction of their geometry and topology is crucial for\napplications such as bronchoscopic navigation and cerebral arterial\nconnectivity assessment. Existing methods often rely on voxel-wise overlap\nmeasures, which fail to capture topological correctness and completeness.\nAlthough topology-aware losses and persistent homology constraints have shown\npromise, they are usually applied patch-wise and cannot guarantee global\npreservation or correct geometric errors at inference. To address these\nlimitations, we propose a novel TopoSculpt, a framework for topological\nrefinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a\nholistic whole-region modeling strategy to capture full spatial context, (ii)\nfirst introduces a Topological Integrity Betti (TIB) constraint that jointly\nenforces Betti number priors and global integrity, and (iii) employs a\ncurriculum refinement scheme with persistent homology to progressively correct\nerrors from coarse to fine scales. Extensive experiments on challenging\npulmonary airway and Circle of Willis datasets demonstrate substantial\nimprovements in both geometry and topology. For instance, $\\beta_{0}$ errors\nare reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on\nthe CoW dataset, with Tree length detected and branch detected rates improving\nby nearly 10\\%. These results highlight the effectiveness of TopoSculpt in\ncorrecting critical topological errors and advancing the high-fidelity modeling\nof complex 3D tubular anatomy. The project homepage is available at:\nhttps://github.com/Puzzled-Hui/TopoSculpt.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03938v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03938v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.255,
      "weak_supervision_score": 0.278,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.295,
      "datasets_score": 0.261,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03940",
      "title": "VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based\n  Role-Playing Agents",
      "authors": [
        "Weihao Wu",
        "Liang Cao",
        "Xinyu Wu",
        "Zhiwei Lin",
        "Rui Niu",
        "Jingbei Li",
        "Zhiyong Wu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.SD (Sound)"
      ],
      "abstract": "Recent significant advancements in Large Language Models (LLMs) have greatly\npropelled the development of Role-Playing Conversational Agents (RPCAs). These\nsystems aim to create immersive user experiences through consistent persona\nadoption. However, current RPCA research faces dual limitations. First,\nexisting work predominantly focuses on the textual modality, entirely\noverlooking critical paralinguistic features including intonation, prosody, and\nrhythm in speech, which are essential for conveying character emotions and\nshaping vivid identities. Second, the speech-based role-playing domain suffers\nfrom a long-standing lack of standardized evaluation benchmarks. Most current\nspoken dialogue datasets target only fundamental capability assessments,\nfeaturing thinly sketched or ill-defined character profiles. Consequently, they\nfail to effectively quantify model performance on core competencies like\nlong-term persona consistency. To address this critical gap, we introduce\nVoxRole, the first comprehensive benchmark specifically designed for the\nevaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn\ndialogues, totaling 65.6 hours of speech from 1228 unique characters across 261\nmovies. To construct this resource, we propose a novel two-stage automated\npipeline that first aligns movie audio with scripts and subsequently employs an\nLLM to systematically build multi-dimensional profiles for each character.\nLeveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary\nspoken dialogue models, revealing crucial insights into their respective\nstrengths and limitations in maintaining persona consistency.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03940v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03940v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.304,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03950",
      "title": "Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer\n  Learning in a U-Net Architecture",
      "authors": [
        "Alvaro Aranibar Roque",
        "Helga Sebastian"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Pneumothorax, the abnormal accumulation of air in the pleural space, can be\nlife-threatening if undetected. Chest X-rays are the first-line diagnostic\ntool, but small cases may be subtle. We propose an automated deep-learning\npipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax\nregions. Trained on the SIIM-ACR dataset with data augmentation and a combined\nbinary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and\nDice score of 0.8241 on the independent PTX-498 dataset. These results\ndemonstrate that the model can accurately localize pneumothoraces and support\nradiologists.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03950v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03950v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.249,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.294,
      "distributed_training_score": 0.311,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03951",
      "title": "ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD\n  Detection",
      "authors": [
        "Wenjie Zhu",
        "Yabin Zhang",
        "Xin Jin",
        "Wenjun Zeng",
        "Lei Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The introduction of negative labels (NLs) has proven effective in enhancing\nOut-of-Distribution (OOD) detection. However, existing methods often lack an\nunderstanding of OOD images, making it difficult to construct an accurate\nnegative space. In addition, the presence of false negative labels\nsignificantly degrades their near-OOD performance. To address these issues, we\npropose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the\nunderstanding and reasoning capabilities of multimodal large language models\n(MLLMs). Specifically, we identify images likely to be OOD samples as negative\nimages and prompt the MLLM to describe these images, generating expressive\nnegative sentences that precisely characterize the OOD distribution and enhance\nfar-OOD detection. For the near-OOD setting, where OOD samples resemble the\nin-distribution (ID) subset, we first identify the subset of ID classes that\nare visually similar to negative images and then leverage the reasoning\ncapability of MLLMs to generate visually similar negative labels tailored to\nthis subset, effectively reducing false negatives and improving near-OOD\ndetection. To balance these two types of negative textual spaces, we design an\nadaptive weighted score that enables the method to handle different OOD task\nsettings (near-OOD and far-OOD) without relying on task-specific prior\nknowledge, making it highly adaptable in open environments. On the ImageNet\nbenchmark, our ANTS significantly reduces the FPR95 by 4.2\\%, establishing a\nnew state-of-the-art. Furthermore, our method is training-free and zero-shot,\nenabling high scalability.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03951v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03951v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.432,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.375,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper uses MLLMs to programmatically generate negative labels from image descriptions and reasoning, which involves creating noisy or imprecise labels without hand-labeling, aligning with weak supervision concepts. However, the primary focus is on OOD detection rather than extensively exploring weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper relies on MLLMs for understanding and reasoning tasks in OOD detection but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. There is no component that adapts diffusion for Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces ANTS, a method for enhancing Out-of-Distribution (OOD) detection in computer vision by leveraging Multimodal Large Language Models (MLLMs) to create an adaptive negative textual space. It addresses limitations in existing approaches by generating expressive negative sentences for far-OOD detection and visually similar negative labels for near-OOD scenarios, while using an adaptive weighted score to balance both without prior knowledge of the task setting; experiments on ImageNet show significant reductions in FPR95, establishing a new state-of-the-art in a training-free and zero-shot manner.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel integration of MLLMs for dynamically shaping adaptive negative textual spaces, addressing key limitations in OOD detection and significantly advancing the state-of-the-art by improving both far-OOD and near-OOD performance.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence OOD detection research within computer vision by demonstrating the effectiveness of MLLMs in open environments, though its impact may be confined to specific subfields rather than broadly across all AI applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution to OOD detection with state-of-the-art results and practical advantages like being training-free, making it valuable for researchers in computer vision and AI safety.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9bb08afe7f0022a2e5896410254223e51a294cfd",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 17,
      "average_h_index": 6.2,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Wen-Qing Zhu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2265742862"
        },
        {
          "name": "Yabin Zhang",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2108151704"
        },
        {
          "name": "Xin Jin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379121204"
        },
        {
          "name": "Wenjun Zeng",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2247938835"
        },
        {
          "name": "Lei Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2309479338"
        }
      ]
    },
    {
      "id": "2509.03953",
      "title": "Handling Infinite Domain Parameters in Planning Through Best-First\n  Search with Delayed Partial Expansions",
      "authors": [
        "Ángel Aso-Mollar",
        "Diego Aineto",
        "Enrico Scala",
        "Eva Onaindia"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SC (Symbolic Computation)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "In automated planning, control parameters extend standard action\nrepresentations through the introduction of continuous numeric decision\nvariables. Existing state-of-the-art approaches have primarily handled control\nparameters as embedded constraints alongside other temporal and numeric\nrestrictions, and thus have implicitly treated them as additional constraints\nrather than as decision points in the search space. In this paper, we propose\nan efficient alternative that explicitly handles control parameters as true\ndecision points within a systematic search scheme. We develop a best-first,\nheuristic search algorithm that operates over infinite decision spaces defined\nby control parameters and prove a notion of completeness in the limit under\ncertain conditions. Our algorithm leverages the concept of delayed partial\nexpansion, where a state is not fully expanded but instead incrementally\nexpands a subset of its successors. Our results demonstrate that this novel\nsearch algorithm is a competitive alternative to existing approaches for\nsolving planning problems involving control parameters.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03953v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03953v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.282,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.301,
      "datasets_score": 0.232,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03956",
      "title": "World Model Implanting for Test-time Adaptation of Embodied Agents",
      "authors": [
        "Minjong Yoo",
        "Jinwoo Jang",
        "Sihyung Yoon",
        "Honguk Woo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In embodied AI, a persistent challenge is enabling agents to robustly adapt\nto novel domains without requiring extensive data collection or retraining. To\naddress this, we present a world model implanting framework (WorMI) that\ncombines the reasoning capabilities of large language models (LLMs) with\nindependently learned, domain-specific world models through test-time\ncomposition. By allowing seamless implantation and removal of the world models,\nthe embodied agent's policy achieves and maintains cross-domain adaptability.\nIn the WorMI framework, we employ a prototype-based world model retrieval\napproach, utilizing efficient trajectory-based abstract representation\nmatching, to incorporate relevant models into test-time composition. We also\ndevelop a world-wise compound attention method that not only integrates the\nknowledge from the retrieved world models but also aligns their intermediate\nrepresentations with the reasoning model's representation within the agent's\npolicy. This framework design effectively fuses domain-specific knowledge from\nmultiple world models, ensuring robust adaptation to unseen domains. We\nevaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating\nsuperior zero-shot and few-shot performance compared to several LLM-based\napproaches across a range of unseen domains. These results highlight the\nframeworks potential for scalable, real-world deployment in embodied agent\nscenarios where adaptability and data efficiency are essential.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03956v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03956v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.353,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on test-time adaptation of embodied agents using world model implanting and LLMs, without any mention of human feedback, reward modeling from human-ranked data, or fine-tuning via reinforcement learning. It relies on domain-specific world models and retrieval methods, not RLHF techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a framework involving LLMs, world model retrieval, and attention mechanisms for reasoning and adaptation, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. There is no component that treats reasoning paths as holistically correctable entities.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03957",
      "title": "CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese\n  Misinformation Fact-Checking",
      "authors": [
        "Ruiling Guo",
        "Xinwei Yang",
        "Chen Huang",
        "Tong Zhang",
        "Yong Hu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The effectiveness of large language models (LLMs) to fact-check\nmisinformation remains uncertain, despite their growing use. To this end, we\npresent CANDY, a benchmark designed to systematically evaluate the capabilities\nand limitations of LLMs in fact-checking Chinese misinformation. Specifically,\nwe curate a carefully annotated dataset of ~20k instances. Our analysis shows\nthat current LLMs exhibit limitations in generating accurate fact-checking\nconclusions, even when enhanced with chain-of-thought reasoning and few-shot\nprompting. To understand these limitations, we develop a taxonomy to categorize\nflawed LLM-generated explanations for their conclusions and identify factual\nfabrication as the most common failure mode. Although LLMs alone are unreliable\nfor fact-checking, our findings indicate their considerable potential to\naugment human performance when deployed as assistive tools in scenarios. Our\ndataset and code can be accessed at https://github.com/SCUNLP/CANDY",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03957v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03957v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.307,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a benchmark (CANDY) for evaluating large language models (LLMs) in fact-checking Chinese misinformation, including dataset creation, performance analysis, and human-assisted studies. It does not discuss or involve reinforcement learning from human feedback (RLHF), such as training a reward model on human-ranked data to fine-tune AI models. The human elements are limited to evaluation and assistance, not RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03961",
      "title": "Multimodal Feature Fusion Network with Text Difference Enhancement for\n  Remote Sensing Change Detection",
      "authors": [
        "Yijun Zhou",
        "Yikui Zhai",
        "Zilu Ying",
        "Tingfeng Xian",
        "Wenlve Zhou",
        "Zhiheng Zhou",
        "Xiaolin Tian",
        "Xudong Jia",
        "Hongsheng Zhang",
        "C. L. Philip Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Although deep learning has advanced remote sensing change detection (RSCD),\nmost methods rely solely on image modality, limiting feature representation,\nchange pattern modeling, and generalization especially under illumination and\nnoise disturbances. To address this, we propose MMChange, a multimodal RSCD\nmethod that combines image and text modalities to enhance accuracy and\nrobustness. An Image Feature Refinement (IFR) module is introduced to highlight\nkey regions and suppress environmental noise. To overcome the semantic\nlimitations of image features, we employ a vision language model (VLM) to\ngenerate semantic descriptions of bitemporal images. A Textual Difference\nEnhancement (TDE) module then captures fine grained semantic shifts, guiding\nthe model toward meaningful changes. To bridge the heterogeneity between\nmodalities, we design an Image Text Feature Fusion (ITFF) module that enables\ndeep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and\nSYSUCD demonstrate that MMChange consistently surpasses state of the art\nmethods across multiple metrics, validating its effectiveness for multimodal\nRSCD. Code is available at: https://github.com/yikuizhai/MMChange.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03961v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03961v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.342,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a multimodal network for remote sensing change detection, focusing on feature fusion of image and text modalities using CNNs and vision-language models. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are central to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03972",
      "title": "Expanding Foundational Language Capabilities in Open-Source LLMs through\n  a Korean Case Study",
      "authors": [
        "Junghwan Lim",
        "Gangwon Jo",
        "Sungmin Lee",
        "Jiyoung Park",
        "Dongseok Kim",
        "Jihwan Kim",
        "Junhyeok Lee",
        "Wai Ting Cheung",
        "Dahye Choi",
        "Kibong Choi",
        "Jaeyeon Huh",
        "Beomgyu Kim",
        "Jangwoong Kim",
        "Taehyun Kim",
        "Haesol Lee",
        "Jeesoo Lee",
        "Dongpin Oh",
        "Changseok Song",
        "Daewon Suh"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We introduce Llama-3-Motif, a language model consisting of 102 billion\nparameters, specifically designed to enhance Korean capabilities while\nretaining strong performance in English. Developed on the Llama 3 architecture,\nLlama-3-Motif employs advanced training techniques, including LlamaPro and\nMasked Structure Growth, to effectively scale the model without altering its\ncore Transformer architecture. Using the MoAI platform for efficient training\nacross hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully\ncurated dataset that maintains a balanced ratio of Korean and English data.\nLlama-3-Motif shows decent performance on Korean-specific benchmarks,\noutperforming existing models and achieving results comparable to GPT-4.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03972v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03972v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.406,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses the use of the MoAI platform for training Llama-3-Motif, which involves managing thousands of GPU clusters with features like automatic parallelization, GPU virtualization, and dynamic GPU allocation. This directly relates to distributed training by partitioning computation across multiple nodes to accelerate model scaling and optimization, making it a core aspect of the paper's methodology for efficient large-scale training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "Llama-3-Motif is a 102 billion-parameter language model built on the Llama 3 architecture, designed to enhance Korean language capabilities while preserving strong English performance through techniques like LlamaPro for depth expansion, Masked Structure Growth for width expansion, and training on a balanced 194 billion-token Korean-English dataset using the MoAI platform. The methodology includes fine-tuning with NEFTune and Kahneman-Tversky Optimization, resulting in superior performance on Korean benchmarks such as KMMLU and KorMedMCQA, where it outperforms existing models by 9-40% and achieves results comparable to GPT-4, thereby addressing gaps in under-resourced languages through efficient model scaling and high-quality data curation.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing techniques like LlamaPro and Masked Structure Growth to scale a model for better Korean performance, offering a clever adaptation for under-resourced languages rather than introducing a entirely new problem or architecture. However, it builds on established methods, making it an incremental rather than groundbreaking contribution.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in multilingual LLMs by providing practical insights into scaling and dataset curation for non-English languages, potentially leading to citations and applications in subfields like Korean NLP. While its impact is significant within this niche, it may not broadly affect general AI or commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a strong, valuable contribution to improving LLMs for under-resourced languages like Korean, with innovative scaling techniques and benchmark results that are relevant for AI researchers in language processing. It is essential for those working in multilingual AI but not universally groundbreaking.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b992f2a50b705ee7f27785f2019274849358dada",
      "total_authors": 19,
      "authors_found": 19,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Junghwan Lim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375865749"
        },
        {
          "name": "Gangwon Jo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378953770"
        },
        {
          "name": "Sungmin Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375941554"
        },
        {
          "name": "Jiyoung Park",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379036827"
        },
        {
          "name": "Dongseok Kim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375860133"
        },
        {
          "name": "Jihwan Kim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375853829"
        },
        {
          "name": "Junhyeok Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375857950"
        },
        {
          "name": "Wai Ting Cheung",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375818111"
        },
        {
          "name": "Dahye Choi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376233020"
        },
        {
          "name": "Kibong Choi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379184320"
        },
        {
          "name": "Jaeyeon Huh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375818623"
        },
        {
          "name": "Beomgyu Kim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375844761"
        },
        {
          "name": "Jangwoong Kim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379193388"
        },
        {
          "name": "Taehyun Kim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379984276"
        },
        {
          "name": "Haesol Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375860222"
        },
        {
          "name": "Jeesoo Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375857933"
        },
        {
          "name": "Dongpin Oh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375819425"
        },
        {
          "name": "Changseok Song",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381149648"
        },
        {
          "name": "Daewon Suh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375818515"
        }
      ]
    },
    {
      "id": "2509.03973",
      "title": "SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for\n  Histopathology Whole Slide Image Classification",
      "authors": [
        "Yu Bai",
        "Zitong Yu",
        "Haowen Tian",
        "Xijing Wang",
        "Shuo Yan",
        "Lin Wang",
        "Honglin Li",
        "Xitong Ling",
        "Bo Zhang",
        "Zheng Zhang",
        "Wufan Wang",
        "Hui Gao",
        "Xiangyang Gong",
        "Wendong Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for\nperforming WSI classification. SAC-MIL consists of a positional encoding module\nto encode position information and a SAC block to perform full instance\ncorrelations. The positional encoding module utilizes the instance coordinates\nwithin the slide to encode the spatial relationships instead of the instance\nindex in the input WSI sequence. The positional encoding module can also handle\nthe length extrapolation issue where the training and testing sequences have\ndifferent lengths. The SAC block is an MLP-based method that performs full\ninstance correlation in linear time complexity with respect to the sequence\nlength. Due to the simple structure of MLP, it is easy to deploy since it does\nnot require custom CUDA kernels, compared to Transformer-based methods for WSI\nclassification. SAC-MIL has achieved state-of-the-art performance on the\nCAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon\nacceptance.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03973v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03973v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.272,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.308,
      "distributed_training_score": 0.304,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03975",
      "title": "Improving Vessel Segmentation with Multi-Task Learning and Auxiliary\n  Data Available Only During Model Training",
      "authors": [
        "Daniel Sobotka",
        "Alexander Herold",
        "Matthias Perkonigg",
        "Lucian Beer",
        "Nina Bastati",
        "Alina Sablatnig",
        "Ahmed Ba-Ssalamah",
        "Georg Langs"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Liver vessel segmentation in magnetic resonance imaging data is important for\nthe computational analysis of vascular remodelling, associated with a wide\nspectrum of diffuse liver diseases. Existing approaches rely on contrast\nenhanced imaging data, but the necessary dedicated imaging sequences are not\nuniformly acquired. Images without contrast enhancement are acquired more\nfrequently, but vessel segmentation is challenging, and requires large-scale\nannotated data. We propose a multi-task learning framework to segment vessels\nin liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data\navailable only during training to reduce the need for annotated training\nexamples. Our approach draws on paired native and contrast enhanced data with\nand without vessel annotations for model training. Results show that auxiliary\ndata improves the accuracy of vessel segmentation, even if they are not\navailable during inference. The advantage is most pronounced if only few\nannotations are available for training, since the feature representation\nbenefits from the shared task structure. A validation of this approach to\naugment a model for brain tumor segmentation confirms its benefits across\ndifferent domains. An auxiliary informative imaging modality can augment expert\nannotations even if it is only available during training.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03975v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03975v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.434,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.376,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper employs a multi-task learning framework that uses auxiliary contrast-enhanced MRI data during training to improve vessel segmentation in non-contrast images, reducing the need for fully annotated data. This approach leverages unannotated or partially annotated pairs, which aligns with weak supervision by incorporating noisy or indirect sources of information (e.g., auxiliary data for implicit learning). However, it primarily focuses on semi-supervised multi-task learning rather than programmatically generating labels from high-level sources, making it moderately relevant rather than a direct match.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces a multi-task learning framework to enhance vessel segmentation in non-contrast enhanced liver MRI by leveraging auxiliary contrast-enhanced MRI data available only during training, aiming to reduce the need for extensive annotated datasets. Using a Y-net model that simultaneously predicts vessel labels and reconstructs contrast-enhanced images from non-contrast inputs, the approach improves segmentation accuracy, particularly with limited annotations, and demonstrates generalizability through validation on brain tumor segmentation tasks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining multi-task learning with auxiliary data for medical image segmentation, offering a practical adaptation of existing techniques to address data scarcity, though it does not introduce a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in medical imaging subfields due to its practical approach for handling limited annotations, potentially influencing applications in areas like tumor segmentation, but its reach may be confined to specialized domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to computer vision in medical imaging by demonstrating an effective use of auxiliary data, making it essential for researchers working on segmentation tasks with data constraints.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a55e8eefccce703091f8a88b541e99e4f94dcb12",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 18,
      "average_h_index": 4.875,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Daniel Sobotka",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/1393444886"
        },
        {
          "name": "Alexander Herold",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2179213072"
        },
        {
          "name": "Matthias Perkonigg",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2130152515"
        },
        {
          "name": "Lucian Beer",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2266727243"
        },
        {
          "name": "N. Bastati",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/11552919"
        },
        {
          "name": "Alina Sablatnig",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2291898756"
        },
        {
          "name": "A. Ba-Ssalamah",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2266727251"
        },
        {
          "name": "Georg Langs",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2266326474"
        }
      ]
    },
    {
      "id": "2509.03985",
      "title": "NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language\n  Models",
      "authors": [
        "Chuhan Zhang",
        "Ye Zhang",
        "Bowen Shi",
        "Yuyou Gan",
        "Tianyu Du",
        "Shouling Ji",
        "Dazhan Deng",
        "Yingcai Wu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In deployment and application, large language models (LLMs) typically undergo\nsafety alignment to prevent illegal and unethical outputs. However, the\ncontinuous advancement of jailbreak attack techniques, designed to bypass\nsafety mechanisms with adversarial prompts, has placed increasing pressure on\nthe security defenses of LLMs. Strengthening resistance to jailbreak attacks\nrequires an in-depth understanding of the security mechanisms and\nvulnerabilities of LLMs. However, the vast number of parameters and complex\nstructure of LLMs make analyzing security weaknesses from an internal\nperspective a challenging task. This paper presents NeuroBreak, a top-down\njailbreak analysis system designed to analyze neuron-level safety mechanisms\nand mitigate vulnerabilities. We carefully design system requirements through\ncollaboration with three experts in the field of AI security. The system\nprovides a comprehensive analysis of various jailbreak attack methods. By\nincorporating layer-wise representation probing analysis, NeuroBreak offers a\nnovel perspective on the model's decision-making process throughout its\ngeneration steps. Furthermore, the system supports the analysis of critical\nneurons from both semantic and functional perspectives, facilitating a deeper\nexploration of security mechanisms. We conduct quantitative evaluations and\ncase studies to verify the effectiveness of our system, offering mechanistic\ninsights for developing next-generation defense strategies against evolving\njailbreak attacks.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03985v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03985v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.443,
      "distributed_training_score": 0.363,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions safety alignment through fine-tuning on curated datasets, which could indirectly relate to RLHF as it often involves human feedback for alignment, but the core contribution is on analyzing jailbreak mechanisms and neuron-level vulnerabilities, not on implementing or evaluating RLHF systems with reward models and reinforcement learning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on neuron-level analysis and visual analytics for understanding jailbreak attacks in LLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03986",
      "title": "Promptception: How Sensitive Are Large Multimodal Models to Prompts?",
      "authors": [
        "Mohamed Insaf Ismithdeen",
        "Muhammad Uzair Khattak",
        "Salman Khan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Despite the success of Large Multimodal Models (LMMs) in recent years, prompt\ndesign for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly\nunderstood. We show that even minor variations in prompt phrasing and structure\ncan lead to accuracy deviations of up to 15% for certain prompts and models.\nThis variability poses a challenge for transparent and fair LMM evaluation, as\nmodels often report their best-case performance using carefully selected\nprompts. To address this, we introduce Promptception, a systematic framework\nfor evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,\nspanning 15 categories and 6 supercategories, each targeting specific aspects\nof prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight\nopen-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:\nMMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit\ngreater sensitivity to prompt phrasing, reflecting tighter alignment with\ninstruction semantics, while open-source models are steadier but struggle with\nnuanced and complex phrasing. Based on this analysis, we propose Prompting\nPrinciples tailored to proprietary and open-source LMMs, enabling more robust\nand fair model evaluation.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03986v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03986v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.341,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses LMMs' sensitivity to prompts and mentions tighter alignment with instruction semantics in proprietary models, which may indirectly relate to RLHF as a training method for such alignment. However, the paper focuses on evaluation frameworks and prompt variations, not on systems using human feedback for training or fine-tuning.",
      "weak_supervision_justification": "The paper introduces a framework for evaluating prompt sensitivity in LMMs using systematically designed prompts, but it does not involve training models with programmatically generated labels or rely on noisy/imprecise sources for supervision. It is solely about testing existing models, not weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper examines prompt variations in LMMs for MCQA tasks and proposes an evaluation framework, but it does not mention or adapt diffusion processes for multi-step logical reasoning or iterative refinement of reasoning paths. There is no component involving diffusion models.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03990",
      "title": "Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility\n  for Resource-Efficient LLM Agent",
      "authors": [
        "Chunlong Wu",
        "Ye Luo",
        "Zhibo Qu",
        "Min Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language model (LLM) agents achieve impressive single-task performance\nbut commonly exhibit repeated failures, inefficient exploration, and limited\ncross-task adaptability. Existing reflective strategies (e.g., Reflexion,\nReAct) improve per-episode behavior but typically produce ephemeral,\ntask-specific traces that are not reused across tasks. Reinforcement-learning\nbased alternatives can produce transferable policies but require substantial\nparameter updates and compute. In this work we introduce Meta-Policy Reflexion\n(MPR): a hybrid framework that consolidates LLM-generated reflections into a\nstructured, predicate-like Meta-Policy Memory (MPM) and applies that memory at\ninference time through two complementary mechanisms soft memory-guided decoding\nand hard rule admissibility checks(HAC). MPR (i) externalizes reusable\ncorrective knowledge without model weight updates, (ii) enforces domain\nconstraints to reduce unsafe or invalid actions, and (iii) retains the\nadaptability of language-based reflection. We formalize the MPM representation,\npresent algorithms for update and decoding, and validate the approach in a\ntext-based agent environment following the experimental protocol described in\nthe provided implementation (AlfWorld-based). Empirical results reported in the\nsupplied material indicate consistent gains in execution accuracy and\nrobustness when compared to Reflexion baselines; rule admissibility further\nimproves stability. We analyze mechanisms that explain these gains, discuss\nscalability and failure modes, and outline future directions for multimodal and\nmulti-agent extensions.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03990v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03990v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.516,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.464,
      "distributed_training_score": 0.354,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions reinforcement learning as a complementary approach for transferable policies but does not implement RLHF. It focuses on Meta-Policy Reflexion, which uses LLM-generated reflections without human feedback or fine-tuning, making the connection indirect.",
      "weak_supervision_justification": "The paper involves programmatically generating rules from LLM-produced reflections, which are noisy and derived from failure analyses, aligning with weak supervision's use of imprecise sources for training or guidance. However, it does not explicitly frame this as a weak supervision method for model training.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for reasoning tasks. It focuses on reflection-based memory and rule mechanisms for LLM agents, with no mention of multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Meta-Policy Reflexion (MPR), a framework to enhance Large Language Model (LLM) agents by creating a reusable Meta-Policy Memory (MPM) from reflections, which stores predicate-like rules to guide actions via soft decoding and hard admissibility checks, aiming to reduce repeated failures and improve adaptability without updating model weights. The methodology involves formalizing MPM, developing algorithms for its update and application, and validating it empirically in a text-based environment like AlfWorld, where results demonstrate improved execution accuracy, robustness, and stability compared to baselines such as Reflexion.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing reflective strategies with a structured, reusable memory system, offering a new way to enhance LLM agents without parameter updates, though it builds on known concepts like Reflexion and ReAct.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in LLM agents by providing a resource-efficient method for better adaptability and safety, potentially leading to citations and applications within this subfield but not broadly across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, practical contribution to LLM agent design with empirical validation, making it valuable for researchers focused on AI agents to understand and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f9d3395aa998257311c9e6724d7b325702be3dae",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Chunlong Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378472103"
        },
        {
          "name": "Ye Luo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379721845"
        },
        {
          "name": "Zhibo Qu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377280766"
        },
        {
          "name": "Min Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380084332"
        }
      ]
    },
    {
      "id": "2509.03995",
      "title": "RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question\n  Answering with Large Language Models",
      "authors": [
        "Zhaoyan Gong",
        "Juan Li",
        "Zhiqiang Liu",
        "Lei Liang",
        "Huajun Chen",
        "Wen Zhang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Current temporal knowledge graph question answering (TKGQA) methods primarily\nfocus on implicit temporal constraints, lacking the capability of handling more\ncomplex temporal queries, and struggle with limited reasoning abilities and\nerror propagation in decomposition frameworks. We propose RTQA, a novel\nframework to address these challenges by enhancing reasoning over TKGs without\nrequiring training. Following recursive thinking, RTQA recursively decomposes\nquestions into sub-problems, solves them bottom-up using LLMs and TKG\nknowledge, and employs multi-path answer aggregation to improve fault\ntolerance. RTQA consists of three core components: the Temporal Question\nDecomposer, the Recursive Solver, and the Answer Aggregator. Experiments on\nMultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements\nin \"Multiple\" and \"Complex\" categories, outperforming state-of-the-art methods.\nOur code and data are available at https://github.com/zjukg/RTQA.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03995v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03995v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.349,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces RTQA, a framework for temporal knowledge graph question answering that uses recursive decomposition and bottom-up reasoning with LLMs and TKG knowledge, along with multi-path answer aggregation for fault tolerance. It does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a single entity for holistic correction. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03999",
      "title": "SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy\n  Representation",
      "authors": [
        "Han Huang",
        "Han Sun",
        "Ningzhong Liu",
        "Huiyu Zhou",
        "Jiaquan Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Driven by autonomous driving's demands for precise 3D perception, 3D semantic\noccupancy prediction has become a pivotal research topic. Unlike\nbird's-eye-view (BEV) methods, which restrict scene representation to a 2D\nplane, occupancy prediction leverages a complete 3D voxel grid to model spatial\nstructures in all dimensions, thereby capturing semantic variations along the\nvertical axis. However, most existing approaches overlook height-axis\ninformation when processing voxel features. And conventional SENet-style\nchannel attention assigns uniform weight across all height layers, limiting\ntheir ability to emphasize features at different heights. To address these\nlimitations, we propose SliceSemOcc, a novel vertical slice based multimodal\nframework for 3D semantic occupancy representation. Specifically, we extract\nvoxel features along the height-axis using both global and local vertical\nslices. Then, a global local fusion module adaptively reconciles fine-grained\nspatial details with holistic contextual information. Furthermore, we propose\nthe SEAttention3D module, which preserves height-wise resolution through\naverage pooling and assigns dynamic channel attention weights to each height\nlayer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy\ndatasets verify that our method significantly enhances mean IoU, achieving\nespecially pronounced gains on most small-object categories. Detailed ablation\nstudies further validate the effectiveness of the proposed SliceSemOcc\nframework.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.03999v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03999v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.331,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04007",
      "title": "AutoPBO: LLM-powered Optimization for Local Search PBO Solvers",
      "authors": [
        "Jinyuan Li",
        "Yi Chu",
        "Yiwen Sun",
        "Mengchuan Zou",
        "Shaowei Cai"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling\ncombinatorial problems through pseudo-Boolean (PB) constraints. Local search\nsolvers have shown excellent performance in PBO solving, and their efficiency\nis highly dependent on their internal heuristics to guide the search. Still,\ntheir design often requires significant expert effort and manual tuning in\npractice. While Large Language Models (LLMs) have demonstrated potential in\nautomating algorithm design, their application to optimizing PBO solvers\nremains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered\nframework to automatically enhance PBO local search solvers. We conduct\nexperiments on a broad range of four public benchmarks, including one\nreal-world benchmark, a benchmark from PB competition, an integer linear\nprogramming optimization benchmark, and a crafted combinatorial benchmark, to\nevaluate the performance improvement achieved by AutoPBO and compare it with\nsix state-of-the-art competitors, including two local search PBO solvers NuPBO\nand OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed\ninteger programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates\nsignificant improvements over previous local search approaches, while\nmaintaining competitive performance compared to state-of-the-art competitors.\nThe results suggest that AutoPBO offers a promising approach to automating\nlocal search solver design.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04007v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04007v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.356,
      "datasets_score": 0.262,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an LLM-powered framework for automating PBO solver optimization using multi-agent systems and greedy search, with feedback-driven processes that appear automated. There is no mention of human feedback, training a reward model on human-ranked data, or using reinforcement learning to fine-tune models based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes using LLMs for iterative code generation and optimization in PBO solvers, but it does not involve diffusion models, iterative refinement of a Chain-of-Thought for logical tasks, or any multi-step reasoning process based on diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04009",
      "title": "Detecting Regional Spurious Correlations in Vision Transformers via\n  Token Discarding",
      "authors": [
        "Solha Kang",
        "Esla Timothy Anzaku",
        "Wesley De Neve",
        "Arnout Van Messem",
        "Joris Vankerschaver",
        "Francois Rameau",
        "Utku Ozbulak"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Due to their powerful feature association capabilities, neural network-based\ncomputer vision models have the ability to detect and exploit unintended\npatterns within the data, potentially leading to correct predictions based on\nincorrect or unintended but statistically relevant signals. These clues may\nvary from simple color aberrations to small texts within the image. In\nsituations where these unintended signals align with the predictive task,\nmodels can mistakenly link these features with the task and rely on them for\nmaking predictions. This phenomenon is referred to as spurious correlations,\nwhere patterns appear to be associated with the task but are actually\ncoincidental. As a result, detection and mitigation of spurious correlations\nhave become crucial tasks for building trustworthy, reliable, and generalizable\nmachine learning models. In this work, we present a novel method to detect\nspurious correlations in vision transformers, a type of neural network\narchitecture that gained significant popularity in recent years. Using both\nsupervised and self-supervised trained models, we present large-scale\nexperiments on the ImageNet dataset demonstrating the ability of the proposed\nmethod to identify spurious correlations. We also find that, even if the same\narchitecture is used, the training methodology has a significant impact on the\nmodel's reliance on spurious correlations. Furthermore, we show that certain\nclasses in the ImageNet dataset contain spurious signals that are easily\ndetected by the models and discuss the underlying reasons for those spurious\nsignals. In light of our findings, we provide an exhaustive list of the\naforementioned images and call for caution in their use in future research\nefforts. Lastly, we present a case study investigating spurious signals in\ninvasive breast mass classification, grounding our work in real-world\nscenarios.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04009v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04009v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.37,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on detecting spurious correlations in Vision Transformers using token discarding methods, emphasizing computer vision tasks like image classification on datasets such as ImageNet. It does not involve diffusion models, iterative refinement processes for logical reasoning, or any adaptation of diffusion techniques for multi-step Chain-of-Thought reasoning. Therefore, there is no overlap with the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04011",
      "title": "NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware\n  Embeddings",
      "authors": [
        "Or Shachar",
        "Uri Katz",
        "Yoav Goldberg",
        "Oren Glickman"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named\nEntity Retrieval, a variant of Named Entity Recognition (NER), where the types\nof interest are not provided in advance, and a user-defined type description is\nused to retrieve documents mentioning entities of that type. Instead of relying\non fixed schemas or fine-tuned models, our method builds on internal\nrepresentations of large language models (LLMs) to embed both entity mentions\nand user-provided open-ended type descriptions into a shared semantic space. We\nshow that internal representations, specifically the value vectors from\nmid-layer transformer blocks, encode fine-grained type information more\neffectively than commonly used top-layer embeddings. To refine these\nrepresentations, we train a lightweight contrastive projection network that\naligns type-compatible entities while separating unrelated types. The resulting\nentity embeddings are compact, type-aware, and well-suited for nearest-neighbor\nsearch. Evaluated on three benchmarks, NER Retriever significantly outperforms\nboth lexical and dense sentence-level retrieval baselines. Our findings provide\nempirical support for representation selection within LLMs and demonstrate a\npractical solution for scalable, schema-free entity retrieval. The NER\nRetriever Codebase is publicly available at\nhttps://github.com/ShacharOr100/ner_retriever",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04011v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04011v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.332,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04013",
      "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
      "authors": [
        "Riccardo Lunardi",
        "Vincenzo Della Mea",
        "Stefano Mizzaro",
        "Kevin Roitero"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04013v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04013v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.447,
      "weak_supervision_score": 0.43,
      "diffusion_reasoning_score": 0.433,
      "distributed_training_score": 0.382,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on evaluating the robustness of LLMs to paraphrased benchmark questions, not on training models using human feedback or reinforcement learning techniques. There is no mention of aligning models with human preferences via a reward model.",
      "weak_supervision_justification": "The paper involves programmatically generating paraphrases of benchmark questions, which could loosely relate to noisy label generation in weak supervision, but it does not address training models with imprecise labels or weak supervision methods; instead, it evaluates existing models' performance.",
      "diffusion_reasoning_justification": "The paper examines LLM robustness to linguistic variations in benchmarks and does not involve diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper analyzes and evaluates existing benchmarks (datasets) by generating paraphrases and assessing their impact on LLM performance, which aligns with dataset analysis and benchmark evaluation, though it does not introduce new datasets or focus on curation methodologies.",
      "llm_score_status": "completed",
      "summary": "This paper examines the robustness of Large Language Models (LLMs) to linguistic variations by generating systematic paraphrases of questions from six common benchmarks and evaluating their impact on 34 state-of-the-art LLMs. The core objectives are to assess whether LLMs maintain performance across reworded inputs and to determine the reliability of benchmark-based evaluations; the methodology involves creating and testing these paraphrases, revealing that while relative model rankings remain stable, absolute performance scores drop significantly, indicating potential overestimation of LLMs' generalization abilities and the need for more robust evaluation methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by systematically evaluating LLM robustness to paraphrases across multiple benchmarks and models, building on existing research to highlight gaps in generalization but not introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in AI and NLP by prompting improvements in benchmark design and evaluation practices, though its applicability may be confined to specific subfields dealing with LLM assessments.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers valuable insights into the limitations of current LLM evaluation methods, making it essential for researchers in AI and computational language to consider for advancing more reliable benchmarking approaches.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c5c7fef575c2a1988047c88084bcb9675bc57458",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 28,
      "average_h_index": 16.75,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Riccardo Lunardi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2336093860"
        },
        {
          "name": "V. D. Mea",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/2335925"
        },
        {
          "name": "Stefano Mizzaro",
          "h_index": 28,
          "profile_url": "https://www.semanticscholar.org/author/1726978"
        },
        {
          "name": "Kevin Roitero",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/3445334"
        }
      ]
    },
    {
      "id": "2509.04023",
      "title": "Learning from Majority Label: A Novel Problem in Multi-class\n  Multiple-Instance Learning",
      "authors": [
        "Shiku Kaito",
        "Shinnosuke Matsuo",
        "Daiki Suehiro",
        "Ryoma Bise"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The paper proposes a novel multi-class Multiple-Instance Learning (MIL)\nproblem called Learning from Majority Label (LML). In LML, the majority class\nof instances in a bag is assigned as the bag-level label. The goal of LML is to\ntrain a classification model that estimates the class of each instance using\nthe majority label. This problem is valuable in a variety of applications,\nincluding pathology image segmentation, political voting prediction, customer\nsentiment analysis, and environmental monitoring. To solve LML, we propose a\nCounting Network trained to produce bag-level majority labels, estimated by\ncounting the number of instances in each class. Furthermore, analysis\nexperiments on the characteristics of LML revealed that bags with a high\nproportion of the majority class facilitate learning. Based on this result, we\ndeveloped a Majority Proportion Enhancement Module (MPEM) that increases the\nproportion of the majority class by removing minority class instances within\nthe bags. Experiments demonstrate the superiority of the proposed method on\nfour datasets compared to conventional MIL methods. Moreover, ablation studies\nconfirmed the effectiveness of each module. The code is available at\n\\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04023v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04023v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.42,
      "diffusion_reasoning_score": 0.287,
      "distributed_training_score": 0.321,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves Multiple-Instance Learning (MIL) with bag-level labels based on the majority class, which aligns closely with weak supervision. In weak supervision, models are trained using high-level, noisy, or imprecise labels rather than precise instance-level annotations. Here, the bag-level majority label serves as a weak signal for instance-level classification, as seen in applications like pathology image segmentation. This directly advances weak supervision techniques by proposing methods like the Counting Network and Majority Proportion Enhancement Module to handle such labels effectively.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Learning from Majority Label (LML), a novel problem in multi-class Multiple-Instance Learning (MIL) where a bag's label is determined by the majority class of its instances, aiming to train a model for instance-level classification. The authors propose a Counting Network that estimates instance classes by counting predictions to match the majority label and a Majority Proportion Enhancement Module (MPEM) to increase the majority class proportion in bags, with experiments on four datasets demonstrating superior performance over conventional MIL methods and confirming the effectiveness of each component through ablation studies.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new problem in MIL with LML and proposes innovative techniques like the Counting Network and MPEM, significantly advancing the state-of-the-art by addressing majority label ambiguities not covered in existing methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in specific subfields like computer vision and MIL applications, such as pathology image analysis and sentiment prediction, due to its practical methods and demonstrated improvements.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a strong, valuable contribution to MIL by introducing a new problem and effective solutions, making it essential for researchers in computer vision and related areas to be aware of its insights and methods.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ba0445fac7a4cedc48ce30bb332b3d60149c3134",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 7,
      "average_h_index": 3.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shiku Kaito",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378955993"
        },
        {
          "name": "Shinnosuke Matsuo",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2059604253"
        },
        {
          "name": "D. Suehiro",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1833688"
        },
        {
          "name": "Ryoma Bise",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261295283"
        }
      ]
    },
    {
      "id": "2509.04027",
      "title": "CoT-Space: A Theoretical Framework for Internal Slow-Thinking via\n  Reinforcement Learning",
      "authors": [
        "Zeyu Gan",
        "Hao Yi",
        "Yong Liu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Reinforcement Learning (RL) has become a pivotal approach for enhancing the\nreasoning capabilities of Large Language Models (LLMs). However, a significant\ntheoretical gap persists, as traditional token-level RL frameworks fail to\nalign with the reasoning-level nature of complex, multi-step thought processes\nlike Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,\na novel theoretical framework that recasts LLM reasoning from a discrete\ntoken-prediction task to an optimization process within a continuous,\nreasoning-level semantic space. By analyzing this process from both a noise\nperspective and a risk perspective, we demonstrate that the convergence to an\noptimal CoT length is a natural consequence of the fundamental trade-off\nbetween underfitting and overfitting. Furthermore, extensive experiments\nprovide strong empirical validation for our theoretical findings. Our framework\nnot only provides a coherent explanation for empirical phenomena such as\noverthinking but also offers a solid theoretical foundation to guide the future\ndevelopment of more effective and generalizable reasoning agents.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04027v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04027v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.572,
      "distributed_training_score": 0.361,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Reinforcement Learning (RL) for enhancing LLM reasoning through self-exploration and high-level rewards, without mentioning human feedback, a reward model trained on human-ranked data, or alignment with human preferences. This does not align with the definition of RLHF, making the paper not relevant.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a theoretical framework for LLM reasoning using RL and analyzes it in a semantic space, but it does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for multi-step correction. Thus, it lacks any components related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04041",
      "title": "Oruga: An Avatar of Representational Systems Theory",
      "authors": [
        "Daniel Raggi",
        "Gem Stapleton",
        "Mateja Jamnik",
        "Aaron Stockdill",
        "Grecia Garcia Garcia",
        "Peter C-H. Cheng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "Humans use representations flexibly. We draw diagrams, change representations\nand exploit creative analogies across different domains. We want to harness\nthis kind of power and endow machines with it to make them more compatible with\nhuman use. Previously we developed Representational Systems Theory (RST) to\nstudy the structure and transformations of representations. In this paper we\npresent Oruga (caterpillar in Spanish; a symbol of transformation), an\nimplementation of various aspects of RST. Oruga consists of a core of data\nstructures corresponding to concepts in RST, a language for communicating with\nthe core, and an engine for producing transformations using a method we call\nstructure transfer. In this paper we present an overview of the core and\nlanguage of Oruga, with a brief example of the kind of transformation that\nstructure transfer can execute.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04041v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04041v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.249,
      "weak_supervision_score": 0.238,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.216,
      "datasets_score": 0.235,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04043",
      "title": "Millisecond-Response Tracking and Gazing System for UAVs: A Domestic\n  Solution Based on \"Phytium + Cambricon\"",
      "authors": [
        "Yuchen Zhu",
        "Longxiang Yin",
        "Kai Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In the frontier research and application of current video surveillance\ntechnology, traditional camera systems exhibit significant limitations of\nresponse delay exceeding 200 ms in dynamic scenarios due to the insufficient\ndeep feature extraction capability of automatic recognition algorithms and the\nefficiency bottleneck of computing architectures, failing to meet the real-time\nrequirements in complex scenes. To address this issue, this study proposes a\nheterogeneous computing architecture based on Phytium processors and Cambricon\naccelerator cards, constructing a UAV tracking and gazing system with\nmillisecond-level response capability. At the hardware level, the system adopts\na collaborative computing architecture of Phytium FT-2000/4 processors and\nMLU220 accelerator cards, enhancing computing power through multi-card\nparallelism. At the software level, it innovatively integrates a lightweight\nYOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming\na closed-loop control chain of \"detection-tracking-feedback\". Experimental\nresults demonstrate that the system achieves a stable single-frame\ncomprehensive processing delay of 50-100 ms in 1920*1080 resolution video\nstream processing, with a multi-scale target recognition accuracy of over\n98.5%, featuring both low latency and high precision. This study provides an\ninnovative solution for UAV monitoring and the application of domestic chips.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04043v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04043v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.286,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.383,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04047",
      "title": "TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface\n  Scattering for Perlin Distributed Heterogeneous Media",
      "authors": [
        "Ashish Tiwari",
        "Satyam Bhardwaj",
        "Yash Bachwana",
        "Parag Sarvoday Sahu",
        "T. M. Feroz Ali",
        "Bhargava Chintalapati",
        "Shanmuganathan Raman"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Estimating scattering parameters of heterogeneous media from images is a\nseverely under-constrained and challenging problem. Most of the existing\napproaches model BSSRDF either through an analysis-by-synthesis approach,\napproximating complex path integrals, or using differentiable volume rendering\ntechniques to account for heterogeneity. However, only a few studies have\napplied learning-based methods to estimate subsurface scattering parameters,\nbut they assume homogeneous media. Interestingly, no specific distribution is\nknown to us that can explicitly model the heterogeneous scattering parameters\nin the real world. Notably, procedural noise models such as Perlin and Fractal\nPerlin noise have been effective in representing intricate heterogeneities of\nnatural, organic, and inorganic surfaces. Leveraging this, we first create\nHeteroSynth, a synthetic dataset comprising photorealistic images of\nheterogeneous media whose scattering parameters are modeled using Fractal\nPerlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a\nlearning-based feed-forward framework to estimate these Perlin-distributed\nheterogeneous scattering parameters from sparse multi-view image observations.\nInstead of directly predicting the 3D scattering parameter volume, TensoIS uses\nlearnable low-rank tensor components to represent the scattering volume. We\nevaluate TensoIS on unseen heterogeneous variations over shapes from the\nHeteroSynth test set, smoke and cloud geometries obtained from open-source\nrealistic volumetric simulations, and some real-world samples to establish its\neffectiveness for inverse scattering. Overall, this study is an attempt to\nexplore Perlin noise distribution, given the lack of any such well-defined\ndistribution in literature, to potentially model real-world heterogeneous\nscattering in a feed-forward manner.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04047v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04047v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.364,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04050",
      "title": "A Re-ranking Method using K-nearest Weighted Fusion for Person\n  Re-identification",
      "authors": [
        "Quang-Huy Che",
        "Le-Chuong Nguyen",
        "Gia-Nghia Tran",
        "Dinh-Duy Phan",
        "Vinh-Tiep Nguyen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In person re-identification, re-ranking is a crucial step to enhance the\noverall accuracy by refining the initial ranking of retrieved results. Previous\nstudies have mainly focused on features from single-view images, which can\ncause view bias and issues like pose variation, viewpoint changes, and\nocclusions. Using multi-view features to present a person can help reduce view\nbias. In this work, we present an efficient re-ranking method that generates\nmulti-view features by aggregating neighbors' features using K-nearest Weighted\nFusion (KWF) method. Specifically, we hypothesize that features extracted from\nre-identification models are highly similar when representing the same\nidentity. Thus, we select K neighboring features in an unsupervised manner to\ngenerate multi-view features. Additionally, this study explores the weight\nselection strategies during feature aggregation, allowing us to identify an\neffective strategy. Our re-ranking approach does not require model fine-tuning\nor extra annotations, making it applicable to large-scale datasets. We evaluate\nour method on the person re-identification datasets Market1501, MSMT17, and\nOccluded-DukeMTMC. The results show that our method significantly improves\nRank@1 and mAP when re-ranking the top M candidates from the initial ranking\nresults. Specifically, compared to the initial results, our re-ranking method\nachieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets:\nMSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach\ndemonstrates substantial enhancements in computational efficiency compared to\nother re-ranking methods.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04050v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04050v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.313,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04051",
      "title": "Neural Video Compression with In-Loop Contextual Filtering and\n  Out-of-Loop Reconstruction Enhancement",
      "authors": [
        "Yaojun Wu",
        "Chaoyi Lin",
        "Yiming Wang",
        "Semih Esenlik",
        "Zhaobin Zhang",
        "Kai Zhang",
        "Li Zhang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper explores the application of enhancement filtering techniques in\nneural video compression. Specifically, we categorize these techniques into\nin-loop contextual filtering and out-of-loop reconstruction enhancement based\non whether the enhanced representation affects the subsequent coding loop.\nIn-loop contextual filtering refines the temporal context by mitigating error\npropagation during frame-by-frame encoding. However, its influence on both the\ncurrent and subsequent frames poses challenges in adaptively applying filtering\nthroughout the sequence. To address this, we introduce an adaptive coding\ndecision strategy that dynamically determines filtering application during\nencoding. Additionally, out-of-loop reconstruction enhancement is employed to\nrefine the quality of reconstructed frames, providing a simple yet effective\nimprovement in coding efficiency. To the best of our knowledge, this work\npresents the first systematic study of enhancement filtering in the context of\nconditional-based neural video compression. Extensive experiments demonstrate a\n7.71% reduction in bit rate compared to state-of-the-art neural video codecs,\nvalidating the effectiveness of the proposed approach.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04051v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04051v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.319,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04058",
      "title": "SMooGPT: Stylized Motion Generation using Large Language Models",
      "authors": [
        "Lei Zhong",
        "Yi Yang",
        "Changjian Li"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04058v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04058v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.505,
      "distributed_training_score": 0.317,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a methodology using fine-tuned LLMs for reasoning, composing, and generating stylized motions via body-part text representations. It references diffusion models in the context of existing works for motion generation, but does not adapt the iterative refinement process of diffusion for multi-step logical reasoning or treat a 'Chain-of-Thought' as a holistic entity. The core approach relies on LLM capabilities, not diffusion-based mechanisms, so it lacks the required components for this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04076",
      "title": "Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot",
      "authors": [
        "Lennart Clasmeier",
        "Jan-Gerrit Habekost",
        "Connor Gäde",
        "Philipp Allgeuer",
        "Stefan Wermter"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose a novel diffusion-based action model for robotic motion planning.\nCommonly, established numerical planning approaches are used to solve general\nmotion planning problems, but have significant runtime requirements. By\nleveraging the power of deep learning, we are able to achieve good results in a\nmuch smaller runtime by learning from a dataset generated by these planners.\nWhile our initial model uses point cloud embeddings in the input to predict\nkeypoint-based joint sequences in its output, we observed in our ablation study\nthat it remained challenging to condition the network on the point cloud\nembeddings. We identified some biases in our dataset and refined it, which\nimproved the model's performance. Our model, even without the use of the point\ncloud encodings, outperforms numerical models by an order of magnitude\nregarding the runtime, while reaching a success rate of up to 90% of collision\nfree solutions on the test set.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04076v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04076v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.552,
      "distributed_training_score": 0.326,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for iterative refinement in generating robotic motion plans, specifically for creating keypoint-based joint sequences. While this involves multi-step processes similar to diffusion-based methods, it applies to physical path planning and environmental adaptation rather than complex logical reasoning or Chain-of-Thought tasks as defined. Thus, it has a loose connection but does not directly address abstract logical problem-solving.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04078",
      "title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging\n  Evaluation of Large Language Models",
      "authors": [
        "Jingjing Liu",
        "Zeming Liu",
        "Zihao Cheng",
        "Mengliang He",
        "Xiaoming Shi",
        "Yuhang Guo",
        "Xiangrong Zhu",
        "Yuanfang Guo",
        "Yunhong Wang",
        "Haifeng Wang"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have exhibited significant proficiency in code\ndebugging, especially in automatic program repair, which may substantially\nreduce the time consumption of developers and enhance their efficiency.\nSignificant advancements in debugging datasets have been made to promote the\ndevelopment of code debugging. However, these datasets primarily focus on\nassessing the LLM's function-level code repair capabilities, neglecting the\nmore complex and realistic repository-level scenarios, which leads to an\nincomplete understanding of the LLM's challenges in repository-level debugging.\nWhile several repository-level datasets have been proposed, they often suffer\nfrom limitations such as limited diversity of tasks, languages, and error\ntypes. To mitigate this challenge, this paper introduces RepoDebug, a\nmulti-task and multi-language repository-level code debugging dataset with 22\nsubtypes of errors that supports 8 commonly used programming languages and 3\ndebugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,\nwhere Claude 3.5 Sonnect, the best-performing model, still cannot perform well\nin repository-level debugging.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04078v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04078v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.39,
      "datasets_score": 0.41,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves programmatically introducing bugs into code files using tools like tree-sitter, which could be seen as a form of noisy label generation, but it does not focus on training models with weak supervision. Instead, it centers on dataset creation for evaluation, making the connection indirect and not central to the main contribution.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion-based models, iterative refinement processes, or any form of multi-step logical reasoning via diffusion. It focuses solely on code debugging datasets and LLM evaluation, with no connection to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution is the introduction and evaluation of a new dataset, RepoDebug, for assessing LLMs in code debugging. It details dataset curation, benchmarking experiments, and comparisons with existing datasets, aligning directly with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces RepoDebug, a comprehensive dataset designed to evaluate large language models (LLMs) on repository-level code debugging, addressing the limitations of existing function-level benchmarks by incorporating 8 programming languages, 3 debugging tasks (Bug Identification, Bug Localization, and Automatic Program Repair), and 22 subtypes of errors across 63 GitHub repositories. The methodology involves constructing bugs using abstract syntax trees, ensuring validity through automated and manual processes, and evaluating 10 LLMs with specific metrics, revealing that even advanced models like Claude 3.5 Sonnet struggle with repository-level tasks, particularly with multiple errors and longer code, while performance varies by language and error type.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset that advances the state-of-the-art by focusing on repository-level debugging with multi-task, multi-language, and diverse error types, addressing significant gaps in existing benchmarks. This represents a substantial innovation in evaluating LLMs for more realistic software engineering scenarios.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI for software engineering, as it provides a robust new benchmark for LLM evaluation, potentially improving future model development. However, its influence may be confined to specialized research rather than broad commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with a valuable new dataset and insightful evaluations that advance LLM debugging research, making it essential for researchers in AI and software engineering. While not groundbreaking for all audiences, it provides important insights that warrant attention in relevant fields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7bcd505061aa10ac1b38cf7479f0070022014d34",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 5,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jingjing Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379048729"
        },
        {
          "name": "Zeming Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2356032542"
        },
        {
          "name": "Zihao Cheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364700237"
        },
        {
          "name": "Mengliang He",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xiaoming Shi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2119203812"
        },
        {
          "name": "Yuhang Guo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2265608879"
        },
        {
          "name": "Xiangrong Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376416683"
        },
        {
          "name": "Yuanfang Guo",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2255826390"
        },
        {
          "name": "Yunhong Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363414454"
        },
        {
          "name": "Haifeng Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362387707"
        }
      ]
    },
    {
      "id": "2509.04083",
      "title": "Intermediate Languages Matter: Formal Languages and LLMs affect\n  Neurosymbolic Reasoning",
      "authors": [
        "Alexander Beiser",
        "David Penz",
        "Nysret Musliu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, the contributing factors to the success of Neurosymbolic LLM\nreasoning remain unclear. This paper demonstrates that one previously\noverlooked factor is the choice of the formal language. We introduce the\nintermediate language challenge: selecting a suitable formal language for\nneurosymbolic reasoning. By comparing four formal languages across three\ndatasets and seven LLMs, we show that the choice of formal language affects\nboth syntactic and semantic reasoning capabilities. We also discuss the varying\neffects across different LLMs.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04083v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04083v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.497,
      "distributed_training_score": 0.311,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper investigates the role of formal languages in neurosymbolic LLM reasoning, focusing on translation from natural language to formal languages and symbolic solving. It does not discuss diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for logical tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04086",
      "title": "TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale\n  Category-Aware Temporal Graph",
      "authors": [
        "Yaru Chen",
        "Faegheh Sardari",
        "Peiliang Zhang",
        "Ruohao Guo",
        "Yang Xiang",
        "Zhenbo Li",
        "Wenwu Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Audio-Visual Video Parsing (AVVP) task aims to identify event categories and\ntheir occurrence times in a given video with weakly supervised labels. Existing\nmethods typically fall into two categories: (i) designing enhanced\narchitectures based on attention mechanism for better temporal modeling, and\n(ii) generating richer pseudo-labels to compensate for the absence of\nframe-level annotations. However, the first type methods treat noisy\nsegment-level pseudo labels as reliable supervision and the second type methods\nlet indiscriminate attention spread them across all frames, the initial errors\nare repeatedly amplified during training. To address this issue, we propose a\nmethod that combines the Bi-Directional Text Fusion (BiT) module and\nCategory-Aware Temporal Graph (CATS) module. Specifically, we integrate the\nstrengths and complementarity of the two previous research directions. We first\nperform semantic injection and dynamic calibration on audio and visual modality\nfeatures through the BiT module, to locate and purify cleaner and richer\nsemantic cues. Then, we leverage the CATS module for semantic propagation and\nconnection to enable precise semantic information dissemination across time.\nExperimental results demonstrate that our proposed method achieves\nstate-of-the-art (SOTA) performance in multiple key indicators on two benchmark\ndatasets, LLP and UnAV-100.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04086v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04086v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.32,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04092",
      "title": "TriLiteNet: Lightweight Model for Multi-Task Visual Perception",
      "authors": [
        "Quang-Huy Che",
        "Duc-Khai Lam"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Efficient perception models are essential for Advanced Driver Assistance\nSystems (ADAS), as these applications require rapid processing and response to\nensure safety and effectiveness in real-world environments. To address the\nreal-time execution needs of such perception models, this study introduces the\nTriLiteNet model. This model can simultaneously manage multiple tasks related\nto panoramic driving perception. TriLiteNet is designed to optimize performance\nwhile maintaining low computational costs. Experimental results on the BDD100k\ndataset demonstrate that the model achieves competitive performance across\nthree key tasks: vehicle detection, drivable area segmentation, and lane line\nsegmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of\n85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for\ndrivable area segmentation, and an Acc of 82.3% for lane line segmentation with\nonly 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed\nmodel includes a tiny configuration with just 0.14M parameters, which provides\na multi-task solution with minimal computational demand. Evaluated for latency\nand power consumption on embedded devices, TriLiteNet in both configurations\nshows low latency and reasonable power during inference. By balancing\nperformance, computational efficiency, and scalability, TriLiteNet offers a\npractical and deployable solution for real-world autonomous driving\napplications. Code is available at https://github.com/chequanghuy/TriLiteNet.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04092v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04092v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.453,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the design of a lightweight multi-task model (TriLiteNet) for visual perception in autonomous driving, focusing on efficient architecture, inference performance, and deployment on embedded devices. It does not address distributed training, parallel computing, multi-node machine learning, or any strategies for partitioning data/computation across processors or nodes during training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04100",
      "title": "Hybrid Reinforcement Learning and Search for Flight Trajectory Planning",
      "authors": [
        "Alberto Luise",
        "Michele Lombardi",
        "Florent Teichteil Koenigsbuch"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper explores the combination of Reinforcement Learning (RL) and\nsearch-based path planners to speed up the optimization of flight paths for\nairliners, where in case of emergency a fast route re-calculation can be\ncrucial. The fundamental idea is to train an RL Agent to pre-compute\nnear-optimal paths based on location and atmospheric data and use those at\nruntime to constrain the underlying path planning solver and find a solution\nwithin a certain distance from the initial guess. The approach effectively\nreduces the size of the solver's search space, significantly speeding up route\noptimization. Although global optimality is not guaranteed, empirical results\nconducted with Airbus aircraft's performance models show that fuel consumption\nremains nearly identical to that of an unconstrained solver, with deviations\ntypically within 1%. At the same time, computation speed can be improved by up\nto 50% as compared to using a conventional solver alone.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04100v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04100v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.292,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.3,
      "datasets_score": 0.236,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a hybrid approach combining Reinforcement Learning (RL) with search-based methods for optimizing flight trajectories, using RL to pre-compute paths based on environmental data like location and atmospheric conditions. However, there is no mention of training a reward model with human-ranked data, fine-tuning based on human preferences, or any involvement of human feedback. RLHF specifically requires human feedback elements, which are absent here, making the paper unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04107",
      "title": "FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data\n  Heterogeneity",
      "authors": [
        "Ozgu Goksu",
        "Nicolas Pugeault"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Federated Learning (FL) provides decentralised model training, which\neffectively tackles problems such as distributed data and privacy preservation.\nHowever, the generalisation of global models frequently faces challenges from\ndata heterogeneity among clients. This challenge becomes even more pronounced\nwhen datasets are limited in size and class imbalance. To address data\nheterogeneity, we propose a novel method, \\textit{FedQuad}, that explicitly\noptimises smaller intra-class variance and larger inter-class variance across\nclients, thereby decreasing the negative impact of model aggregation on the\nglobal model over client representations. Our approach minimises the distance\nbetween similar pairs while maximising the distance between negative pairs,\neffectively disentangling client data in the shared feature space. We evaluate\nour method on the CIFAR-10 and CIFAR-100 datasets under various data\ndistributions and with many clients, demonstrating superior performance\ncompared to existing approaches. Furthermore, we provide a detailed analysis of\nmetric learning-based strategies within both supervised and federated learning\nparadigms, highlighting their efficacy in addressing representational learning\nchallenges in federated settings.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04107v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04107v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.286,
      "distributed_training_score": 0.461,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a novel federated learning method called FedQuad, which addresses data heterogeneity in distributed training scenarios. Federated Learning (FL) is a form of distributed training that partitions data across multiple clients for decentralized model training, aligning directly with topics like parallel computing and multi-node machine learning. The paper discusses algorithms for model aggregation and optimization across clients, which are strategies to handle distributed data and computation, making it highly relevant to accelerating and improving distributed training processes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces FedQuad, a novel federated learning method aimed at addressing data heterogeneity in distributed datasets by using a stochastic quadruplet loss function to minimize intra-class variance and maximize inter-class variance, thereby improving global model generalization. The methodology involves constructing quadruplets with an anchor, positive sample, negative sample, and harder negative sample, and it is evaluated on CIFAR-10 and CIFAR-100 datasets, where it outperforms existing approaches and provides insights into metric learning for federated settings.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of quadruplet loss with federated learning to handle data heterogeneity, offering a notable improvement over existing methods like contrastive learning adaptations. However, it builds on established metric learning techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in federated learning subfields by providing a practical solution to data heterogeneity, potentially leading to citations and further developments in applications like image classification. Nonetheless, its impact may be confined to specific areas of machine learning and computer vision rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a valuable contribution to federated learning by effectively addressing a key challenge, making it worth reading for researchers in the field to understand advancements in handling data heterogeneity. While not essential for all, it provides solid insights and empirical evidence that could inform ongoing work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/dd64cc0b78265251ed50dd94a256ab7482afd447",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 2,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ozgu Goksu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/51211700"
        },
        {
          "name": "Nicolas Pugeault",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2286681045"
        }
      ]
    },
    {
      "id": "2509.04117",
      "title": "DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset",
      "authors": [
        "Mustafa Sakhai",
        "Kaung Sithu",
        "Min Khant Soe Oke",
        "Maciej Wielgosz"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness\nchanges instead of full frames, offering low latency, high dynamic range, and\nmotion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a\nneuromorphic dataset designed for pedestrian detection and crossing-intention\nanalysis in normal and adverse weather conditions across two complementary\nsources: (1) synthetic event streams generated in the CARLA simulator for\ncontrolled \"approach-cross\" scenes under varied weather and lighting; and (2)\nreal-world JAAD dash-cam videos converted to event streams using the v2e tool,\npreserving natural behaviors and backgrounds. Each sequence includes paired RGB\nframes, per-frame DVS \"event frames\" (33 ms accumulations), and frame-level\nlabels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0\nevent files and AVI DVS video files and metadata for flexible re-processing.\nBaseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset\nusability and reveal a sim-to-real gap, motivating domain adaptation and\nmultimodal fusion. DVS-PedX aims to accelerate research in event-based\npedestrian safety, intention prediction, and neuromorphic perception.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04117v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04117v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.243,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.277,
      "datasets_score": 0.414,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new dataset, DVS-PedX, specifically designed for machine learning applications in event-based pedestrian detection. It details the dataset creation process, including synthetic generation using CARLA and real-world conversion from JAAD videos, provides benchmarking with baseline SNNs, and analyzes aspects like sim-to-real gaps. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI and ML.",
      "llm_score_status": "completed",
      "summary": "DVS-PedX is a neuromorphic dataset designed for event-based pedestrian detection and crossing-intention analysis, utilizing Dynamic Vision Sensors (DVS) to capture brightness changes in both synthetic environments via the CARLA simulator and real-world scenarios from JAAD videos converted using the v2e tool. The dataset includes paired RGB frames, event frames, labels for crossing behaviors, and raw event files, with baseline experiments using spiking neural networks (SNNs) from SpikingJelly to demonstrate its applicability, reveal a sim-to-real gap, and promote advancements in event-based pedestrian safety and perception research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a new dataset that combines synthetic and real event-based data for pedestrian analysis, effectively integrating existing simulation and conversion tools to address gaps in event-based vision. While it doesn't introduce a entirely new problem or technique, it offers a clever combination that advances applications in neuromorphic perception.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of event-based computer vision and autonomous driving, as it provides a specialized dataset for pedestrian safety research. However, its influence may be limited to niche applications rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution by introducing a valuable dataset for event-based pedestrian detection, making it essential for researchers in neuromorphic computing and computer vision to be aware of for advancing safety-related applications. While not groundbreaking for all audiences, it offers practical utility and insights that warrant review.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2351a015fd48b8283e259ccc7aff2965392b4d88",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 2,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Mustafa Sakhai",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2127069490"
        },
        {
          "name": "Kaung Sithu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370424201"
        },
        {
          "name": "Min Khant Soe Oke",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370448073"
        },
        {
          "name": "Maciej Wielgosz",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2304470859"
        }
      ]
    },
    {
      "id": "2509.04118",
      "title": "EHVC: Efficient Hierarchical Reference and Quality Structure for Neural\n  Video Coding",
      "authors": [
        "Junqi Liao",
        "Yaojun Wu",
        "Chaoyi Lin",
        "Zhipin Deng",
        "Li Li",
        "Dong Liu",
        "Xiaoyan Sun"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Neural video codecs (NVCs), leveraging the power of end-to-end learning, have\ndemonstrated remarkable coding efficiency improvements over traditional video\ncodecs. Recent research has begun to pay attention to the quality structures in\nNVCs, optimizing them by introducing explicit hierarchical designs. However,\nless attention has been paid to the reference structure design, which\nfundamentally should be aligned with the hierarchical quality structure. In\naddition, there is still significant room for further optimization of the\nhierarchical quality structure. To address these challenges in NVCs, we propose\nEHVC, an efficient hierarchical neural video codec featuring three key\ninnovations: (1) a hierarchical multi-reference scheme that draws on\ntraditional video codec design to align reference and quality structures,\nthereby addressing the reference-quality mismatch; (2) a lookahead strategy to\nutilize an encoder-side context from future frames to enhance the quality\nstructure; (3) a layer-wise quality scale with random quality training strategy\nto stabilize quality structures during inference. With these improvements, EHVC\nachieves significantly superior performance to the state-of-the-art NVCs. Code\nwill be released in: https://github.com/bytedance/NEVC.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04118v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04118v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.276,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.34,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04123",
      "title": "TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering",
      "authors": [
        "Ayan Banerjee",
        "Josep Lladós",
        "Umapada Pal",
        "Anjan Dutta"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-story visualization is challenging due to the need for consistent\ninteraction among multiple characters across frames. Existing methods struggle\nwith character consistency, leading to artifact generation and inaccurate\ndialogue rendering, which results in disjointed storytelling. In response, we\nintroduce TaleDiffusion, a novel framework for generating multi-character\nstories with an iterative process, maintaining character consistency, and\naccurate dialogue assignment via postprocessing. Given a story, we use a\npre-trained LLM to generate per-frame descriptions, character details, and\ndialogues via in-context learning, followed by a bounded attention-based\nper-box mask technique to control character interactions and minimize\nartifacts. We then apply an identity-consistent self-attention mechanism to\nensure character consistency across frames and region-aware cross-attention for\nprecise object placement. Dialogues are also rendered as bubbles and assigned\nto characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion\noutperforms existing methods in consistency, noise reduction, and dialogue\nrendering.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04123v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04123v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.563,
      "distributed_training_score": 0.317,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models for iterative image generation in story visualization, involving refinement processes to maintain character consistency and reduce artifacts. However, it focuses on visual and generative tasks rather than adapting diffusion for multi-step logical reasoning or treating a chain-of-thought as a holistic entity. There is no clear component for solving complex logical tasks, making the connection indirect.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04125",
      "title": "Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker",
      "authors": [
        "Tarik Zaciragic",
        "Aske Plaat",
        "K. Joost Batenburg"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In the game of poker, being unpredictable, or bluffing, is an essential\nskill. When humans play poker, they bluff. However, most works on\ncomputer-poker focus on performance metrics such as win rates, while bluffing\nis overlooked. In this paper we study whether two popular algorithms, DQN\n(based on reinforcement learning) and CFR (based on game theory), exhibit\nbluffing behavior in Leduc Hold'em, a simplified version of poker. We designed\nan experiment where we let the DQN and CFR agent play against each other while\nwe log their actions. We find that both DQN and CFR exhibit bluffing behavior,\nbut they do so in different ways. Although both attempt to perform bluffs at\ndifferent rates, the percentage of successful bluffs (where the opponent folds)\nis roughly the same. This suggests that bluffing is an essential aspect of the\ngame, not of the algorithm. Future work should look at different bluffing\nstyles and at the full game of poker. Code at\nhttps://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04125v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04125v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.28,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04126",
      "title": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image\n  Generation",
      "authors": [
        "Yuan Zhao",
        "Lin Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Text-to-image diffusion models have achieved remarkable image quality, but\nthey still struggle with complex, multiele ment prompts, and limited stylistic\ndiversity. To address these limitations, we propose a Multi-Expert Planning and\nGen eration Framework (MEPG) that synergistically integrates position- and\nstyle-aware large language models (LLMs) with spatial-semantic expert modules.\nThe framework comprises two core components: (1) a Position-Style-Aware (PSA)\nmodule that utilizes a supervised fine-tuned LLM to decom pose input prompts\ninto precise spatial coordinates and style encoded semantic instructions; and\n(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera\ntion through dynamic expert routing across both local regions and global areas.\nDuring the generation process for each lo cal region, specialized models (e.g.,\nrealism experts, styliza tion specialists) are selectively activated for each\nspatial par tition via attention-based gating mechanisms. The architec ture\nsupports lightweight integration and replacement of ex pert models, providing\nstrong extensibility. Additionally, an interactive interface enables real-time\nspatial layout editing and per-region style selection from a portfolio of\nexperts. Ex periments show that MEPG significantly outperforms base line models\nwith the same backbone in both image quality\n  and style diversity.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04126v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04126v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.587,
      "distributed_training_score": 0.367,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing text-to-image diffusion models for generation tasks by integrating LLMs and multi-expert modules for spatial and style control. While it uses the iterative refinement process of diffusion models for image synthesis, it does not adapt this process for multi-step logical reasoning or treating a Chain-of-Thought as an entity. Instead, the diffusion is applied to generative tasks, making the connection indirect and not central to the paper's contributions.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04129",
      "title": "Simplicity Lies in the Eye of the Beholder: A Strategic Perspective on\n  Controllers in Reactive Synthesis",
      "authors": [
        "Mickael Randour"
      ],
      "categories": [
        "cs.LO (Logic in Computer Science)",
        "cs.AI (Artificial Intelligence)",
        "cs.FL (Formal Languages and Automata Theory)",
        "math.PR (Probability)"
      ],
      "abstract": "In the game-theoretic approach to controller synthesis, we model the\ninteraction between a system to be controlled and its environment as a game\nbetween these entities, and we seek an appropriate (e.g., winning or optimal)\nstrategy for the system. This strategy then serves as a formal blueprint for a\nreal-world controller. A common belief is that simple (e.g., using limited\nmemory) strategies are better: corresponding controllers are easier to conceive\nand understand, and cheaper to produce and maintain.\n  This invited contribution focuses on the complexity of strategies in a\nvariety of synthesis contexts. We discuss recent results concerning memory and\nrandomness, and take a brief look at what lies beyond our traditional notions\nof complexity for strategies.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04129v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04129v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.259,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.262,
      "datasets_score": 0.206,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04130",
      "title": "The human biological advantage over AI",
      "authors": [
        "William Stewart"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Recent advances in AI raise the possibility that AI systems will one day be\nable to do anything humans can do, only better. If artificial general\nintelligence (AGI) is achieved, AI systems may be able to understand, reason,\nproblem solve, create, and evolve at a level and speed that humans will\nincreasingly be unable to match, or even understand. These possibilities raise\na natural question as to whether AI will eventually become superior to humans,\na successor \"digital species\", with a rightful claim to assume leadership of\nthe universe. However, a deeper consideration suggests the overlooked\ndifferentiator between human beings and AI is not the brain, but the central\nnervous system (CNS), providing us with an immersive integration with physical\nreality. It is our CNS that enables us to experience emotion including pain,\njoy, suffering, and love, and therefore to fully appreciate the consequences of\nour actions on the world around us. And that emotional understanding of the\nconsequences of our actions is what is required to be able to develop\nsustainable ethical systems, and so be fully qualified to be the leaders of the\nuniverse. A CNS cannot be manufactured or simulated; it must be grown as a\nbiological construct. And so, even the development of consciousness will not be\nsufficient to make AI systems superior to humans. AI systems may become more\ncapable than humans on almost every measure and transform our society. However,\nthe best foundation for leadership of our universe will always be DNA, not\nsilicon.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04130v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04130v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.235,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.233,
      "datasets_score": 0.246,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04139",
      "title": "Enhancing Technical Documents Retrieval for RAG",
      "authors": [
        "Songjiang Lai",
        "Tsun-Hin Cheung",
        "Ka-Chun Fung",
        "Kaiwen Xue",
        "Kwan-Ho Lin",
        "Yan-Ming Choi",
        "Vincent Ng",
        "Kin-Man Lam"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we introduce Technical-Embeddings, a novel framework designed\nto optimize semantic retrieval in technical documentation, with applications in\nboth hardware and software development. Our approach addresses the challenges\nof understanding and retrieving complex technical content by leveraging the\ncapabilities of Large Language Models (LLMs). First, we enhance user queries by\ngenerating expanded representations that better capture user intent and improve\ndataset diversity, thereby enriching the fine-tuning process for embedding\nmodels. Second, we apply summary extraction techniques to encode essential\ncontextual information, refining the representation of technical documents. To\nfurther enhance retrieval performance, we fine-tune a bi-encoder BERT model\nusing soft prompting, incorporating separate learning parameters for queries\nand document context to capture fine-grained semantic nuances. We evaluate our\napproach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that\nTechnical-Embeddings significantly outperforms baseline models in both\nprecision and recall. Our findings highlight the effectiveness of integrating\nquery expansion and contextual summarization to enhance information access and\ncomprehension in technical domains. This work advances the state of\nRetrieval-Augmented Generation (RAG) systems, offering new avenues for\nefficient and accurate technical document retrieval in engineering and product\ndevelopment workflows.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04139v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04139v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.334,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on improving semantic retrieval for technical documents using LLMs, query expansion, summarization, and fine-tuning BERT models for RAG systems. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04145",
      "title": "Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network\n  Weight Space Diffusion",
      "authors": [
        "Dongliang Cao",
        "Guoxing Sun",
        "Marc Habermann",
        "Florian Bernard"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Creating human avatars is a highly desirable yet challenging task. Recent\nadvancements in radiance field rendering have achieved unprecedented\nphotorealism and real-time performance for personalized dynamic human avatars.\nHowever, these approaches are typically limited to person-specific rendering\nmodels trained on multi-view video data for a single individual, limiting their\nability to generalize across different identities. On the other hand,\ngenerative approaches leveraging prior knowledge from pre-trained 2D diffusion\nmodels can produce cartoonish, static human avatars, which are animated through\nsimple skeleton-based articulation. Therefore, the avatars generated by these\nmethods suffer from lower rendering quality compared to person-specific\nrendering methods and fail to capture pose-dependent deformations such as cloth\nwrinkles. In this paper, we propose a novel approach that unites the strengths\nof person-specific rendering and diffusion-based generative modeling to enable\ndynamic human avatar generation with both high photorealism and realistic\npose-dependent deformations. Our method follows a two-stage pipeline: first, we\noptimize a set of person-specific UNets, with each network representing a\ndynamic human avatar that captures intricate pose-dependent deformations. In\nthe second stage, we train a hyper diffusion model over the optimized network\nweights. During inference, our method generates network weights for real-time,\ncontrollable rendering of dynamic human avatars. Using a large-scale,\ncross-identity, multi-view video dataset, we demonstrate that our approach\noutperforms state-of-the-art human avatar generation methods.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04145v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04145v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.526,
      "distributed_training_score": 0.369,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for generating network weights to create dynamic human avatars, which is a generative task in computer graphics for visual rendering. It does not involve adapting diffusion for multi-step logical reasoning, such as processing a 'Chain-of-Thought' for complex logical tasks, as the method is centered on avatar generation and pose-dependent deformations without any logical reasoning components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04150",
      "title": "Revisiting Simple Baselines for In-The-Wild Deepfake Detection",
      "authors": [
        "Orlando Castaneda",
        "Kevin So-Tang",
        "Kshitij Gurung"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The widespread adoption of synthetic media demands accessible deepfake\ndetectors and realistic benchmarks. While most existing research evaluates\ndeepfake detectors on highly controlled datasets, we focus on the recently\nreleased \"in-the-wild\" benchmark, Deepfake-Eval-2024. Initial reporting on\nDeepfake-Eval-2024 showed that three finetuned open-source models achieve\naccuracies between 61% and 69%, significantly lagging behind the leading\ncommercial deepfake detector with 82% accuracy. Our work revisits one of these\nbaseline approaches, originally introduced by Ojha et al., which adapts\nstandard pretrained vision backbones to produce generalizable deepfake\ndetectors. We demonstrate that with better-tuned hyperparameters, this simple\napproach actually yields much higher performance -- 81% accuracy on\nDeepfake-Eval-2024 -- surpassing the previously reported accuracy of this\nbaseline approach by 18% and competing with commercial deepfake detectors. We\ndiscuss tradeoffs in accuracy, computational costs, and interpretability,\nfocusing on how practical these deepfake detectors might be when deployed in\nreal-world settings. Our code can be found at\nhttps://github.com/Deepfake-Detection-KKO/deepfake-detection.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04150v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04150v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.33,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04152",
      "title": "TAGAL: Tabular Data Generation using Agentic LLM Methods",
      "authors": [
        "Benoît Ronval",
        "Pierre Dupont",
        "Siegfried Nijssen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The generation of data is a common approach to improve the performance of\nmachine learning tasks, among which is the training of models for\nclassification. In this paper, we present TAGAL, a collection of methods able\nto generate synthetic tabular data using an agentic workflow. The methods\nleverage Large Language Models (LLMs) for an automatic and iterative process\nthat uses feedback to improve the generated data without any further LLM\ntraining. The use of LLMs also allows for the addition of external knowledge in\nthe generation process. We evaluate TAGAL across diverse datasets and different\naspects of quality for the generated data. We look at the utility of downstream\nML models, both by training classifiers on synthetic data only and by combining\nreal and synthetic data. Moreover, we compare the similarities between the real\nand the generated data. We show that TAGAL is able to perform on par with\nstate-of-the-art approaches that require LLM training and generally outperforms\nother training-free approaches. These findings highlight the potential of\nagentic workflow and open new directions for LLM-based data generation methods.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04152v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04152v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.457,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.378,
      "datasets_score": 0.413,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on agentic LLM workflows for synthetic tabular data generation using automated feedback from another LLM, without involving human-ranked data, a reward model, or reinforcement learning for fine-tuning. There is no element of human feedback or alignment with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "The paper's main contribution is generating synthetic tabular data for training ML models, not programmatically generating labels from noisy or imprecise sources. It does not address training models with weak supervision techniques, focusing instead on data augmentation via LLMs.",
      "diffusion_reasoning_justification": "The paper uses iterative agentic LLM workflows for data generation, but it does not involve diffusion models, multi-step logical reasoning, or treating a chain-of-thought as a single entity for holistic correction. There is no adaptation of diffusion processes for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper evaluates TAGAL on diverse datasets, assesses the utility and quality of generated synthetic data for downstream ML tasks, and analyzes similarities to real data, which aligns with creating and evaluating datasets. However, the primary focus is on the generation method rather than comprehensive dataset curation, benchmarking, or analysis.",
      "llm_score_status": "completed",
      "summary": "The paper introduces TAGAL, a suite of training-free methods that leverage agentic workflows with Large Language Models (LLMs) to generate synthetic tabular data, addressing issues like data scarcity, imbalance, and privacy in domains such as healthcare and finance. By employing iterative feedback loops for data refinement and incorporating external knowledge, TAGAL enhances data quality and utility, demonstrating performance on par with state-of-the-art trained models in downstream machine learning tasks and data similarity metrics, while outperforming other training-free approaches.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining agentic workflows with LLMs for training-free tabular data generation, offering a clever integration of existing ideas to enhance data quality through iterative feedback. However, it builds on established LLM techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in subfields like machine learning for tabular data and privacy-preserving techniques, as it provides effective training-free methods that could be adopted for practical applications. Nonetheless, its broader impact may be limited to specific domains where data generation is challenging.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and practical contribution to LLM-based data generation, making it essential for researchers in AI and machine learning focused on synthetic data techniques. While not revolutionary, its methods and findings provide actionable insights for improving data utility in real-world scenarios.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/35cb3fee92a822b31295c9acd40dc5057d7bdcff",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Benoît Ronval",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2347733362"
        },
        {
          "name": "Pierre Dupont",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2307252688"
        },
        {
          "name": "Siegfried Nijssen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2347733220"
        }
      ]
    },
    {
      "id": "2509.04154",
      "title": "Attention as an Adaptive Filter",
      "authors": [
        "Peter Racioppo"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce Adaptive Filter Attention (AFA), a novel attention mechanism\nthat incorporates a learnable dynamics model directly into the computation of\nattention weights. Rather than comparing queries and keys directly, we model\nthe input sequence as discrete observations of a linear stochastic differential\nequation (SDE). By imposing a linear dynamics model with simultaneously\ndiagonalizable state matrices and noise covariances, we can make use of a\nclosed-form solution to the differential Lyapunov equation to efficiently\npropagate pairwise uncertainties through the dynamics. Attention naturally\narises as the maximum likelihood solution for this linear SDE, with attention\nweights corresponding to robust residual-based reweightings of the propagated\npairwise precisions. Imposing an additional constraint on the state matrix's\neigenvalues leads to a simplified variant with the same computational and\nmemory complexity as standard attention. In the limit of vanishing dynamics and\nprocess noise, and using a small-angle approximation, we recover ordinary\ndot-product attention.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04154v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04154v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.334,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Adaptive Filter Attention, which uses linear stochastic differential equations to enhance attention mechanisms in sequence modeling. It focuses on propagating uncertainties and bridging attention with state space models, but does not involve diffusion models, iterative refinement for logical tasks, or treating a 'Chain-of-Thought' as an entity for holistic correction. There is no component for multi-step logical reasoning using diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04156",
      "title": "YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind\n  Turbine Components",
      "authors": [
        "Serhii Svystun",
        "Pavlo Radiuk",
        "Oleksandr Melnychenko",
        "Oleg Savenko",
        "Anatoliy Sachenko"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up\nnew opportunities for monitoring wind power plants, including blades, towers,\nand other critical components. However, reliable defect detection requires\nhigh-resolution data and efficient methods to process multispectral imagery. In\nthis research, we aim to enhance defect detection accuracy through the\ndevelopment of an ensemble of YOLO-based deep learning models that integrate\nboth visible and thermal channels. We propose an ensemble approach that\nintegrates a general-purpose YOLOv8 model with a specialized thermal model,\nusing a sophisticated bounding box fusion algorithm to combine their\npredictions. Our experiments show this approach achieves a mean Average\nPrecision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone\nYOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that\ncombining multiple YOLO architectures with fused multispectral data provides a\nmore reliable solution, improving the detection of both visual and thermal\ndefects.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04156v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04156v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.346,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04159",
      "title": "Towards an Action-Centric Ontology for Cooking Procedures Using Temporal\n  Graphs",
      "authors": [
        "Aarush Kumbhakern",
        "Saransh Kumar Gupta",
        "Lipika Dey",
        "Partha Pratim Das"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Formalizing cooking procedures remains a challenging task due to their\ninherent complexity and ambiguity. We introduce an extensible domain-specific\nlanguage for representing recipes as directed action graphs, capturing\nprocesses, transfers, environments, concurrency, and compositional structure.\nOur approach enables precise, modular modeling of complex culinary workflows.\nInitial manual evaluation on a full English breakfast recipe demonstrates the\nDSL's expressiveness and suitability for future automated recipe analysis and\nexecution. This work represents initial steps towards an action-centric\nontology for cooking, using temporal graphs to enable structured machine\nunderstanding, precise interpretation, and scalable automation of culinary\nprocesses - both in home kitchens and professional culinary settings.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04159v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04159v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.288,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.252,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04166",
      "title": "Crossing the Species Divide: Transfer Learning from Speech to Animal\n  Sounds",
      "authors": [
        "Jules Cauzinille",
        "Marius Miron",
        "Olivier Pietquin",
        "Masato Hagiwara",
        "Ricard Marxer",
        "Arnaud Rey",
        "Benoit Favre"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.SD (Sound)"
      ],
      "abstract": "Self-supervised speech models have demonstrated impressive performance in\nspeech processing, but their effectiveness on non-speech data remains\nunderexplored. We study the transfer learning capabilities of such models on\nbioacoustic detection and classification tasks. We show that models such as\nHuBERT, WavLM, and XEUS can generate rich latent representations of animal\nsounds across taxa. We analyze the models properties with linear probing on\ntime-averaged representations. We then extend the approach to account for the\neffect of time-wise information with other downstream architectures. Finally,\nwe study the implication of frequency range and noise on performance. Notably,\nour results are competitive with fine-tuned bioacoustic pre-trained models and\nshow the impact of noise-robust pre-training setups. These findings highlight\nthe potential of speech-based self-supervised learning as an efficient\nframework for advancing bioacoustic research.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04166v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04166v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.413,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.351,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper utilizes self-supervised learning (SSL), which involves training on unlabeled data through pretext tasks, aligning with weak supervision by generating supervisory signals without hand-labeled data. However, the main contribution focuses on transfer learning from speech models to bioacoustic tasks, rather than innovating or deeply exploring weak supervision techniques themselves. This makes it relevant but not central to the topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper explores the transferability of self-supervised speech models, such as HuBERT, WavLM, and XEUS, to bioacoustic tasks involving animal sounds from various taxa, aiming to assess their effectiveness in detection and classification. The authors benchmark these models on 10 diverse tasks using methods like linear probing and recurrent neural networks, analyze the impact of noise and frequency ranges, and demonstrate that speech-based representations achieve competitive performance with specialized models, emphasizing the benefits of noise-robust and multilingual pre-training for advancing bioacoustic research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying existing self-supervised speech models to bioacoustic tasks in a systematic way, combining techniques to enhance cross-domain transferability without introducing entirely new architectures or problems. This clever adaptation advances the state-of-the-art in applying speech processing to under-resourced areas like animal vocalizations.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like bioacoustics and AI, as it provides an efficient framework for using speech models on animal sounds, potentially influencing research on noise-robust applications. However, its broader influence on commercial or unrelated fields may be limited.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to machine learning and bioacoustics by demonstrating effective transfer learning techniques, making it essential for researchers interested in cross-domain applications. While not groundbreaking enough to be a must-read, its insights are significant and worth engaging with.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1062880136a695b6acc170e0dc7e1289dd34ab95",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 4,
      "average_h_index": 2.2857142857142856,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jules Cauzinille",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2315469800"
        },
        {
          "name": "Marius Miron",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2282541032"
        },
        {
          "name": "Olivier Pietquin",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2285916934"
        },
        {
          "name": "Masato Hagiwara",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2282540921"
        },
        {
          "name": "Ricard Marxer",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2315467914"
        },
        {
          "name": "Arnaud Rey",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2315467667"
        },
        {
          "name": "Benoît Favre",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2315477908"
        }
      ]
    },
    {
      "id": "2509.04180",
      "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
      "authors": [
        "Safouane El Ghazouali",
        "Umberto Michelucci"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04180v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04180v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.435,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.366,
      "datasets_score": 0.423,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces an AI-assisted annotation tool that generates initial labels programmatically using models like CLIP and Grounding DINO, which could produce noisy or imprecise annotations for human refinement. This aligns peripherally with weak supervision's use of programmatically generated labels, but the paper focuses on annotation tools rather than training models with such labels, making it only tangentially relevant.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper describes a tool for efficient dataset annotation, including benchmarking on diverse datasets and reducing manual effort while maintaining quality, which relates to dataset curation methodologies. However, it does not introduce a new dataset or conduct in-depth analysis, so it is moderately relevant to dataset creation and evaluation rather than a primary focus.",
      "llm_score_status": "completed",
      "summary": "VisioFirm is an open-source, cross-platform web application aimed at streamlining image annotation for computer vision tasks by integrating AI models such as CLIP, Ultralytics, Grounding DINO, and Segment Anything to automate initial labeling. It employs a hybrid filtering pipeline for high-recall pre-annotations, supports interactive refinement tools for bounding boxes, oriented bounding boxes, polygons, and on-the-fly segmentation, and achieves up to 90% reduction in manual effort while maintaining accuracy, with features for offline operation and multiple export formats.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing AI models like CLIP and Grounding DINO into a unified annotation tool, offering a clever integration that enhances efficiency for known problems in data labeling. However, it does not introduce a entirely new problem or technique, making it an incremental advancement rather than a groundbreaking one.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the computer vision subfield due to its open-source nature and practical utility in reducing annotation efforts, potentially influencing tool development for researchers and practitioners. Nonetheless, its impact may be limited to specific applications in AI-assisted labeling rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution for those in computer vision and AI, as it provides an accessible tool that significantly improves annotation efficiency, making it essential for practitioners dealing with large datasets. While not exceptional, its practical innovations warrant attention from relevant audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/007cf380a3ade069b0d413f1a976ff590994b472",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 3,
      "average_h_index": 3.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Safouane El Ghazouali",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2097553862"
        },
        {
          "name": "Umberto Michelucci",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2262221551"
        }
      ]
    },
    {
      "id": "2509.04183",
      "title": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn\n  Mental Health Counseling Sessions",
      "authors": [
        "Aishik Mandal",
        "Tanmoy Chakraborty",
        "Iryna Gurevych"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04183v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04183v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.372,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a multi-agent framework for generating synthetic counseling sessions and fine-tunes models on this data using expert evaluations for assessment, but it does not involve training a reward model or using reinforcement learning based on human-ranked data. The fine-tuning appears to be standard supervised learning, not RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a multi-agent LLM framework for generating counseling dialogues through task decomposition and coordination, but it does not incorporate diffusion models, iterative refinement for logical tasks, or any multi-step reasoning process akin to diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04192",
      "title": "Domain size asymptotics for Markov logic networks",
      "authors": [
        "Vera Koponen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)",
        "math.LO (Logic)"
      ],
      "abstract": "A Markov logic network (MLN) determines a probability distribution on the set\nof structures, or ``possible worlds'', with an arbitrary finite domain. We\nstudy the properties of such distributions as the domain size tends to\ninfinity. Three types of concrete examples of MLNs will be considered, and the\nproperties of random structures with domain sizes tending to infinity will be\nstudied: (1) Arbitrary quantifier-free MLNs over a language with only one\nrelation symbol which has arity 1. In this case we give a pretty complete\ncharacterization of the possible limit behaviours of random structures. (2) An\nMLN that favours graphs with fewer triangles (or more generally, fewer\nk-cliques). As a corollary of the analysis a ``$\\delta$-approximate 0-1 law''\nfor first-order logic is obtained. (3) An MLN that favours graphs with fewer\nvertices with degree higher than a fixed (but arbitrary) number. The analysis\nshows that depending on which ``soft constraints'' an MLN uses the limit\nbehaviour of random structures can be quite different, and the weights of the\nsoft constraints may, or may not, have influence on the limit behaviour. It\nwill also be demonstrated, using (1), that quantifier-free MLNs and lifted\nBayesian networks (in a broad sense) are asymptotically incomparable, roughly\nmeaning that there is a sequence of distributions on possible worlds with\nincreasing domain sizes that can be defined by one of the formalisms but not\neven approximated by the other. In a rather general context it is also shown\nthat on large domains the distribution determined by an MLN concentrates almost\nall its probability mass on a totally different part of the space of possible\nworlds than the uniform distribution does.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04192v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04192v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.304,
      "datasets_score": 0.251,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04193",
      "title": "DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval",
      "authors": [
        "Ruohong Yang",
        "Peng Hu",
        "Yunfan Li",
        "Xi Peng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of\nthe same category across diverse domains without relying on annotations.\nExisting UCIR methods, which align cross-domain features for the entire image,\noften struggle with the domain gap, as the object features critical for\nretrieval are frequently entangled with domain-specific styles. To address this\nchallenge, we propose DUDE, a novel UCIR method building upon feature\ndisentanglement. In brief, DUDE leverages a text-to-image generative model to\ndisentangle object features from domain-specific styles, thus facilitating\nsemantical image retrieval. To further achieve reliable alignment of the\ndisentangled object features, DUDE aligns mutual neighbors from within domains\nto across domains in a progressive manner. Extensive experiments demonstrate\nthat DUDE achieves state-of-the-art performance across three benchmark datasets\nover 13 domains. The code will be released.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04193v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04193v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.333,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes DUDE, which uses a diffusion model for feature disentanglement in unsupervised cross-domain image retrieval, focusing on extracting domain-invariant object features from images. While diffusion models involve iterative refinement, the paper applies this to visual tasks like image generation and retrieval, not to solving complex logical tasks or multi-step reasoning processes such as a 'Chain-of-Thought'. There is no component for holistic correction of reasoning paths, making it unrelated to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04239",
      "title": "Evaluating Quality of Gaming Narratives Co-created with AI",
      "authors": [
        "Arturo Valdivia",
        "Paolo Burelli"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper proposes a structured methodology to evaluate AI-generated game\nnarratives, leveraging the Delphi study structure with a panel of narrative\ndesign experts. Our approach synthesizes story quality dimensions from\nliterature and expert insights, mapping them into the Kano model framework to\nunderstand their impact on player satisfaction. The results can inform game\ndevelopers on prioritizing quality aspects when co-creating game narratives\nwith generative AI.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04239v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04239v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.265,
      "datasets_score": 0.399,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating AI-generated game narratives using expert panels and the Kano model, which involves human feedback for quality assessment. However, it does not describe using human feedback to train or fine-tune AI models via reinforcement learning, as required for RLHF. Thus, while human input is present, it is not in the context of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a methodology for evaluating game narratives using expert insights and the Kano model, but it does not involve diffusion models, iterative refinement for reasoning, or multi-step logical reasoning processes. There is no mention of adapting diffusion techniques for any tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04243",
      "title": "Learning Active Perception via Self-Evolving Preference Optimization for\n  GUI Grounding",
      "authors": [
        "Wanfu Wang",
        "Qipeng Huang",
        "Guangquan Xue",
        "Xiaobo Liang",
        "Juntao Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vision Language Models (VLMs) have recently achieved significant progress in\nbridging visual perception and linguistic reasoning. Recently, OpenAI o3 model\nintroduced a zoom-in search strategy that effectively elicits active perception\ncapabilities in VLMs, improving downstream task performance. However, enabling\nVLMs to reason effectively over appropriate image regions remains a core\nchallenge in GUI grounding, particularly under high-resolution inputs and\ncomplex multi-element visual interactions. In this work, we propose LASER, a\nself-evolving framework that progressively endows VLMs with multi-step\nperception capabilities, enabling precise coordinate prediction. Specifically,\nour approach integrate Monte Carlo quality estimation with\nIntersection-over-Union (IoU)-based region quality evaluation to jointly\nencourage both accuracy and diversity in constructing high-quality preference\ndata. This combination explicitly guides the model to focus on\ninstruction-relevant key regions while adaptively allocating reasoning steps\nbased on task complexity. Comprehensive experiments on the ScreenSpot Pro and\nScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating\nthe effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER\nachieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new\nstate-of-the-art (SoTA) among 7B-scale models.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04243v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04243v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.473,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.361,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's LASER framework uses automated methods like Monte Carlo and IoU-based estimation for preference optimization, without any mention of human feedback, a reward model trained on human-ranked data, or reinforcement learning for fine-tuning. It relies on self-evolving techniques and rejection sampling-based SFT, which do not align with RLHF.",
      "weak_supervision_justification": "The paper employs programmatic generation of preference data using Monte Carlo and IoU-based methods, which are noisy and automated sources, to train the model without extensive human-labeled data. This directly matches weak supervision by deriving labels from high-level, imprecise techniques rather than perfect hand-labeling.",
      "diffusion_reasoning_justification": "The paper involves iterative refinement and multi-step reasoning in LASER for perception tasks, which shares some conceptual similarities with iterative processes in diffusion models. However, it does not explicitly use diffusion models, adapt them for Chain-of-Thought reasoning, or treat reasoning as a holistically corrected entity, making it only loosely related.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces LASER, a self-evolving framework designed to enhance Vision Language Models (VLMs) for GUI grounding by enabling active perception through multi-step reasoning, addressing challenges in high-resolution inputs and complex interactions. The methodology integrates Monte Carlo quality estimation and Intersection-over-Union (IoU)-based evaluation to create high-quality preference data, allowing models to focus on instruction-relevant regions and adapt reasoning steps based on task complexity, with experiments on ScreenSpot Pro and ScreenSpot-v2 benchmarks demonstrating significant performance gains and a new state-of-the-art score of 55.7 using the GTA1-7B model.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework, LASER, that combines Monte Carlo estimation and IoU-based techniques for self-evolving preference optimization, significantly advancing active perception in VLMs for GUI grounding tasks.",
      "impact_score": "High",
      "impact_justification": "The work's innovative approach to active perception could influence future developments in VLMs for GUI interactions and visual reasoning, as evidenced by its state-of-the-art results and potential applicability to broader computer vision tasks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution with practical advancements in VLM capabilities, making it valuable for researchers in computer vision and AI, though it is specialized to GUI grounding.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cc956577ee62212926d26d9b59f218a34777b698",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 8,
      "average_h_index": 1.6,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Wanfu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373547244"
        },
        {
          "name": "Qipeng Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379185664"
        },
        {
          "name": "Guangquan Xue",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378955690"
        },
        {
          "name": "Xiaobo Liang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/48083523"
        },
        {
          "name": "Juntao Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379003004"
        }
      ]
    },
    {
      "id": "2509.04250",
      "title": "How many patients could we save with LLM priors?",
      "authors": [
        "Shota Arai",
        "David Selby",
        "Andrew Vargo",
        "Sebastian Vollmer"
      ],
      "categories": [
        "stat.ME (Methodology)",
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)",
        "cs.IR (Information Retrieval)",
        "stat.AP (Applications)"
      ],
      "abstract": "Imagine a world where clinical trials need far fewer patients to achieve the\nsame statistical power, thanks to the knowledge encoded in large language\nmodels (LLMs). We present a novel framework for hierarchical Bayesian modeling\nof adverse events in multi-center clinical trials, leveraging LLM-informed\nprior distributions. Unlike data augmentation approaches that generate\nsynthetic data points, our methodology directly obtains parametric priors from\nthe model. Our approach systematically elicits informative priors for\nhyperparameters in hierarchical Bayesian models using a pre-trained LLM,\nenabling the incorporation of external clinical expertise directly into\nBayesian safety modeling. Through comprehensive temperature sensitivity\nanalysis and rigorous cross-validation on real-world clinical trial data, we\ndemonstrate that LLM-derived priors consistently improve predictive performance\ncompared to traditional meta-analytical approaches. This methodology paves the\nway for more efficient and expert-informed clinical trial design, enabling\nsubstantial reductions in the number of patients required to achieve robust\nsafety assessment and with the potential to transform drug safety monitoring\nand regulatory decision making.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04250v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04250v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.351,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves using pre-trained large language models (LLMs) to elicit priors for hierarchical Bayesian modeling in clinical trials, focusing on improving statistical analysis of adverse events. It does not involve reinforcement learning, human feedback for model alignment, training a reward model, or fine-tuning based on human-ranked data. As RLHF specifically requires these elements, the paper has no connection to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04260",
      "title": "An Empirical Study of Vulnerabilities in Python Packages and Their\n  Detection",
      "authors": [
        "Haowei Quan",
        "Junjie Wang",
        "Xinzhe Li",
        "Terry Yue Zhuo",
        "Xiao Chen",
        "Xiaoning Du"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "In the rapidly evolving software development landscape, Python stands out for\nits simplicity, versatility, and extensive ecosystem. Python packages, as units\nof organization, reusability, and distribution, have become a pressing concern,\nhighlighted by the considerable number of vulnerability reports. As a scripting\nlanguage, Python often cooperates with other languages for performance or\ninteroperability. This adds complexity to the vulnerabilities inherent to\nPython packages, and the effectiveness of current vulnerability detection tools\nremains underexplored. This paper addresses these gaps by introducing PyVul,\nthe first comprehensive benchmark suite of Python-package vulnerabilities.\nPyVul includes 1,157 publicly reported, developer-verified vulnerabilities,\neach linked to its affected packages. To accommodate diverse detection\ntechniques, it provides annotations at both commit and function levels. An\nLLM-assisted data cleansing method is incorporated to improve label accuracy,\nachieving 100% commit-level and 94% function-level accuracy, establishing PyVul\nas the most precise large-scale Python vulnerability benchmark. We further\ncarry out a distribution analysis of PyVul, which demonstrates that\nvulnerabilities in Python packages involve multiple programming languages and\nexhibit a wide variety of types. Moreover, our analysis reveals that\nmulti-lingual Python packages are potentially more susceptible to\nvulnerabilities. Evaluation of state-of-the-art detectors using this benchmark\nreveals a significant discrepancy between the capabilities of existing tools\nand the demands of effectively identifying real-world security issues in Python\npackages. Additionally, we conduct an empirical review of the top-ranked CWEs\nobserved in Python packages, to diagnose the fine-grained limitations of\ncurrent detection tools and highlight the necessity for future advancements in\nthe field.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04260v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04260v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.247,
      "distributed_training_score": 0.307,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04268",
      "title": "Differential Morphological Profile Neural Networks for Semantic\n  Segmentation",
      "authors": [
        "David Huangal",
        "J. Alex Hurt"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Semantic segmentation of overhead remote sensing imagery enables applications\nin mapping, urban planning, and disaster response. State-of-the-art\nsegmentation networks are typically developed and tuned on ground-perspective\nphotographs and do not directly address remote sensing challenges such as\nextreme scale variation, foreground-background imbalance, and large image\nsizes. We explore the incorporation of the differential morphological profile\n(DMP), a multi-scale shape extraction method based on grayscale morphology,\ninto modern segmentation networks. Prior studies have shown that the DMP can\nprovide critical shape information to Deep Neural Networks to enable superior\ndetection and classification performance in overhead imagery. In this work, we\nextend prior DMPNet work beyond classification and object detection by\nintegrating DMP features into three state-of-the-art convolutional and\ntransformer semantic segmentation architectures. We utilize both direct input,\nwhich adapts the input stem of feature extraction architectures to accept DMP\nchannels, and hybrid architectures, a dual-stream design that fuses RGB and DMP\nencoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP\ndifferentials and structuring element shapes to more effectively provide shape\ninformation to the model. Our results show that while non-DMP models generally\noutperform the direct-input variants, hybrid DMP consistently outperforms\ndirect-input and is capable of surpassing a non-DMP model on mIoU, F1, and\nRecall.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04268v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04268v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.373,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on integrating differential morphological profiles into neural networks for semantic segmentation in remote sensing imagery. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning tasks, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses the iSAID dataset for evaluating its segmentation models, which involves benchmarking and analysis of results on a remote sensing dataset. However, the primary focus is on developing and testing neural network architectures, not on creating, curating, or deeply analyzing datasets themselves.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04269",
      "title": "TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D\n  Diffusion Models",
      "authors": [
        "Yuxin Gong",
        "Se-in Jang",
        "Wei Shao",
        "Yi Su",
        "Kuang Gong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate quantification of tau pathology via tau positron emission tomography\n(PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD).\nHowever, the high cost and limited availability of tau PET restrict its\nwidespread use. In contrast, structural magnetic resonance imaging (MRI) and\nplasma-based biomarkers provide non-invasive and widely available complementary\ninformation related to brain anatomy and disease progression. In this work, we\npropose a text-guided 3D diffusion model for 3D tau PET image synthesis,\nleveraging multimodal conditions from both structural MRI and plasma\nmeasurement. Specifically, the textual prompt is from the plasma p-tau217\nmeasurement, which is a key indicator of AD progression, while MRI provides\nanatomical structure constraints. The proposed framework is trained and\nevaluated using clinical AV1451 tau PET data from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database. Experimental results demonstrate that\nour approach can generate realistic, clinically meaningful 3D tau PET across a\nrange of disease stages. The proposed framework can help perform tau PET data\naugmentation under different settings, provide a non-invasive, cost-effective\nalternative for visualizing tau pathology, and support the simulation of\ndisease progression under varying plasma biomarker levels and cognitive\nconditions.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04269v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04269v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.523,
      "distributed_training_score": 0.363,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for 3D medical image synthesis, specifically generating tau PET images from MRI and plasma data. While diffusion models involve iterative refinement processes, this application is limited to image generation and does not adapt the process for multi-step logical reasoning, chain-of-thought correction, or solving complex logical tasks. As the paper lacks any component related to logical reasoning, it does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04273",
      "title": "Dual-Scale Volume Priors with Wasserstein-Based Consistency for\n  Semi-Supervised Medical Image Segmentation",
      "authors": [
        "Junying Meng",
        "Gangxuan Zhou",
        "Jun Liu",
        "Weihong Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite signi cant progress in semi-supervised medical image segmentation,\nmost existing segmentation networks overlook e ective methodological guidance\nfor feature extraction and important prior information from\n  datasets. In this paper, we develop a semi-supervised medical image\nsegmentation framework that e ectively integrates spatial regularization\nmethods and volume priors. Speci cally, our approach integrates a strong\nexplicit volume prior at the image scale and Threshold Dynamics spatial\nregularization, both derived from variational models, into the backbone\nsegmentation network. The target region volumes for each unlabeled image are\nestimated by a regression network, which e ectively regularizes the backbone\nsegmentation network through an image-scale Wasserstein distance constraint,\nensuring that the class ratios in the segmentation results for each unlabeled\nimage match those predicted by the regression network. Additionally, we design\na dataset-scale Wasserstein distance loss function based on a weak implicit\nvolume prior, which enforces that the volume distribution predicted for the\nunlabeled dataset is similar to that of labeled dataset. Experimental results\non the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset\nshow the superiority of the proposed method.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04273v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04273v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.427,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.365,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on semi-supervised medical image segmentation, where a regression network estimates volumes for unlabeled images, serving as a form of programmatically generated labels or priors. This aligns with weak supervision by using high-level, imprecise sources (e.g., volume predictions) to regularize the model without relying solely on hand-labeled data. However, the primary emphasis is on integrating volume priors and Wasserstein distance for segmentation, rather than directly advancing weak supervision techniques, making it moderately relevant rather than central.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a semi-supervised medical image segmentation framework that integrates dual-scale volume priors—explicit at the image level and implicit at the dataset level—along with Threshold Dynamics spatial regularization into a backbone segmentation network. By employing a regression network to estimate volumes for unlabeled images and enforcing consistency through Wasserstein distance-based loss functions, the method ensures accurate class ratios and volume distributions, demonstrating superior performance on datasets such as ACDC, PROMISE12, and thigh muscle MR images compared to existing approaches.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by innovatively combining volume priors from variational models with Wasserstein-based consistency in a semi-supervised setting, extending beyond previous methods that only addressed image-scale priors.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in semi-supervised medical image segmentation by providing effective ways to incorporate priors, potentially leading to better utilization of unlabeled data in clinical applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and innovative approach to enhancing segmentation accuracy with limited annotations, making it essential for researchers focused on medical image analysis and semi-supervised learning.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b1f9f5f29592954c36ed882e2d9812172e83a175",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Junying Meng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Gangxuan Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381322117"
        },
        {
          "name": "Jun Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379016643"
        },
        {
          "name": "Weihong Guo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379517763"
        }
      ]
    },
    {
      "id": "2509.04276",
      "title": "PAOLI: Pose-free Articulated Object Learning from Sparse-view Images",
      "authors": [
        "Jianning Deng",
        "Kartic Subr",
        "Hakan Bilen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present a novel self-supervised framework for learning articulated object\nrepresentations from sparse-view, unposed images. Unlike prior methods that\nrequire dense multi-view observations and ground-truth camera poses, our\napproach operates with as few as four views per articulation and no camera\nsupervision. To address the inherent challenges, we first reconstruct each\narticulation independently using recent advances in sparse-view 3D\nreconstruction, then learn a deformation field that establishes dense\ncorrespondences across poses. A progressive disentanglement strategy further\nseparates static from moving parts, enabling robust separation of camera and\nobject motion. Finally, we jointly optimize geometry, appearance, and\nkinematics with a self-supervised loss that enforces cross-view and cross-pose\nconsistency. Experiments on the standard benchmark and real-world examples\ndemonstrate that our method produces accurate and detailed articulated object\nrepresentations under significantly weaker input assumptions than existing\napproaches.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04276v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04276v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.316,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04288",
      "title": "Reinforcement Learning for Robust Ageing-Aware Control of Li-ion Battery\n  Systems with Data-Driven Formal Verification",
      "authors": [
        "Rudi Coppola",
        "Hovsep Touloujian",
        "Pierfrancesco Ombrini",
        "Manuel Mazo Jr"
      ],
      "categories": [
        "eess.SY (Systems and Control)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)"
      ],
      "abstract": "Rechargeable lithium-ion (Li-ion) batteries are a ubiquitous element of\nmodern technology. In the last decades, the production and design of such\nbatteries and their adjacent embedded charging and safety protocols, denoted by\nBattery Management Systems (BMS), has taken central stage. A fundamental\nchallenge to be addressed is the trade-off between the speed of charging and\nthe ageing behavior, resulting in the loss of capacity in the battery cell. We\nrely on a high-fidelity physics-based battery model and propose an approach to\ndata-driven charging and safety protocol design. Following a\nCounterexample-Guided Inductive Synthesis scheme, we combine Reinforcement\nLearning (RL) with recent developments in data-driven formal methods to obtain\na hybrid control strategy: RL is used to synthesise the individual controllers,\nand a data-driven abstraction guides their partitioning into a switched\nstructure, depending on the initial output measurements of the battery. The\nresulting discrete selection among RL-based controllers, coupled with the\ncontinuous battery dynamics, realises a hybrid system. When a design meets the\ndesired criteria, the abstraction provides probabilistic guarantees on the\nclosed-loop performance of the cell.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04288v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04288v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.435,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.31,
      "datasets_score": 0.246,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Reinforcement Learning (RL) for designing battery control strategies based on simulations from a physics-based model and data-driven formal methods, without any involvement of human feedback, preferences, or ranked data. RLHF specifically requires training with human-provided rewards or rankings, which is absent here, making the paper unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04298",
      "title": "Noisy Label Refinement with Semantically Reliable Synthetic Images",
      "authors": [
        "Yingxuan Li",
        "Jiafeng Mao",
        "Yusuke Matsui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Semantic noise in image classification datasets, where visually similar\ncategories are frequently mislabeled, poses a significant challenge to\nconventional supervised learning approaches. In this paper, we explore the\npotential of using synthetic images generated by advanced text-to-image models\nto address this issue. Although these high-quality synthetic images come with\nreliable labels, their direct application in training is limited by domain gaps\nand diversity constraints. Unlike conventional approaches, we propose a novel\nmethod that leverages synthetic images as reliable reference points to identify\nand correct mislabeled samples in noisy datasets. Extensive experiments across\nmultiple benchmark datasets show that our approach significantly improves\nclassification accuracy under various noise conditions, especially in\nchallenging scenarios with semantic label noise. Additionally, since our method\nis orthogonal to existing noise-robust learning techniques, when combined with\nstate-of-the-art noise-robust training methods, it achieves superior\nperformance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100\nunder 70% semantic noise, and by 24% on ImageNet-100 under real-world noise\nconditions.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04298v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04298v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.552,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.337,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using synthetic images from text-to-image models to refine noisy labels in datasets, which aligns closely with weak supervision. It programmatically generates and corrects labels from high-level sources (text prompts), reducing reliance on perfectly hand-labeled data and improving training under noisy conditions.",
      "diffusion_reasoning_justification": "The paper uses diffusion-based models like Stable Diffusion solely for generating synthetic images, not for adapting their iterative refinement process to multi-step logical reasoning or solving complex logical tasks. There is no component involving Chain-of-Thought or holistic reasoning correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of semantic noise in image classification datasets by proposing a method that uses high-quality synthetic images generated from text-to-image models as reliable reference points to identify and correct mislabeled samples. The methodology involves a preprocessing step to refine noisy labels, which is orthogonal to existing noise-robust techniques, and experiments on datasets like CIFAR-10, CIFAR-100, and ImageNet-100 demonstrate significant accuracy improvements, such as up to 30% on CIFAR-10 under 70% semantic noise when combined with state-of-the-art methods.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel technique that leverages synthetic images as reference points for correcting semantic noise, significantly advancing the state-of-the-art in noisy label learning by addressing a previously underutilized resource. This approach represents a meaningful innovation beyond incremental refinements, as it combines generative models with label correction in a new way.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a broad range of future research and applications in computer vision by improving robustness to real-world noisy datasets, as evidenced by substantial accuracy gains on benchmark datasets. Its orthogonality to existing methods allows for easy integration, likely leading to widespread adoption and citations in the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with practical implications for handling noisy labels, making it valuable for researchers in computer vision and machine learning. While not essential for all, its innovative approach and demonstrated performance improvements warrant attention from those working in related areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2962e20f289d32167100137b1d88c69eeaa7b08b",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yingxuan Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2247959834"
        },
        {
          "name": "Jiafeng Mao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377960146"
        },
        {
          "name": "Yusuke Matsui",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2366011288"
        }
      ]
    },
    {
      "id": "2509.04303",
      "title": "HumAIne-Chatbot: Real-Time Personalized Conversational AI via\n  Reinforcement Learning",
      "authors": [
        "Georgios Makridis",
        "Georgios Fragiadakis",
        "Jorge Oliveira",
        "Tomaz Saraiva",
        "Philip Mavrepis",
        "Georgios Fatouros",
        "Dimosthenis Kyriazis"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Current conversational AI systems often provide generic, one-size-fits-all\ninteractions that overlook individual user characteristics and lack adaptive\ndialogue management. To address this gap, we introduce\n\\textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes\nresponses through a novel user profiling framework. The system is pre-trained\non a diverse set of GPT-generated virtual personas to establish a broad prior\nover user types. During live interactions, an online reinforcement learning\nagent refines per-user models by combining implicit signals (e.g. typing speed,\nsentiment, engagement duration) with explicit feedback (e.g., likes and\ndislikes). This profile dynamically informs the chatbot dialogue policy,\nenabling real-time adaptation of both content and style. To evaluate the\nsystem, we performed controlled experiments with 50 synthetic personas in\nmultiple conversation domains. The results showed consistent improvements in\nuser satisfaction, personalization accuracy, and task achievement when\npersonalization features were enabled. Statistical analysis confirmed\nsignificant differences between personalized and nonpersonalized conditions,\nwith large effect sizes across key metrics. These findings highlight the\neffectiveness of AI-driven user profiling and provide a strong foundation for\nfuture real-world validation.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04303v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04303v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.509,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.331,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves an online reinforcement learning agent that refines per-user models using both implicit signals and explicit human feedback, such as likes and dislikes, to adapt the chatbot's dialogue policy in real-time. This aligns closely with RLHF, as it uses human feedback to guide the learning process and fine-tune the model for better alignment with user preferences, making the paper's approach a direct application of RLHF principles.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces HumAIne-Chatbot, a conversational AI system that uses reinforcement learning to deliver real-time personalized interactions by profiling users through implicit signals (e.g., typing speed and sentiment) and explicit feedback, building on pre-training with GPT-generated virtual personas. Through controlled experiments involving 50 synthetic personas, the authors demonstrate significant improvements in user satisfaction, personalization accuracy, and task achievement compared to non-personalized versions, highlighting the effectiveness of their adaptive user profiling framework.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining reinforcement learning with user profiling via virtual personas to enhance personalization in conversational AI, though it builds on existing techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in human-computer interaction and AI subfields by providing a framework for personalized chatbots, but its broader impact may be limited without real-world validation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a strong, practical contribution to personalized conversational AI, making it valuable for researchers in AI and HCI to understand and potentially build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3f82181eb5545dcf2c0d7f8322d494f5d3906da3",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 33,
      "average_h_index": 9.857142857142858,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Georgios Makridis",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2042705161"
        },
        {
          "name": "Georgios Fragiadakis",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378954901"
        },
        {
          "name": "Jorge Oliveira",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2331287694"
        },
        {
          "name": "T. Saraiva",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/144357379"
        },
        {
          "name": "Philip Mavrepis",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/1998030340"
        },
        {
          "name": "G. Fatouros",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/90773902"
        },
        {
          "name": "D. Kyriazis",
          "h_index": 33,
          "profile_url": "https://www.semanticscholar.org/author/1681006"
        }
      ]
    },
    {
      "id": "2509.04304",
      "title": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge\n  in Large Language Models",
      "authors": [
        "Juraj Vladika",
        "Mahdi Dhaini",
        "Florian Matthes"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The growing capabilities of Large Language Models (LLMs) show significant\npotential to enhance healthcare by assisting medical researchers and\nphysicians. However, their reliance on static training data is a major risk\nwhen medical recommendations evolve with new research and developments. When\nLLMs memorize outdated medical knowledge, they can provide harmful advice or\nfail at clinical reasoning tasks. To investigate this problem, we introduce two\nnovel question-answering (QA) datasets derived from systematic reviews:\nMedRevQA (16,501 QA pairs covering general biomedical knowledge) and\nMedChangeQA (a subset of 512 QA pairs where medical consensus has changed over\ntime). Our evaluation of eight prominent LLMs on the datasets reveals\nconsistent reliance on outdated knowledge across all models. We additionally\nanalyze the influence of obsolete pre-training data and training strategies to\nexplain this phenomenon and propose future directions for mitigation, laying\nthe groundwork for developing more current and reliable medical AI systems.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04304v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04304v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.339,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates the memorization of outdated medical knowledge in standard Large Language Models (LLMs) using QA datasets, focusing on healthcare applications and temporal knowledge decay. It does not mention, adapt, or involve diffusion-based models, iterative refinement processes, or any mechanism for treating Chain-of-Thought as a holistic entity for multi-step logical reasoning. Therefore, the paper's contributions are unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04310",
      "title": "EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn\n  Negotiation",
      "authors": [
        "Yunbo Long",
        "Liming Xu",
        "Lukas Beckenbauer",
        "Yuhan Liu",
        "Alexandra Brintrup"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04310v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04310v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.296,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces EvoEmo, an evolutionary reinforcement learning framework that uses rewards from simulated negotiations to optimize emotion policies, but it does not involve training a reward model on human-ranked data or aligning the model with human preferences through human feedback. While it mentions RLHF as an existing technique in the introduction, the main contribution does not incorporate human feedback, making it distinct from RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evolutionary reinforcement learning and Chain-of-Thought reasoning for optimizing emotional policies in negotiations, but it does not adapt diffusion models or involve any iterative refinement process for multi-step logical reasoning. There is no mention of treating reasoning paths as entities for holistic correction using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04317",
      "title": "Improving Robustness of AlphaZero Algorithms to Test-Time Environment\n  Changes",
      "authors": [
        "Isidoro Tamassia",
        "Wendelin Böhmer"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The AlphaZero framework provides a standard way of combining Monte Carlo\nplanning with prior knowledge provided by a previously trained policy-value\nneural network. AlphaZero usually assumes that the environment on which the\nneural network was trained will not change at test time, which constrains its\napplicability. In this paper, we analyze the problem of deploying AlphaZero\nagents in potentially changed test environments and demonstrate how the\ncombination of simple modifications to the standard framework can significantly\nboost performance, even in settings with a low planning budget available. The\ncode is publicly available on GitHub.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04317v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04317v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.396,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04324",
      "title": "OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent\n  Detection",
      "authors": [
        "Chen Hu",
        "Shan Luo",
        "Letizia Gionfrida"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Grasping assistance is essential for restoring autonomy in individuals with\nmotor impairments, particularly in unstructured environments where object\ncategories and user intentions are diverse and unpredictable. We present\nOVGrasp, a hierarchical control framework for soft exoskeleton-based grasp\nassistance that integrates RGB-D vision, open-vocabulary prompts, and voice\ncommands to enable robust multimodal interaction. To enhance generalization in\nopen environments, OVGrasp incorporates a vision-language foundation model with\nan open-vocabulary mechanism, allowing zero-shot detection of previously unseen\nobjects without retraining. A multimodal decision-maker further fuses spatial\nand linguistic cues to infer user intent, such as grasp or release, in\nmulti-object scenarios. We deploy the complete framework on a custom\negocentric-view wearable exoskeleton and conduct systematic evaluations on 15\nobjects across three grasp types. Experimental results with ten participants\ndemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,\noutperforming state-of-the-art baselines and achieving improved kinematic\nalignment with natural hand motion.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04324v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04324v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.301,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04326",
      "title": "Efficient Odd-One-Out Anomaly Detection",
      "authors": [
        "Silvio Chito",
        "Paolo Rabino",
        "Tatiana Tommasi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The recently introduced odd-one-out anomaly detection task involves\nidentifying the odd-looking instances within a multi-object scene. This problem\npresents several challenges for modern deep learning models, demanding spatial\nreasoning across multiple views and relational reasoning to understand context\nand generalize across varying object categories and layouts. We argue that\nthese challenges must be addressed with efficiency in mind. To this end, we\npropose a DINO-based model that reduces the number of parameters by one third\nand shortens training time by a factor of three compared to the current\nstate-of-the-art, while maintaining competitive performance. Our experimental\nevaluation also introduces a Multimodal Large Language Model baseline,\nproviding insights into its current limitations in structured visual reasoning\ntasks. The project page can be found at\nhttps://silviochito.github.io/EfficientOddOneOut/",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04326v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04326v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.367,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves developing an efficient DINO-based model for odd-one-out anomaly detection and evaluating a Multimodal Large Language Model baseline, focusing on spatial and relational reasoning. There is no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04334",
      "title": "GeoArena: An Open Platform for Benchmarking Large Vision-language Models\n  on WorldWide Image Geolocalization",
      "authors": [
        "Pengyue Jia",
        "Yingyi Zhang",
        "Xiangyu Zhao",
        "Yixuan Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Image geolocalization aims to predict the geographic location of images\ncaptured anywhere on Earth, but its global nature presents significant\nchallenges. Current evaluation methodologies suffer from two major limitations.\nFirst, data leakage: advanced approaches often rely on large vision-language\nmodels (LVLMs) to predict image locations, yet these models are frequently\npretrained on the test datasets, compromising the accuracy of evaluating a\nmodel's actual geolocalization capability. Second, existing metrics primarily\nrely on exact geographic coordinates to assess predictions, which not only\nneglects the reasoning process but also raises privacy concerns when user-level\nlocation data is required. To address these issues, we propose GeoArena, a\nfirst open platform for evaluating LVLMs on worldwide image geolocalization\ntasks, offering true in-the-wild and human-centered benchmarking. GeoArena\nenables users to upload in-the-wild images for a more diverse evaluation\ncorpus, and it leverages pairwise human judgments to determine which model\noutput better aligns with human expectations. Our platform has been deployed\nonline for two months, during which we collected over thousands voting records.\nBased on this data, we conduct a detailed analysis and establish a leaderboard\nof different LVLMs on the image geolocalization task.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04334v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04334v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.383,
      "datasets_score": 0.414,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper collects human feedback (user votes) to rank models and mentions releasing data for reward modeling, which is a component of RLHF. However, it does not involve training or fine-tuning models using reinforcement learning based on this feedback; it focuses on benchmarking, making it only indirectly related.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and analysis of a new dataset through GeoArena, including in-the-wild images and voting records, which are used for benchmarking LVLMs. It also involves dataset curation, evaluation, and public release, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "GeoArena is an open platform developed to benchmark large vision-language models (LVLMs) for worldwide image geolocalization by overcoming limitations in current methods, such as data leakage and privacy concerns associated with static datasets and GPS-based evaluations. The platform enables users to upload real-world images, generates predictions from various models, and uses pairwise human judgments to rank models based on user preferences, leading to a dynamic leaderboard and insights from collected data, including the dominance of models like Gemini-2.5 and the importance of detailed responses.",
      "novelty_score": "High",
      "novelty_justification": "GeoArena introduces a novel evaluation platform that uses dynamic, user-uploaded images and human preference judgments, significantly advancing beyond traditional static benchmarks by addressing data leakage and privacy issues. This represents a true innovation in benchmarking methodologies for image geolocalization tasks.",
      "impact_score": "High",
      "impact_justification": "The platform has the potential to influence future research in LVLMs and geolocalization by providing a more realistic, privacy-preserving benchmark that could become a standard for evaluations. Its release of real-world data and insights may also spur advancements in related areas like reward modeling and geographic AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a valuable and innovative contribution to AI benchmarking that is relevant for researchers in computer vision and geolocalization, making it important for staying updated on evaluation methodologies. However, while significant, it may not be essential for those outside this specific subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/89cfb11320212f2eb103e8f333e0c04021cb5e4f",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 6,
      "average_h_index": 2.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Pengyue Jia",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2264224432"
        },
        {
          "name": "Yingyi Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2355358738"
        },
        {
          "name": "Xiangyu Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2368574332"
        },
        {
          "name": "Yixuan Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2368475632"
        }
      ]
    },
    {
      "id": "2509.04337",
      "title": "Decoupled Entity Representation Learning for Pinterest Ads Ranking",
      "authors": [
        "Jie Liu",
        "Yinrui Li",
        "Jiankai Sun",
        "Kungang Li",
        "Han Sun",
        "Sihan Wang",
        "Huasen Wu",
        "Siyuan Gao",
        "Paulo Soares",
        "Nan Li",
        "Zhifang Liu",
        "Haoyang Li",
        "Siping Ji",
        "Ling Leng",
        "Prathibha Deshikachar"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In this paper, we introduce a novel framework following an\nupstream-downstream paradigm to construct user and item (Pin) embeddings from\ndiverse data sources, which are essential for Pinterest to deliver personalized\nPins and ads effectively. Our upstream models are trained on extensive data\nsources featuring varied signals, utilizing complex architectures to capture\nintricate relationships between users and Pins on Pinterest. To ensure\nscalability of the upstream models, entity embeddings are learned, and\nregularly refreshed, rather than real-time computation, allowing for\nasynchronous interaction between the upstream and downstream models. These\nembeddings are then integrated as input features in numerous downstream tasks,\nincluding ad retrieval and ranking models for CTR and CVR predictions. We\ndemonstrate that our framework achieves notable performance improvements in\nboth offline and online settings across various downstream tasks. This\nframework has been deployed in Pinterest's production ad ranking systems,\nresulting in significant gains in online metrics.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04337v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04337v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.384,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04338",
      "title": "From Editor to Dense Geometry Estimator",
      "authors": [
        "JiYuan Wang",
        "Chunyu Lin",
        "Lei Sun",
        "Rongying Liu",
        "Lang Nie",
        "Mingxing Li",
        "Kang Liao",
        "Xiangxiang Chu",
        "Yao Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Leveraging visual priors from pre-trained text-to-image (T2I) generative\nmodels has shown success in dense prediction. However, dense prediction is\ninherently an image-to-image task, suggesting that image editing models, rather\nthan T2I generative models, may be a more suitable foundation for fine-tuning.\n  Motivated by this, we conduct a systematic analysis of the fine-tuning\nbehaviors of both editors and generators for dense geometry estimation. Our\nfindings show that editing models possess inherent structural priors, which\nenable them to converge more stably by ``refining\" their innate features, and\nultimately achieve higher performance than their generative counterparts.\n  Based on these findings, we introduce \\textbf{FE2E}, a framework that\npioneeringly adapts an advanced editing model based on Diffusion Transformer\n(DiT) architecture for dense geometry prediction. Specifically, to tailor the\neditor for this deterministic task, we reformulate the editor's original flow\nmatching loss into the ``consistent velocity\" training objective. And we use\nlogarithmic quantization to resolve the precision conflict between the editor's\nnative BFloat16 format and the high precision demand of our tasks.\nAdditionally, we leverage the DiT's global attention for a cost-free joint\nestimation of depth and normals in a single forward pass, enabling their\nsupervisory signals to mutually enhance each other.\n  Without scaling up the training data, FE2E achieves impressive performance\nimprovements in zero-shot monocular depth and normal estimation across multiple\ndatasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset\nand outperforms the DepthAnything series, which is trained on 100$\\times$ data.\nThe project page can be accessed \\href{https://amap-ml.github.io/FE2E/}{here}.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04338v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04338v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.392,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adapting a Diffusion Transformer (DiT) model for dense geometry estimation tasks, such as depth and normal prediction, by leveraging its iterative refinement process. While diffusion models inherently involve multi-step refinement, the paper applies this to image-to-image tasks rather than complex logical reasoning or treating a 'Chain-of-Thought' as a holistic entity for correction. Thus, the connection is tangential through the use of diffusion mechanisms, but it lacks the core elements of logical task-solving.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04343",
      "title": "Psychologically Enhanced AI Agents",
      "authors": [
        "Maciej Besta",
        "Shriram Chandran",
        "Robert Gerstenberger",
        "Mathis Lindner",
        "Marcin Chrapek",
        "Sebastian Hermann Martschat",
        "Taraneh Ghandi",
        "Patrick Iff",
        "Hubert Niewiadomski",
        "Piotr Nyczyk",
        "Jürgen Müller",
        "Torsten Hoefler"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CY (Computers and Society)",
        "cs.HC (Human-Computer Interaction)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of\nLarge Language Model (LLM) agents through psychologically grounded personality\nconditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method\nprimes agents with distinct personality archetypes via prompt engineering,\nenabling control over behavior along two foundational axes of human psychology,\ncognition and affect. We show that such personality priming yields consistent,\ninterpretable behavioral biases across diverse tasks: emotionally expressive\nagents excel in narrative generation, while analytically primed agents adopt\nmore stable strategies in game-theoretic settings. Our framework supports\nexperimenting with structured multi-agent communication protocols and reveals\nthat self-reflection prior to interaction improves cooperation and reasoning\nquality. To ensure trait persistence, we integrate the official 16Personalities\ntest for automated verification. While our focus is on MBTI, we show that our\napproach generalizes seamlessly to other psychological frameworks such as Big\nFive, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior\ndesign, we establish a foundation for psychologically enhanced AI agents\nwithout any fine-tuning.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04343v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04343v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.481,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.33,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on prompt engineering for personality conditioning in LLMs without any fine-tuning, training on human-ranked data, or use of a reward model. It does not involve reinforcement learning or human feedback to align AI models, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses prompt-based psychological priming and reasoning in LLMs but does not mention diffusion models, iterative refinement processes, or treating chain-of-thought as a holistically corrected entity. There is no component for multi-step logical reasoning using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04344",
      "title": "MICACL: Multi-Instance Category-Aware Contrastive Learning for\n  Long-Tailed Dynamic Facial Expression Recognition",
      "authors": [
        "Feng-Qi Cui",
        "Zhen Lin",
        "Xinlong Rao",
        "Anyang Tong",
        "Shiyao Li",
        "Fei Wang",
        "Changlin Chen",
        "Bin Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Dynamic facial expression recognition (DFER) faces significant challenges due\nto long-tailed category distributions and complexity of spatio-temporal feature\nmodeling. While existing deep learning-based methods have improved DFER\nperformance, they often fail to address these issues, resulting in severe model\ninduction bias. To overcome these limitations, we propose a novel\nmulti-instance learning framework called MICACL, which integrates\nspatio-temporal dependency modeling and long-tailed contrastive learning\noptimization. Specifically, we design the Graph-Enhanced Instance Interaction\nModule (GEIIM) to capture intricate spatio-temporal between adjacent instances\nrelationships through adaptive adjacency matrices and multiscale convolutions.\nTo enhance instance-level feature aggregation, we develop the Weighted Instance\nAggregation Network (WIAN), which dynamically assigns weights based on instance\nimportance. Furthermore, we introduce a Multiscale Category-aware Contrastive\nLearning (MCCL) strategy to balance training between major and minor\ncategories. Extensive experiments on in-the-wild datasets (i.e., DFEW and\nFERV39k) demonstrate that MICACL achieves state-of-the-art performance with\nsuperior robustness and generalization.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04344v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04344v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.327,
      "distributed_training_score": 0.364,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04345",
      "title": "AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open\n  Worlds",
      "authors": [
        "Qizhou Wang",
        "Hanxun Huang",
        "Guansong Pang",
        "Sarah Erfani",
        "Christopher Leckie"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Speech generation systems can produce remarkably realistic vocalisations that\nare often indistinguishable from human speech, posing significant authenticity\nchallenges. Although numerous deepfake detection methods have been developed,\ntheir effectiveness in real-world environments remains unrealiable due to the\ndomain shift between training and test samples arising from diverse human\nspeech and fast evolving speech synthesis systems. This is not adequately\naddressed by current datasets, which lack real-world application challenges\nwith diverse and up-to-date audios in both real and deep-fake categories. To\nfill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale,\nhighly diverse deepfake audio dataset for comprehensive evaluation and robust\ndevelopment of generalised models for deepfake audio detection. It consists of\nover 4,500 hours of synthetic audio generated by 11 recent TTS models and 10\nvocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio\nclips, making it the largest deepfake audio dataset by scale. Through extensive\nexperiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods\ntrained on existing datasets struggle to generalise to novel deepfake audio\nsamples and suffer from high false positive rates on unseen human voice,\nunderscoring the need for a comprehensive dataset; and ii) these methods\ntrained on AUDETER achieve highly generalised detection performance and\nsignificantly reduce detection error rate by 44.1% to 51.6%, achieving an error\nrate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild\ndataset, paving the way for training generalist deepfake audio detectors.\nAUDETER is available on GitHub.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04345v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04345v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.369,
      "datasets_score": 0.442,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of AUDETER, a new large-scale dataset for deepfake audio detection, which directly aligns with research on creating datasets for AI applications. It includes dataset curation methodologies, such as generating synthetic audio from diverse TTS models and human voice sources, and provides comparisons with existing datasets for benchmarking and analysis. Additionally, the paper evaluates models using AUDETER to demonstrate its effectiveness in open-world scenarios, fitting the topic's focus on dataset introduction, benchmarking, and evaluation.",
      "llm_score_status": "completed",
      "summary": "AUDETER introduces a large-scale dataset comprising over 4,500 hours of deepfake audio generated from 11 recent Text-to-Speech (TTS) models and 10 vocoders, sourced from diverse human voice corpora, to address the limitations of existing datasets in handling open-world deepfake detection challenges. The paper evaluates state-of-the-art methods, revealing that those trained on current datasets struggle with generalization to novel samples, while models trained on AUDETER achieve significant performance improvements, reducing detection error rates by 44.1% to 51.6% and achieving an error rate of only 4.17% on cross-domain samples, thereby paving the way for more robust deepfake audio detectors.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating the largest and most diverse deepfake audio dataset to date, which cleverly combines existing TTS and vocoder technologies to address domain shift issues in open-world detection, though it does not introduce a fundamentally new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of deepfake audio detection, as the dataset provides a valuable resource for training more generalized models, potentially influencing research in AI-driven audio authentication and misinformation detection.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by introducing a comprehensive dataset that addresses critical gaps in deepfake detection research, making it essential for researchers in AI, machine learning, and sound processing to be aware of for advancing open-world applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ddaa2c18a95e21f9f7e87e4ef33cf26abbd60de3",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 27,
      "average_h_index": 7.4,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Qizhou Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2266421329"
        },
        {
          "name": "Hanxun Huang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/1753845931"
        },
        {
          "name": "Guansong Pang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378953008"
        },
        {
          "name": "S. Erfani",
          "h_index": 27,
          "profile_url": "https://www.semanticscholar.org/author/144757691"
        },
        {
          "name": "Christopher Leckie",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2266397122"
        }
      ]
    },
    {
      "id": "2509.04351",
      "title": "Global-to-Local or Local-to-Global? Enhancing Image Retrieval with\n  Efficient Local Search and Effective Global Re-ranking",
      "authors": [
        "Dror Aiger",
        "Bingyi Cao",
        "Kaifeng Chen",
        "Andre Araujo"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The dominant paradigm in image retrieval systems today is to search large\ndatabases using global image features, and re-rank those initial results with\nlocal image feature matching techniques. This design, dubbed global-to-local,\nstems from the computational cost of local matching approaches, which can only\nbe afforded for a small number of retrieved images. However, emerging efficient\nlocal feature search approaches have opened up new possibilities, in particular\nenabling detailed retrieval at large scale, to find partial matches which are\noften missed by global feature search. In parallel, global feature-based\nre-ranking has shown promising results with high computational efficiency. In\nthis work, we leverage these building blocks to introduce a local-to-global\nretrieval paradigm, where efficient local feature search meets effective global\nfeature re-ranking. Critically, we propose a re-ranking method where global\nfeatures are computed on-the-fly, based on the local feature retrieval\nsimilarities. Such re-ranking-only global features leverage multidimensional\nscaling techniques to create embeddings which respect the local similarities\nobtained during search, enabling a significant re-ranking boost.\nExperimentally, we demonstrate solid retrieval performance, setting new\nstate-of-the-art results on the Revisited Oxford and Paris datasets.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04351v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04351v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.329,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04357",
      "title": "PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity\n  Disambiguation",
      "authors": [
        "Jiajun He",
        "Naoki Sawada",
        "Koichi Miyazaki",
        "Tomoki Toda"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SD (Sound)"
      ],
      "abstract": "Automatic speech recognition (ASR) systems struggle with domain-specific\nnamed entities, especially homophones. Contextual ASR improves recognition but\noften fails to capture fine-grained phoneme variations due to limited entity\ndiversity. Moreover, prior methods treat entities as independent tokens,\nleading to incomplete multi-token biasing. To address these issues, we propose\nPhoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation\n(PARCO), which integrates phoneme-aware encoding, contrastive entity\ndisambiguation, entity-level supervision, and hierarchical entity filtering.\nThese components enhance phonetic discrimination, ensure complete entity\nretrieval, and reduce false positives under uncertainty. Experiments show that\nPARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English\nDATA2 under 1,000 distractors, significantly outperforming baselines. PARCO\nalso demonstrates robust gains on out-of-domain datasets like THCHS-30 and\nLibriSpeech.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04357v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04357v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.378,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is the development of PARCO, a method for improving ASR by incorporating phoneme-aware encoding, contrastive entity disambiguation, entity-level supervision, and hierarchical entity filtering. These techniques rely on standard supervised training with labeled datasets (e.g., AISHELL-1, LibriSpeech) to enhance entity recognition. There is no indication of using weak supervision, such as programmatically generating noisy or imprecise labels from high-level sources, as the approach depends on direct supervision and fine-tuning rather than label-efficient methods.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04362",
      "title": "Parking Availability Prediction via Fusing Multi-Source Data with A\n  Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer",
      "authors": [
        "Yin Huang",
        "Yongqi Dong",
        "Youhua Tang",
        "Li Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "The rapid growth of private car ownership has worsened the urban parking\npredicament, underscoring the need for accurate and effective parking\navailability prediction to support urban planning and management. To address\nkey limitations in modeling spatio-temporal dependencies and exploiting\nmulti-source data for parking availability prediction, this study proposes a\nnovel approach with SST-iTransformer. The methodology leverages K-means\nclustering to establish parking cluster zones (PCZs), extracting and\nintegrating traffic demand characteristics from various transportation modes\n(i.e., metro, bus, online ride-hailing, and taxi) associated with the targeted\nparking lots. Upgraded on vanilla iTransformer, SST-iTransformer integrates\nmasking-reconstruction-based pretext tasks for self-supervised spatio-temporal\nrepresentation learning, and features an innovative dual-branch attention\nmechanism: Series Attention captures long-term temporal dependencies via\npatching operations, while Channel Attention models cross-variate interactions\nthrough inverted dimensions. Extensive experiments using real-world data from\nChengdu, China, demonstrate that SST-iTransformer outperforms baseline deep\nlearning models (including Informer, Autoformer, Crossformer, and\niTransformer), achieving state-of-the-art performance with the lowest mean\nsquared error (MSE) and competitive mean absolute error (MAE). Comprehensive\nablation studies quantitatively reveal the relative importance of different\ndata sources: incorporating ride-hailing data provides the largest performance\ngains, followed by taxi, whereas fixed-route transit features (bus/metro)\ncontribute marginally. Spatial correlation analysis further confirms that\nexcluding historical data from correlated parking lots within PCZs leads to\nsubstantial performance degradation, underscoring the importance of modeling\nspatial dependencies.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04362v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04362v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.36,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04370",
      "title": "Stitching the Story: Creating Panoramic Incident Summaries from\n  Body-Worn Footage",
      "authors": [
        "Dor Cohen",
        "Inga Efrosman",
        "Yehudit Aperstein",
        "Alexander Apartsin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "First responders widely adopt body-worn cameras to document incident scenes\nand support post-event analysis. However, reviewing lengthy video footage is\nimpractical in time-critical situations. Effective situational awareness\ndemands a concise visual summary that can be quickly interpreted. This work\npresents a computer vision pipeline that transforms body-camera footage into\ninformative panoramic images summarizing the incident scene. Our method\nleverages monocular Simultaneous Localization and Mapping (SLAM) to estimate\ncamera trajectories and reconstruct the spatial layout of the environment. Key\nviewpoints are identified by clustering camera poses along the trajectory, and\nrepresentative frames from each cluster are selected. These frames are fused\ninto spatially coherent panoramic images using multi-frame stitching\ntechniques. The resulting summaries enable rapid understanding of complex\nenvironments and facilitate efficient decision-making and incident review.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04370v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04370v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.224,
      "weak_supervision_score": 0.25,
      "diffusion_reasoning_score": 0.315,
      "distributed_training_score": 0.24,
      "datasets_score": 0.26,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04376",
      "title": "AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval\n  for Text-Based Person Anomaly Search",
      "authors": [
        "Hao Ju",
        "Hu Zhang",
        "Zhedong Zheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With growing public safety demands, text-based person anomaly search has\nemerged as a critical task, aiming to retrieve individuals with abnormal\nbehaviors via natural language descriptions. Unlike conventional person search,\nthis task presents two unique challenges: (1) fine-grained cross-modal\nalignment between textual anomalies and visual behaviors, and (2) anomaly\nrecognition under sparse real-world samples. While Large Multi-modal Models\n(LMMs) excel in multi-modal understanding, their potential for fine-grained\nanomaly retrieval remains underexplored, hindered by: (1) a domain gap between\ngenerative knowledge and discriminative retrieval, and (2) the absence of\nefficient adaptation strategies for deployment. In this work, we propose\nAnomalyLMM, the first framework that harnesses LMMs for text-based person\nanomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline\nintegrating LMMs to bridge generative world knowledge with retrieval-centric\nanomaly detection; (2) A training-free adaptation cookbook featuring masked\ncross-modal prompting, behavioral saliency prediction, and knowledge-aware\nre-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study\nto explore LMMs for this task, we conduct a rigorous evaluation on the PAB\ndataset, the only publicly available benchmark for text-based person anomaly\nsearch, with its curated real-world anomalies covering diverse scenarios (e.g.,\nfalling, collision, and being hit). Experiments show the effectiveness of the\nproposed method, surpassing the competitive baseline by +0.96% Recall@1\naccuracy. Notably, our method reveals interpretable alignment between textual\nanomalies and visual behaviors, validated via qualitative analysis. Our code\nand models will be released for future research.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04376v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04376v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.334,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework called AnomalyLMM that uses Large Multi-modal Models (LMMs) for text-based person anomaly search, involving a coarse-to-fine pipeline, masked prompting, and re-ranking. It does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a single entity for holistic correction, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04378",
      "title": "Aesthetic Image Captioning with Saliency Enhanced MLLMs",
      "authors": [
        "Yilin Tao",
        "Jiashui Huang",
        "Huaze Xu",
        "Ling Shao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Aesthetic Image Captioning (AIC) aims to generate textual descriptions of\nimage aesthetics, becoming a key research direction in the field of\ncomputational aesthetics. In recent years, pretrained Multimodal Large Language\nModels (MLLMs) have advanced rapidly, leading to a significant increase in\nimage aesthetics research that integrates both visual and textual modalities.\nHowever, most existing studies on image aesthetics primarily focus on\npredicting aesthetic ratings and have shown limited application in AIC.\nExisting AIC works leveraging MLLMs predominantly rely on fine-tuning methods\nwithout specifically adapting MLLMs to focus on target aesthetic content. To\naddress this limitation, we propose the Aesthetic Saliency Enhanced Multimodal\nLarge Language Model (ASE-MLLM), an end-to-end framework that explicitly\nincorporates aesthetic saliency into MLLMs. Within this framework, we introduce\nthe Image Aesthetic Saliency Module (IASM), which efficiently and effectively\nextracts aesthetic saliency features from images. Additionally, we design\nIAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency\nfeatures with original image features via a cross-attention mechanism. To the\nbest of our knowledge, ASE-MLLM is the first framework to integrate image\naesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments\ndemonstrated that our approach significantly outperformed traditional methods\nand generic MLLMs on current mainstream AIC benchmarks, achieving\nstate-of-the-art (SOTA) performance.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04378v3",
      "pdf_url": "http://arxiv.org/pdf/2509.04378v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.283,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04379",
      "title": "SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer",
      "authors": [
        "Jimin Xu",
        "Bosheng Qin",
        "Tao Jin",
        "Zhou Zhao",
        "Zhenhui Ye",
        "Jun Yu",
        "Fei Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advancements in neural representations, such as Neural Radiance Fields\nand 3D Gaussian Splatting, have increased interest in applying style transfer\nto 3D scenes. While existing methods can transfer style patterns onto\n3D-consistent neural representations, they struggle to effectively extract and\ntransfer high-level style semantics from the reference style image.\nAdditionally, the stylized results often lack structural clarity and\nseparation, making it difficult to distinguish between different instances or\nobjects within the 3D scene. To address these limitations, we propose a novel\n3D style transfer pipeline that effectively integrates prior knowledge from\npretrained 2D diffusion models. Our pipeline consists of two key stages: First,\nwe leverage diffusion priors to generate stylized renderings of key viewpoints.\nThen, we transfer the stylized key views onto the 3D representation. This\nprocess incorporates two innovative designs. The first is cross-view style\nalignment, which inserts cross-view attention into the last upsampling block of\nthe UNet, allowing feature interactions across multiple key views. This ensures\nthat the diffusion model generates stylized key views that maintain both style\nfidelity and instance-level consistency. The second is instance-level style\ntransfer, which effectively leverages instance-level consistency across\nstylized key views and transfers it onto the 3D representation. This results in\na more structured, visually coherent, and artistically enriched stylization.\nExtensive qualitative and quantitative experiments demonstrate that our 3D\nstyle transfer pipeline significantly outperforms state-of-the-art methods\nacross a wide range of scenes, from forward-facing to challenging 360-degree\nenvironments. Visit our project page https://jm-xu.github.io/SSGaussian for\nimmersive visualization.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04379v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04379v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.314,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models for 3D style transfer, leveraging their iterative refinement process to generate stylized images and ensure consistency across views. However, it focuses on visual generation and style application, not on adapting diffusion for complex logical tasks, Chain-of-Thought reasoning, or multi-step problem-solving. The connection is loose, as diffusion's iterative nature is present but repurposed for creative tasks rather than reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04394",
      "title": "Transition Models: Rethinking the Generative Learning Objective",
      "authors": [
        "Zidong Wang",
        "Yiyuan Zhang",
        "Xiaoyu Yue",
        "Xiangyu Yue",
        "Yangguang Li",
        "Wanli Ouyang",
        "Lei Bai"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "A fundamental dilemma in generative modeling persists: iterative diffusion\nmodels achieve outstanding fidelity, but at a significant computational cost,\nwhile efficient few-step alternatives are constrained by a hard quality\nceiling. This conflict between generation steps and output quality arises from\nrestrictive training objectives that focus exclusively on either infinitesimal\ndynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by\nintroducing an exact, continuous-time dynamics equation that analytically\ndefines state transitions across any finite time interval. This leads to a\nnovel generative paradigm, Transition Models (TiM), which adapt to\narbitrary-step transitions, seamlessly traversing the generative trajectory\nfrom single leaps to fine-grained refinement with more steps. Despite having\nonly 865M parameters, TiM achieves state-of-the-art performance, surpassing\nleading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across\nall evaluated step counts. Importantly, unlike previous few-step generators,\nTiM demonstrates monotonic quality improvement as the sampling budget\nincreases. Additionally, when employing our native-resolution strategy, TiM\ndelivers exceptional fidelity at resolutions up to 4096x4096.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04394v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04394v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.559,
      "distributed_training_score": 0.404,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving generative modeling for image generation using diffusion models, specifically introducing Transition Models (TiM) to handle state transitions in diffusion processes. However, it does not involve adapting diffusion for complex logical tasks, such as treating a chain-of-thought as an entity for multi-step reasoning. There is no component for logical reasoning or holistic correction of reasoning paths, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper discusses the architecture and performance of Transition Models (TiM) for generative tasks but does not address distributed training, parallel computing, or multi-node machine learning. There are no mentions of algorithms or systems for partitioning data, architecture, or computation across processors, so it lacks any relevance to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04398",
      "title": "IPA: An Information-Preserving Input Projection Framework for Efficient\n  Foundation Model Adaptation",
      "authors": [
        "Yuan Yin",
        "Shashanka Venkataramanan",
        "Tuan-Hung Vu",
        "Andrei Bursuc",
        "Matthieu Cord"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce\nadaptation cost by injecting low-rank updates into pretrained weights. However,\nLoRA's down-projection is randomly initialized and data-agnostic, discarding\npotentially useful information. Prior analyses show that this projection\nchanges little during training, while the up-projection carries most of the\nadaptation, making the random input compression a performance bottleneck. We\npropose IPA, a feature-aware projection framework that explicitly preserves\ninformation in the reduced hidden space. In the linear case, we instantiate IPA\nwith algorithms approximating top principal components, enabling efficient\nprojector pretraining with negligible inference overhead. Across language and\nvision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on\naverage 1.5 points higher accuracy on commonsense reasoning and 2.3 points on\nVTAB-1k, while matching full LoRA performance with roughly half the trainable\nparameters when the projection is frozen.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04398v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04398v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.386,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses a framework for efficient adaptation of foundation models using information-preserving projections, focusing on parameter-efficient fine-tuning methods like LoRA. It does not involve reinforcement learning, human feedback, reward models, or any alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces an input projection framework for model adaptation and does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. It focuses solely on preserving information in reduced hidden spaces for fine-tuning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04402",
      "title": "Learning neural representations for X-ray ptychography reconstruction\n  with unknown probes",
      "authors": [
        "Tingyou Li",
        "Zixin Xu",
        "Zirui Gao",
        "Hanfei Yan",
        "Xiaojing Huang",
        "Jizhou Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "X-ray ptychography provides exceptional nanoscale resolution and is widely\napplied in materials science, biology, and nanotechnology. However, its full\npotential is constrained by the critical challenge of accurately reconstructing\nimages when the illuminating probe is unknown. Conventional iterative methods\nand deep learning approaches are often suboptimal, particularly under the\nlow-signal conditions inherent to low-dose and high-speed experiments. These\nlimitations compromise reconstruction fidelity and restrict the broader\nadoption of the technique. In this work, we introduce the Ptychographic\nImplicit Neural Representation (PtyINR), a self-supervised framework that\nsimultaneously addresses the object and probe recovery problem. By\nparameterizing both as continuous neural representations, PtyINR performs\nend-to-end reconstruction directly from raw diffraction patterns without\nrequiring any pre-characterization of the probe. Extensive evaluations\ndemonstrate that PtyINR achieves superior reconstruction quality on both\nsimulated and experimental data, with remarkable robustness under challenging\nlow-signal conditions. Furthermore, PtyINR offers a generalizable,\nphysics-informed framework for addressing probe-dependent inverse problems,\nmaking it applicable to a wide range of computational microscopy problems.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04402v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04402v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.316,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04403",
      "title": "Self-adaptive Dataset Construction for Real-World Multimodal Safety\n  Scenarios",
      "authors": [
        "Jingen Qu",
        "Lijun Li",
        "Bo Zhang",
        "Yichen Yan",
        "Jing Shao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Multimodal large language models (MLLMs) are rapidly evolving, presenting\nincreasingly complex safety challenges. However, current dataset construction\nmethods, which are risk-oriented, fail to cover the growing complexity of\nreal-world multimodal safety scenarios (RMS). And due to the lack of a unified\nevaluation metric, their overall effectiveness remains unproven. This paper\nintroduces a novel image-oriented self-adaptive dataset construction method for\nRMS, which starts with images and end constructing paired text and guidance\nresponses. Using the image-oriented method, we automatically generate an RMS\ndataset comprising 35k image-text pairs with guidance responses. Additionally,\nwe introduce a standardized safety dataset evaluation metric: fine-tuning a\nsafety judge model and evaluating its capabilities on other safety\ndatasets.Extensive experiments on various tasks demonstrate the effectiveness\nof the proposed image-oriented pipeline. The results confirm the scalability\nand effectiveness of the image-oriented approach, offering a new perspective\nfor the construction of real-world multimodal safety datasets.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04403v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04403v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.434,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.367,
      "datasets_score": 0.493,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on automated dataset construction and evaluation for multimodal safety scenarios, without any mention of training a reward model on human-ranked data or using reinforcement learning to fine-tune models based on human feedback.",
      "weak_supervision_justification": "The paper's method involves programmatically generating image-text pairs and guidance responses from real-world images using automated processes, which aligns with weak supervision by relying on noisy or imprecise sources rather than hand-labeled data, though it is not the primary focus.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes; it centers on dataset construction for safety in multimodal models without any components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of a new dataset for real-world multimodal safety scenarios, including methodologies for dataset curation, benchmarking through a new evaluation metric, and analysis of its effectiveness, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of existing risk-oriented datasets for Multimodal Large Language Models (MLLMs) by introducing a novel image-oriented self-adaptive method to construct a Real-World Multimodal Safety (RMS) dataset, focusing on real-world scenarios and information complementarity. The methodology involves starting with real-world images, automatically generating paired text and guidance responses to create 35,000 image-text pairs organized into 39 fine-grained scenarios across 12 categories, and proposing a standardized evaluation metric by fine-tuning a safety judge model on the dataset and assessing its performance on other safety benchmarks. Experimental results demonstrate the effectiveness, scalability, and adaptability of this approach in enhancing MLLM safety judgment capabilities, providing a new perspective for multimodal safety dataset construction.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new image-oriented self-adaptive dataset construction method that advances the state-of-the-art by effectively handling real-world multimodal safety scenarios through automated processes and information complementarity, addressing key shortcomings in existing approaches.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of multimodal AI safety, as it provides a scalable dataset and a new evaluation metric that could improve future MLLM safety research, though its influence may remain confined to specialized applications rather than broad commercial ones.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI safety by offering an innovative dataset construction method and evaluation metric, making it essential for researchers in computer vision and multimodal language processing to stay informed on advancements in this area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/639cb8097626250409a34f6ea3c3b20e30fe07a3",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 7,
      "average_h_index": 1.4,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Jingen Qu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379605084"
        },
        {
          "name": "Lijun Li",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2280261471"
        },
        {
          "name": "Bo Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379769297"
        },
        {
          "name": "Yichen Yan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378983447"
        },
        {
          "name": "Jing Shao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379581430"
        }
      ]
    },
    {
      "id": "2509.04404",
      "title": "No Thoughts Just AI: Biased LLM Hiring Recommendations Alter Human\n  Decision Making and Limit Human Autonomy",
      "authors": [
        "Kyra Wilson",
        "Mattea Sim",
        "Anna-Maria Gueorguieva",
        "Aylin Caliskan"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "In this study, we conduct a resume-screening experiment (N=528) where people\ncollaborate with simulated AI models exhibiting race-based preferences (bias)\nto evaluate candidates for 16 high and low status occupations. Simulated AI\nbias approximates factual and counterfactual estimates of racial bias in\nreal-world AI systems. We investigate people's preferences for White, Black,\nHispanic, and Asian candidates (represented through names and affinity groups\non quality-controlled resumes) across 1,526 scenarios and measure their\nunconscious associations between race and status using implicit association\ntests (IATs), which predict discriminatory hiring decisions but have not been\ninvestigated in human-AI collaboration. When making decisions without AI or\nwith AI that exhibits no race-based preferences, people select all candidates\nat equal rates. However, when interacting with AI favoring a particular group,\npeople also favor those candidates up to 90% of the time, indicating a\nsignificant behavioral shift. The likelihood of selecting candidates whose\nidentities do not align with common race-status stereotypes can increase by 13%\nif people complete an IAT before conducting resume screening. Finally, even if\npeople think AI recommendations are low quality or not important, their\ndecisions are still vulnerable to AI bias under certain circumstances. This\nwork has implications for people's autonomy in AI-HITL scenarios, AI and work,\ndesign and evaluation of AI hiring systems, and strategies for mitigating bias\nin collaborative decision-making tasks. In particular, organizational and\nregulatory policy should acknowledge the complex nature of AI-HITL decision\nmaking when implementing these systems, educating people who use them, and\ndetermining which are subject to oversight.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04404v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04404v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.468,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.296,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper examines the effects of biased AI recommendations on human decision-making in hiring processes, focusing on experiments with simulated AI models and human-AI collaboration. It does not involve training AI models using human feedback, reinforcement learning, reward models, or fine-tuning based on human-ranked data, which are core elements of RLHF. Therefore, the paper's contributions are unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04406",
      "title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation",
      "authors": [
        "Zanwei Zhou",
        "Taoran Yi",
        "Jiemin Fang",
        "Chen Yang",
        "Lingxi Xie",
        "Xinggang Wang",
        "Wei Shen",
        "Qi Tian"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04406v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04406v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.298,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.479,
      "distributed_training_score": 0.405,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on accelerating 3D generation models using flow-based distillation techniques, specifically for visual and geometric synthesis. It does not involve adapting diffusion processes for complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as an entity for correction. There is no component for logical reasoning, making it unrelated to this topic.",
      "distributed_training_justification": "The paper discusses model distillation for faster inference in 3D generation but does not address distributed training, parallel computing, or multi-node machine learning. There are no mentions of partitioning data, architecture, or computation across processors or nodes, focusing instead on single-device inference optimizations.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04419",
      "title": "Towards a Unified View of Large Language Model Post-Training",
      "authors": [
        "Xingtai Lv",
        "Yuxin Zuo",
        "Youbang Sun",
        "Hongyi Liu",
        "Yuntian Wei",
        "Zhekai Chen",
        "Lixuan He",
        "Xuekai Zhu",
        "Kaiyan Zhang",
        "Bingning Wang",
        "Ning Ding",
        "Bowen Zhou"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Two major sources of training data exist for post-training modern language\nmodels: online (model-generated rollouts) data, and offline (human or\nother-model demonstrations) data. These two types of data are typically used by\napproaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),\nrespectively. In this paper, we show that these approaches are not in\ncontradiction, but are instances of a single optimization process. We derive a\nUnified Policy Gradient Estimator, and present the calculations of a wide\nspectrum of post-training approaches as the gradient of a common objective\nunder different data distribution assumptions and various bias-variance\ntradeoffs. The gradient estimator is constructed with four interchangeable\nparts: stabilization mask, reference policy denominator, advantage estimate,\nand likelihood gradient. Motivated by our theoretical findings, we propose\nHybrid Post-Training (HPT), an algorithm that dynamically selects different\ntraining signals. HPT is designed to yield both effective exploitation of\ndemonstration and stable exploration without sacrificing learned reasoning\npatterns. We provide extensive experiments and ablation studies to verify the\neffectiveness of our unified theoretical framework and HPT. Across six\nmathematical reasoning benchmarks and two out-of-distribution suites, HPT\nconsistently surpasses strong baselines across models of varying scales and\nfamilies.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04419v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04419v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.501,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.499,
      "distributed_training_score": 0.451,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses Reinforcement Learning (RL) for post-training language models, which could involve human demonstrations in Supervised Fine-Tuning (SFT), but it does not specifically focus on training a separate reward model from human-ranked data as required for RLHF. Instead, it unifies RL and SFT under a common framework, treating RL as using online data without emphasizing human feedback. Thus, while RL is mentioned, the paper does not align with the core elements of RLHF.",
      "weak_supervision_justification": "The paper focuses on unifying RL and SFT for post-training language models using offline demonstrations, but it does not involve programmatically generating noisy or imprecise labels, which is the essence of weak supervision. There is no discussion of alternative labeling methods or reliance on imperfect data sources.",
      "diffusion_reasoning_justification": "The paper addresses post-training techniques like RL and SFT for language models, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It focuses on policy gradients and hybrid optimization, which are unrelated to diffusion-based approaches.",
      "distributed_training_justification": "The paper's main contribution is a unified framework for RL and SFT in post-training, with no discussion of parallel computing, multi-node systems, or strategies for partitioning data or computation across processors. It concentrates on algorithmic aspects rather than training infrastructure.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04434",
      "title": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer",
      "authors": [
        "Hyunsoo Cha",
        "Byungjun Kim",
        "Hanbyul Joo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present Durian, the first method for generating portrait animation videos\nwith facial attribute transfer from a given reference image to a target\nportrait in a zero-shot manner. To enable high-fidelity and spatially\nconsistent attribute transfer across frames, we introduce dual reference\nnetworks that inject spatial features from both the portrait and attribute\nimages into the denoising process of a diffusion model. We train the model\nusing a self-reconstruction formulation, where two frames are sampled from the\nsame portrait video: one is treated as the attribute reference and the other as\nthe target portrait, and the remaining frames are reconstructed conditioned on\nthese inputs and their corresponding masks. To support the transfer of\nattributes with varying spatial extent, we propose a mask expansion strategy\nusing keypoint-conditioned image generation for training. In addition, we\nfurther augment the attribute and portrait images with spatial and\nappearance-level transformations to improve robustness to positional\nmisalignment between them. These strategies allow the model to effectively\ngeneralize across diverse attributes and in-the-wild reference combinations,\ndespite being trained without explicit triplet supervision. Durian achieves\nstate-of-the-art performance on portrait animation with attribute transfer, and\nnotably, its dual reference design enables multi-attribute composition in a\nsingle generation pass without additional training.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04434v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04434v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.311,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models for generating portrait animations and attribute transfers, focusing on image and video synthesis through iterative denoising. However, it does not adapt diffusion for complex logical tasks, such as treating a Chain-of-Thought as a single entity for multi-step reasoning. The work is centered on visual generation, lacking any component for logical inference or reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04437",
      "title": "From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray\n  Collimators via Hough Transform",
      "authors": [
        "Benjamin El-Zein",
        "Dominik Eckert",
        "Andreas Fieselmann",
        "Christopher Syben",
        "Ludwig Ritschl",
        "Steffen Kappler",
        "Sebastian Stober"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Collimation in X-ray imaging restricts exposure to the region-of-interest\n(ROI) and minimizes the radiation dose applied to the patient. The detection of\ncollimator shadows is an essential image-based preprocessing step in digital\nradiography posing a challenge when edges get obscured by scattered X-ray\nradiation. Regardless, the prior knowledge that collimation forms\npolygonal-shaped shadows is evident. For this reason, we introduce a deep\nlearning-based segmentation that is inherently constrained to its geometry. We\nachieve this by incorporating a differentiable Hough transform-based network to\ndetect the collimation borders and enhance its capability to extract the\ninformation about the ROI center. During inference, we combine the information\nof both tasks to enable the generation of refined, line-constrained\nsegmentation masks. We demonstrate robust reconstruction of collimated regions\nachieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real\nXray images. While this application involves at most four shadow borders, our\nmethod is not fundamentally limited by a specific number of edges.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04437v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04437v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.317,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04438",
      "title": "The Telephone Game: Evaluating Semantic Drift in Unified Models",
      "authors": [
        "Sabbir Mollah",
        "Rohit Gupta",
        "Sirnam Swetha",
        "Qingyang Liu",
        "Ahnaf Munir",
        "Mubarak Shah"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Employing a single, unified model (UM) for both visual understanding\n(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened\na new direction in Visual Language Model (VLM) research. While UMs can also\nsupport broader unimodal tasks (e.g., text-to-text, image-to-image), we focus\non the core cross-modal pair T2I and I2T, as consistency between understanding\nand generation is critical for downstream use. Existing evaluations consider\nthese capabilities in isolation: FID and GenEval for T2I, and benchmarks such\nas MME, MMBench for I2T. These single-pass metrics do not reveal whether a\nmodel that understands a concept can also render it, nor whether meaning is\npreserved when cycling between image and text modalities. To address this, we\nintroduce the Unified Consistency Framework for Unified Models (UCF-UM), a\ncyclic evaluation protocol that alternates I2T and T2I over multiple\ngenerations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean\nCumulative Drift (MCD), an embedding-based measure of overall semantic loss;\n(ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii)\nMulti-Generation GenEval (MGG), an object-level compliance score extending\nGenEval. To assess generalization beyond COCO, which is widely used in\ntraining; we create a new benchmark ND400, sampled from NoCaps and DOCCI and\nevaluate on seven recent models. UCF-UM reveals substantial variation in\ncross-modal stability: some models like BAGEL maintain semantics over many\nalternations, whereas others like Vila-u drift quickly despite strong\nsingle-pass scores. Our results highlight cyclic consistency as a necessary\ncomplement to standard I2T and T2I evaluations, and provide practical metrics\nto consistently assess unified model's cross-modal stability and strength of\ntheir shared representations. Code:\nhttps://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04438v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04438v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.481,
      "distributed_training_score": 0.323,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating semantic drift in unified visual-language models through cyclic image-to-text and text-to-image conversions, introducing metrics to measure consistency. It does not involve adapting diffusion processes for multi-step logical reasoning or treating Chain-of-Thought as a holistic entity. While some evaluated models may use diffusion for generation, the paper's core contribution is evaluation-based, not on diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04439",
      "title": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory",
      "authors": [
        "Matthew Ho",
        "Chen Si",
        "Zhaoxiang Feng",
        "Fangxu Yu",
        "Yichi Yang",
        "Zhijian Liu",
        "Zhiting Hu",
        "Lianhui Qin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "While inference-time scaling enables LLMs to carry out increasingly long and\ncapable reasoning traces, the patterns and insights uncovered during these\ntraces are immediately discarded once the context window is reset for a new\nquery. External memory is a natural way to persist these discoveries, and\nrecent work has shown clear benefits for reasoning-intensive tasks. We see an\nopportunity to make such memories more broadly reusable and scalable by moving\nbeyond instance-based memory entries (e.g. exact query/response pairs, or\nsummaries tightly coupled with the original problem context) toward\nconcept-level memory: reusable, modular abstractions distilled from solution\ntraces and stored in natural language. For future queries, relevant concepts\nare selectively retrieved and integrated into the prompt, enabling test-time\ncontinual learning without weight updates. Our design introduces new strategies\nfor abstracting takeaways from rollouts and retrieving entries for new queries,\npromoting reuse and allowing memory to expand with additional experiences. We\nevaluate on ARC-AGI, a benchmark that stresses compositional generalization and\nabstract reasoning, making it a natural fit for concept memory. Our method\nyields a 7.5% relative gain over a strong no-memory baseline with performance\ncontinuing to scale with inference compute. We find abstract concepts to be the\nmost consistent memory design, outscoring the baseline at all tested inference\ncompute scales. Moreover, dynamically updating memory during test-time\noutperforms fixed settings, supporting the hypothesis that accumulating and\nabstracting patterns enables further solutions in a form of self-improvement.\nCode is available at https://github.com/matt-seb-ho/arc_memo.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04439v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04439v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.513,
      "distributed_training_score": 0.337,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing LLMs with external memory for abstract reasoning and compositional generalization, specifically through concept-level memory storage and retrieval. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for logical tasks. Therefore, there is no connection to the topic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04441",
      "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation",
      "authors": [
        "Hao-Shu Fang",
        "Branden Romero",
        "Yichen Xie",
        "Arthur Hu",
        "Bo-Ruei Huang",
        "Juan Alvarez",
        "Matthew Kim",
        "Gabriel Margolis",
        "Kavya Anbarasu",
        "Masayoshi Tomizuka",
        "Edward Adelson",
        "Pulkit Agrawal"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04441v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04441v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.36,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04442",
      "title": "Delta Activations: A Representation for Finetuned Large Language Models",
      "authors": [
        "Zhiqiu Xu",
        "Amish Sethi",
        "Mayur Naik",
        "Ser-Nam Lim"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04442v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04442v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.464,
      "distributed_training_score": 0.442,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses finetuned LLMs, including those optimized via preference alignment techniques (e.g., citing works on RLHF), but its main contribution is a representation method for model embeddings, not the RLHF process itself. It only indirectly relates by considering models that might have been fine-tuned with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on representing finetuned LLMs using activation shifts and does not involve diffusion models, iterative refinement for reasoning, or multi-step logical processes as described in diffusion-based reasoning.",
      "distributed_training_justification": "The paper addresses model representation and clustering of finetuned LLMs, with no discussion of distributed training, parallel computing, or strategies for accelerating training across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04444",
      "title": "One Flight Over the Gap: A Survey from Perspective to Panoramic Vision",
      "authors": [
        "Xin Lin",
        "Xian Ge",
        "Dizhe Zhang",
        "Zhaoliang Wan",
        "Xianshun Wang",
        "Xiangtai Li",
        "Wenjie Jiang",
        "Bo Du",
        "Dacheng Tao",
        "Ming-Hsuan Yang",
        "Lu Qi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Driven by the demand for spatial intelligence and holistic scene perception,\nomnidirectional images (ODIs), which provide a complete 360\\textdegree{} field\nof view, are receiving growing attention across diverse applications such as\nvirtual reality, autonomous driving, and embodied robotics. Despite their\nunique characteristics, ODIs exhibit remarkable differences from perspective\nimages in geometric projection, spatial distribution, and boundary continuity,\nmaking it challenging for direct domain adaption from perspective methods. This\nsurvey reviews recent panoramic vision techniques with a particular emphasis on\nthe perspective-to-panorama adaptation. We first revisit the panoramic imaging\npipeline and projection methods to build the prior knowledge required for\nanalyzing the structural disparities. Then, we summarize three challenges of\ndomain adaptation: severe geometric distortions near the poles, non-uniform\nsampling in Equirectangular Projection (ERP), and periodic boundary continuity.\nBuilding on this, we cover 20+ representative tasks drawn from more than 300\nresearch papers in two dimensions. On one hand, we present a cross-method\nanalysis of representative strategies for addressing panoramic specific\nchallenges across different tasks. On the other hand, we conduct a cross-task\ncomparison and classify panoramic vision into four major categories: visual\nquality enhancement and assessment, visual understanding, multimodal\nunderstanding, and visual generation. In addition, we discuss open challenges\nand future directions in data, models, and applications that will drive the\nadvancement of panoramic vision research. We hope that our work can provide new\ninsight and forward looking perspectives to advance the development of\npanoramic vision technologies. Our project page is\nhttps://insta360-research-team.github.io/Survey-of-Panorama",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04444v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04444v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.323,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04446",
      "title": "Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing\n  with Text-to-Image Diffusion Models",
      "authors": [
        "Kiymet Akdemir",
        "Jing Shi",
        "Kushal Kafle",
        "Brian Price",
        "Pinar Yanardag"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-image diffusion models have demonstrated significant capabilities to\ngenerate diverse and detailed visuals in various domains, and story\nvisualization is emerging as a particularly promising application. However, as\ntheir use in real-world creative domains increases, the need for providing\nenhanced control, refinement, and the ability to modify images post-generation\nin a consistent manner becomes an important challenge. Existing methods often\nlack the flexibility to apply fine or coarse edits while maintaining visual and\nnarrative consistency across multiple frames, preventing creators from\nseamlessly crafting and refining their visual stories. To address these\nchallenges, we introduce Plot'n Polish, a zero-shot framework that enables\nconsistent story generation and provides fine-grained control over story\nvisualizations at various levels of detail.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04446v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04446v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.534,
      "distributed_training_score": 0.313,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using text-to-image diffusion models for story visualization and editing, leveraging the iterative refinement process of diffusion for image generation and modifications. However, it does not adapt this process to solve complex logical tasks or treat a Chain-of-Thought as a single entity for holistic correction. Instead, the emphasis is on visual consistency and narrative coherence, making it only loosely related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04448",
      "title": "TRUST-VL: An Explainable News Assistant for General Multimodal\n  Misinformation Detection",
      "authors": [
        "Zehong Yan",
        "Peng Qi",
        "Wynne Hsu",
        "Mong Li Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Multimodal misinformation, encompassing textual, visual, and cross-modal\ndistortions, poses an increasing societal threat that is amplified by\ngenerative AI. Existing methods typically focus on a single type of distortion\nand struggle to generalize to unseen scenarios. In this work, we observe that\ndifferent distortion types share common reasoning capabilities while also\nrequiring task-specific skills. We hypothesize that joint training across\ndistortion types facilitates knowledge sharing and enhances the model's ability\nto generalize. To this end, we introduce TRUST-VL, a unified and explainable\nvision-language model for general multimodal misinformation detection. TRUST-VL\nincorporates a novel Question-Aware Visual Amplifier module, designed to\nextract task-specific visual features. To support training, we also construct\nTRUST-Instruct, a large-scale instruction dataset containing 198K samples\nfeaturing structured reasoning chains aligned with human fact-checking\nworkflows. Extensive experiments on both in-domain and zero-shot benchmarks\ndemonstrate that TRUST-VL achieves state-of-the-art performance, while also\noffering strong generalization and interpretability.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04448v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04448v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.344,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a unified vision-language model for multimodal misinformation detection, incorporating modules like the Question-Aware Visual Amplifier and structured reasoning chains from a dataset. However, it does not involve diffusion-based methods, iterative refinement processes, or any adaptation of diffusion models for logical reasoning tasks. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction over multiple steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04449",
      "title": "ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset",
      "authors": [
        "Adrian Catalin Lutu",
        "Ioana Pintilie",
        "Elena Burceanu",
        "Andrei Manolache"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present ChronoGraph, a graph-structured multivariate time series\nforecasting dataset built from real-world production microservices. Each node\nis a service that emits a multivariate stream of system-level performance\nmetrics, capturing CPU, memory, and network usage patterns, while directed\nedges encode dependencies between services. The primary task is forecasting\nfuture values of these signals at the service level. In addition, ChronoGraph\nprovides expert-annotated incident windows as anomaly labels, enabling\nevaluation of anomaly detection methods and assessment of forecast robustness\nduring operational disruptions. Compared to existing benchmarks from industrial\ncontrol systems or traffic and air-quality domains, ChronoGraph uniquely\ncombines (i) multivariate time series, (ii) an explicit, machine-readable\ndependency graph, and (iii) anomaly labels aligned with real incidents. We\nreport baseline results spanning forecasting models, pretrained time-series\nfoundation models, and standard anomaly detectors. ChronoGraph offers a\nrealistic benchmark for studying structure-aware forecasting and incident-aware\nevaluation in microservice systems.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04449v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04449v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.299,
      "distributed_training_score": 0.369,
      "datasets_score": 0.391,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04450",
      "title": "Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual\n  Try-On from a Single Image -- Technical Preview",
      "authors": [
        "Jun-Kun Chen",
        "Aayush Bansal",
        "Minh Phuoc Vo",
        "Yu-Xiong Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We introduce the Virtual Fitting Room (VFR), a novel video generative model\nthat produces arbitrarily long virtual try-on videos. Our VFR models long video\ngeneration tasks as an auto-regressive, segment-by-segment generation process,\neliminating the need for resource-intensive generation and lengthy video data,\nwhile providing the flexibility to generate videos of arbitrary length. The key\nchallenges of this task are twofold: ensuring local smoothness between adjacent\nsegments and maintaining global temporal consistency across different segments.\nTo address these challenges, we propose our VFR framework, which ensures\nsmoothness through a prefix video condition and enforces consistency with the\nanchor video -- a 360-degree video that comprehensively captures the human's\nwholebody appearance. Our VFR generates minute-scale virtual try-on videos with\nboth local smoothness and global temporal consistency under various motions,\nmaking it a pioneering work in long virtual try-on video generation.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04450v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04450v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.278,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.292,
      "datasets_score": 0.253,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04534",
      "title": "Quantized Large Language Models in Biomedical Natural Language\n  Processing: Evaluation and Recommendation",
      "authors": [
        "Zaifu Zhan",
        "Shuang Zhou",
        "Min Zeng",
        "Kai Yu",
        "Meijia Song",
        "Xiaoyi Chen",
        "Jun Wang",
        "Yu Hou",
        "Rui Zhang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models have demonstrated remarkable capabilities in biomedical\nnatural language processing, yet their rapid growth in size and computational\nrequirements present a major barrier to adoption in healthcare settings where\ndata privacy precludes cloud deployment and resources are limited. In this\nstudy, we systematically evaluated the impact of quantization on 12\nstate-of-the-art large language models, including both general-purpose and\nbiomedical-specific models, across eight benchmark datasets covering four key\ntasks: named entity recognition, relation extraction, multi-label\nclassification, and question answering. We show that quantization substantially\nreduces GPU memory requirements-by up to 75%-while preserving model performance\nacross diverse tasks, enabling the deployment of 70B-parameter models on 40GB\nconsumer-grade GPUs. In addition, domain-specific knowledge and responsiveness\nto advanced prompting methods are largely maintained. These findings provide\nsignificant practical and guiding value, highlighting quantization as a\npractical and effective strategy for enabling the secure, local deployment of\nlarge yet high-capacity language models in biomedical contexts, bridging the\ngap between technical advances in AI and real-world clinical translation.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04534v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04534v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.439,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating quantization techniques for large language models in biomedical NLP, emphasizing efficiency in deployment and performance on tasks like named entity recognition. It does not involve diffusion models, iterative refinement processes, or any multi-step logical reasoning mechanisms, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper's main contribution is the assessment of quantization for reducing memory and computational demands during inference of large language models in biomedical contexts. It does not address distributed training, parallel computing, or strategies for accelerating model training across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04535",
      "title": "In-Context Policy Adaptation via Cross-Domain Skill Diffusion",
      "authors": [
        "Minjong Yoo",
        "Woo Kyung Kim",
        "Honguk Woo"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In this work, we present an in-context policy adaptation (ICPAD) framework\ndesigned for long-horizon multi-task environments, exploring diffusion-based\nskill learning techniques in cross-domain settings. The framework enables rapid\nadaptation of skill-based reinforcement learning policies to diverse target\ndomains, especially under stringent constraints on no model updates and only\nlimited target domain data. Specifically, the framework employs a cross-domain\nskill diffusion scheme, where domain-agnostic prototype skills and a\ndomain-grounded skill adapter are learned jointly and effectively from an\noffline dataset through cross-domain consistent diffusion processes. The\nprototype skills act as primitives for common behavior representations of\nlong-horizon policies, serving as a lingua franca to bridge different domains.\nFurthermore, to enhance the in-context adaptation performance, we develop a\ndynamic domain prompting scheme that guides the diffusion-based skill adapter\ntoward better alignment with the target domain. Through experiments with\nrobotic manipulation in Metaworld and autonomous driving in CARLA, we show that\nour $\\oursol$ framework achieves superior policy adaptation performance under\nlimited target domain data conditions for various cross-domain configurations\nincluding differences in environment dynamics, agent embodiment, and task\nhorizon.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04535v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04535v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.398,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on in-context policy adaptation using diffusion-based skill learning in reinforcement learning for cross-domain settings, without any mention of human feedback, human-ranked data, or a reward model trained on human preferences. It relies on offline datasets and limited target data from simulated environments, not aligning with RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs diffusion processes for skill learning and policy adaptation in reinforcement learning, such as in the cross-domain skill diffusion scheme, which involves iterative refinement. However, this is applied to sequential decision-making tasks like robotic manipulation and autonomous driving, not to multi-step logical reasoning or holistic correction of a Chain-of-Thought, lacking the core elements of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04537",
      "title": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem",
      "authors": [
        "Ryosuke Takata",
        "Atsushi Masumori",
        "Takashi Ikegami"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "We investigate the emergent social dynamics of Large Language Model (LLM)\nagents in a spatially extended El Farol Bar problem, observing how they\nautonomously navigate this classic social dilemma. As a result, the LLM agents\ngenerated a spontaneous motivation to go to the bar and changed their decision\nmaking by becoming a collective. We also observed that the LLM agents did not\nsolve the problem completely, but rather behaved more like humans. These\nfindings reveal a complex interplay between external incentives\n(prompt-specified constraints such as the 60% threshold) and internal\nincentives (culturally-encoded social preferences derived from pre-training),\ndemonstrating that LLM agents naturally balance formal game-theoretic\nrationality with social motivations that characterize human behavior. These\nfindings suggest that a new model of group decision making, which could not be\nhandled in the previous game-theoretic problem setting, can be realized by LLM\nagents.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04537v3",
      "pdf_url": "http://arxiv.org/pdf/2509.04537v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.295,
      "datasets_score": 0.279,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained LLM agents in a simulated social dilemma without any mention of training processes involving human feedback, reward models, or reinforcement learning for alignment. It discusses emergent behaviors from existing LLMs, not RLHF techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines LLM agents in a game-theoretic scenario but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It centers on social dynamics and decision-making, not holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04544",
      "title": "i-Mask: An Intelligent Mask for Breath-Driven Activity Recognition",
      "authors": [
        "Ashutosh Kumar Sinha",
        "Ayush Patel",
        "Mitul Dudhat",
        "Pritam Anand",
        "Rahul Mishra"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The patterns of inhalation and exhalation contain important physiological\nsignals that can be used to anticipate human behavior, health trends, and vital\nparameters. Human activity recognition (HAR) is fundamentally connected to\nthese vital signs, providing deeper insights into well-being and enabling\nreal-time health monitoring. This work presents i-Mask, a novel HAR approach\nthat leverages exhaled breath patterns captured using a custom-developed mask\nequipped with integrated sensors. Data collected from volunteers wearing the\nmask undergoes noise filtering, time-series decomposition, and labeling to\ntrain predictive models. Our experimental results validate the effectiveness of\nthe approach, achieving over 95\\% accuracy and highlighting its potential in\nhealthcare and fitness applications.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04544v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04544v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.289,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04545",
      "title": "PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via\n  Chain-of-Thought Prompt Rewriting",
      "authors": [
        "Linqing Wang",
        "Ximing Xing",
        "Yiji Cheng",
        "Zhiyuan Zhao",
        "Donghao Li",
        "Tiankai Hang",
        "Jiale Tao",
        "Qixun Wang",
        "Ruihuang Li",
        "Comi Chen",
        "Xin Li",
        "Mingrui Wu",
        "Xinchi Deng",
        "Shuyang Gu",
        "Chunyu Wang",
        "Qinglin Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advancements in text-to-image (T2I) diffusion models have demonstrated\nremarkable capabilities in generating high-fidelity images. However, these\nmodels often struggle to faithfully render complex user prompts, particularly\nin aspects like attribute binding, negation, and compositional relationships.\nThis leads to a significant mismatch between user intent and the generated\noutput. To address this challenge, we introduce PromptEnhancer, a novel and\nuniversal prompt rewriting framework that enhances any pretrained T2I model\nwithout requiring modifications to its weights. Unlike prior methods that rely\non model-specific fine-tuning or implicit reward signals like image-reward\nscores, our framework decouples the rewriter from the generator. We achieve\nthis by training a Chain-of-Thought (CoT) rewriter through reinforcement\nlearning, guided by a dedicated reward model we term the AlignEvaluator. The\nAlignEvaluator is trained to provide explicit and fine-grained feedback based\non a systematic taxonomy of 24 key points, which are derived from a\ncomprehensive analysis of common T2I failure modes. By optimizing the CoT\nrewriter to maximize the reward from our AlignEvaluator, our framework learns\nto generate prompts that are more precisely interpreted by T2I models.\nExtensive experiments on the HunyuanImage 2.1 model demonstrate that\nPromptEnhancer significantly improves image-text alignment across a wide range\nof semantic and compositional challenges. Furthermore, we introduce a new,\nhigh-quality human preference benchmark to facilitate future research in this\ndirection.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04545v5",
      "pdf_url": "http://arxiv.org/pdf/2509.04545v5",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.51,
      "distributed_training_score": 0.334,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves training a Chain-of-Thought rewriter using reinforcement learning, guided by a reward model (AlignEvaluator) that is based on a taxonomy derived from human analysis of T2I failures. This aligns closely with RLHF, as the reward model provides explicit feedback from human-preference data to fine-tune the rewriter, optimizing for human-aligned prompts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Chain-of-Thought rewriter for prompt enhancement in text-to-image models, but it does not adapt the iterative refinement process of diffusion for multi-step logical reasoning tasks. There is no indication that diffusion is used for reasoning; it is only mentioned in the context of image generation.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "PromptEnhancer is a universal framework designed to improve text-to-image (T2I) diffusion models by rewriting user prompts using a Chain-of-Thought (CoT) approach, which is trained via reinforcement learning with a new reward model called AlignEvaluator that evaluates based on 24 key points covering various failure modes. This method enhances prompt clarity and model performance without modifying the T2I model's weights, as demonstrated through experiments on the HunyuanImage 2.1 model that show significant improvements in image-text alignment, and it introduces a new human preference benchmark for future research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of Chain-of-Thought reasoning and reinforcement learning to address prompt issues in T2I models, offering a notable improvement over existing methods without requiring model fine-tuning. While it builds on established techniques, it introduces a new reward model with 24 key points, making it a practical advancement rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in text-to-image generation by providing a versatile prompt rewriting framework and a new benchmark, potentially improving practical applications in AI content creation. However, its impact may be confined to specific subfields like computer vision, limiting broader adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality, practical contribution that addresses a key limitation in T2I models, making it valuable for researchers working on generative AI. While not essential for all, it offers insights and tools that could enhance ongoing work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/70a0f72b06327998b9274a5d7da6f89159b75d16",
      "total_authors": 16,
      "authors_found": 15,
      "highest_h_index": 6,
      "average_h_index": 1.4,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Linqing Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355930541"
        },
        {
          "name": "Ximing Xing",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379640106"
        },
        {
          "name": "Yiji Cheng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2352776123"
        },
        {
          "name": "Zhiyuan Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380250554"
        },
        {
          "name": "Donghao Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2345323647"
        },
        {
          "name": "Tiankai Hang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2141507950"
        },
        {
          "name": "Jiale Tao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356376838"
        },
        {
          "name": "Qixun Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2362517658"
        },
        {
          "name": "Ruihuang Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356100198"
        },
        {
          "name": "Comi Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2362426722"
        },
        {
          "name": "Xin Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379643032"
        },
        {
          "name": "Mingrui Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379759997"
        },
        {
          "name": "Xinchi Deng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2291203760"
        },
        {
          "name": "Shuyang Gu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chunyu Wang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2276270775"
        },
        {
          "name": "Qinglin Lu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365376135"
        }
      ]
    },
    {
      "id": "2509.04548",
      "title": "Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified\n  Multimodal Model",
      "authors": [
        "Hongyang Wei",
        "Baixin Xu",
        "Hongbo Liu",
        "Cyrus Wu",
        "Jie Liu",
        "Yi Peng",
        "Peiyu Wang",
        "Zexiang Liu",
        "Jingwen He",
        "Yidan Xietian",
        "Chuanxin Tang",
        "Zidong Wang",
        "Yichen Wei",
        "Liang Hu",
        "Boyi Jiang",
        "William Li",
        "Ying He",
        "Yang Liu",
        "Xuchen Song",
        "Eric Li",
        "Yahui Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in multimodal models have demonstrated impressive\ncapabilities in unified image generation and editing. However, many prominent\nopen-source models prioritize scaling model parameters over optimizing training\nstrategies, limiting their efficiency and performance. In this work, we present\nUniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which\nachieves state-of-the-art image generation and editing while extending\nseamlessly into a unified multimodal framework. Our approach begins with\narchitectural modifications to SD3.5-Medium and large-scale pre-training on\nhigh-quality data, enabling joint text-to-image generation and editing\ncapabilities. To enhance instruction following and editing consistency, we\npropose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which\neffectively strengthens both tasks in a staged manner. We empirically validate\nthat the reinforcement phases for different tasks are mutually beneficial and\ndo not induce negative interference. After pre-training and reinforcement\nstrategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and\nediting capabilities than models with significantly larger generation\nparameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following\nthe MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a\nconnector and perform joint training to launch a unified multimodal model\nUniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and\nediting, achieving top-tier performance across diverse tasks with a simple and\nscalable training paradigm. This consistently validates the effectiveness and\ngeneralizability of our proposed training paradigm, which we formalize as\nSkywork UniPic 2.0.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04548v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04548v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.429,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs Progressive Dual-Task Reinforcement (PDTR) using online reinforcement learning with reward models like Skywork-EditReward and GPT-4.1, which may incorporate elements of human preferences. However, it does not explicitly describe training a separate reward model on human-ranked data, focusing instead on self-trained and external evaluators, making it only moderately aligned with RLHF definitions.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a Diffusion Transformer (DiT) model for image generation and editing, but it does not adapt diffusion for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for iterative refinement. The focus is on visual tasks, not complex logical problem-solving.",
      "distributed_training_justification": "The paper mentions large-scale pre-training on high-quality data but does not discuss specific algorithms, systems, or strategies for distributed training, parallel computing, or partitioning across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper presents Skywork UniPic 2.0, a lightweight 2B-parameter multimodal model based on SD3.5-Medium, designed to unify image generation, editing, and understanding through architectural modifications, large-scale pre-training, and a novel Progressive Dual-Task Reinforcement (PDTR) strategy. The methodology involves enhancing instruction following and editing consistency via PDTR, integrating with Qwen2.5-VL-7B for broader capabilities, and demonstrating superior performance over larger models like BAGEL (7B) and Flux-Kontext (12B) in benchmarks, thus advancing efficient and scalable multimodal intelligence.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces PDTR, a novel reinforcement learning strategy for synergistically optimizing image generation and editing without interference, representing a significant advancement in unified multimodal modeling. This addresses a previously unexplored challenge in multitask RL for these tasks, pushing the state-of-the-art forward.",
      "impact_score": "Moderate",
      "impact_justification": "The work's demonstration of a lightweight model outperforming larger ones could influence research in efficient multimodal systems and inspire similar training paradigms in subfields like computer vision. However, its impact is primarily confined to specific areas of AI rather than broad commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable innovations in efficient multimodal modeling and reinforcement learning strategies, making it a significant contribution for researchers in AI and computer vision. While not essential for all, it provides insights that could guide future work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4912a5fdaf4d04e55cefb343608059584fbb6637",
      "total_authors": 21,
      "authors_found": 21,
      "highest_h_index": 8,
      "average_h_index": 1.3333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Hongyang Wei",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2221231512"
        },
        {
          "name": "Baixin Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375392473"
        },
        {
          "name": "Hongbo Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2240188482"
        },
        {
          "name": "Cyrus Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376206595"
        },
        {
          "name": "Jie Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380078677"
        },
        {
          "name": "Yi Peng",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2354283589"
        },
        {
          "name": "Peiyu Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2365442774"
        },
        {
          "name": "Zexiang Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376510137"
        },
        {
          "name": "Jingwen He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380017595"
        },
        {
          "name": "Yidan Xietian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379549307"
        },
        {
          "name": "Chuanxin Tang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2375061951"
        },
        {
          "name": "Zidong Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354868495"
        },
        {
          "name": "Yichen Wei",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2354517832"
        },
        {
          "name": "Liang Hu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2375721908"
        },
        {
          "name": "Boyi Jiang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380442380"
        },
        {
          "name": "William Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379602538"
        },
        {
          "name": "Ying He",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2288007172"
        },
        {
          "name": "Yang Liu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2354287011"
        },
        {
          "name": "Xuchen Song",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2354290518"
        },
        {
          "name": "Eric Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2375390476"
        },
        {
          "name": "Yahui Zhou",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2263493641"
        }
      ]
    },
    {
      "id": "2509.04549",
      "title": "Manipulating Transformer-Based Models: Controllability, Steerability,\n  and Robust Interventions",
      "authors": [
        "Faruk Alpay",
        "Taylan Alpay"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Transformer-based language models excel in NLP tasks, but fine-grained\ncontrol remains challenging. This paper explores methods for manipulating\ntransformer models through principled interventions at three levels: prompts,\nactivations, and weights. We formalize controllable text generation as an\noptimization problem addressable via prompt engineering, parameter-efficient\nfine-tuning, model editing, and reinforcement learning. We introduce a unified\nframework encompassing prompt-level steering, activation interventions, and\nweight-space edits. We analyze robustness and safety implications, including\nadversarial attacks and alignment mitigations. Theoretically, we show minimal\nweight updates can achieve targeted behavior changes with limited side-effects.\nEmpirically, we demonstrate >90% success in sentiment control and factual edits\nwhile preserving base performance, though generalization-specificity trade-offs\nexist. We discuss ethical dual-use risks and the need for rigorous evaluation.\nThis work lays groundwork for designing controllable and robust language\nmodels.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04549v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04549v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.468,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.355,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions reinforcement learning with human or AI-defined rewards as one of the techniques for steering model behavior, specifically under alignment via feedback, which aligns with RLHF. However, it is not the primary focus, as the paper covers a broader framework including prompts, activations, and weights, making RLHF a supporting element rather than the main contribution.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on manipulating transformer-based models through prompts, activations, weights, and related techniques like reinforcement learning, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. There is no component involving diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates methods for manipulating transformer-based language models to enhance controllability, steerability, and robustness by intervening at the levels of prompts, activations, and weights, formalizing controllable text generation as an optimization problem using techniques like prompt engineering, parameter-efficient fine-tuning, model editing, and reinforcement learning. The authors introduce a unified framework, provide theoretical analysis showing minimal weight updates can achieve targeted changes with limited side-effects, and demonstrate empirically over 90% success in tasks such as sentiment control and factual edits on models like GPT-2 and LLaMA-7B, while discussing ethical implications and dual-use risks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by unifying existing techniques into a systematic framework for model manipulation and providing new theoretical analysis, though it builds on known methods rather than introducing a entirely new problem or architecture. This clever combination addresses controllability in a more comprehensive way, advancing the state-of-the-art incrementally.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI safety and NLP due to its practical techniques for model steering and robustness analysis, potentially influencing research on controllable language models. However, its impact may be limited to specific applications rather than broadly across all AI or commercial sectors.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution with its unified framework and empirical results on model controllability, making it essential for researchers in AI and NLP to understand advancements in steering and safety. While not groundbreaking enough to be a must-read, it provides important insights that could guide future work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/277036f94d166649f7b11e633eba5d0e24eed8b1",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 2,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Faruk Alpay",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2366070098"
        },
        {
          "name": "Taylan Alpay",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2372232219"
        }
      ]
    },
    {
      "id": "2509.04582",
      "title": "Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing\n  via Bidirectional Warping",
      "authors": [
        "Jingyi Lu",
        "Kai Han"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Drag-based image editing has emerged as a powerful paradigm for intuitive\nimage manipulation. However, existing approaches predominantly rely on\nmanipulating the latent space of generative models, leading to limited\nprecision, delayed feedback, and model-specific constraints. Accordingly, we\npresent Inpaint4Drag, a novel framework that decomposes drag-based editing into\npixel-space bidirectional warping and image inpainting. Inspired by elastic\nobject deformation in the physical world, we treat image regions as deformable\nmaterials that maintain natural shape under user manipulation. Our method\nachieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at\n512x512 resolution, significantly improving the interaction experience compared\nto existing methods that require minutes per edit. By transforming drag inputs\ndirectly into standard inpainting formats, our approach serves as a universal\nadapter for any inpainting model without architecture modification,\nautomatically inheriting all future improvements in inpainting technology.\nExtensive experiments demonstrate that our method achieves superior visual\nquality and precise control while maintaining real-time performance. Project\npage: https://visual-ai.github.io/inpaint4drag/",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04582v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04582v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.29,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04588",
      "title": "Toward Faithfulness-guided Ensemble Interpretation of Neural Network",
      "authors": [
        "Siyu Zhang",
        "Kenneth Mcmillan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Interpretable and faithful explanations for specific neural inferences are\ncrucial for understanding and evaluating model behavior. Our work introduces\n\\textbf{F}aithfulness-guided \\textbf{E}nsemble \\textbf{I}nterpretation\n(\\textbf{FEI}), an innovative framework that enhances the breadth and\neffectiveness of faithfulness, advancing interpretability by providing superior\nvisualization. Through an analysis of existing evaluation benchmarks,\n\\textbf{FEI} employs a smooth approximation to elevate quantitative\nfaithfulness scores. Diverse variations of \\textbf{FEI} target enhanced\nfaithfulness in hidden layer encodings, expanding interpretability.\nAdditionally, we propose a novel qualitative metric that assesses hidden layer\nfaithfulness. In extensive experiments, \\textbf{FEI} surpasses existing\nmethods, demonstrating substantial advances in qualitative visualization and\nquantitative faithfulness scores. Our research establishes a comprehensive\nframework for elevating faithfulness in neural network explanations,\nemphasizing both breadth and precision",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04588v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04588v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.354,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on enhancing the interpretability and faithfulness of neural network explanations through methods like ensemble interpretation and gradient clipping, primarily in computer vision. It does not involve training models with human feedback, reward models, or reinforcement learning techniques, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a framework for improving neural network explanations using smooth approximations and ensemble methods, but it does not adapt diffusion models for multi-step logical reasoning or iterative refinement processes. There is no component involving diffusion-based techniques for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04597",
      "title": "DisPatch: Disarming Adversarial Patches in Object Detection with\n  Diffusion Models",
      "authors": [
        "Jin Ma",
        "Mohammed Aldeen",
        "Christopher Salas",
        "Feng Luo",
        "Mashrur Chowdhury",
        "Mert Pesé",
        "Long Cheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Object detection is fundamental to various real-world applications, such as\nsecurity monitoring and surveillance video analysis. Despite their\nadvancements, state-of-theart object detectors are still vulnerable to\nadversarial patch attacks, which can be easily applied to real-world objects to\neither conceal actual items or create non-existent ones, leading to severe\nconsequences. Given the current diversity of adversarial patch attacks and\npotential unknown threats, an ideal defense method should be effective,\ngeneralizable, and robust against adaptive attacks. In this work, we introduce\nDISPATCH, the first diffusion-based defense framework for object detection.\nUnlike previous works that aim to \"detect and remove\" adversarial patches,\nDISPATCH adopts a \"regenerate and rectify\" strategy, leveraging generative\nmodels to disarm attack effects while preserving the integrity of the input\nimage. Specifically, we utilize the in-distribution generative power of\ndiffusion models to regenerate the entire image, aligning it with benign data.\nA rectification process is then employed to identify and replace adversarial\nregions with their regenerated benign counterparts. DISPATCH is attack-agnostic\nand requires no prior knowledge of the existing patches. Extensive experiments\nacross multiple detectors and attacks demonstrate that DISPATCH consistently\noutperforms state-of-the-art defenses on both hiding attacks and creating\nattacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and\nlowering the attack success rate to 24.8% on untargeted creating attacks.\nMoreover, it maintains strong robustness against adaptive attacks, making it a\npractical and reliable defense for object detection systems.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04597v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04597v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.482,
      "distributed_training_score": 0.331,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for image regeneration and rectification in object detection defenses, leveraging their iterative refinement process to generate benign images. However, it does not apply this to multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks; instead, it focuses on visual data manipulation for security purposes. Thus, while diffusion models are involved, the application is not aligned with the topic's emphasis on reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04600",
      "title": "WATCH: World-aware Allied Trajectory and pose reconstruction for Camera\n  and Human",
      "authors": [
        "Qijun Ying",
        "Zhongyuan Hu",
        "Rui Zhang",
        "Ronghui Li",
        "Yu Lu",
        "Zijiao Zeng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Global human motion reconstruction from in-the-wild monocular videos is\nincreasingly demanded across VR, graphics, and robotics applications, yet\nrequires accurate mapping of human poses from camera to world coordinates-a\ntask challenged by depth ambiguity, motion ambiguity, and the entanglement\nbetween camera and human movements. While human-motion-centric approaches excel\nin preserving motion details and physical plausibility, they suffer from two\ncritical limitations: insufficient exploitation of camera orientation\ninformation and ineffective integration of camera translation cues. We present\nWATCH (World-aware Allied Trajectory and pose reconstruction for Camera and\nHuman), a unified framework addressing both challenges. Our approach introduces\nan analytical heading angle decomposition technique that offers superior\nefficiency and extensibility compared to existing geometric methods.\nAdditionally, we design a camera trajectory integration mechanism inspired by\nworld models, providing an effective pathway for leveraging camera translation\ninformation beyond naive hard-decoding approaches. Through experiments on\nin-the-wild benchmarks, WATCH achieves state-of-the-art performance in\nend-to-end trajectory reconstruction. Our work demonstrates the effectiveness\nof jointly modeling camera-human motion relationships and offers new insights\nfor addressing the long-standing challenge of camera translation integration in\nglobal human motion reconstruction. The code will be available publicly.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04600v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04600v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.316,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04601",
      "title": "Quantum-Enhanced Multi-Task Learning with Learnable Weighting for\n  Pharmacokinetic and Toxicity Prediction",
      "authors": [
        "Han Zhang",
        "Fengji Ma",
        "Jiamin Su",
        "Xinyue Yang",
        "Lei Wang",
        "Wen-Cai Ye",
        "Li Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Prediction for ADMET (Absorption, Distribution, Metabolism, Excretion, and\nToxicity) plays a crucial role in drug discovery and development, accelerating\nthe screening and optimization of new drugs. Existing methods primarily rely on\nsingle-task learning (STL), which often fails to fully exploit the\ncomplementarities between tasks. Besides, it requires more computational\nresources while training and inference of each task independently. To address\nthese issues, we propose a new unified Quantum-enhanced and task-Weighted\nMulti-Task Learning (QW-MTL) framework, specifically designed for ADMET\nclassification tasks. Built upon the Chemprop-RDKit backbone, QW-MTL adopts\nquantum chemical descriptors to enrich molecular representations with\nadditional information about the electronic structure and interactions.\nMeanwhile, it introduces a novel exponential task weighting scheme that\ncombines dataset-scale priors with learnable parameters to achieve dynamic loss\nbalancing across tasks. To the best of our knowledge, this is the first work to\nsystematically conduct joint multi-task training across all 13 Therapeutics\nData Commons (TDC) classification benchmarks, using leaderboard-style data\nsplits to ensure a standardized and realistic evaluation setting. Extensive\nexperimental results show that QW-MTL significantly outperforms single-task\nbaselines on 12 out of 13 tasks, achieving high predictive performance with\nminimal model complexity and fast inference, demonstrating the effectiveness\nand efficiency of multi-task molecular learning enhanced by quantum-informed\nfeatures and adaptive task weighting.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04601v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04601v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.409,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a quantum-enhanced multi-task learning framework for ADMET prediction, focusing on improving molecular representations and task weighting for better performance in drug discovery. It does not address distributed training, parallel computing, multi-node machine learning, or any strategies for partitioning data/computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04602",
      "title": "Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption\n  Retrieval for Dense Video Captioning",
      "authors": [
        "MinJu Jeon",
        "Si-Woo Kim",
        "Ye-Chan Kim",
        "HyunGee Kim",
        "Dong-Jin Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Dense video captioning aims to temporally localize events in video and\ngenerate captions for each event. While recent works propose end-to-end models,\nthey suffer from two limitations: (1) applying timestamp supervision only to\ntext while treating all video frames equally, and (2) retrieving captions from\nfixed-size video chunks, overlooking scene transitions. To address these, we\npropose Sali4Vid, a simple yet effective saliency-aware framework. We introduce\nSaliency-aware Video Reweighting, which converts timestamp annotations into\nsigmoid-based frame importance weights, and Semantic-based Adaptive Caption\nRetrieval, which segments videos by frame similarity to capture scene\ntransitions and improve caption retrieval. Sali4Vid achieves state-of-the-art\nresults on YouCook2 and ViTT, demonstrating the benefit of jointly improving\nvideo weighting and retrieval for dense video captioning",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04602v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04602v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.307,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04606",
      "title": "Sample-efficient Integration of New Modalities into Large Language\n  Models",
      "authors": [
        "Osman Batur İnce",
        "André F. T. Martins",
        "Oisin Mac Aodha",
        "Edoardo M. Ponti"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal foundation models can process several modalities. However, since\nthe space of possible modalities is large and evolving over time, training a\nmodel from scratch to encompass all modalities is unfeasible. Moreover,\nintegrating a modality into a pre-existing foundation model currently requires\na significant amount of paired data, which is often not available for\nlow-resource modalities. In this paper, we introduce a method for\nsample-efficient modality integration (SEMI) into Large Language Models (LLMs).\nTo this end, we devise a hypernetwork that can adapt a shared projector --\nplaced between modality-specific encoders and an LLM -- to any modality. The\nhypernetwork, trained on high-resource modalities (i.e., text, speech, audio,\nvideo), is conditioned on a few samples from any arbitrary modality at\ninference time to generate a suitable adapter. To increase the diversity of\ntraining modalities, we artificially multiply the number of encoders through\nisometric transformations. We find that SEMI achieves a significant boost in\nsample efficiency during few-shot integration of new modalities (i.e.,\nsatellite images, astronomical images, inertial measurements, and molecules)\nwith encoders of arbitrary embedding dimensionality. For instance, to reach the\nsame accuracy as 32-shot SEMI, training the projector from scratch needs\n64$\\times$ more data. As a result, SEMI holds promise to extend the modality\ncoverage of foundation models.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04606v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04606v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.404,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on sample-efficient integration of new modalities into LLMs using hypernetworks and few-shot learning, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper involves using minimal paired data for modality integration, which could loosely relate to handling limited or noisy supervision, but it does not center on programmatically generating labels from high-level sources, as is core to weak supervision.",
      "diffusion_reasoning_justification": "The paper deals with hypernetworks for adapting projectors in multimodal models and does not involve diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "The paper's main contribution is on efficient modality integration techniques, with no discussion of distributed systems, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04622",
      "title": "Measuring the Measures: Discriminative Capacity of Representational\n  Similarity Metrics Across Model Families",
      "authors": [
        "Jialin Wu",
        "Shreya Saha",
        "Yiqing Bo",
        "Meenakshi Khosla"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NE (Neural and Evolutionary Computing)"
      ],
      "abstract": "Representational similarity metrics are fundamental tools in neuroscience and\nAI, yet we lack systematic comparisons of their discriminative power across\nmodel families. We introduce a quantitative framework to evaluate\nrepresentational similarity measures based on their ability to separate model\nfamilies-across architectures (CNNs, Vision Transformers, Swin Transformers,\nConvNeXt) and training regimes (supervised vs. self-supervised). Using three\ncomplementary separability measures-dprime from signal detection theory,\nsilhouette coefficients and ROC-AUC, we systematically assess the\ndiscriminative capacity of commonly used metrics including RSA, linear\npredictivity, Procrustes, and soft matching. We show that separability\nsystematically increases as metrics impose more stringent alignment\nconstraints. Among mapping-based approaches, soft-matching achieves the highest\nseparability, followed by Procrustes alignment and linear predictivity.\nNon-fitting methods such as RSA also yield strong separability across families.\nThese results provide the first systematic comparison of similarity metrics\nthrough a separability lens, clarifying their relative sensitivity and guiding\nmetric choice for large-scale model and brain comparisons.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04622v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04622v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.367,
      "datasets_score": 0.415,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the evaluation of representational similarity metrics for distinguishing between model families, using various models and metrics. It does not involve creating, analyzing, benchmarking, or evaluating datasets for machine learning and AI applications; instead, it focuses on model representations and metric performance.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04624",
      "title": "UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle\n  Detection, Classification, Tracking, and Behavioral Analysis",
      "authors": [
        "Ali Khanpour",
        "Tianyi Wang",
        "Afra Vahidi-Shams",
        "Wim Ectors",
        "Farzam Nakhaie",
        "Amirhossein Taheri",
        "Christian Claudel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.ET (Emerging Technologies)",
        "cs.RO (Robotics)",
        "cs.SY (Systems and Control)",
        "eess.IV (Image and Video Processing)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Traffic congestion and violations pose significant challenges for urban\nmobility and road safety. Traditional traffic monitoring systems, such as fixed\ncameras and sensor-based methods, are often constrained by limited coverage,\nlow adaptability, and poor scalability. To address these challenges, this paper\nintroduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance\nsystem capable of accurate vehicle detection, classification, tracking, and\nbehavioral analysis in real-world, unconstrained urban environments. The system\nleverages multi-scale and multi-angle template matching, Kalman filtering, and\nhomography-based calibration to process aerial video data collected from\naltitudes of approximately 200 meters. A case study in urban area demonstrates\nrobust performance, achieving a detection precision of 91.8%, an F1-score of\n90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.\nBeyond precise detection, the system classifies five vehicle types and\nautomatically detects critical traffic violations, including unsafe lane\nchanges, illegal double parking, and crosswalk obstructions, through the fusion\nof geofencing, motion filtering, and trajectory deviation analysis. The\nintegrated analytics module supports origin-destination tracking, vehicle count\nvisualization, inter-class correlation analysis, and heatmap-based congestion\nmodeling. Additionally, the system enables entry-exit trajectory profiling,\nvehicle density estimation across road segments, and movement direction\nlogging, supporting comprehensive multi-scale urban mobility analytics.\nExperimental results confirms the system's scalability, accuracy, and practical\nrelevance, highlighting its potential as an enforcement-aware,\ninfrastructure-independent traffic monitoring solution for next-generation\nsmart cities.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04624v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04624v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.293,
      "distributed_training_score": 0.307,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04628",
      "title": "Action Chunking with Transformers for Image-Based Spacecraft Guidance\n  and Control",
      "authors": [
        "Alejandro Posadas-Nava",
        "Andrea Scorsoglio",
        "Luca Ghilardi",
        "Roberto Furfaro",
        "Richard Linares"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present an imitation learning approach for spacecraft guidance,\nnavigation, and control(GNC) that achieves high performance from limited data.\nUsing only 100 expert demonstrations, equivalent to 6,300 environment\ninteractions, our method, which implements Action Chunking with Transformers\n(ACT), learns a control policy that maps visual and state observations to\nthrust and torque commands. ACT generates smoother, more consistent\ntrajectories than a meta-reinforcement learning (meta-RL) baseline trained with\n40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking\nwith the International Space Station (ISS). We show that our approach achieves\ngreater accuracy, smoother control, and greater sample efficiency.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04628v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04628v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.334,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04632",
      "title": "Schema Inference for Tabular Data Repositories Using Large Language\n  Models",
      "authors": [
        "Zhenyu Wu",
        "Jiaoyan Chen",
        "Norman W. Paton"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Minimally curated tabular data often contain representational inconsistencies\nacross heterogeneous sources, and are accompanied by sparse metadata. Working\nwith such data is intimidating. While prior work has advanced dataset discovery\nand exploration, schema inference remains difficult when metadata are limited.\nWe present SI-LLM (Schema Inference using Large Language Models), which infers\na concise conceptual schema for tabular data using only column headers and cell\nvalues. The inferred schema comprises hierarchical entity types, attributes,\nand inter-type relationships. In extensive evaluation on two datasets from web\ntables and open data, SI-LLM achieves promising end-to-end results, as well as\nbetter or comparable results to state-of-the-art methods at each step. All\nsource code, full prompts, and datasets of SI-LLM are available at\nhttps://github.com/PierreWoL/SILLM.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04632v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04632v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.431,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.327,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces SI-LLM, a prompt-based framework using LLMs for schema inference without bespoke training or domain ontologies, relying on column headers and cell values. While LLMs are often trained on large, noisy datasets that align with weak supervision concepts, the paper itself does not involve programmatically generating labels for training a model. Instead, it focuses on direct inference via prompts, making the connection indirect rather than central.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04633",
      "title": "Scaling Environments for Organoid Intelligence with LLM-Automated Design\n  and Plasticity-Based Evaluation",
      "authors": [
        "Brennen Hill"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "As the complexity of artificial agents increases, the design of environments\nthat can effectively shape their behavior and capabilities has become a\ncritical research frontier. We propose a framework that extends this principle\nto a novel class of agents: biological neural networks in the form of neural\norganoids. This paper introduces three scalable, closed-loop virtual\nenvironments designed to train organoid-based biological agents and probe the\nunderlying mechanisms of learning, such as long-term potentiation (LTP) and\nlong-term depression (LTD). We detail the design of three distinct task\nenvironments with increasing complexity: (1) a conditional avoidance task, (2)\na one-dimensional predator-prey scenario, and (3) a replication of the classic\nPong game. For each environment, we formalize the state and action spaces, the\nsensory encoding and motor decoding mechanisms, and the feedback protocols\nbased on predictable (reward) and unpredictable (punishment) stimulation.\nFurthermore, we propose a novel meta-learning approach where a Large Language\nModel (LLM) is used to automate the generation and optimization of experimental\nprotocols, scaling the process of environment and curriculum design. Finally,\nwe outline a multi-modal approach for evaluating learning by measuring synaptic\nplasticity at electrophysiological, cellular, and molecular levels. This work\nbridges the gap between computational neuroscience and agent-based AI, offering\na unique platform for studying embodiment, learning, and intelligence in a\ncontrolled biological substrate.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04633v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04633v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.455,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.393,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on designing virtual environments for training biological neural organoids using rewards and punishments, and employs an LLM for automating protocols. However, it does not involve human feedback, a reward model trained on human-ranked data, or fine-tuning based on human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses an LLM to automate the generation and optimization of experimental protocols for environment design, but it does not incorporate diffusion models, iterative refinement processes for logical tasks, or multi-step reasoning as a holistically corrected entity, which are essential for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04642",
      "title": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents",
      "authors": [
        "Wenxiao Wang",
        "Priyatham Kattakinda",
        "Soheil Feizi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Building reliable LLM agents requires decisions at two levels: the graph\n(which modules exist and how information flows) and the configuration of each\nnode (models, prompts, tools, control knobs). Most existing optimizers tune\nconfigurations while holding the graph fixed, leaving structural failure modes\nunaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for\nLLM agents that jointly searches over graphs and configurations to maximize\nagent quality, subject to explicit rollout/token budgets. Beyond numeric\nmetrics, Maestro leverages reflective textual feedback from traces to\nprioritize edits, improving sample efficiency and targeting specific failure\nmodes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses\nleading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,\n4.9%, and 4.86%, respectively; even when restricted to prompt-only\noptimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these\nresults with far fewer rollouts than GEPA. We further show large gains on two\napplications (interviewer & RAG agents), highlighting that joint graph &\nconfiguration search addresses structural failure modes that prompt tuning\nalone cannot fix.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04642v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04642v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.471,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.4,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reflective textual feedback to guide agent optimization, which involves feedback signals similar to human preferences, but it does not employ a reward model trained on human-ranked data or fine-tune models via reinforcement learning. Maestro focuses on search-based optimization rather than RLHF techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Maestro for optimizing agent graphs and configurations through iterative search and feedback, but it does not involve diffusion models, iterative refinement processes for reasoning, or treating Chain-of-Thought as a holistic entity for correction. There is no component related to diffusion-based methods.",
      "distributed_training_justification": "The paper focuses on optimizing AI agent structures and configurations, such as graphs and prompts, but does not address distributed training, parallel computing, or strategies for accelerating model training across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04646",
      "title": "Towards Personalized Explanations for Health Simulations: A\n  Mixed-Methods Framework for Stakeholder-Centric Summarization",
      "authors": [
        "Philippe J. Giabbanelli",
        "Ameeta Agrawal"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "Modeling & Simulation (M&S) approaches such as agent-based models hold\nsignificant potential to support decision-making activities in health, with\nrecent examples including the adoption of vaccines, and a vast literature on\nhealthy eating behaviors and physical activity behaviors. These models are\npotentially usable by different stakeholder groups, as they support\npolicy-makers to estimate the consequences of potential interventions and they\ncan guide individuals in making healthy choices in complex environments.\nHowever, this potential may not be fully realized because of the models'\ncomplexity, which makes them inaccessible to the stakeholders who could benefit\nthe most. While Large Language Models (LLMs) can translate simulation outputs\nand the design of models into text, current approaches typically rely on\none-size-fits-all summaries that fail to reflect the varied informational needs\nand stylistic preferences of clinicians, policymakers, patients, caregivers,\nand health advocates. This limitation stems from a fundamental gap: we lack a\nsystematic understanding of what these stakeholders need from explanations and\nhow to tailor them accordingly. To address this gap, we present a step-by-step\nframework to identify stakeholder needs and guide LLMs in generating tailored\nexplanations of health simulations. Our procedure uses a mixed-methods design\nby first eliciting the explanation needs and stylistic preferences of diverse\nhealth stakeholders, then optimizing the ability of LLMs to generate tailored\noutputs (e.g., via controllable attribute tuning), and then evaluating through\na comprehensive range of metrics to further improve the tailored generation of\nsummaries.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04646v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04646v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.323,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper involves human feedback to optimize LLMs for tailored summaries, such as eliciting stakeholder needs and iterative evaluation, which aligns loosely with RLHF concepts. However, it does not explicitly describe training a reward model or using reinforcement learning for fine-tuning, focusing instead on general optimization techniques like controllable attribute tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using LLMs for generating personalized explanations of health simulations through a mixed-methods framework, with no mention of diffusion models, iterative refinement for logical tasks, or multi-step Chain-of-Thought reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04650",
      "title": "Comparative Analysis of Transformer Models in Disaster Tweet\n  Classification for Public Safety",
      "authors": [
        "Sharif Noor Zisad",
        "N. M. Istiak Chowdhury",
        "Ragib Hasan"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Twitter and other social media platforms have become vital sources of real\ntime information during disasters and public safety emergencies. Automatically\nclassifying disaster related tweets can help emergency services respond faster\nand more effectively. Traditional Machine Learning (ML) models such as Logistic\nRegression, Naive Bayes, and Support Vector Machines have been widely used for\nthis task, but they often fail to understand the context or deeper meaning of\nwords, especially when the language is informal, metaphorical, or ambiguous. We\nposit that, in this context, transformer based models can perform better than\ntraditional ML models. In this paper, we evaluate the effectiveness of\ntransformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for\nclassifying disaster related tweets. These models are compared with traditional\nML approaches to highlight the performance gap. Experimental results show that\nBERT achieved the highest accuracy (91%), significantly outperforming\ntraditional models like Logistic Regression and Naive Bayes (both at 82%). The\nuse of contextual embeddings and attention mechanisms allows transformer models\nto better understand subtle language in tweets, where traditional ML models\nfall short. This research demonstrates that transformer architectures are far\nmore suitable for public safety applications, offering improved accuracy,\ndeeper language understanding, and better generalization across real world\nsocial media text.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04650v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04650v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.378,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04653",
      "title": "Interpreting Transformer Architectures as Implicit Multinomial\n  Regression",
      "authors": [
        "Jonas A. Actor",
        "Anthony Gruber",
        "Eric C. Cyr"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)",
        "math.NA (Numerical Analysis)"
      ],
      "abstract": "Mechanistic interpretability aims to understand how internal components of\nmodern machine learning models, such as weights, activations, and layers, give\nrise to the model's overall behavior. One particularly opaque mechanism is\nattention: despite its central role in transformer models, its mathematical\nunderpinnings and relationship to concepts like feature polysemanticity,\nsuperposition, and model performance remain poorly understood. This paper\nestablishes a novel connection between attention mechanisms and multinomial\nregression. Specifically, we show that in a fixed multinomial regression\nsetting, optimizing over latent features yields optimal solutions that align\nwith the dynamics induced by attention blocks. In other words, the evolution of\nrepresentations through a transformer can be interpreted as a trajectory that\nrecovers the optimal features for classification.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04653v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04653v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.468,
      "distributed_training_score": 0.371,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is to interpret transformer architectures as implicit multinomial regression, focusing on attention mechanisms, feature optimization, and mechanistic interpretability. It discusses concepts like sparsity and superposition in transformers but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought tasks. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04655",
      "title": "Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs",
      "authors": [
        "Ayush Gupta",
        "Ramneet Kaur",
        "Anirban Roy",
        "Adam D. Cobb",
        "Rama Chellappa",
        "Susmit Jha"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose a novel inference-time out-of-domain (OOD) detection algorithm for\nspecialized large language models (LLMs). Despite achieving state-of-the-art\nperformance on in-domain tasks through fine-tuning, specialized LLMs remain\nvulnerable to incorrect or unreliable outputs when presented with OOD inputs,\nposing risks in critical applications. Our method leverages the Inductive\nConformal Anomaly Detection (ICAD) framework, using a new non-conformity\nmeasure based on the model's dropout tolerance. Motivated by recent findings on\npolysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs\nexhibit higher dropout tolerance than OOD inputs. We aggregate dropout\ntolerance across multiple layers via a valid ensemble approach, improving\ndetection while maintaining theoretical false alarm bounds from ICAD.\nExperiments with medical-specialized LLMs show that our approach detects OOD\ninputs better than baseline methods, with AUROC improvements of $2\\%$ to $37\\%$\nwhen treating OOD datapoints as positives and in-domain test datapoints as\nnegatives.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04655v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04655v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.344,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a novel OOD detection method for specialized LLMs using dropout tolerance and the ICAD framework, focusing on anomaly detection in medical domains. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04657",
      "title": "Evaluating NL2SQL via SQL2NL",
      "authors": [
        "Mohammadtaher Safarzadeh",
        "Afshin Oroojlooyjadid",
        "Dan Roth"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.DB (Databases)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Robust evaluation in the presence of linguistic variation is key to\nunderstanding the generalization capabilities of Natural Language to SQL\n(NL2SQL) models, yet existing benchmarks rarely address this factor in a\nsystematic or controlled manner. We propose a novel schema-aligned paraphrasing\nframework that leverages SQL-to-NL (SQL2NL) to automatically generate\nsemantically equivalent, lexically diverse queries while maintaining alignment\nwith the original schema and intent. This enables the first targeted evaluation\nof NL2SQL robustness to linguistic variation in isolation-distinct from prior\nwork that primarily investigates ambiguity or schema perturbations. Our\nanalysis reveals that state-of-the-art models are far more brittle than\nstandard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop\nin execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries,\nwhile LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to\n42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We\nalso find that robustness degradation varies significantly with query\ncomplexity, dataset, and domain -- highlighting the need for evaluation\nframeworks that explicitly measure linguistic generalization to ensure reliable\nperformance in real-world settings.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04657v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04657v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.341,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves programmatically generating schema-aligned queries using SQL2NL, which could be used to create noisy or imprecise labels for training NL2SQL models, aligning somewhat with weak supervision concepts. However, the main focus is on evaluation and robustness analysis, not primarily on training with weakly supervised data.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning as described. Its contributions center on NL2SQL evaluation via paraphrasing and schema alignment, with no connection to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04669",
      "title": "VCMamba: Bridging Convolutions with Multi-Directional Mamba for\n  Efficient Visual Representation",
      "authors": [
        "Mustafa Munir",
        "Alex Zhang",
        "Radu Marculescu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs)\nhave challenged the dominance of Convolutional Neural Networks (CNNs) in\ncomputer vision. ViTs excel at capturing global context, and SSMs like Mamba\noffer linear complexity for long sequences, yet they do not capture\nfine-grained local features as effectively as CNNs. Conversely, CNNs possess\nstrong inductive biases for local features but lack the global reasoning\ncapabilities of transformers and Mamba. To bridge this gap, we introduce\n\\textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs\nand multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a\nhierarchical structure with convolutional blocks in its early stages to extract\nrich local features. These convolutional blocks are then processed by later\nstages incorporating multi-directional Mamba blocks designed to efficiently\nmodel long-range dependencies and global context. This hybrid design allows for\nsuperior feature representation while maintaining linear complexity with\nrespect to image resolution. We demonstrate VCMamba's effectiveness through\nextensive experiments on ImageNet-1K classification and ADE20K semantic\nsegmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K,\nsurpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming\nVision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains\n47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing\n62% fewer parameters. Code is available at\nhttps://github.com/Wertyuui345/VCMamba.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04669v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04669v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.348,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces VCMamba, a vision backbone combining convolutions and State Space Models for efficient visual feature extraction and global context modeling in tasks like image classification and segmentation. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as the topic requires.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04676",
      "title": "An Approach to Grounding AI Model Evaluations in Human-derived Criteria",
      "authors": [
        "Sasha Mitts"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "In the rapidly evolving field of artificial intelligence (AI), traditional\nbenchmarks can fall short in attempting to capture the nuanced capabilities of\nAI models. We focus on the case of physical world modeling and propose a novel\napproach to augment existing benchmarks with human-derived evaluation criteria,\naiming to enhance the interpretability and applicability of model behaviors.\nGrounding our study in the Perception Test and OpenEQA benchmarks, we conducted\nin-depth interviews and large-scale surveys to identify key cognitive skills,\nsuch as Prioritization, Memorizing, Discerning, and Contextualizing, that are\ncritical for both AI and human reasoning. Our findings reveal that participants\nperceive AI as lacking in interpretive and empathetic skills yet hold high\nexpectations for AI performance. By integrating insights from our findings into\nbenchmark design, we offer a framework for developing more human-aligned means\nof defining and measuring progress. This work underscores the importance of\nuser-centered evaluation in AI development, providing actionable guidelines for\nresearchers and practitioners aiming to align AI capabilities with human\ncognitive processes. Our approach both enhances current benchmarking practices\nand sets the stage for future advancements in AI model evaluation.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04676v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04676v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.489,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.324,
      "datasets_score": 0.481,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper uses human feedback from interviews and surveys to inform benchmark design, which shares the concept of human input, but it does not involve training a reward model or fine-tuning AI models via reinforcement learning. Thus, it only touches on human feedback peripherally without aligning with core RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves analyzing and augmenting benchmarks like Perception Test and OpenEQA through human-derived criteria, which relates to evaluating and improving datasets for AI applications. However, it does not primarily focus on creating new datasets or detailed curation methodologies, making it relevant but not central.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of traditional AI benchmarks by proposing a novel approach to incorporate human-derived evaluation criteria, particularly for physical world modeling, to better capture AI's nuanced capabilities. The authors conducted in-depth interviews and large-scale surveys based on benchmarks like the Perception Test and OpenEQA to identify critical cognitive skills such as Prioritization, Memorizing, Discerning, and Contextualizing, revealing that while participants have high expectations for AI, they perceive it as deficient in interpretive and empathetic skills; the study offers a framework for human-aligned benchmarks and guidelines to align AI development with human cognitive processes.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by augmenting existing benchmarks with human-derived criteria through surveys and interviews, significantly advancing how AI evaluations are grounded in human cognition. This represents a meaningful shift from traditional methods, potentially setting a new standard in AI assessment.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a broad range of future research in AI and human-computer interaction by promoting more interpretable and human-aligned evaluations, which could lead to improved AI applications and ethical developments. Its actionable guidelines make it likely to be adopted and built upon in various subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights and a practical framework for AI researchers and practitioners focused on evaluation and human alignment, making it a significant contribution worth engaging with. While not essential for all audiences, it provides important advancements that could inform ongoing work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/28e7dd8a06d9b910e9a656d7f6a54f32cafa5c04",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 2,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Sasha Mitts",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2327097073"
        }
      ]
    },
    {
      "id": "2509.04677",
      "title": "Inferring the Graph Structure of Images for Graph Neural Networks",
      "authors": [
        "Mayur S Gowda",
        "John Shi",
        "Augusto Santos",
        "José M. F. Moura"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Image datasets such as MNIST are a key benchmark for testing Graph Neural\nNetwork (GNN) architectures. The images are traditionally represented as a grid\ngraph with each node representing a pixel and edges connecting neighboring\npixels (vertically and horizontally). The graph signal is the values\n(intensities) of each pixel in the image. The graphs are commonly used as input\nto graph neural networks (e.g., Graph Convolutional Neural Networks (Graph\nCNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify the\nimages. In this work, we improve the accuracy of downstream graph neural\nnetwork tasks by finding alternative graphs to the grid graph and superpixel\nmethods to represent the dataset images, following the approach in [5, 6]. We\nfind row correlation, column correlation, and product graphs for each image in\nMNIST and Fashion-MNIST using correlations between the pixel values building on\nthe method in [5, 6]. Experiments show that using these different graph\nrepresentations and features as input into downstream GNN models improves the\naccuracy over using the traditional grid graph and superpixel methods in the\nliterature.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04677v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04677v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.27,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.332,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04682",
      "title": "Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine\n  Bioacoustic Monitoring",
      "authors": [
        "Nicholas R. Rasmussen",
        "Rodrigue Rizk",
        "Longwei Wang",
        "KC Santosh"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.IR (Information Retrieval)",
        "cs.LG (Machine Learning)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal\ndata for long-term ecological analysis, but intrinsic noise and complex signal\ndependencies hinder model stability and generalization. Multilayered windowing\nhas improved target sound localization, yet variability from shifting ambient\nnoise, diverse propagation effects, and mixed biological and anthropogenic\nsources demands robust architectures and rigorous evaluation. We introduce\nGetNetUPAM, a hierarchical nested cross-validation framework designed to\nquantify model stability under ecologically realistic variability. Data are\npartitioned into distinct site-year segments, preserving recording\nheterogeneity and ensuring each validation fold reflects a unique environmental\nsubset, reducing overfitting to localized noise and sensor artifacts. Site-year\nblocking enforces evaluation against genuine environmental diversity, while\nstandard cross-validation on random subsets measures generalization across\nUPAM's full signal distribution, a dimension absent from current benchmarks.\nUsing GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution\nPooling and Attention Network (ARPA-N), a neural architecture for irregular\nspectrogram dimensions. Adaptive pooling with spatial attention extends the\nreceptive field, capturing global context without excessive parameters. Under\nGetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet\nbaselines and a log2-scale order-of-magnitude drop in variability across all\nmetrics, enabling consistent detection across site-year folds and advancing\nscalable, accurate bioacoustic monitoring.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04682v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04682v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.42,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.398,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contributions involve developing a hierarchical cross-validation framework (GetNetUPAM) and a neural architecture (ARPA-N) for improving stability and generalization in underwater acoustic monitoring. It does not discuss or utilize weak supervision techniques, such as programmatically generating labels from noisy or imprecise sources, relying instead on standard supervised learning with partitioned datasets.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04687",
      "title": "Guideline-Consistent Segmentation via Multi-Agent Refinement",
      "authors": [
        "Vanshika Vats",
        "Ashwani Rathee",
        "James Davis"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Semantic segmentation in real-world applications often requires not only\naccurate masks but also strict adherence to textual labeling guidelines. These\nguidelines are typically complex and long, and both human and automated\nlabeling often fail to follow them faithfully. Traditional approaches depend on\nexpensive task-specific retraining that must be repeated as the guidelines\nevolve. Although recent open-vocabulary segmentation methods excel with simple\nprompts, they often fail when confronted with sets of paragraph-length\nguidelines that specify intricate segmentation rules. To address this, we\nintroduce a multi-agent, training-free framework that coordinates\ngeneral-purpose vision-language models within an iterative Worker-Supervisor\nrefinement architecture. The Worker performs the segmentation, the Supervisor\ncritiques it against the retrieved guidelines, and a lightweight reinforcement\nlearning stop policy decides when to terminate the loop, ensuring\nguideline-consistent masks while balancing resource use. Evaluated on the Waymo\nand ReasonSeg datasets, our method notably outperforms state-of-the-art\nbaselines, demonstrating strong generalization and instruction adherence.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04687v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04687v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.461,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.369,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper presents a training-free framework for semantic segmentation that uses existing models and iterative refinement to adhere to guidelines, without involving the generation or use of weak labels for training. Weak supervision specifically entails programmatically creating noisy labels to train models, which is not a component of this work.",
      "diffusion_reasoning_justification": "The paper's iterative Worker-Supervisor loop involves multi-step refinement for segmentation, but it does not employ diffusion models or adapt the diffusion process for logical reasoning. Instead, it uses vision-language models and reinforcement learning for critique and stopping, lacking any diffusion-based components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04696",
      "title": "ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs",
      "authors": [
        "Samira Khorshidi",
        "Azadeh Nikfarjam",
        "Suprita Shankar",
        "Yisi Sang",
        "Yash Govind",
        "Hyun Jang",
        "Ali Kasgari",
        "Alexis McClimans",
        "Mohamed Soliman",
        "Vishnu Konda",
        "Ahmed Fakhry",
        "Xiaoguang Qi"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Knowledge graphs (KGs) are foundational to many AI applications, but\nmaintaining their freshness and completeness remains costly. We present ODKE+,\na production-grade system that automatically extracts and ingests millions of\nopen-domain facts from web sources with high precision. ODKE+ combines modular\ncomponents into a scalable pipeline: (1) the Extraction Initiator detects\nmissing or stale facts, (2) the Evidence Retriever collects supporting\ndocuments, (3) hybrid Knowledge Extractors apply both pattern-based rules and\nontology-guided prompting for large language models (LLMs), (4) a lightweight\nGrounder validates extracted facts using a second LLM, and (5) the Corroborator\nranks and normalizes candidate facts for ingestion. ODKE+ dynamically generates\nontology snippets tailored to each entity type to align extractions with schema\nconstraints, enabling scalable, type-consistent fact extraction across 195\npredicates. The system supports batch and streaming modes, processing over 9\nmillion Wikipedia pages and ingesting 19 million high-confidence facts with\n98.8% precision. ODKE+ significantly improves coverage over traditional\nmethods, achieving up to 48% overlap with third-party KGs and reducing update\nlag by 50 days on average. Our deployment demonstrates that LLM-based\nextraction, grounded in ontological structure and verification workflows, can\ndeliver trustworthiness, production-scale knowledge ingestion with broad\nreal-world applicability. A recording of the system demonstration is included\nwith the submission and is also available at https://youtu.be/UcnE3_GsTWs.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04696v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04696v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.31,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper describes a system using ontology-guided prompting for LLMs to extract knowledge, which involves programmatically generated rules and prompts to guide extractions. This indirectly relates to weak supervision, as it leverages high-level rules (from ontologies) to generate and validate facts from noisy web sources, similar to using imprecise sources for labeling. However, the paper's main focus is on building a scalable knowledge extraction pipeline, not on training ML models with weak labels, making the connection peripheral.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04711",
      "title": "Domain Adaptation for Different Sensor Configurations in 3D Object\n  Detection",
      "authors": [
        "Satoshi Tanaka",
        "Kok Seang Tan",
        "Isamu Yamashita"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Recent advances in autonomous driving have underscored the importance of\naccurate 3D object detection, with LiDAR playing a central role due to its\nrobustness under diverse visibility conditions. However, different vehicle\nplatforms often deploy distinct sensor configurations, causing performance\ndegradation when models trained on one configuration are applied to another\nbecause of shifts in the point cloud distribution. Prior work on multi-dataset\ntraining and domain adaptation for 3D object detection has largely addressed\nenvironmental domain gaps and density variation within a single LiDAR; in\ncontrast, the domain gap for different sensor configurations remains largely\nunexplored. In this work, we address domain adaptation across different sensor\nconfigurations in 3D object detection. We propose two techniques: Downstream\nFine-tuning (dataset-specific fine-tuning after multi-dataset training) and\nPartial Layer Fine-tuning (updating only a subset of layers to improve\ncross-configuration generalization). Using paired datasets collected in the\nsame geographic region with multiple sensor configurations, we show that joint\ntraining with Downstream Fine-tuning and Partial Layer Fine-tuning consistently\noutperforms naive joint training for each configuration. Our findings provide a\npractical and scalable solution for adapting 3D object detection models to the\ndiverse vehicle platforms.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04711v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04711v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.406,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution focuses on domain adaptation techniques, such as Downstream Fine-tuning and Partial Layer Fine-tuning, for 3D object detection across different sensor configurations. It discusses multi-dataset training strategies but does not address distributed training, parallel computing, multi-node setups, or methods for partitioning data or computation across processors. Therefore, it lacks any direct or indirect relevance to distributed training topics.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04712",
      "title": "Bootstrapping Reinforcement Learning with Sub-optimal Policies for\n  Autonomous Driving",
      "authors": [
        "Zhihao Zhang",
        "Chengyang Peng",
        "Ekim Yurtsever",
        "Keith A. Redmill"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Automated vehicle control using reinforcement learning (RL) has attracted\nsignificant attention due to its potential to learn driving policies through\nenvironment interaction. However, RL agents often face training challenges in\nsample efficiency and effective exploration, making it difficult to discover an\noptimal driving strategy. To address these issues, we propose guiding the RL\ndriving agent with a demonstration policy that need not be a highly optimized\nor expert-level controller. Specifically, we integrate a rule-based lane change\ncontroller with the Soft Actor Critic (SAC) algorithm to enhance exploration\nand learning efficiency. Our approach demonstrates improved driving performance\nand can be extended to other driving scenarios that can similarly benefit from\ndemonstration-based guidance.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.04712v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04712v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.348,
      "datasets_score": 0.26,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on guiding RL agents for autonomous driving using a predefined rule-based controller to improve exploration and efficiency, as seen in the integration with the Soft Actor Critic (SAC) algorithm. While the rule-based controller is derived from human expertise, it does not involve human feedback in the form of ranked data to train a separate reward model for fine-tuning, which is the core of RLHF. Instead, it uses a static demonstration policy, making it distinct from RLHF methods.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05285",
      "title": "Improved 3D Scene Stylization via Text-Guided Generative Image Editing\n  with Region-Based Control",
      "authors": [
        "Haruo Fujiwara",
        "Yusuke Mukuta",
        "Tatsuya Harada"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in text-driven 3D scene editing and stylization, which\nleverage the powerful capabilities of 2D generative models, have demonstrated\npromising outcomes. However, challenges remain in ensuring high-quality\nstylization and view consistency simultaneously. Moreover, applying style\nconsistently to different regions or objects in the scene with semantic\ncorrespondence is a challenging task. To address these limitations, we\nintroduce techniques that enhance the quality of 3D stylization while\nmaintaining view consistency and providing optional region-controlled style\ntransfer. Our method achieves stylization by re-training an initial 3D\nrepresentation using stylized multi-view 2D images of the source views.\nTherefore, ensuring both style consistency and view consistency of stylized\nmulti-view images is crucial. We achieve this by extending the style-aligned\ndepth-conditioned view generation framework, replacing the fully shared\nattention mechanism with a single reference-based attention-sharing mechanism,\nwhich effectively aligns style across different viewpoints. Additionally,\ninspired by recent 3D inpainting methods, we utilize a grid of multiple depth\nmaps as a single-image reference to further strengthen view consistency among\nstylized images. Finally, we propose Multi-Region Importance-Weighted Sliced\nWasserstein Distance Loss, allowing styles to be applied to distinct image\nregions using segmentation masks from off-the-shelf models. We demonstrate that\nthis optional feature enhances the faithfulness of style transfer and enables\nthe mixing of different styles across distinct regions of the scene.\nExperimental evaluations, both qualitative and quantitative, demonstrate that\nour pipeline effectively improves the results of text-driven 3D stylization.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05285v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05285v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.346,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models (e.g., Stable Diffusion) for generative image editing and stylization, which involves iterative refinement processes. However, it applies these models to visual tasks like 3D scene editing and view consistency, not to complex logical reasoning or Chain-of-Thought processes. There is no component for multi-step logical reasoning, making the connection indirect and not central to the paper's contributions.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05362",
      "title": "AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and\n  Conversational Scambaiting by Leveraging LLMs and Federated Learning",
      "authors": [
        "Ismail Hossain",
        "Sai Puppala",
        "Sajedul Talukder",
        "Md Jahangir Alam"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "Scams exploiting real-time social engineering -- such as phishing,\nimpersonation, and phone fraud -- remain a persistent and evolving threat\nacross digital platforms. Existing defenses are largely reactive, offering\nlimited protection during active interactions. We propose a privacy-preserving,\nAI-in-the-loop framework that proactively detects and disrupts scam\nconversations in real time. The system combines instruction-tuned artificial\nintelligence with a safety-aware utility function that balances engagement with\nharm minimization, and employs federated learning to enable continual model\nupdates without raw data sharing. Experimental evaluations show that the system\nproduces fluent and engaging responses (perplexity as low as 22.3, engagement\n$\\approx$0.80), while human studies confirm significant gains in realism,\nsafety, and effectiveness over strong baselines. In federated settings, models\ntrained with FedAvg sustain up to 30 rounds while preserving high engagement\n($\\approx$0.80), strong relevance ($\\approx$0.74), and low PII leakage\n($\\leq$0.0085). Even with differential privacy, novelty and safety remain\nstable, indicating that robust privacy can be achieved without sacrificing\nperformance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,\nMD-Judge) shows a straightforward pattern: stricter moderation settings reduce\nthe chance of exposing personal information, but they also limit how much the\nmodel engages in conversation. In contrast, more relaxed settings allow longer\nand richer interactions, which improve scam detection, but at the cost of\nhigher privacy risk. To our knowledge, this is the first framework to unify\nreal-time scam-baiting, federated privacy preservation, and calibrated safety\nmoderation into a proactive defense paradigm.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05362v2",
      "pdf_url": "http://arxiv.org/pdf/2509.05362v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.472,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.358,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a framework using instruction-tuned LLMs and a utility function for scam detection, which involves balancing engagement and risk, and incorporates human studies for evaluation. While human evaluations are mentioned to assess realism and effectiveness, there is no evidence of using human-ranked data to train a reward model or fine-tune the main model via reinforcement learning, which are core to RLHF. Thus, the paper only loosely connects through human feedback in assessments, making it tangentially relevant.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05363",
      "title": "SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis",
      "authors": [
        "Lijie Ding",
        "Changwoo Do"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "We introduce SasAgent, a multi-agent AI system powered by large language\nmodels (LLMs) that automates small-angle scattering (SAS) data analysis by\nleveraging tools from the SasView software and enables user interaction via\ntext input. SasAgent features a coordinator agent that interprets user prompts\nand delegates tasks to three specialized agents for scattering length density\n(SLD) calculation, synthetic data generation, and experimental data fitting.\nThese agents utilize LLM-friendly tools to execute tasks efficiently. These\ntools, including the model data tool, Retrieval-Augmented Generation (RAG)\ndocumentation tool, bump fitting tool, and SLD calculator tool, are derived\nfrom the SasView Python library. A user-friendly Gradio-based interface\nenhances user accessibility. Through diverse examples, we demonstrate\nSasAgent's ability to interpret complex prompts, calculate SLDs, generate\naccurate scattering data, and fit experimental datasets with high precision.\nThis work showcases the potential of LLM-driven AI systems to streamline\nscientific workflows and enhance automation in SAS research.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05363v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05363v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.336,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05364",
      "title": "Prototyping an AI-powered Tool for Energy Efficiency in New Zealand\n  Homes",
      "authors": [
        "Abdollah Baghaei Daemei"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "Residential buildings contribute significantly to energy use, health\noutcomes, and carbon emissions. In New Zealand, housing quality has\nhistorically been poor, with inadequate insulation and inefficient heating\ncontributing to widespread energy hardship. Recent reforms, including the\nWarmer Kiwi Homes program, Healthy Homes Standards, and H1 Building Code\nupgrades, have delivered health and comfort improvements, yet challenges\npersist. Many retrofits remain partial, data on household performance are\nlimited, and decision-making support for homeowners is fragmented. This study\npresents the design and evaluation of an AI-powered decision-support tool for\nresidential energy efficiency in New Zealand. The prototype, developed using\nPython and Streamlit, integrates data ingestion, anomaly detection, baseline\nmodeling, and scenario simulation (e.g., LED retrofits, insulation upgrades)\ninto a modular dashboard. Fifteen domain experts, including building\nscientists, consultants, and policy practitioners, tested the tool through\nsemi-structured interviews. Results show strong usability (M = 4.3), high value\nof scenario outputs (M = 4.5), and positive perceptions of its potential to\ncomplement subsidy programs and regulatory frameworks. The tool demonstrates\nhow AI can translate national policies into personalized, household-level\nguidance, bridging the gap between funding, standards, and practical\ndecision-making. Its significance lies in offering a replicable framework for\nreducing energy hardship, improving health outcomes, and supporting climate\ngoals. Future development should focus on carbon metrics, tariff modeling,\nintegration with national datasets, and longitudinal trials to assess\nreal-world adoption.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05364v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05364v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.292,
      "distributed_training_score": 0.294,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05367",
      "title": "Between a Rock and a Hard Place: Exploiting Ethical Reasoning to\n  Jailbreak LLMs",
      "authors": [
        "Shei Pern Chua",
        "Zhen Leng Thai",
        "Teh Kai Jun",
        "Xiao Li",
        "Xiaolin Hu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have undergone safety alignment efforts to\nmitigate harmful outputs. However, as LLMs become more sophisticated in\nreasoning, their intelligence may introduce new security risks. While\ntraditional jailbreak attacks relied on singlestep attacks, multi-turn\njailbreak strategies that adapt dynamically to context remain underexplored. In\nthis work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack\nLogic), a framework that leverages LLMs ethical reasoning to bypass their\nsafeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on\nthe trolley problem. TRIAL demonstrates high jailbreak success rates towards\nboth open and close-source models. Our findings underscore a fundamental\nlimitation in AI safety: as models gain advanced reasoning abilities, the\nnature of their alignment may inadvertently allow for more covert security\nvulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating\nsafety alignment oversight strategies, as current safeguards may prove\ninsufficient against context-aware adversarial attack.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05367v2",
      "pdf_url": "http://arxiv.org/pdf/2509.05367v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.434,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.314,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses safety-aligned LLMs, which often involve RLHF for fine-tuning based on human preferences, but its main contribution is on exploiting vulnerabilities in these models through jailbreak attacks, not on the RLHF process itself. It assumes alignment exists without directly analyzing, implementing, or evaluating RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on multi-turn jailbreak attacks using ethical dilemmas, but it does not involve diffusion models, iterative refinement of reasoning paths, or any adaptation of diffusion processes for logical tasks. There is no mention of treating a chain-of-thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05368",
      "title": "Long-Horizon Visual Imitation Learning via Plan and Code Reflection",
      "authors": [
        "Quan Chen",
        "Chenrui Shi",
        "Qi Chen",
        "Yuwei Wu",
        "Zhi Gao",
        "Xintong Zhang",
        "Rui Gao",
        "Kun Wu",
        "Yunde Jia"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Learning from long-horizon demonstrations with complex action sequences\npresents significant challenges for visual imitation learning, particularly in\nunderstanding temporal relationships of actions and spatial relationships\nbetween objects. In this paper, we propose a new agent framework that\nincorporates two dedicated reflection modules to enhance both plan and code\ngeneration. The plan generation module produces an initial action sequence,\nwhich is then verified by the plan reflection module to ensure temporal\ncoherence and spatial alignment with the demonstration video. The code\ngeneration module translates the plan into executable code, while the code\nreflection module verifies and refines the generated code to ensure correctness\nand consistency with the generated plan. These two reflection modules jointly\nenable the agent to detect and correct errors in both the plan generation and\ncode generation, improving performance in tasks with intricate temporal and\nspatial dependencies. To support systematic evaluation, we introduce\nLongVILBench, a benchmark comprising 300 human demonstrations with action\nsequences of up to 18 steps. LongVILBench emphasizes temporal and spatial\ncomplexity across multiple task types. Experimental results demonstrate that\nexisting methods perform poorly on this benchmark, whereas our new framework\nestablishes a strong baseline for long-horizon visual imitation learning.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05368v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05368v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.299,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on visual imitation learning using demonstration videos to generate and refine plans and codes, but it does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune a model based on human preferences. Instead, it relies on direct imitation from videos without any RLHF mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes iterative reflection modules for verifying and refining plans and codes, which involves multi-step reasoning, but it does not adapt the iterative refinement process of diffusion models. There is no mention of diffusion-based techniques or treating a Chain-of-Thought as a single entity for holistic correction using diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05374",
      "title": "A Synthetic-to-Real Dehazing Method based on Domain Unification",
      "authors": [
        "Zhiqiang Yuan",
        "Jinchao Zhang",
        "Jie Zhou"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Due to distribution shift, the performance of deep learning-based method for\nimage dehazing is adversely affected when applied to real-world hazy images. In\nthis paper, we find that such deviation in dehazing task between real and\nsynthetic domains may come from the imperfect collection of clean data. Owing\nto the complexity of the scene and the effect of depth, the collected clean\ndata cannot strictly meet the ideal conditions, which makes the atmospheric\nphysics model in the real domain inconsistent with that in the synthetic\ndomain. For this reason, we come up with a synthetic-to-real dehazing method\nbased on domain unification, which attempts to unify the relationship between\nthe real and synthetic domain, thus to let the dehazing model more in line with\nthe actual situation. Extensive experiments qualitatively and quantitatively\ndemonstrate that the proposed dehazing method significantly outperforms\nstate-of-the-art methods on real-world images.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05374v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05374v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.331,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05375",
      "title": "Characterizing Fitness Landscape Structures in Prompt Engineering",
      "authors": [
        "Arend Hintze"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While prompt engineering has emerged as a crucial technique for optimizing\nlarge language model performance, the underlying optimization landscape remains\npoorly understood. Current approaches treat prompt optimization as a black-box\nproblem, applying sophisticated search algorithms without characterizing the\nlandscape topology they navigate. We present a systematic analysis of fitness\nlandscape structures in prompt engineering using autocorrelation analysis\nacross semantic embedding spaces. Through experiments on error detection tasks\nwith two distinct prompt generation strategies -- systematic enumeration (1,024\nprompts) and novelty-driven diversification (1,000 prompts) -- we reveal\nfundamentally different landscape topologies. Systematic prompt generation\nyields smoothly decaying autocorrelation, while diversified generation exhibits\nnon-monotonic patterns with peak correlation at intermediate semantic\ndistances, indicating rugged, hierarchically structured landscapes.\nTask-specific analysis across 10 error detection categories reveals varying\ndegrees of ruggedness across different error types. Our findings provide an\nempirical foundation for understanding the complexity of optimization in prompt\nengineering landscapes.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05375v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05375v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.457,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.391,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on analyzing fitness landscapes in prompt engineering through empirical methods like autocorrelation analysis, without any mention of human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines prompt optimization landscapes and their topologies but does not involve diffusion models, iterative refinement for logical reasoning, or any multi-step reasoning processes as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05376",
      "title": "Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye\n  Tracking for Interactive Learning Environments",
      "authors": [
        "Abdul Rehman",
        "Are Dæhlen",
        "Ilona Heldal",
        "Jerry Chun-wei Lin"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Eye-tracking technology can aid in understanding neurodevelopmental disorders\nand tracing a person's identity. However, this technology poses a significant\nrisk to privacy, as it captures sensitive information about individuals and\nincreases the likelihood that data can be traced back to them. This paper\nproposes a human-centered framework designed to prevent identity backtracking\nwhile preserving the pedagogical benefits of AI-powered eye tracking in\ninteractive learning environments. We explore how real-time data anonymization,\nethical design principles, and regulatory compliance (such as GDPR) can be\nintegrated to build trust and transparency. We first demonstrate the potential\nfor backtracking student IDs and diagnoses in various scenarios using serious\ngame-based eye-tracking data. We then provide a two-stage privacy-preserving\nframework that prevents participants from being tracked while still enabling\ndiagnostic classification. The first phase covers four scenarios: I) Predicting\ndisorder diagnoses based on different game levels. II) Predicting student IDs\nbased on different game levels. III) Predicting student IDs based on randomized\ndata. IV) Utilizing K-Means for out-of-sample data. In the second phase, we\npresent a two-stage framework that preserves privacy. We also employ Federated\nLearning (FL) across multiple clients, incorporating a secure identity\nmanagement system with dummy IDs and administrator-only access controls. In the\nfirst phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%\naccuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully\nidentifying and assigning a new student ID in scenario 4. In phase 2, we\neffectively prevented backtracking and established a secure identity management\nsystem with dummy IDs and administrator-only access controls, achieving an\noverall accuracy of 99.40%.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05376v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05376v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.373,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for privacy preservation in AI-driven eye tracking, focusing on techniques like Federated Learning, data anonymization, and machine learning models such as Decision Trees and Random Forest for diagnosing neurodevelopmental disorders. It does not involve training AI models with human feedback, creating a reward model based on human-ranked data, or using reinforcement learning to fine-tune models, which are core elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05378",
      "title": "Code Like Humans: A Multi-Agent Solution for Medical Coding",
      "authors": [
        "Andreas Motzfeldt",
        "Joakim Edin",
        "Casper L. Christensen",
        "Christian Hardmeier",
        "Lars Maaløe",
        "Anna Rogers"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "In medical coding, experts map unstructured clinical notes to alphanumeric\ncodes for diagnoses and procedures. We introduce Code Like Humans: a new\nagentic framework for medical coding with large language models. It implements\nofficial coding guidelines for human experts, and it is the first solution that\ncan support the full ICD-10 coding system (+70K labels). It achieves the best\nperformance to date on rare diagnosis codes (fine-tuned discriminative\nclassifiers retain an advantage for high-frequency codes, to which they are\nlimited). Towards future work, we also contribute an analysis of system\nperformance and identify its `blind spots' (codes that are systematically\nundercoded).",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05378v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05378v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.334,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces an agentic framework using large language models for medical coding, but it does not mention or involve reinforcement learning, human feedback, reward models, or any training process aligned with RLHF. The focus is on mimicking human coding workflows without feedback-based fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a multi-agent system with sequential steps for reasoning in medical coding, but it does not use diffusion models, iterative refinement processes, or treat reasoning as a holistically corrected chain-of-thought via diffusion. It is a standard agentic approach inspired by human workflows, not diffusion-based.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05379",
      "title": "ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through\n  Threat Modeling",
      "authors": [
        "Sharif Noor Zisad",
        "Ragib Hasan"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As our cities and communities become smarter, the systems that keep us safe,\nsuch as traffic control centers, emergency response networks, and public\ntransportation, also become more complex. With this complexity comes a greater\nrisk of security threats that can affect not just machines but real people's\nlives. To address this challenge, we present ThreatGPT, an agentic Artificial\nIntelligence (AI) assistant built to help people whether they are engineers,\nsafety officers, or policy makers to understand and analyze threats in public\nsafety systems. Instead of requiring deep cybersecurity expertise, it allows\nusers to simply describe the components of a system they are concerned about,\nsuch as login systems, data storage, or communication networks. Then, with the\nclick of a button, users can choose how they want the system to be analyzed by\nusing popular frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, or\nCISA. ThreatGPT is unique because it does not just provide threat information,\nbut rather it acts like a knowledgeable partner. Using few-shot learning, the\nAI learns from examples and generates relevant smart threat models. It can\nhighlight what might go wrong, how attackers could take advantage, and what can\nbe done to prevent harm. Whether securing a city's infrastructure or a local\nhealth service, this tool adapts to users' needs. In simple terms, ThreatGPT\nbrings together AI and human judgment to make our public systems safer. It is\ndesigned not just to analyze threats, but to empower people to understand and\nact on them, faster, smarter, and with more confidence.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05379v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05379v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.38,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces ThreatGPT, which uses few-shot learning and large language models trained on specific frameworks like STRIDE and MITRE ATT&CK, but it does not mention reinforcement learning, human feedback for training a reward model, or fine-tuning via RLHF. The focus is on generating threat models from user inputs, not aligning AI with human preferences through ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes ThreatGPT as an agentic AI that uses few-shot learning to generate threat models and guide users, but it does not involve diffusion models, iterative refinement processes, or treating a chain-of-thought as a holistic entity for multi-step logical reasoning. The approach is based on standard AI frameworks, not diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05380",
      "title": "Cumplimiento del Reglamento (UE) 2024/1689 en robótica y sistemas\n  autónomos: una revisión sistemática de la literatura",
      "authors": [
        "Yoana Pita Lorenzo"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)",
        "cs.RO (Robotics)"
      ],
      "abstract": "This systematic literature review analyzes the current state of compliance\nwith Regulation (EU) 2024/1689 in autonomous robotic systems, focusing on\ncybersecurity frameworks and methodologies. Using the PRISMA protocol, 22\nstudies were selected from 243 initial records across IEEE Xplore, ACM DL,\nScopus, and Web of Science. Findings reveal partial regulatory alignment: while\nprogress has been made in risk management and encrypted communications,\nsignificant gaps persist in explainability modules, real-time human oversight,\nand knowledge base traceability. Only 40% of reviewed solutions explicitly\naddress transparency requirements, and 30% implement failure intervention\nmechanisms. The study concludes that modular approaches integrating risk,\nsupervision, and continuous auditing are essential to meet the AI Act mandates\nin autonomous robotics.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05380v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05380v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.262,
      "diffusion_reasoning_score": 0.243,
      "distributed_training_score": 0.279,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05381",
      "title": "Murphys Laws of AI Alignment: Why the Gap Always Wins",
      "authors": [
        "Madhava Gaikwad"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We study reinforcement learning from human feedback under misspecification.\nSometimes human feedback is systematically wrong on certain types of inputs,\nlike a broken compass that points the wrong way in specific regions. We prove\nthat when feedback is biased on a fraction alpha of contexts with bias strength\nepsilon, any learning algorithm needs exponentially many samples\nexp(n*alpha*epsilon^2) to distinguish between two possible \"true\" reward\nfunctions that differ only on these problematic contexts. However, if you can\nidentify where feedback is unreliable (a \"calibration oracle\"), you can focus\nyour limited questions there and overcome the exponential barrier with just\nO(1/(alpha*epsilon^2)) queries. This quantifies why alignment is hard: rare\nedge cases with subtly biased feedback create an exponentially hard learning\nproblem unless you know where to look.\n  The gap between what we optimize (proxy from human feedback) and what we want\n(true objective) is fundamentally limited by how common the problematic\ncontexts are (alpha), how wrong the feedback is there (epsilon), and how much\nthe true objectives disagree there (gamma). Murphy's Law for AI alignment: the\ngap always wins unless you actively route around misspecification.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.05381v3",
      "pdf_url": "http://arxiv.org/pdf/2509.05381v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.546,
      "weak_supervision_score": 0.43,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.316,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper directly focuses on reinforcement learning from human feedback (RLHF), analyzing its limitations due to misspecified feedback, providing theoretical lower and upper bounds, and discussing practical implications for AI alignment. This aligns closely with RLHF systems that use human-ranked data to train reward models for policy optimization.",
      "weak_supervision_justification": "The paper addresses noisy or biased feedback in RLHF, which shares similarities with weak supervision's use of imprecise labels. However, it is primarily centered on RLHF contexts rather than the broader techniques of programmatically generating labels, making it only moderately relevant to weak supervision as a whole.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates the challenges of reinforcement learning from human feedback (RLHF) due to misspecified feedback on certain contexts, demonstrating that biased feedback on a fraction alpha of inputs with bias strength epsilon requires an exponential number of samples, specifically exp(n*alpha*epsilon^2), to distinguish between true reward functions. It employs theoretical analysis using Kullback-Leibler divergence to establish lower bounds on sample complexity and shows that with a calibration oracle to identify unreliable contexts, the problem can be overcome using only O(1/(alpha*epsilon^2)) queries, highlighting the fundamental difficulties in AI alignment and emphasizing the need to address gaps between proxy and true objectives.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel theoretical framework and proofs for sample complexity in RLHF under misspecification, significantly advancing the state-of-the-art by quantifying the exponential barriers in AI alignment.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in AI alignment and machine learning subfields due to its insights on handling biased feedback, though its influence may be limited to specific applications rather than broad commercial or research areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers high-quality theoretical insights into the core challenges of AI alignment, making it essential for researchers in reinforcement learning and AI safety to understand and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/129dea44270e8cbc0a2de70696d99d58711e9e6e",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Madhava Gaikwad",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379666374"
        }
      ]
    },
    {
      "id": "2509.06996",
      "title": "Visible Yet Unreadable: A Systematic Blind Spot of Vision Language\n  Models Across Writing Systems",
      "authors": [
        "Jie Zhang",
        "Ting Xu",
        "Gelei Deng",
        "Runyi Hu",
        "Han Qiu",
        "Tianwei Zhang",
        "Qing Guo",
        "Ivor Tsang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Writing is a universal cultural technology that reuses vision for symbolic\ncommunication. Humans display striking resilience: we readily recognize words\neven when characters are fragmented, fused, or partially occluded. This paper\ninvestigates whether advanced vision language models (VLMs) share this\nresilience. We construct two psychophysics inspired benchmarks across distinct\nwriting systems, Chinese logographs and English alphabetic words, by splicing,\nrecombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli\nfor models while remaining legible to humans. Despite strong performance on\nclean text, contemporary VLMs show a severe drop under these perturbations,\nfrequently producing unrelated or incoherent outputs. The pattern suggests a\nstructural limitation: models heavily leverage generic visual invariances but\nunder rely on compositional priors needed for robust literacy. We release\nstimuli generation code, prompts, and evaluation protocols to facilitate\ntransparent replication and follow up work. Our findings motivate architectures\nand training strategies that encode symbol segmentation, composition, and\nbinding across scripts, and they delineate concrete challenges for deploying\nmultimodal systems in education, accessibility, cultural heritage, and\nsecurity.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.06996v2",
      "pdf_url": "http://arxiv.org/pdf/2509.06996v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.358,
      "datasets_score": 0.38,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines the resilience of vision language models (VLMs) to visual perturbations in text, focusing on benchmarks for human-readable but machine-unreadable stimuli. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.06997",
      "title": "K-Syn: K-space Data Synthesis in Ultra Low-data Regimes",
      "authors": [
        "Guan Yu",
        "Zhang Jianhua",
        "Liang Dong",
        "Liu Qiegen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Owing to the inherently dynamic and complex characteristics of cardiac\nmagnetic resonance (CMR) imaging, high-quality and diverse k-space data are\nrarely available in practice, which in turn hampers robust reconstruction of\ndynamic cardiac MRI. To address this challenge, we perform feature-level\nlearning directly in the frequency domain and employ a temporal-fusion strategy\nas the generative guidance to synthesize k-space data. Specifically, leveraging\nthe global representation capacity of the Fourier transform, the frequency\ndomain can be considered a natural global feature space. Therefore, unlike\ntraditional methods that use pixel-level convolution for feature learning and\nmodeling in the image domain, this letter focuses on feature-level modeling in\nthe frequency domain, enabling stable and rich generation even with ultra\nlow-data regimes. Moreover, leveraging the advantages of feature-level modeling\nin the frequency domain, we integrate k-space data across time frames with\nmultiple fusion strategies to steer and further optimize the generative\ntrajectory. Experimental results demonstrate that the proposed method possesses\nstrong generative ability in low-data regimes, indicating practical potential\nto alleviate data scarcity in dynamic MRI reconstruction.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.06997v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06997v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.25,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.313,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.06998",
      "title": "Not All Splits Are Equal: Rethinking Attribute Generalization Across\n  Unrelated Categories",
      "authors": [
        "Liviu Nicolae Fircă",
        "Antonio Bărbălau",
        "Dan Oneata",
        "Elena Burceanu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Can models generalize attribute knowledge across semantically and\nperceptually dissimilar categories? While prior work has addressed attribute\nprediction within narrow taxonomic or visually similar domains, it remains\nunclear whether current models can abstract attributes and apply them to\nconceptually distant categories. This work presents the first explicit\nevaluation for the robustness of the attribute prediction task under such\nconditions, testing whether models can correctly infer shared attributes\nbetween unrelated object types: e.g., identifying that the attribute \"has four\nlegs\" is common to both \"dogs\" and \"chairs\". To enable this evaluation, we\nintroduce train-test split strategies that progressively reduce correlation\nbetween training and test sets, based on: LLM-driven semantic grouping,\nembedding similarity thresholding, embedding-based clustering, and\nsupercategory-based partitioning using ground-truth labels. Results show a\nsharp drop in performance as the correlation between training and test\ncategories decreases, indicating strong sensitivity to split design. Among the\nevaluated methods, clustering yields the most effective trade-off, reducing\nhidden correlations while preserving learnability. These findings offer new\ninsights into the limitations of current representations and inform future\nbenchmark construction for attribute reasoning.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.06998v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06998v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.366,
      "datasets_score": 0.408,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper mentions using LLM-driven semantic grouping for dataset splits, which could involve weak supervision through high-level, programmatically generated labels. However, its main focus is on evaluating attribute generalization rather than training models with weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's core contribution involves creating and evaluating new train-test splits for datasets to assess attribute generalization, including methodologies for dataset curation, benchmarking, and analysis to reduce correlations and improve evaluation robustness.",
      "llm_score_status": "completed",
      "summary": "This paper investigates whether machine learning models can generalize attribute knowledge, such as \"has four legs,\" across semantically and perceptually dissimilar categories like dogs and chairs, which prior work has not adequately addressed. The authors introduce novel train-test split strategies, including LLM-driven semantic grouping, embedding similarity thresholding, embedding-based clustering, and supercategory-based partitioning, to evaluate this generalization; their findings reveal a significant performance drop as training-test correlations decrease, with clustering offering an effective balance by minimizing leakage while maintaining learnability, thus highlighting limitations in current models and guiding future benchmark designs.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new evaluation framework for attribute generalization across unrelated categories and novel split strategies, significantly advancing the state-of-the-art in attribute prediction and zero-shot learning by addressing previously unexplored aspects of model robustness.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like computer vision and machine learning for improving attribute reasoning benchmarks and model generalization techniques. However, its influence may be limited to specific applications rather than broadly across all AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers high-quality insights into the limitations of current attribute generalization methods, making it a valuable read for researchers in AI and computer vision to inform their work on model robustness and benchmark design.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/74eb585cbde87d691fa4597691e69f0ac23f5abc",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 13,
      "average_h_index": 3.25,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Liviu Nicolae Fircua",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379760085"
        },
        {
          "name": "Antonio Buarbualau",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2327336769"
        },
        {
          "name": "Dan Oneaţă",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/3095774"
        },
        {
          "name": "Elena Burceanu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379760055"
        }
      ]
    },
    {
      "id": "2509.07996",
      "title": "3D and 4D World Modeling: A Survey",
      "authors": [
        "Lingdong Kong",
        "Wesley Yang",
        "Jianbiao Mei",
        "Youquan Liu",
        "Ao Liang",
        "Dekai Zhu",
        "Dongyue Lu",
        "Wei Yin",
        "Xiaotao Hu",
        "Mingkai Jia",
        "Junyuan Deng",
        "Kaiwen Zhang",
        "Yang Wu",
        "Tianyi Yan",
        "Shenyuan Gao",
        "Song Wang",
        "Linfeng Li",
        "Liang Pan",
        "Yong Liu",
        "Jianke Zhu",
        "Wei Tsang Ooi",
        "Steven C. H. Hoi",
        "Ziwei Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "World modeling has become a cornerstone in AI research, enabling agents to\nunderstand, represent, and predict the dynamic environments they inhabit. While\nprior work largely emphasizes generative methods for 2D image and video data,\nthey overlook the rapidly growing body of work that leverages native 3D and 4D\nrepresentations such as RGB-D imagery, occupancy grids, and LiDAR point clouds\nfor large-scale scene modeling. At the same time, the absence of a standardized\ndefinition and taxonomy for ``world models'' has led to fragmented and\nsometimes inconsistent claims in the literature. This survey addresses these\ngaps by presenting the first comprehensive review explicitly dedicated to 3D\nand 4D world modeling and generation. We establish precise definitions,\nintroduce a structured taxonomy spanning video-based (VideoGen),\noccupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and\nsystematically summarize datasets and evaluation metrics tailored to 3D/4D\nsettings. We further discuss practical applications, identify open challenges,\nand highlight promising research directions, aiming to provide a coherent and\nfoundational reference for advancing the field. A systematic summary of\nexisting literature is available at https://github.com/worldbench/survey",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.07996v2",
      "pdf_url": "http://arxiv.org/pdf/2509.07996v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.349,
      "datasets_score": 0.398,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09699",
      "title": "Structured Information Matters: Explainable ICD Coding with\n  Patient-Level Knowledge Graphs",
      "authors": [
        "Mingyang Li",
        "Viktor Schlegel",
        "Tingting Mu",
        "Warren Del-Pinto",
        "Goran Nenadic"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Mapping clinical documents to standardised clinical vocabularies is an\nimportant task, as it provides structured data for information retrieval and\nanalysis, which is essential to clinical research, hospital administration and\nimproving patient care. However, manual coding is both difficult and\ntime-consuming, making it impractical at scale. Automated coding can\npotentially alleviate this burden, improving the availability and accuracy of\nstructured clinical data. The task is difficult to automate, as it requires\nmapping to high-dimensional and long-tailed target spaces, such as the\nInternational Classification of Diseases (ICD). While external knowledge\nsources have been readily utilised to enhance output code representation, the\nuse of external resources for representing the input documents has been\nunderexplored. In this work, we compute a structured representation of the\ninput documents, making use of document-level knowledge graphs (KGs) that\nprovide a comprehensive structured view of a patient's condition. The resulting\nknowledge graph efficiently represents the patient-centred input documents with\n23\\% of the original text while retaining 90\\% of the information. We assess\nthe effectiveness of this graph for automated ICD-9 coding by integrating it\ninto the state-of-the-art ICD coding architecture PLM-ICD. Our experiments\nyield improved Macro-F1 scores by up to 3.20\\% on popular benchmarks, while\nimproving training efficiency. We attribute this improvement to different types\nof entities and relationships in the KG, and demonstrate the improved\nexplainability potential of the approach over the text-only baseline.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.09699v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09699v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.291,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on constructing and integrating patient-level knowledge graphs for improved ICD coding, emphasizing structured representations, entity recognition, and graph neural networks for classification tasks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no mention of adapting diffusion for reasoning paths or holistic corrections, making the paper unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09700",
      "title": "Cross-Layer Attention Probing for Fine-Grained Hallucination Detection",
      "authors": [
        "Malavika Suresh",
        "Rahaf Aljundi",
        "Ikechukwu Nkisi-Orji",
        "Nirmalie Wiratunga"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the large-scale adoption of Large Language Models (LLMs) in various\napplications, there is a growing reliability concern due to their tendency to\ngenerate inaccurate text, i.e. hallucinations. In this work, we propose\nCross-Layer Attention Probing (CLAP), a novel activation probing technique for\nhallucination detection, which processes the LLM activations across the entire\nresidual stream as a joint sequence. Our empirical evaluations using five LLMs\nand three tasks show that CLAP improves hallucination detection compared to\nbaselines on both greedy decoded responses as well as responses sampled at\nhigher temperatures, thus enabling fine-grained detection, i.e. the ability to\ndisambiguate hallucinations and non-hallucinations among different sampled\nresponses to a given prompt. This allows us to propose a detect-then-mitigate\nstrategy using CLAP to reduce hallucinations and improve LLM reliability\ncompared to direct mitigation approaches. Finally, we show that CLAP maintains\nhigh reliability even when applied out-of-distribution.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.09700v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09700v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.354,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a novel technique for detecting and mitigating hallucinations in LLMs using activation probing, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning processes based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates LLMs on tasks including chain-of-thought reasoning but does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for logical reasoning; it instead proposes an attention-based probing method for hallucination detection.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10522",
      "title": "Multimodal Deep Learning for ATCO Command Lifecycle Modeling and\n  Workload Prediction",
      "authors": [
        "Kaizhen Tan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Air traffic controllers (ATCOs) issue high-intensity voice commands in dense\nairspace, where accurate workload modeling is critical for safety and\nefficiency. This paper proposes a multimodal deep learning framework that\nintegrates structured data, trajectory sequences, and image features to\nestimate two key parameters in the ATCO command lifecycle: the time offset\nbetween a command and the resulting aircraft maneuver, and the command\nduration. A high-quality dataset was constructed, with maneuver points detected\nusing sliding window and histogram-based methods. A CNN-Transformer ensemble\nmodel was developed for accurate, generalizable, and interpretable predictions.\nBy linking trajectories to voice commands, this work offers the first model of\nits kind to support intelligent command generation and provides practical value\nfor workload assessment, staffing, and scheduling.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.10522v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10522v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.358,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10523",
      "title": "From Predictions to Explanations: Explainable AI for Autism Diagnosis\n  and Identification of Critical Brain Regions",
      "authors": [
        "Kush Gupta",
        "Amir Aly",
        "Emmanuel Ifeachor",
        "Rohit Shankar"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autism spectrum disorder (ASD) is a neurodevelopmental condition\ncharacterized by atypical brain maturation. However, the adaptation of transfer\nlearning paradigms in machine learning for ASD research remains notably\nlimited. In this study, we propose a computer-aided diagnostic framework with\ntwo modules. This chapter presents a two-module framework combining deep\nlearning and explainable AI for ASD diagnosis. The first module leverages a\ndeep learning model fine-tuned through cross-domain transfer learning for ASD\nclassification. The second module focuses on interpreting the model decisions\nand identifying critical brain regions. To achieve this, we employed three\nexplainable AI (XAI) techniques: saliency mapping, Gradient-weighted Class\nActivation Mapping, and SHapley Additive exPlanations (SHAP) analysis. This\nframework demonstrates that cross-domain transfer learning can effectively\naddress data scarcity in ASD research. In addition, by applying three\nestablished explainability techniques, the approach reveals how the model makes\ndiagnostic decisions and identifies brain regions most associated with ASD.\nThese findings were compared against established neurobiological evidence,\nhighlighting strong alignment and reinforcing the clinical relevance of the\nproposed approach.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.10523v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10523v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.351,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on deep learning, transfer learning, and explainable AI techniques (e.g., saliency mapping, Grad-CAM, and SHAP) for ASD diagnosis using fMRI data. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no component related to adapting diffusion for complex logical tasks or Chain-of-Thought processing.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10524",
      "title": "Data-Efficient Psychiatric Disorder Detection via Self-supervised\n  Learning on Frequency-enhanced Brain Networks",
      "authors": [
        "Mujie Liu",
        "Mengchu Zhu",
        "Qichao Dong",
        "Ting Dang",
        "Jiangang Ma",
        "Jing Ren",
        "Feng Xia"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Psychiatric disorders involve complex neural activity changes, with\nfunctional magnetic resonance imaging (fMRI) data serving as key diagnostic\nevidence. However, data scarcity and the diverse nature of fMRI information\npose significant challenges. While graph-based self-supervised learning (SSL)\nmethods have shown promise in brain network analysis, they primarily focus on\ntime-domain representations, often overlooking the rich information embedded in\nthe frequency domain. To overcome these limitations, we propose\nFrequency-Enhanced Network (FENet), a novel SSL framework specially designed\nfor fMRI data that integrates time-domain and frequency-domain information to\nimprove psychiatric disorder detection in small-sample datasets. FENet\nconstructs multi-view brain networks based on the inherent properties of fMRI\ndata, explicitly incorporating frequency information into the learning process\nof representation. Additionally, it employs domain-specific encoders to capture\ntemporal-spectral characteristics, including an efficient frequency-domain\nencoder that highlights disease-relevant frequency features. Finally, FENet\nintroduces a domain consistency-guided learning objective, which balances the\nutilization of diverse information and generates frequency-enhanced brain graph\nrepresentations. Experiments on two real-world medical datasets demonstrate\nthat FENet outperforms state-of-the-art methods while maintaining strong\nperformance in minimal data conditions. Furthermore, we analyze the correlation\nbetween various frequency-domain features and psychiatric disorders,\nemphasizing the critical role of high-frequency information in disorder\ndetection.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.10524v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10524v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.356,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10526",
      "title": "Resource-Aware Neural Network Pruning Using Graph-based Reinforcement\n  Learning",
      "authors": [
        "Dieter Balemans",
        "Thomas Huybrechts",
        "Jan Steckel",
        "Siegfried Mercelis"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents a novel approach to neural network pruning by integrating\na graph-based observation space into an AutoML framework to address the\nlimitations of existing methods. Traditional pruning approaches often depend on\nhand-crafted heuristics and local optimization perspectives, which can lead to\nsuboptimal performance and inefficient pruning strategies. Our framework\ntransforms the pruning process by introducing a graph representation of the\ntarget neural network that captures complete topological relationships between\nlayers and channels, replacing the limited layer-wise observation space with a\nglobal view of network structure. The core innovations include a Graph\nAttention Network (GAT) encoder that processes the network's graph\nrepresentation and generates a rich embedding. Additionally, for the action\nspace we transition from continuous pruning ratios to fine-grained binary\naction spaces which enables the agent to learn optimal channel importance\ncriteria directly from data, moving away from predefined scoring functions.\nThese contributions are modelled within a Constrained Markov Decision Process\n(CMDP) framework, allowing the agent to make informed pruning decisions while\nadhering to resource constraints such as target compression rates. For this, we\ndesign a self-competition reward system that encourages the agent to outperform\nits previous best performance while satisfying the defined constraints. We\ndemonstrate the effectiveness of our approach through extensive experiments on\nbenchmark datasets including CIFAR-10, CIFAR-100, and ImageNet. The experiments\nshow that our method consistently outperforms traditional pruning techniques,\nshowing state-of-the-art results while learning task-specific pruning\nstrategies that identify functionally redundant connections beyond simple\nweight magnitude considerations.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.10526v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10526v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.417,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on neural network pruning using graph-based reinforcement learning, emphasizing global network topology and resource-aware decisions. It does not address distributed training, parallel computing, or multi-node machine learning techniques for accelerating model training, as its contributions are limited to pruning strategies and RL frameworks without any mention of partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10528",
      "title": "STM-Graph: A Python Framework for Spatio-Temporal Mapping and Graph\n  Neural Network Predictions",
      "authors": [
        "Amirhossein Ghaffari",
        "Huong Nguyen",
        "Lauri Lovén",
        "Ekaterina Gilman"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Urban spatio-temporal data present unique challenges for predictive analytics\ndue to their dynamic and complex nature. We introduce STM-Graph, an open-source\nPython framework that transforms raw spatio-temporal urban event data into\ngraph representations suitable for Graph Neural Network (GNN) training and\nprediction. STM-Graph integrates diverse spatial mapping methods, urban\nfeatures from OpenStreetMap, multiple GNN models, comprehensive visualization\ntools, and a graphical user interface (GUI) suitable for professional and\nnon-professional users. This modular and extensible framework facilitates rapid\nexperimentation and benchmarking. It allows integration of new mapping methods\nand custom models, making it a valuable resource for researchers and\npractitioners in urban computing. The source code of the framework and GUI are\navailable at: https://github.com/Ahghaffari/stm_graph and\nhttps://github.com/tuminguyen/stm_graph_gui.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.10528v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10528v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.341,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10529",
      "title": "Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image\n  Diffusion via Latent Replay",
      "authors": [
        "Aoi Otani"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Continual learning -- the ability to acquire knowledge incrementally without\nforgetting previous skills -- is fundamental to natural intelligence. While the\nhuman brain excels at this, artificial neural networks struggle with\n\"catastrophic forgetting,\" where learning new tasks erases previously acquired\nknowledge. This challenge is particularly severe for text-to-image diffusion\nmodels, which generate images from textual prompts. Additionally, these models\nface \"mode collapse,\" where their outputs become increasingly repetitive over\ntime. To address these challenges, we apply Latent Replay, a\nneuroscience-inspired approach, to diffusion models. Traditional replay methods\nmitigate forgetting by storing and revisiting past examples, typically\nrequiring large collections of images. Latent Replay instead retains only\ncompact, high-level feature representations extracted from the model's internal\narchitecture. This mirrors the hippocampal process of storing neural activity\npatterns rather than raw sensory inputs, reducing memory usage while preserving\ncritical information. Through experiments with five sequentially learned visual\nconcepts, we demonstrate that Latent Replay significantly outperforms existing\nmethods in maintaining model versatility. After learning all concepts, our\napproach retained 77.59% Image Alignment (IA) on the earliest concept, 14%\nhigher than baseline methods, while maintaining diverse outputs. Surprisingly,\nrandom selection of stored latent examples outperforms similarity-based\nstrategies. Our findings suggest that Latent Replay enables efficient continual\nlearning for generative AI models, paving the way for personalized\ntext-to-image models that evolve with user needs without excessive\ncomputational costs.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.10529v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10529v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.567,
      "distributed_training_score": 0.371,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on continual learning in text-to-image diffusion models using Latent Replay to mitigate forgetting, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for image generation and addresses forgetting in sequential learning, but it does not involve multi-step logical reasoning, iterative refinement for complex tasks, or treating chains of thought as entities.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12221",
      "title": "MEUV: Achieving Fine-Grained Capability Activation in Large Language\n  Models via Mutually Exclusive Unlock Vectors",
      "authors": [
        "Xin Tong",
        "Zhi Lin",
        "Jingya Wang",
        "Meng Han",
        "Bo Jin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Large language models (LLMs) enforce safety alignment to reliably refuse\nmalicious requests, yet the same blanket safeguards also block legitimate uses\nin policing, defense, and other high-stakes settings. Earlier\n\"refusal-direction\" edits can bypass those layers, but they rely on a single\nvector that indiscriminately unlocks all hazardous topics, offering no semantic\ncontrol. We introduce Mutually Exclusive Unlock Vectors (MEUV), a lightweight\nframework that factorizes the monolithic refusal direction into topic-aligned,\nnearly orthogonal vectors, each dedicated to one sensitive capability. MEUV is\nlearned in a single epoch with a multi-task objective that blends a\ndifferential-ablation margin, cross-topic and orthogonality penalties, and\nseveral auxiliary terms. On bilingual malicious-prompt benchmarks, MEUV\nachieves an attack success rate of no less than 87% on Gemma-2-2B, LLaMA-3-8B,\nand Qwen-7B, yet cuts cross-topic leakage by up to 90% compared with the best\nsingle-direction baseline. Vectors trained in Chinese transfer almost unchanged\nto English (and vice versa), suggesting a language-agnostic refusal subspace.\nThe results show that fine-grained, topic-level capability activation is\nachievable with minimal utility loss, paving the way for controlled LLMs\ndeployment in security-sensitive domains.",
      "published_date": "2025-09-04",
      "arxiv_url": "http://arxiv.org/abs/2509.12221v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12221v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.372,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on fine-grained capability activation in LLMs using vector factorization and optimization techniques, without any involvement of human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses safety alignment and vector-based unlocking in LLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 203,
  "date": "2025-09-04"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
